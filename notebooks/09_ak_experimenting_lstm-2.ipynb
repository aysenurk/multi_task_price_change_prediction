{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aysenur\n",
    "#experiment 1 de jupyter iÃ§erisinde weight update edilmediÄŸini farkettim\n",
    "#detayÄ±nÄ± gitmedim ama gpu da accelerator kullanmakla alakalÄ± o yÃ¼zden her ÅŸeyi .py scripte aldÄ±m\n",
    "#scripte aldÄ±klarÄ±m aÅŸaÄŸÄ±da sÄ±rasÄ±yla verildi, bir kere denemesi yapÄ±ldÄ±\n",
    "#sonrasÄ±nda belirlenen parametrelerle script Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±\n",
    "#CosineWarmupScheduler eklendi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataPreparation import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 currency_list,\n",
    "                 x: np.ndarray, \n",
    "                 y: np.ndarray,\n",
    "                 data_use_type,\n",
    "                 train_percentage,\n",
    "                 val_percentage,\n",
    "                 test_percentage,\n",
    "                 seq_len, \n",
    "                 ):\n",
    "        self.currencies = currency_list\n",
    "        self.n_currencies = len(self.currencies)\n",
    "        self.x = torch.tensor(x[:self.n_currencies]).float()\n",
    "        self.y = torch.tensor(y[:self.n_currencies]).long()\n",
    "        self.seq_len = seq_len\n",
    "        self.data_use_type = data_use_type\n",
    "        \n",
    "        \n",
    "        #self.train_size = int(len(self.x[0]) * train_percentage)\n",
    "        self.val_size = int(len(self.x[0]) * val_percentage)\n",
    "        self.test_size = int(len(self.x[0]) * test_percentage)\n",
    "        self.train_size = len(self.x[0]) - self.val_size - self.test_size \n",
    "        \n",
    "        self.train_mean = [self.x[i][:self.train_size].mean() for i in range(self.n_currencies)]\n",
    "        self.train_std = [self.x[i][:self.train_size].std() for i in range(self.n_currencies)]\n",
    "        \n",
    "#         self.train_min = [self.x[i][:self.train_size].min() for i in range(n_currencies)]\n",
    "#         self.train_max = [self.x[i][:self.train_size].max() for i in range(n_currencies)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        if self.data_use_type == \"train\":\n",
    "            return self.train_size - ( self.seq_len)\n",
    "\n",
    "        elif self.data_use_type == \"val\":\n",
    "            return self.val_size\n",
    "  \n",
    "        else:\n",
    "            return self.test_size\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        item = dict()\n",
    "        \n",
    "        if self.data_use_type ==\"val\":\n",
    "            index = self.train_size + index - self.seq_len\n",
    "            \n",
    "        elif self.data_use_type ==\"test\":\n",
    "            index = self.train_size + self.val_size + index - self.seq_len\n",
    "        \n",
    "        for i in range(self.n_currencies):\n",
    "            window = self.x[i][index:index+self.seq_len]\n",
    "            window = (window -self.train_mean[i]) / self.train_std[i]\n",
    "            \n",
    "            item[self.currencies[i] + \"_window\"] = window\n",
    "            item[self.currencies[i] + \"_label\"]  = self.y[i][index+self.seq_len]\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "        \n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "    \n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_based_classification_model(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 train_dataset,\n",
    "                 val_dataset,\n",
    "                 test_dataset,\n",
    "                 calculate_loss_weights,\n",
    "                 currencies,\n",
    "                 num_classes,\n",
    "                 window_size,\n",
    "                 input_size,\n",
    "                 batch_size,\n",
    "                 lstm_hidden_sizes,\n",
    "                 bidirectional,\n",
    "                 learning_rate = 1e-3,\n",
    "                 scheduler_step = 10,\n",
    "                 scheduler_gamma = 0.1,\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.currencies = currencies\n",
    "        self.num_tasks = len(currencies)\n",
    "        self.window_size = window_size\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.lstm_hidden_sizes = lstm_hidden_sizes\n",
    "        self.bidirectional = bidirectional \n",
    "        \n",
    "        if calculate_loss_weights:\n",
    "            loss_weights = []\n",
    "            for i in range(self.num_tasks):\n",
    "                train_labels = [int(train_dataset[n][self.currencies[i] +\"_label\"] )for n in range(train_dataset.__len__())]\n",
    "                samples_size = pd.DataFrame({\"label\": train_labels}).groupby(\"label\").size().to_numpy()\n",
    "                loss_weights.append((1 / samples_size) * sum(samples_size)/2)\n",
    "            self.weights = loss_weights\n",
    "        else:\n",
    "            self.weights = None\n",
    "        \n",
    "        self.lstm_1 = nn.LSTM(input_size = self.input_size, \n",
    "                              num_layers=1, \n",
    "                              batch_first=True, \n",
    "                              hidden_size = self.lstm_hidden_sizes[0], \n",
    "                              bidirectional = bidirectional)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=self.lstm_hidden_sizes[0])\n",
    "        \n",
    "        if len(self.lstm_hidden_sizes) > 1:\n",
    "            self.lstm_2 = nn.LSTM(input_size = self.lstm_hidden_sizes[0], \n",
    "                                  num_layers=1, \n",
    "                                  batch_first=True, \n",
    "                                  hidden_size = self.lstm_hidden_sizes[1], \n",
    "                                  bidirectional = bidirectional)\n",
    "            self.batch_norm2 = nn.BatchNorm2d(num_features=self.lstm_hidden_sizes[1])\n",
    "\n",
    "            self.lstm_3 = nn.LSTM(input_size = self.lstm_hidden_sizes[1], \n",
    "                                  num_layers=1, \n",
    "                                  batch_first=True, \n",
    "                                  hidden_size = self.lstm_hidden_sizes[2], \n",
    "                                  bidirectional = bidirectional)\n",
    "            self.batch_norm3 = nn.BatchNorm2d(num_features=self.lstm_hidden_sizes[2])\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear1 =[nn.Linear(self.lstm_hidden_sizes[-1], int(self.lstm_hidden_sizes[-1]/2))] * self.num_tasks\n",
    "        self.linear1 = torch.nn.ModuleList(self.linear1)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.output_layers = [nn.Linear(int(self.lstm_hidden_sizes[-1]/2), self.num_classes)] * self.num_tasks\n",
    "        self.output_layers = torch.nn.ModuleList(self.output_layers)\n",
    "        \n",
    "        if self.weights != None:\n",
    "            self.cross_entropy_loss = [nn.CrossEntropyLoss(weight= torch.tensor(weights).float()) for weights in self.weights]\n",
    "        else:\n",
    "            self.cross_entropy_loss = [nn.CrossEntropyLoss() for _ in range(self.num_tasks)]\n",
    "        \n",
    "        self.cross_entropy_loss = torch.nn.ModuleList(self.cross_entropy_loss)\n",
    "        \n",
    "        self.f1_score = pl.metrics.F1(num_classes=self.num_classes, average=\"macro\")\n",
    "        self.accuracy_score = pl.metrics.Accuracy()\n",
    "        \n",
    "        self.train_dl = DataLoader(train_dataset, batch_size=self.batch_size, shuffle = True)\n",
    "        self.val_dl = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "        self.test_dl = DataLoader(test_dataset, batch_size=self.batch_size)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.scheduler_step = scheduler_step\n",
    "        self.scheduler_gamma = scheduler_gamma\n",
    "        \n",
    "    def forward(self, x, i):\n",
    "\n",
    "        batch_size = x.size()[0]\n",
    "        \n",
    "        x = x.view(batch_size, self.window_size, self.input_size) #(batch, window_len, feature_size)\n",
    "        x, _  = self.lstm_1(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x.reshape(x.size()[-1], batch_size, self.window_size) #(feature_size, batch, window_len)\n",
    "        x = self.batch_norm1(x.unsqueeze(0))\n",
    "        \n",
    "        if len(self.lstm_hidden_sizes) > 1:\n",
    "            \n",
    "            x = x.view(batch_size, self.window_size, x.size()[1])\n",
    "            x, _  = self.lstm_2(x)\n",
    "\n",
    "            x = self.dropout(x)\n",
    "\n",
    "            x = x.reshape(x.size()[-1], batch_size, self.window_size) #(feature_size, batch, window_len)\n",
    "            x = self.batch_norm2(x.unsqueeze(0))\n",
    "\n",
    "            x = x.view(batch_size, self.window_size, x.size()[1])\n",
    "            x, _  = self.lstm_3(x)\n",
    "\n",
    "            x = self.dropout(x)\n",
    "\n",
    "            x = x.reshape(x.size()[-1], batch_size, self.window_size) #(feature_size, batch, window_len)\n",
    "            x = self.batch_norm3(x.unsqueeze(0))\n",
    "        \n",
    "        x = x.view(batch_size, self.window_size, x.size()[1])\n",
    "        x = x[:, -1, :] # equivalent to return sequence = False on keras :)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear1[i](x)\n",
    "        x = self.activation(x)\n",
    "                 \n",
    "        output = self.output_layers[i](x)\n",
    "    \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        \n",
    "        loss = (torch.tensor(0.0, device=\"cuda:0\", requires_grad=True) + \\\n",
    "                torch.tensor(0.0, device=\"cuda:0\", requires_grad=True)) \n",
    "        # araÅŸtÄ±rÄ±labilir\n",
    "        for i in range(self.num_tasks):\n",
    "            x, y = batch[self.currencies[i] + \"_window\"], batch[self.currencies[i] + \"_label\"]\n",
    "\n",
    "            output = self.forward(x, i)\n",
    "            #loss = F.nll_loss(output, y)\n",
    "            loss += self.cross_entropy_loss[i](output, y)\n",
    "            \n",
    "            acc = self.accuracy_score(torch.max(output, dim=1)[1], y)\n",
    "            self.log(self.currencies[i] +'_train_acc', acc, on_epoch=True, prog_bar=True)\n",
    "\n",
    "            f1 = self.f1_score(torch.max(output, dim=1)[1], y)\n",
    "            self.log(self.currencies[i] +'_train_f1', f1, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        loss = loss / torch.tensor(self.num_tasks)\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss \n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        loss = torch.tensor(0.0, device=\"cuda:0\") + torch.tensor(0.0, device=\"cuda:0\")\n",
    "        \n",
    "        for i in range(self.num_tasks):\n",
    "            x, y = batch[self.currencies[i] + \"_window\"], batch[self.currencies[i] + \"_label\"]\n",
    "\n",
    "            output = self(x, i)\n",
    "            #loss = F.nll_loss(output, y)\n",
    "            loss += self.cross_entropy_loss[i](output, y)\n",
    " \n",
    "            acc = self.accuracy_score(torch.max(output, dim=1)[1], y)\n",
    "            self.log(self.currencies[i] +'_val_acc', acc, on_epoch=True, prog_bar=True, reduce_fx=torch.mean)\n",
    "\n",
    "            f1 = self.f1_score(torch.max(output, dim=1)[1], y)\n",
    "            self.log(self.currencies[i] +'_val_f1', f1, on_epoch=True, prog_bar=True, reduce_fx=torch.mean)\n",
    "        \n",
    "        loss = loss / torch.tensor(self.num_tasks)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        loss = torch.tensor(0.0, device=\"cuda:0\") + torch.tensor(0.0, device=\"cuda:0\")\n",
    "        \n",
    "        for i in range(self.num_tasks):\n",
    "            x, y = batch[ self.currencies[i] + \"_window\"], batch[self.currencies[i] + \"_label\"]\n",
    "\n",
    "            output = self(x, i)\n",
    "#             print(y, torch.max(output, dim=1)[1])\n",
    "#             print(F.softmax(output)) # mantÄ±ken fark etmiyor\n",
    "            loss += self.cross_entropy_loss[i](output, y)\n",
    "            \n",
    "            acc = self.accuracy_score(torch.max(output, dim=1)[1], y)\n",
    "            self.log(self.currencies[i] +'_test_acc', acc, on_epoch=True, reduce_fx=torch.mean)\n",
    "\n",
    "            f1 = self.f1_score(torch.max(output, dim=1)[1], y)\n",
    "            self.log(self.currencies[i] +'_test_f1', f1, on_epoch=True, reduce_fx=torch.mean)\n",
    "        \n",
    "        loss = loss / torch.tensor(self.num_tasks)\n",
    "        self.log('test_loss', loss, on_epoch=True, reduce_fx=torch.mean)\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr= self.learning_rate)#AdamW does weight decay\n",
    "#         scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
    "#                                                     step_size=self.scheduler_step, \n",
    "#                                                     gamma=self.scheduler_gamma)\n",
    "        \n",
    "        self.lr_scheduler = CosineWarmupScheduler(optimizer, \n",
    "                                                  warmup=50, \n",
    "                                                  max_iters=150* self.train_dl.__len__())\n",
    "        return [optimizer]#, [{\"scheduler\": scheduler}]\n",
    "    \n",
    "    def optimizer_step(self, *args, **kwargs):\n",
    "        super().optimizer_step(*args, **kwargs)\n",
    "        self.lr_scheduler.step() # Step per iteration\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_dl\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_dl\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_model(config):\n",
    "    task = \"multi_task_\" + \"_\".join(config[\"currency_list\"]) if len(config[\"currency_list\"]) > 1 else \"single_task_\" + config[\"currency_list\"][0]\n",
    "    classification = \"multi_classification\" if config[\"n_classes\"] > 2 else \"binary_classification\"\n",
    "    lstm = \"stack_lstm\" if len(config[\"lstm_hidden_sizes\"]) > 1 else \"single_lstm\"\n",
    "    trend_removed = \"trend_removed\" if config[\"remove_trend\"] else \"\"\n",
    "    loss_weighted = \"loss_weighted\" if config[\"loss_weight_calculate\"] else \"\"\n",
    "\n",
    "    return \"_\".join([task, lstm, loss_weighted, classification, trend_removed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deneme 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {#fix for this project\n",
    "          \"window_size\": 50, \n",
    "          \"dataset_percentages\": [0.97, 0.007, 0.023],\n",
    "          \"frenquency\": \"D\", \n",
    "          \"neutral_quantile\": 0.33,\n",
    "          \"batch_size\": 16,\n",
    "          \"bidirectional\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CONFIG.copy()\n",
    "config.update({\"n_classes\": 2,\n",
    "          \"currency_list\": ['BTC'],\n",
    "          \"remove_trend\": True,\n",
    "          \"lstm_hidden_sizes\": [128, 128, 128],\n",
    "          \"loss_weight_calculate\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:27z0n5tz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 47469<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210519_232415-27z0n5tz/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210519_232415-27z0n5tz/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">single_task_BTC__stack_lstm__binary_classification_trend_removed</strong>: <a href=\"https://wandb.ai/aysenurk/deneme/runs/27z0n5tz\" target=\"_blank\">https://wandb.ai/aysenurk/deneme/runs/27z0n5tz</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:27z0n5tz). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">single_task_BTC__stack_lstm__binary_classification_trend_removed</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/aysenurk/deneme\" target=\"_blank\">https://wandb.ai/aysenurk/deneme</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/aysenurk/deneme/runs/2uyp322n\" target=\"_blank\">https://wandb.ai/aysenurk/deneme/runs/2uyp322n</a><br/>\n",
       "                Run data is saved locally in <code>/home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210519_232500-2uyp322n</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layoutâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1a7ffb4b824b4b820f1c19342ff3ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.664\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.082 >= min_delta = 0.003. New best score: 0.582\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.135 >= min_delta = 0.003. New best score: 0.447\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291d9a9e99ac425aa3c267b2e3213cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6774193644523621,\n",
      " 'BTC_test_f1': 0.6693548560142517,\n",
      " 'test_loss': 0.6429551839828491}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 47600<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210519_232500-2uyp322n/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210519_232500-2uyp322n/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>BTC_train_acc_step</td><td>0.6875</td></tr><tr><td>BTC_train_f1_step</td><td>0.67611</td></tr><tr><td>train_loss_step</td><td>0.50698</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>trainer/global_step</td><td>1557</td></tr><tr><td>_runtime</td><td>61</td></tr><tr><td>_timestamp</td><td>1621455965</td></tr><tr><td>_step</td><td>69</td></tr><tr><td>BTC_train_acc_epoch</td><td>0.71576</td></tr><tr><td>BTC_train_f1_epoch</td><td>0.69879</td></tr><tr><td>train_loss_epoch</td><td>0.57761</td></tr><tr><td>BTC_val_acc</td><td>0.77778</td></tr><tr><td>BTC_val_f1</td><td>0.775</td></tr><tr><td>val_loss</td><td>0.53603</td></tr><tr><td>BTC_test_acc</td><td>0.67742</td></tr><tr><td>BTC_test_f1</td><td>0.66935</td></tr><tr><td>test_loss</td><td>0.64296</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>BTC_train_acc_step</td><td>â–ƒâ–â–ƒâ–…â–…â–…â–‡â–„â–…â–…â–ˆâ–…â–ƒâ–ƒâ–â–†â–„â–…â–‡â–…â–†â–ƒâ–…â–ƒâ–…â–…â–…â–…â–…â–…â–…</td></tr><tr><td>BTC_train_f1_step</td><td>â–‚â–â–ƒâ–…â–…â–…â–‡â–‚â–„â–…â–ˆâ–„â–ƒâ–ƒâ–â–†â–ƒâ–…â–‡â–…â–†â–‚â–„â–ƒâ–…â–„â–…â–„â–„â–„â–„</td></tr><tr><td>train_loss_step</td><td>â–…â–†â–…â–„â–…â–‚â–â–„â–„â–„â–â–„â–‡â–†â–ˆâ–‚â–†â–„â–â–ƒâ–ƒâ–„â–…â–…â–ƒâ–„â–ƒâ–…â–„â–…â–ƒ</td></tr><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>trainer/global_step</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>_runtime</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>_timestamp</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>_step</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>BTC_train_acc_epoch</td><td>â–â–ƒâ–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡</td></tr><tr><td>BTC_train_f1_epoch</td><td>â–â–ƒâ–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡</td></tr><tr><td>train_loss_epoch</td><td>â–ˆâ–†â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–â–â–</td></tr><tr><td>BTC_val_acc</td><td>â–â–…â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>BTC_val_f1</td><td>â–â–†â–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_loss</td><td>â–ˆâ–…â–†â–â–‚â–ƒâ–‚â–„â–„â–ƒâ–‚â–ƒâ–ƒâ–…â–ƒâ–ƒâ–„â–ƒâ–„</td></tr><tr><td>BTC_test_acc</td><td>â–</td></tr><tr><td>BTC_test_f1</td><td>â–</td></tr><tr><td>test_loss</td><td>â–</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">single_task_BTC__stack_lstm__binary_classification_trend_removed</strong>: <a href=\"https://wandb.ai/aysenurk/deneme/runs/2uyp322n\" target=\"_blank\">https://wandb.ai/aysenurk/deneme/runs/2uyp322n</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = name_model(config)\n",
    "\n",
    "CURRENCY_LST = config[\"currency_list\"]\n",
    "N_CLASSES = config[\"n_classes\"]\n",
    "LSTM_HIDDEN_SIZES = config[\"lstm_hidden_sizes\"]\n",
    "BIDIRECTIONAL = config[\"bidirectional\"]\n",
    "REMOVE_TREND =config[\"remove_trend\"]\n",
    "LOSS_WEIGHT_CALCULATE = config[\"loss_weight_calculate\"]\n",
    "\n",
    "TRAIN_PERCENTAGE, VAL_PERCENTAGE, TEST_PERCENTAGE = config[\"dataset_percentages\"] \n",
    "WINDOW_SIZE = config[\"window_size\"]\n",
    "FREQUENCY = config[\"frenquency\"]\n",
    "NEUTRAL_QUANTILE = config[\"neutral_quantile\"] if N_CLASSES > 2 else 0 \n",
    "BATCH_SIZE= config[\"batch_size\"]\n",
    "#####\n",
    "\n",
    "X, y, features, dfs = get_data(CURRENCY_LST,\n",
    "                            N_CLASSES,\n",
    "                             FREQUENCY, \n",
    "                             WINDOW_SIZE,\n",
    "                             neutral_quantile = NEUTRAL_QUANTILE,\n",
    "                             log_price=True,\n",
    "                             remove_trend=REMOVE_TREND,\n",
    "                             include_indicators = False,\n",
    "                             include_imfs = False\n",
    "                            )\n",
    "INPUT_FEATURE_SIZE = X.shape[-1]\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = [TimeSeriesDataset(CURRENCY_LST, \n",
    "                                                          X, \n",
    "                                                          y, \n",
    "                                                          dtype, \n",
    "                                                          TRAIN_PERCENTAGE, \n",
    "                                                          VAL_PERCENTAGE, \n",
    "                                                          TEST_PERCENTAGE, \n",
    "                                                          WINDOW_SIZE) for dtype in ['train', 'val', 'test']]\n",
    "\n",
    "config[\"dataset_sizes\"] = [len(train_dataset), len(val_dataset), len(test_dataset)]\n",
    "####\n",
    "wandb.init(project=\"price_change_2\",\n",
    "           config=config,\n",
    "           name = MODEL_NAME)\n",
    "logger = WandbLogger()\n",
    "#     #logger = TensorBoardLogger(\"../output/models/lstm_model_logs\", name=\"lstm_multi_task\")\n",
    "\n",
    "model = LSTM_based_classification_model(\n",
    "    train_dataset = train_dataset,\n",
    "     val_dataset = val_dataset,\n",
    "     test_dataset = test_dataset,\n",
    "     calculate_loss_weights = LOSS_WEIGHT_CALCULATE,\n",
    "     currencies = CURRENCY_LST,\n",
    "     num_classes = N_CLASSES,\n",
    "     window_size = WINDOW_SIZE,\n",
    "     input_size = INPUT_FEATURE_SIZE,\n",
    "     batch_size=BATCH_SIZE,\n",
    "     lstm_hidden_sizes = LSTM_HIDDEN_SIZES,\n",
    "     bidirectional = BIDIRECTIONAL)\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='val_loss',\n",
    "   min_delta=0.003,\n",
    "   patience=20,\n",
    "   verbose=True,\n",
    "   mode='min'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(gpus=-1, \n",
    "                     max_epochs= 150,\n",
    "                     logger = logger, \n",
    "                     callbacks=[early_stop_callback])\n",
    "trainer.fit(model)\n",
    "\n",
    "trainer.test()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#denemeleri tamamlandÄ± experiment_lstm.py dosyasÄ±na yazÄ±ldÄ± iÅŸlemler\n",
    "def experiment(script):\n",
    "    !python ../pipelines/multi_task_price_change_prediction/experiment_lstm.py $script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {#fix for this project\n",
    "          \"window_size\": 50, \n",
    "          \"dataset_percentages\": [0.97, 0.007, 0.023],\n",
    "          \"frenquency\": \"D\", \n",
    "          \"neutral_quantile\": 0.33,\n",
    "          \"batch_size\": 16,\n",
    "          \"bidirectional\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "param_grid = {\"n_classes\": [2,3],\n",
    "          \"currency_list\": [['BTC'], ['ETH'], ['LTC'], ['BTC', 'ETH'],  ['BTC', 'ETH', 'LTC']],\n",
    "          \"remove_trend\": [True, False],\n",
    "          \"lstm_hidden_sizes\": [[128], [128, 128, 128]],\n",
    "          \"loss_weight_calculate\": [True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 00:55:10.435653: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_single_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1eprgmz4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005508-1eprgmz4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 130   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.7 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 79/80 [00:01<00:00, 64.35it/s, loss=0.583, v_num=gmz4, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.522\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:01<00:00, 58.94it/s, loss=0.583, v_num=gmz4, BTC_val_\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 49.57it/s, loss=0.574, v_num=gmz4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.518\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 49.31it/s, loss=0.574, v_num=gmz4, BTC_val_\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 66.29it/s, loss=0.597, v_num=gmz4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 65.69it/s, loss=0.597, v_num=gmz4, BTC_val_\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:01<00:00, 57.03it/s, loss=0.605, v_num=gmz4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:01<00:00, 56.59it/s, loss=0.605, v_num=gmz4, BTC_val_\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 65.17it/s, loss=0.568, v_num=gmz4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 64.54it/s, loss=0.568, v_num=gmz4, BTC_val_\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:01<00:00, 65.26it/s, loss=0.589, v_num=gmz4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.513\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:01<00:00, 64.73it/s, loss=0.589, v_num=gmz4, BTC_val_\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:01<00:00, 59.09it/s, loss=0.602, v_num=gmz4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:01<00:00, 58.63it/s, loss=0.602, v_num=gmz4, BTC_val_\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:01<00:00, 61.36it/s, loss=0.577, v_num=gmz4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:01<00:00, 60.86it/s, loss=0.577, v_num=gmz4, BTC_val_\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:01<00:00, 64.61it/s, loss=0.563, v_num=gmz4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:01<00:00, 64.11it/s, loss=0.563, v_num=gmz4, BTC_val_\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:01<00:00, 60.67it/s, loss=0.6, v_num=gmz4, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:01<00:00, 60.18it/s, loss=0.6, v_num=gmz4, BTC_val_ac\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:01<00:00, 62.58it/s, loss=0.532, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.500\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:01<00:00, 62.18it/s, loss=0.532, v_num=gmz4, BTC_val\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:01<00:00, 57.30it/s, loss=0.525, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:01<00:00, 56.74it/s, loss=0.525, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:01<00:00, 56.69it/s, loss=0.579, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:01<00:00, 56.33it/s, loss=0.579, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:01<00:00, 64.55it/s, loss=0.539, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:01<00:00, 64.01it/s, loss=0.539, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:01<00:00, 63.39it/s, loss=0.587, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:01<00:00, 62.93it/s, loss=0.587, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:01<00:00, 63.79it/s, loss=0.586, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:01<00:00, 63.09it/s, loss=0.586, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:01<00:00, 61.62it/s, loss=0.562, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:01<00:00, 61.07it/s, loss=0.562, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:01<00:00, 64.26it/s, loss=0.577, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:01<00:00, 63.81it/s, loss=0.577, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:01<00:00, 63.12it/s, loss=0.564, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:01<00:00, 62.59it/s, loss=0.564, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:01<00:00, 64.54it/s, loss=0.601, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:01<00:00, 64.00it/s, loss=0.601, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:01<00:00, 65.16it/s, loss=0.574, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:01<00:00, 64.73it/s, loss=0.574, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:01<00:00, 63.60it/s, loss=0.585, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:01<00:00, 63.17it/s, loss=0.585, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:01<00:00, 65.31it/s, loss=0.581, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:01<00:00, 64.62it/s, loss=0.581, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:01<00:00, 64.89it/s, loss=0.581, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:01<00:00, 64.28it/s, loss=0.581, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:01<00:00, 64.74it/s, loss=0.605, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:01<00:00, 64.08it/s, loss=0.605, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:01<00:00, 64.34it/s, loss=0.603, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:01<00:00, 63.76it/s, loss=0.603, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:01<00:00, 68.81it/s, loss=0.54, v_num=gmz4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:01<00:00, 68.37it/s, loss=0.54, v_num=gmz4, BTC_val_\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 80/80 [00:01<00:00, 72.99it/s, loss=0.579, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 80/80 [00:01<00:00, 71.80it/s, loss=0.579, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:01<00:00, 62.39it/s, loss=0.545, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:01<00:00, 62.00it/s, loss=0.545, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:01<00:00, 58.77it/s, loss=0.565, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:01<00:00, 58.22it/s, loss=0.565, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 80/80 [00:01<00:00, 64.64it/s, loss=0.574, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.496\n",
      "Epoch 30: 100%|â–ˆ| 80/80 [00:01<00:00, 64.20it/s, loss=0.574, v_num=gmz4, BTC_val\n",
      "Epoch 31: 100%|â–ˆ| 80/80 [00:01<00:00, 63.25it/s, loss=0.57, v_num=gmz4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 80/80 [00:01<00:00, 62.62it/s, loss=0.57, v_num=gmz4, BTC_val_\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 80/80 [00:01<00:00, 59.01it/s, loss=0.561, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 80/80 [00:01<00:00, 58.59it/s, loss=0.561, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 80/80 [00:01<00:00, 64.66it/s, loss=0.581, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 80/80 [00:01<00:00, 64.15it/s, loss=0.581, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 80/80 [00:01<00:00, 71.50it/s, loss=0.602, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 80/80 [00:01<00:00, 70.95it/s, loss=0.602, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 80/80 [00:01<00:00, 64.59it/s, loss=0.572, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 80/80 [00:01<00:00, 64.03it/s, loss=0.572, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 80/80 [00:01<00:00, 65.71it/s, loss=0.521, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 80/80 [00:01<00:00, 65.27it/s, loss=0.521, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 37: 100%|â–ˆ| 80/80 [00:01<00:00, 63.20it/s, loss=0.536, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|â–ˆ| 80/80 [00:01<00:00, 62.69it/s, loss=0.536, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 38: 100%|â–ˆ| 80/80 [00:01<00:00, 70.88it/s, loss=0.548, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|â–ˆ| 80/80 [00:01<00:00, 70.37it/s, loss=0.548, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 39: 100%|â–ˆ| 80/80 [00:01<00:00, 64.81it/s, loss=0.567, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|â–ˆ| 80/80 [00:01<00:00, 64.23it/s, loss=0.567, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 40: 100%|â–ˆ| 80/80 [00:01<00:00, 60.07it/s, loss=0.604, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|â–ˆ| 80/80 [00:01<00:00, 59.62it/s, loss=0.604, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 41: 100%|â–ˆ| 80/80 [00:01<00:00, 62.65it/s, loss=0.573, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|â–ˆ| 80/80 [00:01<00:00, 62.08it/s, loss=0.573, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 42: 100%|â–ˆ| 80/80 [00:01<00:00, 63.66it/s, loss=0.531, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|â–ˆ| 80/80 [00:01<00:00, 63.21it/s, loss=0.531, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 43: 100%|â–ˆ| 80/80 [00:01<00:00, 64.76it/s, loss=0.569, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|â–ˆ| 80/80 [00:01<00:00, 64.23it/s, loss=0.569, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 44: 100%|â–ˆ| 80/80 [00:01<00:00, 62.39it/s, loss=0.549, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|â–ˆ| 80/80 [00:01<00:00, 61.78it/s, loss=0.549, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 45: 100%|â–ˆ| 80/80 [00:01<00:00, 71.66it/s, loss=0.589, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|â–ˆ| 80/80 [00:01<00:00, 71.01it/s, loss=0.589, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 46: 100%|â–ˆ| 80/80 [00:01<00:00, 71.16it/s, loss=0.517, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|â–ˆ| 80/80 [00:01<00:00, 70.62it/s, loss=0.517, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 47: 100%|â–ˆ| 80/80 [00:01<00:00, 70.49it/s, loss=0.569, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|â–ˆ| 80/80 [00:01<00:00, 69.78it/s, loss=0.569, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 48: 100%|â–ˆ| 80/80 [00:01<00:00, 70.31it/s, loss=0.568, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|â–ˆ| 80/80 [00:01<00:00, 69.72it/s, loss=0.568, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 49: 100%|â–ˆ| 80/80 [00:01<00:00, 68.86it/s, loss=0.586, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|â–ˆ| 80/80 [00:01<00:00, 68.01it/s, loss=0.586, v_num=gmz4, BTC_val\u001b[A\n",
      "Epoch 50: 100%|â–ˆ| 80/80 [00:01<00:00, 64.09it/s, loss=0.575, v_num=gmz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.496. Signaling Trainer to stop.\n",
      "Epoch 50: 100%|â–ˆ| 80/80 [00:01<00:00, 63.63it/s, loss=0.575, v_num=gmz4, BTC_val\n",
      "Epoch 50: 100%|â–ˆ| 80/80 [00:01<00:00, 63.47it/s, loss=0.575, v_num=gmz4, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Testing: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 88.43it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.7096773982048035,\n",
      " 'BTC_test_f1': 0.7047099471092224,\n",
      " 'test_loss': 0.6141742467880249}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 62830\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005508-1eprgmz4/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005508-1eprgmz4/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.74603\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.42823\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4029\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 73\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621461382\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.72922\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.7152\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.55561\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.77778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.5299\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.70968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.70471\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.61417\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–…â–„â–…â–‡â–‡â–„â–…â–„â–…â–…â–†â–…â–…â–†â–†â–†â–„â–‡â–„â–…â–†â–‡â–‡â–…â–â–…â–…â–†â–„â–„â–†â–„â–…â–…â–…â–…â–†â–ˆâ–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–…â–„â–…â–‡â–‡â–„â–…â–„â–…â–…â–†â–…â–…â–†â–†â–†â–„â–‡â–„â–…â–†â–‡â–‡â–…â–â–…â–…â–†â–„â–„â–†â–„â–„â–„â–…â–„â–†â–ˆâ–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–„â–†â–ƒâ–‚â–‚â–†â–„â–†â–„â–„â–‚â–ƒâ–…â–ƒâ–ƒâ–ƒâ–„â–ƒâ–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ƒâ–„â–„â–…â–…â–ƒâ–…â–„â–ƒâ–„â–„â–‚â–â–ƒâ–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–†â–†â–†â–†â–†â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–â–†â–†â–…â–†â–†â–†â–‡â–‡â–†â–‡â–‡â–‡â–†â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–„â–ƒâ–†â–‡â–ƒâ–ˆâ–ƒâ–…â–â–ƒâ–„â–ƒâ–ƒâ–ƒâ–„â–…â–„â–ƒâ–‚â–‚â–ˆâ–„â–â–†â–â–â–‚â–…â–‚â–„â–…â–„â–…â–‚â–…â–ƒâ–„â–…â–ƒâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_single_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1eprgmz4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 00:56:32.154796: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_single_lstm_loss_weighted_binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/34r9vs22\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005630-34r9vs22\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 130   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.7 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 80/81 [00:01<00:00, 60.08it/s, loss=0.694, v_num=vs22, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.695\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:01<00:00, 58.93it/s, loss=0.694, v_num=vs22, BTC_val_\n",
      "Epoch 1:  99%|â–‰| 80/81 [00:01<00:00, 62.76it/s, loss=0.704, v_num=vs22, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:01<00:00, 61.48it/s, loss=0.704, v_num=vs22, BTC_val_\u001b[A\n",
      "Epoch 2:  99%|â–‰| 80/81 [00:01<00:00, 52.67it/s, loss=0.695, v_num=vs22, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:01<00:00, 51.93it/s, loss=0.695, v_num=vs22, BTC_val_\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:01<00:00, 49.11it/s, loss=0.711, v_num=vs22, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:01<00:00, 48.69it/s, loss=0.711, v_num=vs22, BTC_val_\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:01<00:00, 47.58it/s, loss=0.692, v_num=vs22, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:01<00:00, 47.02it/s, loss=0.692, v_num=vs22, BTC_val_\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:02<00:00, 39.14it/s, loss=0.692, v_num=vs22, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:02<00:00, 38.99it/s, loss=0.692, v_num=vs22, BTC_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.691\n",
      "Epoch 6:  99%|â–‰| 80/81 [00:01<00:00, 53.35it/s, loss=0.693, v_num=vs22, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:01<00:00, 52.42it/s, loss=0.693, v_num=vs22, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 80/81 [00:01<00:00, 67.74it/s, loss=0.691, v_num=vs22, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:01<00:00, 66.59it/s, loss=0.691, v_num=vs22, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 80/81 [00:01<00:00, 73.49it/s, loss=0.699, v_num=vs22, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:01<00:00, 71.75it/s, loss=0.699, v_num=vs22, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 80/81 [00:01<00:00, 57.54it/s, loss=0.696, v_num=vs22, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:01<00:00, 56.77it/s, loss=0.696, v_num=vs22, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 80/81 [00:01<00:00, 70.71it/s, loss=0.693, v_num=vs22, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:01<00:00, 69.31it/s, loss=0.693, v_num=vs22, BTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 80/81 [00:00<00:00, 85.69it/s, loss=0.692, v_num=vs22, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:00<00:00, 83.33it/s, loss=0.692, v_num=vs22, BTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 80/81 [00:00<00:00, 89.13it/s, loss=0.697, v_num=vs22, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:00<00:00, 86.81it/s, loss=0.697, v_num=vs22, BTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 80/81 [00:00<00:00, 80.54it/s, loss=0.695, v_num=vs22, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:01<00:00, 77.84it/s, loss=0.695, v_num=vs22, BTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 80/81 [00:00<00:00, 89.85it/s, loss=0.695, v_num=vs22, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:00<00:00, 86.24it/s, loss=0.695, v_num=vs22, BTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 80/81 [00:01<00:00, 56.41it/s, loss=0.694, v_num=vs22, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:01<00:00, 53.26it/s, loss=0.694, v_num=vs22, BTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 80/81 [00:01<00:00, 77.37it/s, loss=0.691, v_num=vs22, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:01<00:00, 75.83it/s, loss=0.691, v_num=vs22, BTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 80/81 [00:00<00:00, 84.71it/s, loss=0.698, v_num=vs22, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:00<00:00, 82.69it/s, loss=0.698, v_num=vs22, BTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 80/81 [00:00<00:00, 85.99it/s, loss=0.7, v_num=vs22, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:00<00:00, 83.07it/s, loss=0.7, v_num=vs22, BTC_val_a\u001b[A\n",
      "Epoch 19:  99%|â–‰| 80/81 [00:01<00:00, 76.43it/s, loss=0.697, v_num=vs22, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:01<00:00, 74.96it/s, loss=0.697, v_num=vs22, BTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 80/81 [00:01<00:00, 78.43it/s, loss=0.694, v_num=vs22, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:01<00:00, 76.52it/s, loss=0.694, v_num=vs22, BTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 80/81 [00:01<00:00, 79.96it/s, loss=0.695, v_num=vs22, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:01<00:00, 77.81it/s, loss=0.695, v_num=vs22, BTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 80/81 [00:01<00:00, 76.90it/s, loss=0.694, v_num=vs22, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 81/81 [00:01<00:00, 75.19it/s, loss=0.694, v_num=vs22, BTC_val\u001b[A\n",
      "Epoch 23:  99%|â–‰| 80/81 [00:00<00:00, 84.77it/s, loss=0.694, v_num=vs22, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 81/81 [00:00<00:00, 82.72it/s, loss=0.694, v_num=vs22, BTC_val\u001b[A\n",
      "Epoch 24:  99%|â–‰| 80/81 [00:00<00:00, 85.56it/s, loss=0.696, v_num=vs22, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 81/81 [00:00<00:00, 83.26it/s, loss=0.696, v_num=vs22, BTC_val\u001b[A\n",
      "Epoch 25:  99%|â–‰| 80/81 [00:01<00:00, 79.30it/s, loss=0.692, v_num=vs22, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.691. Signaling Trainer to stop.\n",
      "Epoch 25: 100%|â–ˆ| 81/81 [00:01<00:00, 77.14it/s, loss=0.692, v_num=vs22, BTC_val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|â–ˆ| 81/81 [00:01<00:00, 76.89it/s, loss=0.692, v_num=vs22, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 98.63it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5806451439857483,\n",
      " 'BTC_test_f1': 0.36725807189941406,\n",
      " 'test_loss': 0.6884135603904724}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 63051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005630-34r9vs22/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005630-34r9vs22/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.69882\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2080\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 41\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621461431\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 93\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.50827\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.4836\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69266\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.58462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.69276\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.58065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.36726\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.68841\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–†â–„â–…â–†â–ƒâ–‡â–…â–ˆâ–â–ƒâ–ƒâ–…â–ƒâ–…â–…â–ƒâ–†â–‚â–„â–…â–„â–ƒâ–ƒâ–…â–…â–„â–„â–ƒâ–‡â–†â–†â–…â–„â–ƒâ–†â–„â–„â–ƒâ–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–†â–…â–†â–‡â–„â–‡â–…â–„â–â–„â–„â–†â–ƒâ–†â–†â–‚â–ƒâ–‚â–…â–†â–„â–ƒâ–‚â–†â–…â–…â–„â–ƒâ–ˆâ–†â–‡â–ƒâ–‚â–„â–‡â–…â–…â–‚â–†â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–ƒâ–…â–…â–„â–‡â–‚â–…â–â–ˆâ–‡â–†â–…â–‡â–†â–†â–†â–…â–ˆâ–†â–†â–†â–†â–†â–‡â–†â–‡â–†â–†â–…â–…â–…â–„â–†â–†â–†â–‡â–†â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–„â–…â–ƒâ–„â–ƒâ–â–„â–„â–„â–â–„â–â–ƒâ–…â–‚â–â–ˆâ–…â–‚â–‚â–…â–‚â–…â–„â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–…â–ƒâ–„â–ƒâ–ƒâ–â–…â–…â–ƒâ–ƒâ–ƒâ–‚â–„â–ƒâ–ƒâ–â–ˆâ–…â–ƒâ–‚â–„â–ƒâ–…â–…â–ƒâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–‡â–ˆâ–…â–„â–„â–„â–‚â–‚â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–…â–â–â–â–…â–â–â–â–…â–…â–…â–…â–…â–…â–…â–…â–…â–â–ˆâ–â–â–…â–ˆâ–…â–â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–‚â–â–â–â–‚â–â–â–â–‚â–‚â–†â–†â–‚â–‚â–‚â–‚â–‚â–â–ˆâ–â–â–‚â–ˆâ–†â–â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–„â–ˆâ–„â–ƒâ–ˆâ–â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_single_lstm_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/34r9vs22\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 00:57:21.529125: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_single_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2ae5c60y\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005720-2ae5c60y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 195   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.8 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 79/80 [00:00<00:00, 84.83it/s, loss=1.11, v_num=c60y, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.077\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:01<00:00, 76.98it/s, loss=1.11, v_num=c60y, BTC_val_a\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 73.57it/s, loss=1.1, v_num=c60y, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 72.73it/s, loss=1.1, v_num=c60y, BTC_val_ac\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:00<00:00, 84.88it/s, loss=1.11, v_num=c60y, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 1.073\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:00<00:00, 84.12it/s, loss=1.11, v_num=c60y, BTC_val_a\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:00<00:00, 82.21it/s, loss=1.07, v_num=c60y, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:00<00:00, 81.11it/s, loss=1.07, v_num=c60y, BTC_val_a\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 65.46it/s, loss=1.02, v_num=c60y, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 1.062\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 64.98it/s, loss=1.02, v_num=c60y, BTC_val_a\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:01<00:00, 78.39it/s, loss=1.02, v_num=c60y, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:01<00:00, 77.46it/s, loss=1.02, v_num=c60y, BTC_val_a\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:01<00:00, 78.44it/s, loss=1.02, v_num=c60y, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:01<00:00, 77.54it/s, loss=1.02, v_num=c60y, BTC_val_a\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:00<00:00, 86.44it/s, loss=1.02, v_num=c60y, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:00<00:00, 85.69it/s, loss=1.02, v_num=c60y, BTC_val_a\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:00<00:00, 89.43it/s, loss=0.998, v_num=c60y, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:00<00:00, 88.60it/s, loss=0.998, v_num=c60y, BTC_val_\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:00<00:00, 85.94it/s, loss=1.02, v_num=c60y, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:00<00:00, 85.20it/s, loss=1.02, v_num=c60y, BTC_val_a\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:00<00:00, 85.53it/s, loss=0.988, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:00<00:00, 84.37it/s, loss=0.988, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:01<00:00, 76.37it/s, loss=1.01, v_num=c60y, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:01<00:00, 75.71it/s, loss=1.01, v_num=c60y, BTC_val_\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:01<00:00, 75.98it/s, loss=0.997, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:01<00:00, 75.34it/s, loss=0.997, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:00<00:00, 83.49it/s, loss=0.999, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:00<00:00, 82.76it/s, loss=0.999, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:00<00:00, 87.50it/s, loss=0.966, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:00<00:00, 86.81it/s, loss=0.966, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:01<00:00, 75.64it/s, loss=0.957, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:01<00:00, 74.97it/s, loss=0.957, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:01<00:00, 78.53it/s, loss=0.976, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:01<00:00, 77.90it/s, loss=0.976, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:00<00:00, 83.19it/s, loss=0.999, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:00<00:00, 82.43it/s, loss=0.999, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:00<00:00, 87.44it/s, loss=0.957, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:00<00:00, 86.64it/s, loss=0.957, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:00<00:00, 90.99it/s, loss=0.956, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:00<00:00, 90.09it/s, loss=0.956, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:00<00:00, 89.70it/s, loss=0.961, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:00<00:00, 88.84it/s, loss=0.961, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:00<00:00, 88.79it/s, loss=0.969, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:00<00:00, 88.03it/s, loss=0.969, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:00<00:00, 86.23it/s, loss=0.974, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:00<00:00, 85.33it/s, loss=0.974, v_num=c60y, BTC_val\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.037 >= min_delta = 0.003. New best score: 1.025\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:00<00:00, 87.11it/s, loss=0.941, v_num=c60y, BTC_val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:00<00:00, 86.12it/s, loss=0.941, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:01<00:00, 77.40it/s, loss=0.944, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:01<00:00, 76.62it/s, loss=0.944, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:00<00:00, 84.82it/s, loss=0.977, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:00<00:00, 84.09it/s, loss=0.977, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:00<00:00, 87.49it/s, loss=0.955, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:00<00:00, 86.62it/s, loss=0.955, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 80/80 [00:01<00:00, 77.31it/s, loss=0.964, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 80/80 [00:01<00:00, 76.31it/s, loss=0.964, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:01<00:00, 76.52it/s, loss=0.922, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:01<00:00, 75.82it/s, loss=0.922, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:01<00:00, 78.04it/s, loss=0.91, v_num=c60y, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:01<00:00, 77.30it/s, loss=0.91, v_num=c60y, BTC_val_\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 80/80 [00:00<00:00, 86.47it/s, loss=0.936, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 80/80 [00:00<00:00, 85.67it/s, loss=0.936, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 80/80 [00:00<00:00, 88.89it/s, loss=0.893, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 80/80 [00:00<00:00, 88.11it/s, loss=0.893, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 80/80 [00:00<00:00, 87.91it/s, loss=0.976, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 80/80 [00:00<00:00, 87.08it/s, loss=0.976, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 80/80 [00:00<00:00, 90.09it/s, loss=0.937, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 80/80 [00:00<00:00, 89.03it/s, loss=0.937, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 80/80 [00:00<00:00, 87.40it/s, loss=0.882, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 80/80 [00:00<00:00, 86.62it/s, loss=0.882, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 80/80 [00:00<00:00, 89.72it/s, loss=0.906, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 80/80 [00:00<00:00, 88.83it/s, loss=0.906, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 80/80 [00:00<00:00, 89.64it/s, loss=0.982, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 80/80 [00:00<00:00, 88.76it/s, loss=0.982, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 37: 100%|â–ˆ| 80/80 [00:00<00:00, 88.28it/s, loss=0.97, v_num=c60y, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|â–ˆ| 80/80 [00:00<00:00, 87.48it/s, loss=0.97, v_num=c60y, BTC_val_\u001b[A\n",
      "Epoch 38: 100%|â–ˆ| 80/80 [00:00<00:00, 88.19it/s, loss=0.943, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|â–ˆ| 80/80 [00:00<00:00, 87.35it/s, loss=0.943, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 39: 100%|â–ˆ| 80/80 [00:01<00:00, 79.93it/s, loss=0.928, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|â–ˆ| 80/80 [00:01<00:00, 79.22it/s, loss=0.928, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 40: 100%|â–ˆ| 80/80 [00:01<00:00, 73.34it/s, loss=0.913, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|â–ˆ| 80/80 [00:01<00:00, 72.77it/s, loss=0.913, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 41: 100%|â–ˆ| 80/80 [00:00<00:00, 82.23it/s, loss=0.975, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|â–ˆ| 80/80 [00:00<00:00, 81.45it/s, loss=0.975, v_num=c60y, BTC_val\u001b[A\n",
      "Epoch 42: 100%|â–ˆ| 80/80 [00:00<00:00, 87.90it/s, loss=0.917, v_num=c60y, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.025. Signaling Trainer to stop.\n",
      "Epoch 42: 100%|â–ˆ| 80/80 [00:00<00:00, 86.96it/s, loss=0.917, v_num=c60y, BTC_val\n",
      "Epoch 42: 100%|â–ˆ| 80/80 [00:00<00:00, 86.58it/s, loss=0.917, v_num=c60y, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 118.86it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.4516128897666931,\n",
      " 'BTC_test_f1': 0.40499287843704224,\n",
      " 'test_loss': 0.9798224568367004}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 63252\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005720-2ae5c60y/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005720-2ae5c60y/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.61355\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.70869\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3397\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621461490\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 153\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.49406\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.47716\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.93209\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.33968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.2079\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.45161\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.40499\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.97982\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–‚â–ƒâ–‚â–‚â–â–„â–„â–ƒâ–ƒâ–ˆâ–ƒâ–„â–ƒâ–„â–ƒâ–ƒâ–‚â–ƒâ–…â–„â–„â–…â–„â–ƒâ–ƒâ–…â–ƒâ–ƒâ–†â–ƒâ–‚â–…â–„â–…â–ƒâ–„â–ƒâ–ƒâ–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–â–‚â–‚â–‚â–â–„â–ƒâ–ƒâ–‚â–ˆâ–„â–„â–ƒâ–„â–ƒâ–ƒâ–‚â–ƒâ–†â–„â–„â–…â–ƒâ–ƒâ–ƒâ–…â–„â–ƒâ–†â–ƒâ–‚â–…â–…â–†â–ƒâ–„â–ƒâ–„â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–†â–…â–†â–†â–ˆâ–„â–„â–ƒâ–…â–â–…â–„â–…â–‚â–…â–„â–„â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–„â–‚â–…â–ˆâ–…â–ƒâ–‡â–ƒâ–ƒâ–â–‡â–‚â–ƒâ–ƒâ–„â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–â–ƒâ–ƒâ–ƒâ–…â–…â–…â–†â–†â–‡â–†â–†â–†â–‡â–‡â–†â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–â–â–‚â–ƒâ–„â–…â–…â–…â–†â–†â–‡â–†â–†â–‡â–†â–‡â–‡â–†â–‡â–ˆâ–ˆâ–‡â–ˆâ–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–ˆâ–‡â–‡â–…â–…â–„â–„â–ƒâ–„â–ƒâ–„â–„â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–ˆâ–â–†â–ƒâ–†â–†â–ƒâ–â–ƒâ–ƒâ–†â–ƒâ–ƒâ–ƒâ–ˆâ–ƒâ–ƒâ–ƒâ–†â–ˆâ–†â–ˆâ–ƒâ–ƒâ–†â–†â–ƒâ–†â–ˆâ–†â–ƒâ–ƒâ–ƒâ–†â–ƒâ–â–†â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–…â–â–ƒâ–„â–†â–†â–ƒâ–‚â–ƒâ–„â–†â–ƒâ–ƒâ–„â–ˆâ–„â–„â–ƒâ–†â–ˆâ–†â–ˆâ–„â–„â–†â–†â–„â–†â–ˆâ–†â–„â–„â–„â–…â–„â–‚â–†â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ƒâ–…â–ƒâ–ƒâ–‚â–ƒâ–„â–…â–„â–„â–ƒâ–†â–‡â–†â–…â–…â–†â–†â–„â–„â–„â–â–„â–„â–ƒâ–…â–†â–‚â–ƒâ–…â–ˆâ–‡â–„â–…â–‡â–‡â–…â–…â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_single_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2ae5c60y\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 00:58:21.463723: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_single_lstm_loss_weighted_multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3u51l2tj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005820-3u51l2tj\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 195   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.8 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 80/81 [00:00<00:00, 91.74it/s, loss=1.12, v_num=l2tj, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.172\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:00<00:00, 88.76it/s, loss=1.12, v_num=l2tj, BTC_val_a\n",
      "Epoch 1:  99%|â–‰| 80/81 [00:00<00:00, 80.17it/s, loss=1.1, v_num=l2tj, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:01<00:00, 77.90it/s, loss=1.1, v_num=l2tj, BTC_val_ac\u001b[A\n",
      "Epoch 2:  99%|â–‰| 80/81 [00:01<00:00, 68.42it/s, loss=1.11, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.023 >= min_delta = 0.003. New best score: 1.148\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:01<00:00, 66.22it/s, loss=1.11, v_num=l2tj, BTC_val_a\n",
      "Epoch 3:  99%|â–‰| 80/81 [00:01<00:00, 77.62it/s, loss=1.1, v_num=l2tj, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:01<00:00, 75.65it/s, loss=1.1, v_num=l2tj, BTC_val_ac\u001b[A\n",
      "Epoch 4:  99%|â–‰| 80/81 [00:01<00:00, 74.42it/s, loss=1.15, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:01<00:00, 72.34it/s, loss=1.15, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Epoch 5:  99%|â–‰| 80/81 [00:00<00:00, 91.82it/s, loss=1.1, v_num=l2tj, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:00<00:00, 89.43it/s, loss=1.1, v_num=l2tj, BTC_val_ac\u001b[A\n",
      "Epoch 6:  99%|â–‰| 80/81 [00:00<00:00, 91.29it/s, loss=1.11, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:00<00:00, 88.56it/s, loss=1.11, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Epoch 7:  99%|â–‰| 80/81 [00:00<00:00, 92.71it/s, loss=1.09, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:00<00:00, 90.01it/s, loss=1.09, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Epoch 8:  99%|â–‰| 80/81 [00:00<00:00, 91.53it/s, loss=1.09, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 1.136\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:00<00:00, 89.13it/s, loss=1.09, v_num=l2tj, BTC_val_a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  99%|â–‰| 80/81 [00:00<00:00, 87.09it/s, loss=1.1, v_num=l2tj, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:00<00:00, 84.18it/s, loss=1.1, v_num=l2tj, BTC_val_ac\u001b[A\n",
      "Epoch 10:  99%|â–‰| 80/81 [00:01<00:00, 79.60it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:01<00:00, 77.27it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Epoch 11:  99%|â–‰| 80/81 [00:00<00:00, 87.03it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:00<00:00, 84.85it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Epoch 12:  99%|â–‰| 80/81 [00:00<00:00, 92.35it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:00<00:00, 89.88it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.018 >= min_delta = 0.003. New best score: 1.119\n",
      "Epoch 13:  99%|â–‰| 80/81 [00:00<00:00, 84.05it/s, loss=1.09, v_num=l2tj, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:00<00:00, 81.68it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Epoch 14:  99%|â–‰| 80/81 [00:01<00:00, 78.14it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:01<00:00, 76.04it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Epoch 15:  99%|â–‰| 80/81 [00:00<00:00, 86.18it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:01<00:00, 79.19it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Epoch 16:  99%|â–‰| 80/81 [00:00<00:00, 89.59it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:00<00:00, 87.30it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Epoch 17:  99%|â–‰| 80/81 [00:00<00:00, 88.52it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:00<00:00, 85.76it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Epoch 18:  99%|â–‰| 80/81 [00:00<00:00, 90.70it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:00<00:00, 88.30it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Epoch 19:  99%|â–‰| 80/81 [00:00<00:00, 88.19it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:00<00:00, 85.78it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Epoch 20:  99%|â–‰| 80/81 [00:00<00:00, 90.62it/s, loss=1.08, v_num=l2tj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:00<00:00, 88.19it/s, loss=1.08, v_num=l2tj, BTC_val_\u001b[A\n",
      "Epoch 21:  99%|â–‰| 80/81 [00:00<00:00, 90.38it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:00<00:00, 88.11it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Epoch 22:  99%|â–‰| 80/81 [00:00<00:00, 91.36it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 81/81 [00:00<00:00, 89.04it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Epoch 23:  99%|â–‰| 80/81 [00:00<00:00, 92.50it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 81/81 [00:00<00:00, 90.06it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Epoch 24:  99%|â–‰| 80/81 [00:00<00:00, 88.94it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 81/81 [00:00<00:00, 86.24it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Epoch 25:  99%|â–‰| 80/81 [00:00<00:00, 89.93it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 81/81 [00:00<00:00, 87.67it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Epoch 26:  99%|â–‰| 80/81 [00:00<00:00, 92.18it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 81/81 [00:00<00:00, 89.72it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Epoch 27:  99%|â–‰| 80/81 [00:00<00:00, 93.06it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 81/81 [00:00<00:00, 90.66it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n",
      "Epoch 28:  99%|â–‰| 80/81 [00:00<00:00, 88.75it/s, loss=1.08, v_num=l2tj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 81/81 [00:00<00:00, 86.52it/s, loss=1.08, v_num=l2tj, BTC_val_\u001b[A\n",
      "Epoch 29:  99%|â–‰| 80/81 [00:00<00:00, 90.61it/s, loss=1.11, v_num=l2tj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 81/81 [00:00<00:00, 88.08it/s, loss=1.11, v_num=l2tj, BTC_val_\u001b[A\n",
      "Epoch 30:  99%|â–‰| 80/81 [00:00<00:00, 81.74it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 81/81 [00:01<00:00, 79.39it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Epoch 31:  99%|â–‰| 80/81 [00:01<00:00, 75.22it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 81/81 [00:01<00:00, 73.42it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Epoch 32:  99%|â–‰| 80/81 [00:00<00:00, 83.52it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.119. Signaling Trainer to stop.\n",
      "Epoch 32: 100%|â–ˆ| 81/81 [00:00<00:00, 81.35it/s, loss=1.09, v_num=l2tj, BTC_val_\n",
      "Epoch 32: 100%|â–ˆ| 81/81 [00:00<00:00, 81.09it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 100.74it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.29032257199287415,\n",
      " 'BTC_test_f1': 0.21669228374958038,\n",
      " 'test_loss': 1.1167445182800293}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 63464\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005820-3u51l2tj/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005820-3u51l2tj/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.4375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.28571\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.07213\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2640\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621461540\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 118\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.35461\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.30432\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09402\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.37635\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.29032\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.21669\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.11674\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–„â–„â–„â–„â–‡â–…â–â–„â–„â–ˆâ–…â–…â–ƒâ–†â–„â–‡â–…â–„â–ƒâ–ƒâ–‡â–†â–„â–…â–‡â–„â–„â–„â–„â–„â–‡â–…â–„â–„â–„â–‚â–ƒâ–†â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–„â–ƒâ–„â–„â–†â–„â–â–„â–ƒâ–ˆâ–…â–…â–ƒâ–†â–ƒâ–†â–„â–„â–ƒâ–ƒâ–†â–…â–ƒâ–…â–‡â–ƒâ–„â–ƒâ–„â–„â–…â–…â–„â–„â–ƒâ–‚â–‚â–†â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–„â–„â–ƒâ–ƒâ–â–â–ˆâ–‚â–„â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–â–‚â–‚â–ƒâ–ƒâ–‚â–â–ƒâ–ƒâ–‚â–„â–‚â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–„â–‚â–ƒâ–‚â–ƒâ–ˆâ–ƒâ–ƒâ–ˆâ–ƒâ–…â–…â–ƒâ–‚â–‡â–„â–…â–ƒâ–‚â–ƒâ–ƒâ–‡â–†â–…â–ˆâ–‡â–…â–…â–…â–…â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–â–…â–ƒâ–…â–ƒâ–„â–‡â–„â–…â–ˆâ–„â–†â–…â–„â–ƒâ–†â–…â–…â–ƒâ–…â–„â–…â–ˆâ–‡â–†â–‡â–ˆâ–†â–†â–…â–†â–…â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–†â–†â–ƒâ–…â–ƒâ–‚â–„â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–ƒâ–‚â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–†â–†â–†â–â–†â–â–â–â–†â–†â–†â–†â–â–†â–†â–†â–†â–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–ƒâ–ƒâ–ƒâ–â–ƒâ–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–‚â–‚â–â–„â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–â–‚â–‚â–ƒâ–„â–ƒâ–ƒâ–„â–â–‚â–‚â–ƒâ–„â–ƒâ–ˆâ–„â–†â–‚â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_single_lstm_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3u51l2tj\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 00:59:11.607819: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/28qzo7z2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005909-28qzo7z2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 79/80 [00:01<00:00, 44.84it/s, loss=0.705, v_num=o7z2, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.656\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:01<00:00, 44.14it/s, loss=0.705, v_num=o7z2, BTC_val_\n",
      "Epoch 1:  99%|â–‰| 79/80 [00:01<00:00, 46.54it/s, loss=0.623, v_num=o7z2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.102 >= min_delta = 0.003. New best score: 0.553\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 46.12it/s, loss=0.623, v_num=o7z2, BTC_val_\n",
      "Epoch 2:  99%|â–‰| 79/80 [00:01<00:00, 44.42it/s, loss=0.642, v_num=o7z2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.029 >= min_delta = 0.003. New best score: 0.524\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 43.67it/s, loss=0.642, v_num=o7z2, BTC_val_\n",
      "Epoch 3:  99%|â–‰| 79/80 [00:01<00:00, 44.18it/s, loss=0.576, v_num=o7z2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.516\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:01<00:00, 43.30it/s, loss=0.576, v_num=o7z2, BTC_val_\n",
      "Epoch 4:  99%|â–‰| 79/80 [00:01<00:00, 44.81it/s, loss=0.617, v_num=o7z2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 44.27it/s, loss=0.617, v_num=o7z2, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 79/80 [00:01<00:00, 44.51it/s, loss=0.598, v_num=o7z2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.499\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:01<00:00, 44.14it/s, loss=0.598, v_num=o7z2, BTC_val_\n",
      "Epoch 6:  99%|â–‰| 79/80 [00:01<00:00, 42.60it/s, loss=0.595, v_num=o7z2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:01<00:00, 41.79it/s, loss=0.595, v_num=o7z2, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 79/80 [00:01<00:00, 43.45it/s, loss=0.59, v_num=o7z2, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:01<00:00, 43.05it/s, loss=0.59, v_num=o7z2, BTC_val_a\u001b[A\n",
      "Epoch 8:  99%|â–‰| 79/80 [00:01<00:00, 45.45it/s, loss=0.556, v_num=o7z2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:01<00:00, 44.96it/s, loss=0.556, v_num=o7z2, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 79/80 [00:02<00:00, 38.42it/s, loss=0.589, v_num=o7z2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:02<00:00, 38.14it/s, loss=0.589, v_num=o7z2, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 79/80 [00:01<00:00, 39.98it/s, loss=0.591, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:02<00:00, 39.60it/s, loss=0.591, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 79/80 [00:01<00:00, 46.67it/s, loss=0.556, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:01<00:00, 46.25it/s, loss=0.556, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 79/80 [00:01<00:00, 39.60it/s, loss=0.56, v_num=o7z2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.482\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:02<00:00, 39.33it/s, loss=0.56, v_num=o7z2, BTC_val_\n",
      "Epoch 13:  99%|â–‰| 79/80 [00:02<00:00, 37.15it/s, loss=0.605, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:02<00:00, 36.86it/s, loss=0.605, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 79/80 [00:02<00:00, 33.95it/s, loss=0.576, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:02<00:00, 33.79it/s, loss=0.576, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 79/80 [00:02<00:00, 39.17it/s, loss=0.566, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:02<00:00, 38.97it/s, loss=0.566, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 79/80 [00:02<00:00, 37.73it/s, loss=0.568, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:02<00:00, 37.55it/s, loss=0.568, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 79/80 [00:02<00:00, 38.88it/s, loss=0.57, v_num=o7z2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:02<00:00, 38.65it/s, loss=0.57, v_num=o7z2, BTC_val_\u001b[A\n",
      "Epoch 18:  99%|â–‰| 79/80 [00:02<00:00, 39.12it/s, loss=0.593, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:02<00:00, 38.84it/s, loss=0.593, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 79/80 [00:02<00:00, 39.39it/s, loss=0.517, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:02<00:00, 39.15it/s, loss=0.517, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 79/80 [00:01<00:00, 45.12it/s, loss=0.628, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:01<00:00, 44.32it/s, loss=0.628, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 79/80 [00:02<00:00, 38.63it/s, loss=0.642, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:02<00:00, 38.34it/s, loss=0.642, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 79/80 [00:02<00:00, 34.84it/s, loss=0.54, v_num=o7z2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:02<00:00, 34.70it/s, loss=0.54, v_num=o7z2, BTC_val_\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:  99%|â–‰| 79/80 [00:02<00:00, 38.15it/s, loss=0.574, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 0.468\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:02<00:00, 37.50it/s, loss=0.574, v_num=o7z2, BTC_val\n",
      "Epoch 24:  99%|â–‰| 79/80 [00:02<00:00, 30.02it/s, loss=0.591, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:02<00:00, 29.87it/s, loss=0.591, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 25:  99%|â–‰| 79/80 [00:02<00:00, 34.04it/s, loss=0.562, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:02<00:00, 33.89it/s, loss=0.562, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 26:  99%|â–‰| 79/80 [00:02<00:00, 35.86it/s, loss=0.557, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.015 >= min_delta = 0.003. New best score: 0.454\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:02<00:00, 35.67it/s, loss=0.557, v_num=o7z2, BTC_val\n",
      "Epoch 27:  99%|â–‰| 79/80 [00:02<00:00, 30.79it/s, loss=0.591, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 80/80 [00:02<00:00, 30.45it/s, loss=0.591, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 28:  99%|â–‰| 79/80 [00:02<00:00, 33.67it/s, loss=0.579, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:02<00:00, 33.51it/s, loss=0.579, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 29:  99%|â–‰| 79/80 [00:02<00:00, 34.03it/s, loss=0.599, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:02<00:00, 33.45it/s, loss=0.599, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 30:  99%|â–‰| 79/80 [00:02<00:00, 30.53it/s, loss=0.522, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 80/80 [00:02<00:00, 30.43it/s, loss=0.522, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 31:  99%|â–‰| 79/80 [00:02<00:00, 33.20it/s, loss=0.594, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 80/80 [00:02<00:00, 33.10it/s, loss=0.594, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 32:  99%|â–‰| 79/80 [00:02<00:00, 31.71it/s, loss=0.565, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 80/80 [00:02<00:00, 31.63it/s, loss=0.565, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 33:  99%|â–‰| 79/80 [00:02<00:00, 34.49it/s, loss=0.572, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 80/80 [00:02<00:00, 34.35it/s, loss=0.572, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 34:  99%|â–‰| 79/80 [00:02<00:00, 27.96it/s, loss=0.702, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 80/80 [00:02<00:00, 27.89it/s, loss=0.702, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 35:  99%|â–‰| 79/80 [00:01<00:00, 44.00it/s, loss=0.698, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 80/80 [00:01<00:00, 43.58it/s, loss=0.698, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 36:  99%|â–‰| 79/80 [00:01<00:00, 42.63it/s, loss=0.699, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 80/80 [00:01<00:00, 42.25it/s, loss=0.699, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 37:  99%|â–‰| 79/80 [00:02<00:00, 33.52it/s, loss=0.694, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|â–ˆ| 80/80 [00:02<00:00, 33.12it/s, loss=0.694, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 38:  99%|â–‰| 79/80 [00:02<00:00, 31.89it/s, loss=0.666, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|â–ˆ| 80/80 [00:02<00:00, 31.74it/s, loss=0.666, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 39:  99%|â–‰| 79/80 [00:02<00:00, 29.79it/s, loss=0.657, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|â–ˆ| 80/80 [00:02<00:00, 29.44it/s, loss=0.657, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 40:  99%|â–‰| 79/80 [00:02<00:00, 36.60it/s, loss=0.651, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|â–ˆ| 80/80 [00:02<00:00, 36.17it/s, loss=0.651, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 41:  99%|â–‰| 79/80 [00:02<00:00, 30.10it/s, loss=0.593, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|â–ˆ| 80/80 [00:02<00:00, 30.05it/s, loss=0.593, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 42:  99%|â–‰| 79/80 [00:01<00:00, 44.42it/s, loss=0.671, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|â–ˆ| 80/80 [00:01<00:00, 44.05it/s, loss=0.671, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 43:  99%|â–‰| 79/80 [00:01<00:00, 44.26it/s, loss=0.598, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|â–ˆ| 80/80 [00:01<00:00, 43.87it/s, loss=0.598, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 44:  99%|â–‰| 79/80 [00:02<00:00, 29.72it/s, loss=0.588, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|â–ˆ| 80/80 [00:02<00:00, 29.45it/s, loss=0.588, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 45:  99%|â–‰| 79/80 [00:01<00:00, 44.02it/s, loss=0.633, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|â–ˆ| 80/80 [00:01<00:00, 43.65it/s, loss=0.633, v_num=o7z2, BTC_val\u001b[A\n",
      "Epoch 46:  99%|â–‰| 79/80 [00:01<00:00, 40.12it/s, loss=0.596, v_num=o7z2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.454. Signaling Trainer to stop.\n",
      "Epoch 46: 100%|â–ˆ| 80/80 [00:02<00:00, 39.86it/s, loss=0.596, v_num=o7z2, BTC_val\n",
      "Epoch 46: 100%|â–ˆ| 80/80 [00:02<00:00, 39.80it/s, loss=0.596, v_num=o7z2, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 33.33it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6774193644523621,\n",
      " 'BTC_test_f1': 0.6765552759170532,\n",
      " 'test_loss': 0.5754806995391846}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 63622\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005909-28qzo7z2/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005909-28qzo7z2/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.86667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.53766\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 46\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3713\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621461660\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 168\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.68567\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.67186\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.58554\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.64935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.58857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.67742\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.67656\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.57548\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–ƒâ–…â–†â–‡â–ƒâ–„â–„â–ƒâ–…â–†â–†â–†â–†â–†â–„â–„â–‡â–…â–†â–†â–†â–†â–†â–„â–‡â–ƒâ–„â–ˆâ–„â–„â–â–„â–‚â–†â–…â–†â–„â–…â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–ƒâ–…â–†â–‡â–ƒâ–„â–„â–ƒâ–…â–†â–…â–†â–†â–†â–„â–„â–‡â–…â–†â–†â–†â–†â–†â–„â–‡â–ƒâ–„â–ˆâ–„â–ƒâ–â–„â–‚â–†â–„â–†â–ƒâ–ƒâ–†â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–†â–…â–…â–‚â–†â–†â–‡â–…â–†â–„â–ƒâ–‡â–…â–„â–…â–‡â–ƒâ–ƒâ–…â–ƒâ–„â–ƒâ–ƒâ–ˆâ–ƒâ–ˆâ–…â–‚â–ˆâ–†â–†â–†â–†â–…â–„â–ƒâ–…â–ƒâ–â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–‚â–„â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–â–‚â–‚â–ƒâ–…â–†â–†â–‡â–†â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–‚â–„â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–â–‚â–ƒâ–„â–†â–†â–†â–‡â–†â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–†â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–â–†â–ˆâ–ˆâ–‡â–‡â–„â–ƒâ–„â–ƒâ–ƒâ–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–ƒâ–…â–†â–ˆâ–…â–†â–†â–†â–†â–…â–†â–†â–†â–†â–†â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–…â–â–ƒâ–ƒâ–†â–†â–†â–†â–†â–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–‚â–…â–‡â–ˆâ–…â–‡â–‡â–‡â–‡â–…â–‡â–‡â–‡â–‡â–‡â–…â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–„â–â–‚â–ƒâ–‡â–‡â–‡â–‡â–‡â–‡â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–‡â–„â–ƒâ–ƒâ–…â–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–â–‚â–â–â–â–‚â–‚â–ƒâ–‚â–ˆâ–ˆâ–ˆâ–ˆâ–†â–…â–ƒâ–„â–ƒâ–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/28qzo7z2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:01:15.522870: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_loss_weighted_binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2ve67el4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010114-2ve67el4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:01<00:00, 46.46it/s, loss=0.698, v_num=7el4, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.706\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:01<00:00, 46.13it/s, loss=0.698, v_num=7el4, BTC_val_\n",
      "Epoch 1:  99%|â–‰| 80/81 [00:01<00:00, 46.00it/s, loss=0.717, v_num=7el4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 0.694\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:01<00:00, 45.55it/s, loss=0.717, v_num=7el4, BTC_val_\n",
      "Epoch 2:  99%|â–‰| 80/81 [00:01<00:00, 44.63it/s, loss=0.695, v_num=7el4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:01<00:00, 43.99it/s, loss=0.695, v_num=7el4, BTC_val_\u001b[A\n",
      "Epoch 3:  99%|â–‰| 80/81 [00:02<00:00, 39.70it/s, loss=0.701, v_num=7el4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:02<00:00, 39.38it/s, loss=0.701, v_num=7el4, BTC_val_\u001b[A\n",
      "Epoch 4:  99%|â–‰| 80/81 [00:01<00:00, 44.75it/s, loss=0.694, v_num=7el4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:01<00:00, 44.28it/s, loss=0.694, v_num=7el4, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 80/81 [00:01<00:00, 45.19it/s, loss=0.693, v_num=7el4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:01<00:00, 44.78it/s, loss=0.693, v_num=7el4, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 80/81 [00:02<00:00, 38.24it/s, loss=0.697, v_num=7el4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:02<00:00, 38.01it/s, loss=0.697, v_num=7el4, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 80/81 [00:01<00:00, 44.90it/s, loss=0.694, v_num=7el4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:01<00:00, 44.48it/s, loss=0.694, v_num=7el4, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 80/81 [00:01<00:00, 44.86it/s, loss=0.697, v_num=7el4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:01<00:00, 44.46it/s, loss=0.697, v_num=7el4, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 80/81 [00:01<00:00, 44.94it/s, loss=0.697, v_num=7el4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:01<00:00, 44.37it/s, loss=0.697, v_num=7el4, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 80/81 [00:02<00:00, 34.44it/s, loss=0.694, v_num=7el4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:02<00:00, 34.29it/s, loss=0.694, v_num=7el4, BTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 80/81 [00:02<00:00, 33.66it/s, loss=0.69, v_num=7el4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:02<00:00, 33.48it/s, loss=0.69, v_num=7el4, BTC_val_\u001b[A\n",
      "Epoch 12:  99%|â–‰| 80/81 [00:01<00:00, 44.04it/s, loss=0.697, v_num=7el4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:01<00:00, 43.61it/s, loss=0.697, v_num=7el4, BTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 80/81 [00:01<00:00, 42.64it/s, loss=0.694, v_num=7el4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:01<00:00, 42.23it/s, loss=0.694, v_num=7el4, BTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 80/81 [00:01<00:00, 44.79it/s, loss=0.695, v_num=7el4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:01<00:00, 44.22it/s, loss=0.695, v_num=7el4, BTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 80/81 [00:01<00:00, 42.49it/s, loss=0.696, v_num=7el4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:01<00:00, 40.76it/s, loss=0.696, v_num=7el4, BTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 80/81 [00:02<00:00, 38.18it/s, loss=0.695, v_num=7el4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:02<00:00, 37.89it/s, loss=0.695, v_num=7el4, BTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 80/81 [00:02<00:00, 34.43it/s, loss=0.696, v_num=7el4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:02<00:00, 34.26it/s, loss=0.696, v_num=7el4, BTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 80/81 [00:02<00:00, 32.51it/s, loss=0.696, v_num=7el4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:02<00:00, 32.39it/s, loss=0.696, v_num=7el4, BTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 80/81 [00:01<00:00, 44.62it/s, loss=0.695, v_num=7el4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:01<00:00, 44.09it/s, loss=0.695, v_num=7el4, BTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 80/81 [00:01<00:00, 44.38it/s, loss=0.693, v_num=7el4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:01<00:00, 43.90it/s, loss=0.693, v_num=7el4, BTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 80/81 [00:02<00:00, 37.87it/s, loss=0.692, v_num=7el4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:02<00:00, 37.53it/s, loss=0.692, v_num=7el4, BTC_val\u001b[A\n",
      "                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.694. Signaling Trainer to stop.\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:02<00:00, 37.48it/s, loss=0.692, v_num=7el4, BTC_val\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 95.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5806451439857483,\n",
      " 'BTC_test_f1': 0.36725807189941406,\n",
      " 'test_loss': 0.6858224272727966}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 63928\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010114-2ve67el4/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010114-2ve67el4/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.68493\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1760\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621461726\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 79\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.48857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.46133\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69365\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.30769\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.69426\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.58065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.36726\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.68582\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–…â–‡â–„â–…â–„â–…â–†â–‡â–†â–‡â–…â–ˆâ–†â–†â–†â–â–„â–‡â–…â–…â–…â–†â–„â–…â–†â–†â–…â–†â–‡â–‡â–ƒâ–‡â–†â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–„â–‡â–„â–…â–„â–…â–†â–‡â–†â–‡â–…â–ˆâ–…â–†â–†â–â–ƒâ–‡â–…â–…â–…â–…â–„â–„â–†â–„â–…â–†â–‡â–‡â–ƒâ–…â–†â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–‡â–‚â–ˆâ–„â–…â–„â–ƒâ–â–„â–ƒâ–…â–ƒâ–ƒâ–„â–„â–„â–„â–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–„â–„â–„â–ƒâ–„â–„â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–‡â–‚â–ƒâ–ƒâ–„â–…â–â–„â–†â–ƒâ–ƒâ–„â–ƒâ–â–„â–ƒâ–ƒâ–â–ƒâ–†â–ˆâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–ˆâ–ƒâ–ƒâ–ƒâ–…â–†â–„â–…â–„â–…â–â–†â–…â–â–†â–„â–„â–ƒâ–…â–‡â–ˆâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–„â–ˆâ–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–â–‚â–‚â–â–â–â–‚â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–â–„â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2ve67el4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:02:21.455998: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1i8md9yy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010220-1i8md9yy\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Training: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:01<00:00, 41.46it/s, loss=1.13, v_num=d9yy, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.081\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:01<00:00, 41.17it/s, loss=1.13, v_num=d9yy, BTC_val_a\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 44.67it/s, loss=1.11, v_num=d9yy, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 44.39it/s, loss=1.11, v_num=d9yy, BTC_val_a\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 45.77it/s, loss=1.11, v_num=d9yy, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 45.47it/s, loss=1.11, v_num=d9yy, BTC_val_a\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:01<00:00, 45.92it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:01<00:00, 45.62it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 46.28it/s, loss=1.09, v_num=d9yy, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 46.00it/s, loss=1.09, v_num=d9yy, BTC_val_a\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:02<00:00, 33.79it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:02<00:00, 33.61it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:02<00:00, 38.02it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:02<00:00, 37.82it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:02<00:00, 36.93it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:02<00:00, 36.30it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:01<00:00, 43.09it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:01<00:00, 42.83it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:01<00:00, 44.67it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:01<00:00, 44.38it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:01<00:00, 44.10it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:01<00:00, 43.78it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:02<00:00, 38.89it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:02<00:00, 38.67it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:02<00:00, 38.67it/s, loss=1.11, v_num=d9yy, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:02<00:00, 38.44it/s, loss=1.11, v_num=d9yy, BTC_val_\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:02<00:00, 34.10it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:02<00:00, 33.93it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:02<00:00, 34.64it/s, loss=1.09, v_num=d9yy, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:02<00:00, 34.47it/s, loss=1.09, v_num=d9yy, BTC_val_\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:02<00:00, 34.10it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:02<00:00, 33.94it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:02<00:00, 39.78it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:02<00:00, 39.55it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:02<00:00, 35.69it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:02<00:00, 35.48it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:02<00:00, 34.52it/s, loss=1.11, v_num=d9yy, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:02<00:00, 34.35it/s, loss=1.11, v_num=d9yy, BTC_val_\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:02<00:00, 38.64it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:02<00:00, 38.41it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:01<00:00, 43.81it/s, loss=1.09, v_num=d9yy, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.081. Signaling Trainer to stop.\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:01<00:00, 43.55it/s, loss=1.09, v_num=d9yy, BTC_val_\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:01<00:00, 43.47it/s, loss=1.09, v_num=d9yy, BTC_val_\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 93.99it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.35483869910240173,\n",
      " 'BTC_test_f1': 0.17448680102825165,\n",
      " 'test_loss': 1.0824896097183228}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 64134\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010220-1i8md9yy/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010220-1i8md9yy/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.39057\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.07944\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1659\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 51\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621461791\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.43785\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.30441\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09759\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.09321\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.35484\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.17449\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.08249\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–…â–ƒâ–ƒâ–„â–‚â–†â–ƒâ–…â–ˆâ–ƒâ–ƒâ–†â–‚â–…â–†â–ˆâ–ƒâ–„â–ƒâ–‚â–ƒâ–â–†â–‡â–ƒâ–…â–‚â–„â–ˆâ–„â–‡â–…â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–†â–„â–ƒâ–ƒâ–‚â–‡â–ƒâ–„â–†â–ƒâ–ƒâ–…â–ƒâ–…â–‡â–ˆâ–„â–ƒâ–ƒâ–‚â–ƒâ–â–…â–†â–ƒâ–…â–ƒâ–…â–…â–‚â–†â–„â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–…â–‡â–…â–â–ˆâ–ƒâ–ˆâ–â–‚â–„â–ƒâ–…â–„â–…â–ƒâ–…â–…â–…â–…â–†â–„â–†â–ƒâ–ƒâ–‡â–†â–‡â–…â–ƒâ–…â–„â–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–ƒâ–‚â–„â–‚â–‚â–„â–„â–…â–â–ƒâ–ƒâ–†â–ƒâ–â–…â–„â–‡â–…â–„â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–†â–‡â–ˆâ–†â–„â–‡â–ˆâ–ˆâ–…â–…â–†â–‡â–†â–…â–…â–ƒâ–ƒâ–‡â–â–…â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–…â–„â–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–ƒâ–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–â–…â–‡â–„â–…â–„â–†â–„â–„â–‡â–†â–…â–ˆâ–†â–†â–†â–†â–†â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1i8md9yy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:03:27.328989: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_loss_weighted_multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/ubu3zph0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010325-ubu3zph0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|â–‰| 80/81 [00:01<00:00, 41.94it/s, loss=1.13, v_num=zph0, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.155\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:01<00:00, 41.50it/s, loss=1.13, v_num=zph0, BTC_val_a\n",
      "Epoch 1:  99%|â–‰| 80/81 [00:01<00:00, 45.16it/s, loss=1.11, v_num=zph0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.034 >= min_delta = 0.003. New best score: 1.121\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:01<00:00, 44.75it/s, loss=1.11, v_num=zph0, BTC_val_a\n",
      "Epoch 2:  99%|â–‰| 80/81 [00:01<00:00, 44.53it/s, loss=1.08, v_num=zph0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.020 >= min_delta = 0.003. New best score: 1.101\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:01<00:00, 43.97it/s, loss=1.08, v_num=zph0, BTC_val_a\n",
      "Epoch 3:  99%|â–‰| 80/81 [00:01<00:00, 43.05it/s, loss=1.11, v_num=zph0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:01<00:00, 42.39it/s, loss=1.11, v_num=zph0, BTC_val_a\u001b[A\n",
      "Epoch 4:  99%|â–‰| 80/81 [00:02<00:00, 32.60it/s, loss=1.1, v_num=zph0, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:02<00:00, 32.41it/s, loss=1.1, v_num=zph0, BTC_val_ac\u001b[A\n",
      "Epoch 5:  99%|â–‰| 80/81 [00:02<00:00, 39.01it/s, loss=1.09, v_num=zph0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:02<00:00, 38.73it/s, loss=1.09, v_num=zph0, BTC_val_a\u001b[A\n",
      "Epoch 6:  99%|â–‰| 80/81 [00:02<00:00, 37.63it/s, loss=1.09, v_num=zph0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:02<00:00, 37.39it/s, loss=1.09, v_num=zph0, BTC_val_a\u001b[A\n",
      "Epoch 7:  99%|â–‰| 80/81 [00:02<00:00, 33.96it/s, loss=1.11, v_num=zph0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:02<00:00, 33.80it/s, loss=1.11, v_num=zph0, BTC_val_a\u001b[A\n",
      "Epoch 8:  99%|â–‰| 80/81 [00:01<00:00, 41.59it/s, loss=1.11, v_num=zph0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:01<00:00, 41.28it/s, loss=1.11, v_num=zph0, BTC_val_a\u001b[A\n",
      "Epoch 9:  99%|â–‰| 80/81 [00:02<00:00, 38.13it/s, loss=1.11, v_num=zph0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:02<00:00, 37.81it/s, loss=1.11, v_num=zph0, BTC_val_a\u001b[A\n",
      "Epoch 10:  99%|â–‰| 80/81 [00:02<00:00, 28.36it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:02<00:00, 28.31it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n",
      "Epoch 11:  99%|â–‰| 80/81 [00:01<00:00, 45.42it/s, loss=1.11, v_num=zph0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:01<00:00, 44.82it/s, loss=1.11, v_num=zph0, BTC_val_\u001b[A\n",
      "Epoch 12:  99%|â–‰| 80/81 [00:01<00:00, 43.91it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:01<00:00, 43.49it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n",
      "Epoch 13:  99%|â–‰| 80/81 [00:01<00:00, 44.07it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:01<00:00, 43.66it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n",
      "Epoch 14:  99%|â–‰| 80/81 [00:02<00:00, 33.13it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:02<00:00, 32.58it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n",
      "Epoch 15:  99%|â–‰| 80/81 [00:02<00:00, 32.04it/s, loss=1.09, v_num=zph0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:02<00:00, 31.24it/s, loss=1.09, v_num=zph0, BTC_val_\u001b[A\n",
      "Epoch 16:  99%|â–‰| 80/81 [00:02<00:00, 39.35it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:02<00:00, 39.03it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n",
      "Epoch 17:  99%|â–‰| 80/81 [00:01<00:00, 45.41it/s, loss=1.12, v_num=zph0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:01<00:00, 44.96it/s, loss=1.12, v_num=zph0, BTC_val_\u001b[A\n",
      "Epoch 18:  99%|â–‰| 80/81 [00:01<00:00, 45.16it/s, loss=1.08, v_num=zph0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:01<00:00, 44.69it/s, loss=1.08, v_num=zph0, BTC_val_\u001b[A\n",
      "Epoch 19:  99%|â–‰| 80/81 [00:01<00:00, 45.80it/s, loss=1.08, v_num=zph0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:01<00:00, 45.21it/s, loss=1.08, v_num=zph0, BTC_val_\u001b[A\n",
      "Epoch 20:  99%|â–‰| 80/81 [00:02<00:00, 31.81it/s, loss=1.09, v_num=zph0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:02<00:00, 31.71it/s, loss=1.09, v_num=zph0, BTC_val_\u001b[A\n",
      "Epoch 21:  99%|â–‰| 80/81 [00:02<00:00, 34.03it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:02<00:00, 33.88it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n",
      "Epoch 22:  99%|â–‰| 80/81 [00:02<00:00, 33.77it/s, loss=1.11, v_num=zph0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.101. Signaling Trainer to stop.\n",
      "Epoch 22: 100%|â–ˆ| 81/81 [00:02<00:00, 33.64it/s, loss=1.11, v_num=zph0, BTC_val_\n",
      "Epoch 22: 100%|â–ˆ| 81/81 [00:02<00:00, 33.60it/s, loss=1.11, v_num=zph0, BTC_val_\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 88.19it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.25806450843811035,\n",
      " 'BTC_test_f1': 0.1367289274930954,\n",
      " 'test_loss': 1.135680913925171}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 64340\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010325-ubu3zph0/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010325-ubu3zph0/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.43275\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.0438\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 22\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1840\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 58\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621461863\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 82\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.35225\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.32388\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.10167\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.22222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.12121\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.11175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.25806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.13673\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.13568\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–„â–…â–„â–…â–„â–‚â–„â–…â–„â–„â–ƒâ–ƒâ–…â–…â–„â–â–„â–…â–‚â–„â–…â–„â–…â–ƒâ–ƒâ–…â–…â–ƒâ–ƒâ–…â–„â–ˆâ–ƒâ–„â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–„â–„â–ƒâ–„â–„â–‚â–„â–ƒâ–„â–ƒâ–ƒâ–‚â–…â–„â–„â–â–„â–…â–‚â–ƒâ–…â–ƒâ–„â–‚â–ƒâ–„â–…â–ƒâ–‚â–„â–ƒâ–ˆâ–‚â–„â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–†â–…â–†â–…â–ˆâ–‡â–…â–†â–†â–†â–…â–„â–…â–…â–…â–‡â–…â–ƒâ–‡â–…â–„â–…â–„â–ƒâ–…â–„â–„â–…â–…â–„â–„â–â–†â–†â–‚â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–ƒâ–ƒâ–‡â–…â–‡â–†â–†â–‡â–†â–„â–†â–‡â–†â–†â–‡â–†â–‡â–â–‡â–ˆâ–…â–ˆâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–ƒâ–‚â–„â–ƒâ–†â–…â–‚â–†â–„â–„â–…â–„â–…â–…â–„â–†â–…â–ƒâ–†â–ˆâ–â–‡â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–‡â–ƒâ–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–â–‚â–ƒâ–â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–„â–â–â–ƒâ–â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–â–ƒâ–„â–„â–„â–…â–ƒâ–„â–†â–†â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/ubu3zph0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:04:39.216279: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_single_lstm__binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3edrij0v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010437-3edrij0v\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 130   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.7 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 79/80 [00:01<00:00, 77.47it/s, loss=0.527, v_num=ij0v, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.505\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:01<00:00, 70.47it/s, loss=0.527, v_num=ij0v, BTC_val_\n",
      "Epoch 1:  99%|â–‰| 79/80 [00:01<00:00, 78.11it/s, loss=0.6, v_num=ij0v, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 75.79it/s, loss=0.6, v_num=ij0v, BTC_val_ac\u001b[A\n",
      "Epoch 2:  99%|â–‰| 79/80 [00:00<00:00, 83.99it/s, loss=0.573, v_num=ij0v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:00<00:00, 82.07it/s, loss=0.573, v_num=ij0v, BTC_val_\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:00<00:00, 87.57it/s, loss=0.6, v_num=ij0v, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:00<00:00, 86.81it/s, loss=0.6, v_num=ij0v, BTC_val_ac\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:00<00:00, 90.43it/s, loss=0.588, v_num=ij0v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:00<00:00, 89.56it/s, loss=0.588, v_num=ij0v, BTC_val_\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:00<00:00, 88.50it/s, loss=0.622, v_num=ij0v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:00<00:00, 87.74it/s, loss=0.622, v_num=ij0v, BTC_val_\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:00<00:00, 89.26it/s, loss=0.629, v_num=ij0v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:00<00:00, 88.49it/s, loss=0.629, v_num=ij0v, BTC_val_\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:00<00:00, 89.64it/s, loss=0.582, v_num=ij0v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:00<00:00, 88.81it/s, loss=0.582, v_num=ij0v, BTC_val_\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:00<00:00, 87.96it/s, loss=0.572, v_num=ij0v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:00<00:00, 87.15it/s, loss=0.572, v_num=ij0v, BTC_val_\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:01<00:00, 78.80it/s, loss=0.592, v_num=ij0v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:01<00:00, 78.07it/s, loss=0.592, v_num=ij0v, BTC_val_\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:01<00:00, 70.60it/s, loss=0.525, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:01<00:00, 69.75it/s, loss=0.525, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:01<00:00, 78.54it/s, loss=0.582, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:01<00:00, 77.74it/s, loss=0.582, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:01<00:00, 75.02it/s, loss=0.615, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:01<00:00, 74.34it/s, loss=0.615, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:00<00:00, 83.84it/s, loss=0.578, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:00<00:00, 83.08it/s, loss=0.578, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:00<00:00, 90.43it/s, loss=0.626, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:00<00:00, 89.64it/s, loss=0.626, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:00<00:00, 88.69it/s, loss=0.546, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:00<00:00, 87.85it/s, loss=0.546, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:00<00:00, 91.32it/s, loss=0.564, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:00<00:00, 90.46it/s, loss=0.564, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:00<00:00, 88.97it/s, loss=0.566, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:00<00:00, 88.18it/s, loss=0.566, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:00<00:00, 88.99it/s, loss=0.586, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.501\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:00<00:00, 88.18it/s, loss=0.586, v_num=ij0v, BTC_val\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:00<00:00, 83.94it/s, loss=0.579, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:00<00:00, 83.06it/s, loss=0.579, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:01<00:00, 75.09it/s, loss=0.543, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:01<00:00, 74.48it/s, loss=0.543, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:01<00:00, 74.36it/s, loss=0.579, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:01<00:00, 73.73it/s, loss=0.579, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:01<00:00, 66.50it/s, loss=0.585, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:01<00:00, 65.89it/s, loss=0.585, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:00<00:00, 81.85it/s, loss=0.574, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:00<00:00, 81.13it/s, loss=0.574, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:01<00:00, 78.09it/s, loss=0.584, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:01<00:00, 77.44it/s, loss=0.584, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:01<00:00, 79.33it/s, loss=0.608, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:01<00:00, 78.57it/s, loss=0.608, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:00<00:00, 89.78it/s, loss=0.601, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:00<00:00, 88.97it/s, loss=0.601, v_num=ij0v, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|â–ˆ| 80/80 [00:00<00:00, 88.61it/s, loss=0.574, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 80/80 [00:00<00:00, 87.77it/s, loss=0.574, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:00<00:00, 88.22it/s, loss=0.561, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:00<00:00, 87.34it/s, loss=0.561, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:00<00:00, 86.90it/s, loss=0.588, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:00<00:00, 86.05it/s, loss=0.588, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 80/80 [00:00<00:00, 80.19it/s, loss=0.568, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 80/80 [00:01<00:00, 79.28it/s, loss=0.568, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 80/80 [00:00<00:00, 87.90it/s, loss=0.587, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 80/80 [00:00<00:00, 87.09it/s, loss=0.587, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 80/80 [00:00<00:00, 88.21it/s, loss=0.577, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 80/80 [00:00<00:00, 87.44it/s, loss=0.577, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 80/80 [00:00<00:00, 88.24it/s, loss=0.576, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 80/80 [00:00<00:00, 86.98it/s, loss=0.576, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 80/80 [00:00<00:00, 88.06it/s, loss=0.534, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 80/80 [00:00<00:00, 87.18it/s, loss=0.534, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 80/80 [00:00<00:00, 87.19it/s, loss=0.587, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 80/80 [00:00<00:00, 86.46it/s, loss=0.587, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 80/80 [00:00<00:00, 88.16it/s, loss=0.572, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 80/80 [00:00<00:00, 87.29it/s, loss=0.572, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 37: 100%|â–ˆ| 80/80 [00:00<00:00, 87.13it/s, loss=0.546, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|â–ˆ| 80/80 [00:00<00:00, 86.32it/s, loss=0.546, v_num=ij0v, BTC_val\u001b[A\n",
      "Epoch 38: 100%|â–ˆ| 80/80 [00:01<00:00, 70.08it/s, loss=0.548, v_num=ij0v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.501. Signaling Trainer to stop.\n",
      "Epoch 38: 100%|â–ˆ| 80/80 [00:01<00:00, 69.31it/s, loss=0.548, v_num=ij0v, BTC_val\n",
      "Epoch 38: 100%|â–ˆ| 80/80 [00:01<00:00, 69.12it/s, loss=0.548, v_num=ij0v, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 106.29it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.7096773982048035,\n",
      " 'BTC_test_f1': 0.7012333869934082,\n",
      " 'test_loss': 0.6024167537689209}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 64543\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010437-3edrij0v/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010437-3edrij0v/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.56078\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.78639\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 38\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3081\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 47\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621461924\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 139\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.71496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.69673\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.56223\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.77778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.51207\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.70968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.70123\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.60242\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–„â–‡â–‡â–…â–…â–†â–â–†â–ˆâ–…â–„â–…â–†â–ƒâ–‡â–‡â–…â–ˆâ–‚â–…â–‡â–ˆâ–ˆâ–…â–†â–ˆâ–†â–ƒâ–ˆâ–‡â–‡â–…â–„â–ƒâ–…â–†â–…â–†â–‡â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–„â–‡â–‡â–†â–…â–†â–â–‡â–ˆâ–„â–„â–…â–‡â–ƒâ–‡â–‡â–†â–ˆâ–‚â–…â–‡â–ˆâ–ˆâ–…â–‡â–ˆâ–‡â–ƒâ–‡â–‡â–‡â–…â–„â–„â–†â–†â–…â–†â–‡â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–„â–‚â–ƒâ–…â–ƒâ–ƒâ–ˆâ–‚â–‚â–„â–†â–ƒâ–ƒâ–†â–„â–‚â–„â–ƒâ–†â–…â–‚â–â–‚â–…â–‚â–â–„â–†â–â–â–‚â–ƒâ–„â–†â–„â–ƒâ–„â–„â–ƒâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–…â–…â–…â–…â–…â–†â–†â–…â–†â–†â–‡â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–â–†â–†â–…â–†â–…â–†â–†â–†â–‡â–†â–‡â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–†â–…â–„â–„â–„â–„â–ƒâ–…â–ƒâ–„â–ƒâ–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–ˆâ–ˆâ–â–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–ˆâ–ˆâ–‚â–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‚â–‚â–ˆâ–ˆâ–ˆâ–‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–â–ƒâ–„â–‡â–ˆâ–„â–†â–†â–†â–„â–â–‚â–…â–ƒâ–…â–…â–„â–ƒâ–â–…â–ˆâ–…â–‚â–†â–…â–„â–ƒâ–…â–ƒâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–†â–â–â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_single_lstm__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3edrij0v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:05:33.523984: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_single_lstm__binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/10fa4e86\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010532-10fa4e86\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 130   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.7 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 80/81 [00:00<00:00, 85.95it/s, loss=0.696, v_num=4e86, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.704\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:00<00:00, 83.41it/s, loss=0.696, v_num=4e86, BTC_val_\n",
      "Epoch 1:  99%|â–‰| 80/81 [00:01<00:00, 79.12it/s, loss=0.695, v_num=4e86, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:01<00:00, 76.31it/s, loss=0.695, v_num=4e86, BTC_val_\u001b[A\n",
      "Epoch 2:  99%|â–‰| 80/81 [00:00<00:00, 88.42it/s, loss=0.693, v_num=4e86, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.695\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:00<00:00, 86.32it/s, loss=0.693, v_num=4e86, BTC_val_\n",
      "Epoch 3:  99%|â–‰| 80/81 [00:00<00:00, 87.65it/s, loss=0.699, v_num=4e86, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.692\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:00<00:00, 84.93it/s, loss=0.699, v_num=4e86, BTC_val_\n",
      "Epoch 4:  99%|â–‰| 80/81 [00:00<00:00, 80.24it/s, loss=0.693, v_num=4e86, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:01<00:00, 78.05it/s, loss=0.693, v_num=4e86, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 80/81 [00:01<00:00, 79.83it/s, loss=0.696, v_num=4e86, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:01<00:00, 77.52it/s, loss=0.696, v_num=4e86, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 80/81 [00:00<00:00, 83.77it/s, loss=0.691, v_num=4e86, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:00<00:00, 81.46it/s, loss=0.691, v_num=4e86, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 80/81 [00:01<00:00, 71.08it/s, loss=0.7, v_num=4e86, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:01<00:00, 69.46it/s, loss=0.7, v_num=4e86, BTC_val_ac\u001b[A\n",
      "Epoch 8:  99%|â–‰| 80/81 [00:00<00:00, 80.15it/s, loss=0.697, v_num=4e86, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:01<00:00, 78.22it/s, loss=0.697, v_num=4e86, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 80/81 [00:00<00:00, 86.91it/s, loss=0.689, v_num=4e86, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:00<00:00, 84.75it/s, loss=0.689, v_num=4e86, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 80/81 [00:00<00:00, 92.72it/s, loss=0.683, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:00<00:00, 90.04it/s, loss=0.683, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 80/81 [00:00<00:00, 92.41it/s, loss=0.687, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:00<00:00, 89.87it/s, loss=0.687, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 80/81 [00:00<00:00, 92.38it/s, loss=0.696, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:00<00:00, 89.79it/s, loss=0.696, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 80/81 [00:00<00:00, 91.42it/s, loss=0.686, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:00<00:00, 89.14it/s, loss=0.686, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 80/81 [00:00<00:00, 92.24it/s, loss=0.695, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:00<00:00, 89.52it/s, loss=0.695, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 80/81 [00:00<00:00, 91.70it/s, loss=0.693, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:00<00:00, 88.73it/s, loss=0.693, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 80/81 [00:00<00:00, 82.38it/s, loss=0.695, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:01<00:00, 79.83it/s, loss=0.695, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 80/81 [00:01<00:00, 76.28it/s, loss=0.693, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:01<00:00, 74.35it/s, loss=0.693, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 80/81 [00:01<00:00, 73.00it/s, loss=0.69, v_num=4e86, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:01<00:00, 70.65it/s, loss=0.69, v_num=4e86, BTC_val_\u001b[A\n",
      "Epoch 19:  99%|â–‰| 80/81 [00:01<00:00, 69.18it/s, loss=0.688, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:01<00:00, 67.60it/s, loss=0.688, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 80/81 [00:01<00:00, 76.31it/s, loss=0.69, v_num=4e86, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:01<00:00, 74.46it/s, loss=0.69, v_num=4e86, BTC_val_\u001b[A\n",
      "Epoch 21:  99%|â–‰| 80/81 [00:00<00:00, 85.92it/s, loss=0.688, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:00<00:00, 83.35it/s, loss=0.688, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 80/81 [00:01<00:00, 78.76it/s, loss=0.691, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.688\n",
      "Epoch 22: 100%|â–ˆ| 81/81 [00:01<00:00, 76.71it/s, loss=0.691, v_num=4e86, BTC_val\n",
      "Epoch 23:  99%|â–‰| 80/81 [00:01<00:00, 77.47it/s, loss=0.689, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 81/81 [00:01<00:00, 74.87it/s, loss=0.689, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 24:  99%|â–‰| 80/81 [00:01<00:00, 78.58it/s, loss=0.691, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 81/81 [00:01<00:00, 76.13it/s, loss=0.691, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 25:  99%|â–‰| 80/81 [00:00<00:00, 81.11it/s, loss=0.693, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 81/81 [00:01<00:00, 79.31it/s, loss=0.693, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 26:  99%|â–‰| 80/81 [00:00<00:00, 90.84it/s, loss=0.687, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 81/81 [00:00<00:00, 88.24it/s, loss=0.687, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 27:  99%|â–‰| 80/81 [00:00<00:00, 90.02it/s, loss=0.692, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 81/81 [00:00<00:00, 87.74it/s, loss=0.692, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 28:  99%|â–‰| 80/81 [00:00<00:00, 90.80it/s, loss=0.689, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 81/81 [00:00<00:00, 88.17it/s, loss=0.689, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 29:  99%|â–‰| 80/81 [00:00<00:00, 83.80it/s, loss=0.693, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 81/81 [00:00<00:00, 81.07it/s, loss=0.693, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 30:  99%|â–‰| 80/81 [00:01<00:00, 58.45it/s, loss=0.695, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 81/81 [00:01<00:00, 57.67it/s, loss=0.695, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 31:  99%|â–‰| 80/81 [00:01<00:00, 74.02it/s, loss=0.686, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 81/81 [00:01<00:00, 72.33it/s, loss=0.686, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 32:  99%|â–‰| 80/81 [00:00<00:00, 84.73it/s, loss=0.687, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 81/81 [00:00<00:00, 82.70it/s, loss=0.687, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 33:  99%|â–‰| 80/81 [00:00<00:00, 82.46it/s, loss=0.693, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 81/81 [00:01<00:00, 80.61it/s, loss=0.693, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 34:  99%|â–‰| 80/81 [00:00<00:00, 91.49it/s, loss=0.689, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 81/81 [00:00<00:00, 89.07it/s, loss=0.689, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 35:  99%|â–‰| 80/81 [00:00<00:00, 89.36it/s, loss=0.692, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 81/81 [00:00<00:00, 86.92it/s, loss=0.692, v_num=4e86, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36:  99%|â–‰| 80/81 [00:00<00:00, 90.62it/s, loss=0.696, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 81/81 [00:00<00:00, 88.01it/s, loss=0.696, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 37:  99%|â–‰| 80/81 [00:00<00:00, 88.52it/s, loss=0.687, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|â–ˆ| 81/81 [00:00<00:00, 86.11it/s, loss=0.687, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 38:  99%|â–‰| 80/81 [00:00<00:00, 91.19it/s, loss=0.694, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|â–ˆ| 81/81 [00:00<00:00, 88.75it/s, loss=0.694, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 39:  99%|â–‰| 80/81 [00:00<00:00, 90.08it/s, loss=0.686, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|â–ˆ| 81/81 [00:00<00:00, 87.51it/s, loss=0.686, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 40:  99%|â–‰| 80/81 [00:00<00:00, 83.32it/s, loss=0.691, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|â–ˆ| 81/81 [00:01<00:00, 80.83it/s, loss=0.691, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 41:  99%|â–‰| 80/81 [00:01<00:00, 72.15it/s, loss=0.696, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|â–ˆ| 81/81 [00:01<00:00, 70.40it/s, loss=0.696, v_num=4e86, BTC_val\u001b[A\n",
      "Epoch 42:  99%|â–‰| 80/81 [00:01<00:00, 77.08it/s, loss=0.688, v_num=4e86, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.688. Signaling Trainer to stop.\n",
      "Epoch 42: 100%|â–ˆ| 81/81 [00:01<00:00, 75.27it/s, loss=0.688, v_num=4e86, BTC_val\n",
      "Epoch 42: 100%|â–ˆ| 81/81 [00:01<00:00, 75.03it/s, loss=0.688, v_num=4e86, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 136.33it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.4193548262119293,\n",
      " 'BTC_test_f1': 0.29533159732818604,\n",
      " 'test_loss': 0.6970884799957275}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 64852\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010532-10fa4e86/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010532-10fa4e86/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.4375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.30435\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.71306\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3440\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621461984\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.53822\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.34643\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.68955\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.35714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.69086\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.41935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.29533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.69709\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–ƒâ–…â–ƒâ–‚â–ƒâ–‚â–„â–„â–…â–ˆâ–â–ˆâ–‡â–ƒâ–ƒâ–†â–…â–†â–ƒâ–‚â–ˆâ–ƒâ–…â–…â–†â–ˆâ–„â–ƒâ–†â–…â–ƒâ–†â–ˆâ–„â–…â–ƒâ–‡â–…â–„â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–„â–†â–„â–ƒâ–„â–ƒâ–„â–ƒâ–„â–…â–â–ˆâ–…â–‚â–‚â–„â–…â–‡â–ƒâ–‚â–…â–ƒâ–„â–„â–„â–…â–ƒâ–ƒâ–„â–„â–‚â–„â–…â–ƒâ–„â–ƒâ–…â–„â–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–‡â–†â–…â–ˆâ–ˆâ–†â–…â–…â–„â–â–†â–ƒâ–ƒâ–†â–†â–„â–„â–…â–…â–†â–ƒâ–…â–…â–…â–„â–ƒâ–…â–…â–„â–…â–†â–„â–ƒâ–…â–…â–…â–ƒâ–ƒâ–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–„â–„â–ƒâ–ƒâ–ˆâ–â–…â–…â–‡â–†â–‡â–‡â–ˆâ–‡â–‡â–‡â–†â–‡â–‡â–ˆâ–‡â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–ˆâ–ˆâ–†â–ˆâ–…â–…â–„â–ƒâ–ƒâ–†â–„â–†â–…â–â–â–‚â–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–ˆâ–„â–…â–„â–„â–ƒâ–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–‚â–â–â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–ˆâ–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–‡â–ˆâ–„â–‚â–ƒâ–…â–…â–ƒâ–„â–ƒâ–…â–†â–„â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_single_lstm__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/10fa4e86\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:06:34.207615: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_single_lstm__multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2uzkcaq3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010632-2uzkcaq3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 195   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.8 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 79/80 [00:00<00:00, 85.90it/s, loss=1.03, v_num=caq3, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.004\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:01<00:00, 78.24it/s, loss=1.03, v_num=caq3, BTC_val_a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 79.51it/s, loss=1.01, v_num=caq3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 78.81it/s, loss=1.01, v_num=caq3, BTC_val_a\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.991\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:00<00:00, 89.34it/s, loss=1.01, v_num=caq3, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:00<00:00, 88.45it/s, loss=1.01, v_num=caq3, BTC_val_a\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:01<00:00, 65.37it/s, loss=1.02, v_num=caq3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:01<00:00, 64.85it/s, loss=1.02, v_num=caq3, BTC_val_a\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 74.51it/s, loss=0.983, v_num=caq3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.033 >= min_delta = 0.003. New best score: 0.958\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 73.94it/s, loss=0.983, v_num=caq3, BTC_val_\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:00<00:00, 80.84it/s, loss=0.983, v_num=caq3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:00<00:00, 80.13it/s, loss=0.983, v_num=caq3, BTC_val_\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:01<00:00, 75.02it/s, loss=0.957, v_num=caq3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:01<00:00, 74.40it/s, loss=0.957, v_num=caq3, BTC_val_\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:01<00:00, 76.80it/s, loss=0.977, v_num=caq3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:01<00:00, 76.22it/s, loss=0.977, v_num=caq3, BTC_val_\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:01<00:00, 73.65it/s, loss=0.979, v_num=caq3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:01<00:00, 73.08it/s, loss=0.979, v_num=caq3, BTC_val_\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:01<00:00, 74.30it/s, loss=0.928, v_num=caq3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:01<00:00, 73.73it/s, loss=0.928, v_num=caq3, BTC_val_\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:01<00:00, 74.87it/s, loss=0.916, v_num=caq3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:01<00:00, 74.33it/s, loss=0.916, v_num=caq3, BTC_val\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:00<00:00, 88.30it/s, loss=0.968, v_num=caq3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:00<00:00, 87.48it/s, loss=0.968, v_num=caq3, BTC_val\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:00<00:00, 89.85it/s, loss=0.924, v_num=caq3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:00<00:00, 89.02it/s, loss=0.924, v_num=caq3, BTC_val\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:00<00:00, 84.82it/s, loss=0.974, v_num=caq3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:00<00:00, 83.86it/s, loss=0.974, v_num=caq3, BTC_val\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:00<00:00, 82.61it/s, loss=0.984, v_num=caq3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:00<00:00, 81.55it/s, loss=0.984, v_num=caq3, BTC_val\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:01<00:00, 71.28it/s, loss=0.944, v_num=caq3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:01<00:00, 70.76it/s, loss=0.944, v_num=caq3, BTC_val\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:01<00:00, 74.36it/s, loss=0.91, v_num=caq3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:01<00:00, 73.47it/s, loss=0.91, v_num=caq3, BTC_val_\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:00<00:00, 83.81it/s, loss=0.947, v_num=caq3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:00<00:00, 83.11it/s, loss=0.947, v_num=caq3, BTC_val\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:00<00:00, 89.35it/s, loss=0.959, v_num=caq3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:00<00:00, 88.55it/s, loss=0.959, v_num=caq3, BTC_val\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:00<00:00, 87.22it/s, loss=0.886, v_num=caq3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:00<00:00, 86.46it/s, loss=0.886, v_num=caq3, BTC_val\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:00<00:00, 90.41it/s, loss=0.895, v_num=caq3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:00<00:00, 89.57it/s, loss=0.895, v_num=caq3, BTC_val\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:00<00:00, 89.13it/s, loss=0.892, v_num=caq3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:00<00:00, 88.31it/s, loss=0.892, v_num=caq3, BTC_val\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:00<00:00, 90.13it/s, loss=0.944, v_num=caq3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:00<00:00, 89.35it/s, loss=0.944, v_num=caq3, BTC_val\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:00<00:00, 88.78it/s, loss=0.909, v_num=caq3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:00<00:00, 87.99it/s, loss=0.909, v_num=caq3, BTC_val\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:00<00:00, 88.99it/s, loss=0.952, v_num=caq3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:00<00:00, 88.15it/s, loss=0.952, v_num=caq3, BTC_val\u001b[A\n",
      "                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.958. Signaling Trainer to stop.\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:00<00:00, 87.86it/s, loss=0.952, v_num=caq3, BTC_val\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 141.36it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5483871102333069,\n",
      " 'BTC_test_f1': 0.23566308617591858,\n",
      " 'test_loss': 0.9320668578147888}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 65034\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010632-2uzkcaq3/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010632-2uzkcaq3/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.31746\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.01229\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 34\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462026\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 89\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.55424\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.40886\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.90553\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.04314\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.54839\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.23566\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.93207\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–ƒâ–ƒâ–†â–„â–„â–…â–ƒâ–ƒâ–„â–†â–‚â–„â–…â–ƒâ–…â–…â–…â–…â–„â–„â–„â–ƒâ–…â–†â–ƒâ–â–ƒâ–ƒâ–„â–‡â–ƒâ–ƒâ–ˆâ–ƒâ–ƒâ–…â–ƒâ–‡â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–„â–ƒâ–‚â–„â–ƒâ–„â–ƒâ–‚â–…â–‚â–„â–„â–‚â–â–‚â–ƒâ–„â–…â–„â–ƒâ–ˆâ–„â–ƒâ–„â–„â–…â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–‡â–‡â–„â–…â–†â–†â–†â–†â–†â–…â–…â–„â–„â–†â–…â–…â–‚â–„â–ƒâ–„â–„â–ˆâ–‚â–‚â–…â–…â–…â–†â–„â–â–‡â–†â–â–…â–ˆâ–…â–‡â–â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–‡â–†â–†â–†â–†â–‡â–†â–†â–†â–‡â–†â–ˆâ–…â–ˆâ–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–ƒâ–‚â–‚â–â–‚â–‚â–ƒâ–„â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–†â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–‡â–‡â–†â–†â–…â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–‚â–â–‚â–â–‚â–‚â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–„â–„â–„â–„â–„â–„â–„â–„â–â–„â–„â–ˆâ–„â–„â–ˆâ–„â–ˆâ–„â–„â–„â–â–ˆâ–„â–ˆâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–ƒâ–ˆâ–ƒâ–ƒâ–ˆâ–ƒâ–ˆâ–‚â–‡â–†â–‚â–ˆâ–†â–ˆâ–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ƒâ–‚â–ƒâ–ƒâ–â–ƒâ–‚â–„â–„â–†â–‡â–„â–‡â–…â–ƒâ–‡â–…â–‡â–„â–…â–ˆâ–„â–„â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_single_lstm__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2uzkcaq3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:07:15.662317: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_single_lstm__multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2md3k6a8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010714-2md3k6a8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 195   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.8 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 80/81 [00:00<00:00, 93.81it/s, loss=1.12, v_num=k6a8, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:00<00:00, 90.68it/s, loss=1.12, v_num=k6a8, BTC_val_a\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved. New best score: 1.301\n",
      "Epoch 1:  99%|â–‰| 80/81 [00:01<00:00, 78.28it/s, loss=1.1, v_num=k6a8, BTC_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.103 >= min_delta = 0.003. New best score: 1.199\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:01<00:00, 75.97it/s, loss=1.1, v_num=k6a8, BTC_val_ac\n",
      "Epoch 2:  99%|â–‰| 80/81 [00:00<00:00, 81.81it/s, loss=1.12, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 1.189\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:01<00:00, 79.44it/s, loss=1.12, v_num=k6a8, BTC_val_a\n",
      "Epoch 3:  99%|â–‰| 80/81 [00:01<00:00, 79.28it/s, loss=1.1, v_num=k6a8, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:01<00:00, 77.12it/s, loss=1.1, v_num=k6a8, BTC_val_ac\u001b[A\n",
      "Epoch 4:  99%|â–‰| 80/81 [00:01<00:00, 76.05it/s, loss=1.11, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:01<00:00, 73.91it/s, loss=1.11, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Epoch 5:  99%|â–‰| 80/81 [00:00<00:00, 84.25it/s, loss=1.09, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:00<00:00, 82.30it/s, loss=1.09, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Epoch 6:  99%|â–‰| 80/81 [00:00<00:00, 90.15it/s, loss=1.09, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:00<00:00, 87.82it/s, loss=1.09, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Epoch 7:  99%|â–‰| 80/81 [00:01<00:00, 75.81it/s, loss=1.09, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:01<00:00, 73.68it/s, loss=1.09, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Epoch 8:  99%|â–‰| 80/81 [00:01<00:00, 68.70it/s, loss=1.08, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:01<00:00, 67.31it/s, loss=1.08, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Epoch 9:  99%|â–‰| 80/81 [00:01<00:00, 77.13it/s, loss=1.09, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.033 >= min_delta = 0.003. New best score: 1.155\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:01<00:00, 75.38it/s, loss=1.09, v_num=k6a8, BTC_val_a\n",
      "Epoch 10:  99%|â–‰| 80/81 [00:00<00:00, 89.14it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:00<00:00, 86.50it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Epoch 11:  99%|â–‰| 80/81 [00:01<00:00, 79.37it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:01<00:00, 77.43it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n",
      "Epoch 12:  99%|â–‰| 80/81 [00:00<00:00, 86.04it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:00<00:00, 84.02it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n",
      "Epoch 13:  99%|â–‰| 80/81 [00:00<00:00, 91.42it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:00<00:00, 89.09it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n",
      "Epoch 14:  99%|â–‰| 80/81 [00:00<00:00, 81.07it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:01<00:00, 79.09it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Epoch 15:  99%|â–‰| 80/81 [00:00<00:00, 81.64it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 1.134\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:01<00:00, 78.87it/s, loss=1.1, v_num=k6a8, BTC_val_a\n",
      "Epoch 16:  99%|â–‰| 80/81 [00:01<00:00, 73.13it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:01<00:00, 70.64it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n",
      "Epoch 17:  99%|â–‰| 80/81 [00:01<00:00, 63.96it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:01<00:00, 62.67it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n",
      "Epoch 18:  99%|â–‰| 80/81 [00:01<00:00, 72.37it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:01<00:00, 70.68it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n",
      "Epoch 19:  99%|â–‰| 80/81 [00:00<00:00, 82.06it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:01<00:00, 80.04it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Epoch 20:  99%|â–‰| 80/81 [00:00<00:00, 82.28it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:01<00:00, 80.02it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Epoch 21:  99%|â–‰| 80/81 [00:00<00:00, 90.93it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:00<00:00, 88.35it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n",
      "Epoch 22:  99%|â–‰| 80/81 [00:00<00:00, 90.37it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 81/81 [00:00<00:00, 87.79it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n",
      "Epoch 23:  99%|â–‰| 80/81 [00:00<00:00, 89.54it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 81/81 [00:00<00:00, 86.95it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n",
      "Epoch 24:  99%|â–‰| 80/81 [00:00<00:00, 89.79it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 81/81 [00:00<00:00, 87.41it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Epoch 25:  99%|â–‰| 80/81 [00:00<00:00, 91.33it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 81/81 [00:00<00:00, 88.99it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n",
      "Epoch 26:  99%|â–‰| 80/81 [00:00<00:00, 90.44it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 81/81 [00:00<00:00, 88.00it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n",
      "Epoch 27:  99%|â–‰| 80/81 [00:00<00:00, 90.64it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 81/81 [00:00<00:00, 88.19it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n",
      "Epoch 28:  99%|â–‰| 80/81 [00:00<00:00, 86.51it/s, loss=1.11, v_num=k6a8, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 81/81 [00:00<00:00, 83.86it/s, loss=1.11, v_num=k6a8, BTC_val_\u001b[A\n",
      "Epoch 29:  99%|â–‰| 80/81 [00:01<00:00, 58.95it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 81/81 [00:01<00:00, 57.96it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Epoch 30:  99%|â–‰| 80/81 [00:00<00:00, 83.17it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 81/81 [00:00<00:00, 81.07it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n",
      "Epoch 31:  99%|â–‰| 80/81 [00:00<00:00, 88.10it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 81/81 [00:00<00:00, 85.89it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n",
      "Epoch 32:  99%|â–‰| 80/81 [00:00<00:00, 92.25it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 81/81 [00:00<00:00, 89.41it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n",
      "Epoch 33:  99%|â–‰| 80/81 [00:00<00:00, 90.20it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 81/81 [00:00<00:00, 87.62it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n",
      "Epoch 34:  99%|â–‰| 80/81 [00:00<00:00, 89.39it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 81/81 [00:00<00:00, 86.96it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n",
      "Epoch 35:  99%|â–‰| 80/81 [00:00<00:00, 89.97it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.134. Signaling Trainer to stop.\n",
      "Epoch 35: 100%|â–ˆ| 81/81 [00:00<00:00, 87.19it/s, loss=1.08, v_num=k6a8, BTC_val_\n",
      "Epoch 35: 100%|â–ˆ| 81/81 [00:00<00:00, 86.81it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 145.81it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.25806450843811035,\n",
      " 'BTC_test_f1': 0.1367289274930954,\n",
      " 'test_loss': 1.1771870851516724}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 65178\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010714-2md3k6a8/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010714-2md3k6a8/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.3125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.16667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.06331\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 35\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2880\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 45\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462079\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.37273\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.24619\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.08933\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.44169\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.25806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.13673\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.17719\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–‚â–ƒâ–„â–„â–„â–â–ƒâ–„â–†â–ƒâ–ƒâ–ˆâ–…â–„â–„â–…â–„â–„â–ƒâ–„â–ƒâ–…â–†â–†â–†â–„â–„â–ƒâ–„â–ƒâ–„â–„â–†â–…â–…â–„â–…â–„â–…â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–â–ƒâ–ƒâ–„â–„â–â–ƒâ–„â–„â–ƒâ–ƒâ–ˆâ–„â–„â–„â–„â–„â–ƒâ–ƒâ–„â–‚â–„â–…â–…â–†â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–„â–…â–ƒâ–…â–ƒâ–„â–ƒâ–„â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–…â–„â–„â–ƒâ–ƒâ–ˆâ–ƒâ–„â–‚â–„â–„â–â–‚â–ƒâ–„â–â–‚â–‚â–„â–„â–ƒâ–‚â–‚â–â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–ƒâ–â–ƒâ–„â–„â–…â–‡â–‚â–…â–…â–…â–ˆâ–…â–…â–„â–ƒâ–‡â–…â–„â–†â–…â–…â–…â–‡â–ˆâ–…â–†â–‡â–ˆâ–…â–‡â–‡â–‡â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–…â–ˆâ–…â–‡â–ˆâ–‡â–†â–„â–‡â–†â–…â–„â–‡â–‡â–…â–†â–„â–…â–‚â–„â–…â–‚â–ƒâ–ƒâ–‚â–„â–‚â–„â–„â–…â–ƒâ–…â–…â–…â–ƒâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–…â–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–â–â–â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–…â–‚â–‚â–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–â–ƒâ–‚â–…â–‚â–â–â–ƒâ–‚â–ƒâ–ƒâ–„â–â–„â–‚â–‚â–ƒâ–„â–…â–„â–„â–‚â–„â–ˆâ–…â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_single_lstm__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2md3k6a8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:08:09.843103: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm__binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/32175azg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010808-32175azg\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆ| 80/80 [00:01<00:00, 43.06it/s, loss=0.701, v_num=5azg, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.657\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:01<00:00, 42.71it/s, loss=0.701, v_num=5azg, BTC_val_\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 45.55it/s, loss=0.657, v_num=5azg, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 45.25it/s, loss=0.657, v_num=5azg, BTC_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.093 >= min_delta = 0.003. New best score: 0.564\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 44.26it/s, loss=0.618, v_num=5azg, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.032 >= min_delta = 0.003. New best score: 0.532\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 43.93it/s, loss=0.618, v_num=5azg, BTC_val_\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:02<00:00, 33.46it/s, loss=0.58, v_num=5azg, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.063 >= min_delta = 0.003. New best score: 0.469\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:02<00:00, 33.29it/s, loss=0.58, v_num=5azg, BTC_val_a\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 43.25it/s, loss=0.593, v_num=5azg, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 42.89it/s, loss=0.593, v_num=5azg, BTC_val_\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:01<00:00, 44.46it/s, loss=0.577, v_num=5azg, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:01<00:00, 44.18it/s, loss=0.577, v_num=5azg, BTC_val_\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:01<00:00, 43.28it/s, loss=0.641, v_num=5azg, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:01<00:00, 43.02it/s, loss=0.641, v_num=5azg, BTC_val_\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:02<00:00, 39.16it/s, loss=0.605, v_num=5azg, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:02<00:00, 38.93it/s, loss=0.605, v_num=5azg, BTC_val_\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:01<00:00, 44.14it/s, loss=0.602, v_num=5azg, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:01<00:00, 43.87it/s, loss=0.602, v_num=5azg, BTC_val_\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:01<00:00, 43.86it/s, loss=0.594, v_num=5azg, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:01<00:00, 43.57it/s, loss=0.594, v_num=5azg, BTC_val_\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:02<00:00, 39.90it/s, loss=0.569, v_num=5azg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:02<00:00, 39.67it/s, loss=0.569, v_num=5azg, BTC_val\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:01<00:00, 43.14it/s, loss=0.551, v_num=5azg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:01<00:00, 42.86it/s, loss=0.551, v_num=5azg, BTC_val\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:01<00:00, 46.14it/s, loss=0.612, v_num=5azg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:01<00:00, 45.84it/s, loss=0.612, v_num=5azg, BTC_val\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:01<00:00, 45.27it/s, loss=0.614, v_num=5azg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:01<00:00, 44.99it/s, loss=0.614, v_num=5azg, BTC_val\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:01<00:00, 44.96it/s, loss=0.576, v_num=5azg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:01<00:00, 44.66it/s, loss=0.576, v_num=5azg, BTC_val\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:02<00:00, 31.70it/s, loss=0.581, v_num=5azg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:02<00:00, 31.53it/s, loss=0.581, v_num=5azg, BTC_val\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:02<00:00, 36.85it/s, loss=0.588, v_num=5azg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:02<00:00, 36.66it/s, loss=0.588, v_num=5azg, BTC_val\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:02<00:00, 33.90it/s, loss=0.583, v_num=5azg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:02<00:00, 33.73it/s, loss=0.583, v_num=5azg, BTC_val\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:02<00:00, 32.11it/s, loss=0.603, v_num=5azg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:02<00:00, 31.74it/s, loss=0.603, v_num=5azg, BTC_val\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:02<00:00, 37.47it/s, loss=0.543, v_num=5azg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:02<00:00, 37.27it/s, loss=0.543, v_num=5azg, BTC_val\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:01<00:00, 42.53it/s, loss=0.58, v_num=5azg, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:01<00:00, 42.27it/s, loss=0.58, v_num=5azg, BTC_val_\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:02<00:00, 29.83it/s, loss=0.589, v_num=5azg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:02<00:00, 29.67it/s, loss=0.589, v_num=5azg, BTC_val\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:02<00:00, 29.88it/s, loss=0.584, v_num=5azg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:02<00:00, 29.75it/s, loss=0.584, v_num=5azg, BTC_val\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:02<00:00, 33.95it/s, loss=0.521, v_num=5azg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.469. Signaling Trainer to stop.\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:02<00:00, 33.79it/s, loss=0.521, v_num=5azg, BTC_val\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:02<00:00, 33.57it/s, loss=0.521, v_num=5azg, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 88.87it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.7096773982048035,\n",
      " 'BTC_test_f1': 0.7047099471092224,\n",
      " 'test_loss': 0.602120041847229}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 65375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010808-32175azg/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010808-32175azg/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.74603\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.59597\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 23\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1896\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 59\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462147\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 85\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.72447\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.71129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.56374\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.77778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.466\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.70968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.70471\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.60212\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–‚â–†â–ƒâ–†â–ˆâ–ƒâ–‡â–‡â–‡â–‚â–â–†â–ƒâ–â–‡â–ˆâ–…â–†â–ƒâ–†â–†â–‡â–ƒâ–‡â–‡â–†â–â–â–ƒâ–†â–‡â–â–ƒâ–ƒâ–ƒâ–ˆâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–ƒâ–†â–ƒâ–†â–ˆâ–„â–†â–†â–‡â–ƒâ–‚â–†â–‚â–‚â–†â–ˆâ–…â–†â–„â–†â–†â–‡â–„â–‡â–†â–†â–‚â–‚â–‚â–…â–‡â–â–„â–„â–„â–ˆâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–…â–…â–…â–ƒâ–„â–…â–„â–ƒâ–ƒâ–†â–‡â–‚â–„â–‡â–‚â–â–„â–†â–…â–ƒâ–‚â–…â–†â–ƒâ–â–„â–…â–ˆâ–†â–ƒâ–‚â–‡â–†â–…â–„â–‚â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–„â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–â–„â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–†â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–ƒâ–†â–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–ƒâ–†â–†â–†â–†â–†â–†â–ƒâ–ƒâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–…â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–…â–‡â–‡â–‡â–‡â–‡â–‡â–…â–…â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–…â–ƒâ–â–ƒâ–ƒâ–„â–‚â–ƒâ–ƒâ–â–â–ƒâ–‚â–ƒâ–‚â–„â–ƒâ–ƒâ–ƒâ–â–„â–„â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/32175azg\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:09:23.869660: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm__binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3cc6sl3t\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010922-3cc6sl3t\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 80/81 [00:01<00:00, 46.50it/s, loss=0.713, v_num=sl3t, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.691\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:01<00:00, 45.91it/s, loss=0.713, v_num=sl3t, BTC_val_\n",
      "Epoch 1:  99%|â–‰| 80/81 [00:01<00:00, 45.46it/s, loss=0.694, v_num=sl3t, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:01<00:00, 45.08it/s, loss=0.694, v_num=sl3t, BTC_val_\u001b[A\n",
      "Epoch 2:  99%|â–‰| 80/81 [00:01<00:00, 47.32it/s, loss=0.716, v_num=sl3t, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:01<00:00, 46.83it/s, loss=0.716, v_num=sl3t, BTC_val_\u001b[A\n",
      "Epoch 3:  99%|â–‰| 80/81 [00:01<00:00, 44.63it/s, loss=0.685, v_num=sl3t, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:01<00:00, 44.14it/s, loss=0.685, v_num=sl3t, BTC_val_\u001b[A\n",
      "Epoch 4:  99%|â–‰| 80/81 [00:01<00:00, 41.40it/s, loss=0.683, v_num=sl3t, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:01<00:00, 40.91it/s, loss=0.683, v_num=sl3t, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 80/81 [00:01<00:00, 45.59it/s, loss=0.706, v_num=sl3t, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:01<00:00, 45.12it/s, loss=0.706, v_num=sl3t, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 80/81 [00:02<00:00, 39.34it/s, loss=0.694, v_num=sl3t, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:02<00:00, 39.07it/s, loss=0.694, v_num=sl3t, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 80/81 [00:01<00:00, 45.74it/s, loss=0.691, v_num=sl3t, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:01<00:00, 45.30it/s, loss=0.691, v_num=sl3t, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 80/81 [00:01<00:00, 46.02it/s, loss=0.684, v_num=sl3t, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:01<00:00, 45.55it/s, loss=0.684, v_num=sl3t, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 80/81 [00:01<00:00, 45.57it/s, loss=0.692, v_num=sl3t, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:01<00:00, 45.10it/s, loss=0.692, v_num=sl3t, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 80/81 [00:02<00:00, 33.04it/s, loss=0.692, v_num=sl3t, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:02<00:00, 32.79it/s, loss=0.692, v_num=sl3t, BTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 80/81 [00:01<00:00, 41.05it/s, loss=0.692, v_num=sl3t, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:01<00:00, 40.63it/s, loss=0.692, v_num=sl3t, BTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 80/81 [00:01<00:00, 44.33it/s, loss=0.686, v_num=sl3t, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:01<00:00, 43.91it/s, loss=0.686, v_num=sl3t, BTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 80/81 [00:02<00:00, 39.36it/s, loss=0.687, v_num=sl3t, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:02<00:00, 39.09it/s, loss=0.687, v_num=sl3t, BTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 80/81 [00:02<00:00, 33.54it/s, loss=0.694, v_num=sl3t, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:02<00:00, 33.36it/s, loss=0.694, v_num=sl3t, BTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 80/81 [00:01<00:00, 45.83it/s, loss=0.686, v_num=sl3t, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:01<00:00, 43.95it/s, loss=0.686, v_num=sl3t, BTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 80/81 [00:02<00:00, 34.48it/s, loss=0.69, v_num=sl3t, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:02<00:00, 34.22it/s, loss=0.69, v_num=sl3t, BTC_val_\u001b[A\n",
      "Epoch 17:  99%|â–‰| 80/81 [00:02<00:00, 39.05it/s, loss=0.693, v_num=sl3t, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:02<00:00, 38.61it/s, loss=0.693, v_num=sl3t, BTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 80/81 [00:02<00:00, 38.85it/s, loss=0.695, v_num=sl3t, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:02<00:00, 38.59it/s, loss=0.695, v_num=sl3t, BTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 80/81 [00:02<00:00, 32.96it/s, loss=0.691, v_num=sl3t, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:02<00:00, 32.57it/s, loss=0.691, v_num=sl3t, BTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 80/81 [00:02<00:00, 36.17it/s, loss=0.689, v_num=sl3t, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.691. Signaling Trainer to stop.\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:02<00:00, 35.97it/s, loss=0.689, v_num=sl3t, BTC_val\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:02<00:00, 35.92it/s, loss=0.689, v_num=sl3t, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 78.72it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.4193548262119293,\n",
      " 'BTC_test_f1': 0.29533159732818604,\n",
      " 'test_loss': 0.710812509059906}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 65576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010922-3cc6sl3t/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010922-3cc6sl3t/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.70306\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1680\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 51\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462213\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.5398\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.3528\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69123\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.35714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.68851\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.41935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.29533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.71081\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–„â–…â–…â–„â–ƒâ–„â–…â–ˆâ–…â–„â–…â–„â–„â–…â–ƒâ–â–ƒâ–…â–…â–„â–ƒâ–ƒâ–…â–â–†â–…â–…â–…â–„â–„â–…â–ƒâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–ƒâ–…â–…â–ƒâ–‚â–ƒâ–…â–ˆâ–„â–ƒâ–„â–ƒâ–‚â–„â–‚â–â–‚â–ƒâ–„â–„â–‚â–‚â–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–…â–ƒâ–…â–†â–…â–…â–„â–â–„â–„â–†â–„â–…â–…â–…â–…â–…â–…â–„â–…â–…â–†â–„â–ˆâ–„â–…â–…â–„â–…â–…â–„â–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–ƒâ–…â–â–„â–„â–†â–‚â–ƒâ–†â–„â–…â–‡â–ƒâ–‡â–†â–‡â–†â–ˆâ–‡â–†â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–‡â–ˆâ–‡â–‡â–†â–„â–„â–„â–„â–„â–ƒâ–…â–…â–‚â–â–â–â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–†â–†â–‚â–ƒâ–„â–ƒâ–‚â–â–ƒâ–‚â–â–ƒâ–‚â–â–â–‚â–‚â–‚â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–ˆâ–„â–â–†â–†â–‚â–…â–‚â–ƒâ–‚â–‚â–ƒâ–„â–‚â–„â–„â–ƒâ–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3cc6sl3t\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:10:28.686761: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm__multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1rkw3uss\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011027-1rkw3uss\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Training:   0%|                                          | 0/80 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:01<00:00, 44.21it/s, loss=1.05, v_num=3uss, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.981\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:01<00:00, 43.93it/s, loss=1.05, v_num=3uss, BTC_val_a\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 46.00it/s, loss=1.05, v_num=3uss, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 45.71it/s, loss=1.05, v_num=3uss, BTC_val_a\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 46.45it/s, loss=1.04, v_num=3uss, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 46.17it/s, loss=1.04, v_num=3uss, BTC_val_a\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:01<00:00, 46.22it/s, loss=1.03, v_num=3uss, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:01<00:00, 45.92it/s, loss=1.03, v_num=3uss, BTC_val_a\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 45.60it/s, loss=1.03, v_num=3uss, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 45.31it/s, loss=1.03, v_num=3uss, BTC_val_a\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:01<00:00, 45.54it/s, loss=1.05, v_num=3uss, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:01<00:00, 45.24it/s, loss=1.05, v_num=3uss, BTC_val_a\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:01<00:00, 45.73it/s, loss=1.02, v_num=3uss, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:01<00:00, 45.45it/s, loss=1.02, v_num=3uss, BTC_val_a\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:02<00:00, 38.43it/s, loss=1.02, v_num=3uss, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:02<00:00, 38.21it/s, loss=1.02, v_num=3uss, BTC_val_a\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:02<00:00, 38.68it/s, loss=0.984, v_num=3uss, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:02<00:00, 38.47it/s, loss=0.984, v_num=3uss, BTC_val_\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:01<00:00, 45.73it/s, loss=1, v_num=3uss, BTC_val_acc=\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:01<00:00, 45.43it/s, loss=1, v_num=3uss, BTC_val_acc=\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:01<00:00, 44.81it/s, loss=1.01, v_num=3uss, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:01<00:00, 44.52it/s, loss=1.01, v_num=3uss, BTC_val_\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:01<00:00, 45.80it/s, loss=1.02, v_num=3uss, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:01<00:00, 45.51it/s, loss=1.02, v_num=3uss, BTC_val_\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|â–ˆ| 80/80 [00:01<00:00, 44.63it/s, loss=1.01, v_num=3uss, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:01<00:00, 44.34it/s, loss=1.01, v_num=3uss, BTC_val_\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:01<00:00, 44.93it/s, loss=1.03, v_num=3uss, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:01<00:00, 44.63it/s, loss=1.03, v_num=3uss, BTC_val_\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:01<00:00, 42.45it/s, loss=0.995, v_num=3uss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:01<00:00, 42.18it/s, loss=0.995, v_num=3uss, BTC_val\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:01<00:00, 43.77it/s, loss=1.03, v_num=3uss, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:01<00:00, 43.47it/s, loss=1.03, v_num=3uss, BTC_val_\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:01<00:00, 44.22it/s, loss=1.02, v_num=3uss, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:01<00:00, 43.90it/s, loss=1.02, v_num=3uss, BTC_val_\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:02<00:00, 37.29it/s, loss=1, v_num=3uss, BTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:02<00:00, 36.86it/s, loss=1, v_num=3uss, BTC_val_acc\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:02<00:00, 31.72it/s, loss=1.02, v_num=3uss, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:02<00:00, 31.51it/s, loss=1.02, v_num=3uss, BTC_val_\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:02<00:00, 33.13it/s, loss=1.02, v_num=3uss, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:02<00:00, 32.98it/s, loss=1.02, v_num=3uss, BTC_val_\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:02<00:00, 30.15it/s, loss=1.04, v_num=3uss, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.981. Signaling Trainer to stop.\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:02<00:00, 30.01it/s, loss=1.04, v_num=3uss, BTC_val_\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:02<00:00, 29.98it/s, loss=1.04, v_num=3uss, BTC_val_\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 87.71it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5483871102333069,\n",
      " 'BTC_test_f1': 0.23566308617591858,\n",
      " 'test_loss': 0.941138505935669}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 65761\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011027-1rkw3uss/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011027-1rkw3uss/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.18182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.13358\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1659\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 49\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462276\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.49802\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.21888\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.01995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.01299\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.54839\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.23566\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.94114\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–â–†â–„â–„â–†â–…â–‚â–„â–„â–ˆâ–‡â–â–…â–†â–…â–‡â–ƒâ–„â–…â–â–„â–ƒâ–ƒâ–†â–…â–…â–…â–ƒâ–…â–…â–…â–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–â–ˆâ–ƒâ–…â–…â–…â–‚â–ƒâ–ƒâ–†â–†â–â–„â–…â–…â–†â–ƒâ–ƒâ–„â–â–ƒâ–ƒâ–ƒâ–…â–„â–„â–…â–ƒâ–…â–…â–„â–…â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–‡â–‚â–„â–„â–ƒâ–„â–†â–†â–†â–ƒâ–â–ˆâ–ˆâ–‚â–„â–‚â–…â–„â–ƒâ–ˆâ–…â–…â–†â–‚â–ƒâ–‡â–„â–†â–„â–„â–„â–‚â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–…â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–ˆâ–„â–…â–ƒâ–‚â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–…â–ƒâ–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–â–‚â–â–‚â–‚â–‚â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–â–†â–‡â–†â–…â–‡â–„â–‡â–…â–‡â–‡â–ˆâ–†â–‡â–…â–†â–…â–†â–†â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1rkw3uss\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:11:33.562941: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm__multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/q7jhhfaz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011132-q7jhhfaz\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|â–‰| 80/81 [00:01<00:00, 41.97it/s, loss=1.11, v_num=hfaz, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.087\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:01<00:00, 41.02it/s, loss=1.11, v_num=hfaz, BTC_val_a\n",
      "Epoch 1:  99%|â–‰| 80/81 [00:01<00:00, 42.73it/s, loss=1.12, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:01<00:00, 42.31it/s, loss=1.12, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:01<00:00, 45.77it/s, loss=1.08, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:01<00:00, 45.42it/s, loss=1.08, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Epoch 3:  99%|â–‰| 80/81 [00:01<00:00, 47.47it/s, loss=1.09, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:01<00:00, 47.06it/s, loss=1.09, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Epoch 4:  99%|â–‰| 80/81 [00:01<00:00, 40.22it/s, loss=1.11, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:02<00:00, 39.97it/s, loss=1.11, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Epoch 5:  99%|â–‰| 80/81 [00:02<00:00, 37.94it/s, loss=1.09, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:02<00:00, 37.64it/s, loss=1.09, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Epoch 6:  99%|â–‰| 80/81 [00:01<00:00, 41.11it/s, loss=1.1, v_num=hfaz, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:01<00:00, 40.68it/s, loss=1.1, v_num=hfaz, BTC_val_ac\u001b[A\n",
      "Epoch 7:  99%|â–‰| 80/81 [00:01<00:00, 45.26it/s, loss=1.1, v_num=hfaz, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:01<00:00, 44.88it/s, loss=1.1, v_num=hfaz, BTC_val_ac\u001b[A\n",
      "Epoch 8:  99%|â–‰| 80/81 [00:01<00:00, 45.81it/s, loss=1.1, v_num=hfaz, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:01<00:00, 45.35it/s, loss=1.1, v_num=hfaz, BTC_val_ac\u001b[A\n",
      "Epoch 9:  99%|â–‰| 80/81 [00:01<00:00, 45.65it/s, loss=1.09, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:01<00:00, 45.24it/s, loss=1.09, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Epoch 10:  99%|â–‰| 80/81 [00:01<00:00, 44.92it/s, loss=1.09, v_num=hfaz, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:01<00:00, 44.46it/s, loss=1.09, v_num=hfaz, BTC_val_\u001b[A\n",
      "Epoch 11:  99%|â–‰| 80/81 [00:01<00:00, 44.08it/s, loss=1.1, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:01<00:00, 43.74it/s, loss=1.1, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Epoch 12:  99%|â–‰| 80/81 [00:01<00:00, 42.46it/s, loss=1.11, v_num=hfaz, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:01<00:00, 41.99it/s, loss=1.11, v_num=hfaz, BTC_val_\u001b[A\n",
      "Epoch 13:  99%|â–‰| 80/81 [00:02<00:00, 35.03it/s, loss=1.1, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:02<00:00, 34.83it/s, loss=1.1, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Epoch 14:  99%|â–‰| 80/81 [00:02<00:00, 34.27it/s, loss=1.1, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:02<00:00, 34.13it/s, loss=1.1, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Epoch 15:  99%|â–‰| 80/81 [00:01<00:00, 44.29it/s, loss=1.08, v_num=hfaz, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:01<00:00, 42.47it/s, loss=1.08, v_num=hfaz, BTC_val_\u001b[A\n",
      "Epoch 16:  99%|â–‰| 80/81 [00:02<00:00, 36.61it/s, loss=1.09, v_num=hfaz, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:02<00:00, 36.40it/s, loss=1.09, v_num=hfaz, BTC_val_\u001b[A\n",
      "Epoch 17:  99%|â–‰| 80/81 [00:01<00:00, 44.18it/s, loss=1.09, v_num=hfaz, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:01<00:00, 43.55it/s, loss=1.09, v_num=hfaz, BTC_val_\u001b[A\n",
      "Epoch 18:  99%|â–‰| 80/81 [00:02<00:00, 33.08it/s, loss=1.1, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:02<00:00, 32.75it/s, loss=1.1, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Epoch 19:  99%|â–‰| 80/81 [00:02<00:00, 28.20it/s, loss=1.1, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:02<00:00, 28.01it/s, loss=1.1, v_num=hfaz, BTC_val_a\u001b[A\n",
      "Epoch 20:  99%|â–‰| 80/81 [00:02<00:00, 33.36it/s, loss=1.11, v_num=hfaz, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.087. Signaling Trainer to stop.\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:02<00:00, 33.22it/s, loss=1.11, v_num=hfaz, BTC_val_\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:02<00:00, 33.17it/s, loss=1.11, v_num=hfaz, BTC_val_\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 101.07it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.25806450843811035,\n",
      " 'BTC_test_f1': 0.1367289274930954,\n",
      " 'test_loss': 1.1524884700775146}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 65944\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011132-q7jhhfaz/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011132-q7jhhfaz/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.2967\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.08042\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1680\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 51\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462343\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.3554\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.27011\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09526\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.11437\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.25806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.13673\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.15249\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–…â–†â–…â–…â–„â–‚â–„â–ˆâ–†â–„â–…â–†â–ƒâ–‚â–ƒâ–ˆâ–‚â–„â–ƒâ–â–ƒâ–‡â–‡â–…â–ƒâ–„â–ƒâ–‡â–…â–ƒâ–†â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–†â–‡â–…â–…â–‚â–‚â–„â–‡â–…â–…â–…â–…â–ƒâ–â–ƒâ–ˆâ–‚â–„â–‚â–â–ƒâ–…â–†â–ˆâ–ƒâ–„â–ƒâ–‡â–…â–ƒâ–ƒâ–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–„â–„â–„â–„â–„â–…â–„â–â–ƒâ–„â–…â–„â–„â–ƒâ–…â–â–„â–ƒâ–ƒâ–„â–„â–‚â–ƒâ–ˆâ–„â–ƒâ–„â–„â–ƒâ–„â–ƒâ–…â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–‚â–ƒâ–„â–â–„â–†â–‡â–ƒâ–„â–ƒâ–…â–ƒâ–ˆâ–ƒâ–…â–†â–†â–ƒâ–„â–ƒâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–†â–‡â–†â–…â–†â–…â–ˆâ–…â–‡â–…â–…â–„â–†â–ƒâ–„â–‡â–…â–„â–…â–â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–†â–‚â–…â–„â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–‚â–ƒâ–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–â–‚â–…â–…â–ƒâ–„â–…â–ˆâ–„â–ƒâ–†â–‡â–†â–ˆâ–ˆâ–‡â–†â–…â–…â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/q7jhhfaz\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:12:38.247830: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_single_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3si4xqi8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011236-3si4xqi8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 130   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.7 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 79/80 [00:00<00:00, 88.76it/s, loss=0.626, v_num=xqi8, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.591\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:00<00:00, 80.25it/s, loss=0.626, v_num=xqi8, ETH_val_\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 77.17it/s, loss=0.604, v_num=xqi8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.015 >= min_delta = 0.003. New best score: 0.577\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 76.45it/s, loss=0.604, v_num=xqi8, ETH_val_\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 78.85it/s, loss=0.622, v_num=xqi8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.047 >= min_delta = 0.003. New best score: 0.529\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 78.12it/s, loss=0.622, v_num=xqi8, ETH_val_\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:01<00:00, 69.24it/s, loss=0.568, v_num=xqi8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:01<00:00, 68.66it/s, loss=0.568, v_num=xqi8, ETH_val_\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 71.90it/s, loss=0.608, v_num=xqi8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 71.32it/s, loss=0.608, v_num=xqi8, ETH_val_\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:01<00:00, 73.79it/s, loss=0.598, v_num=xqi8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.522\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:01<00:00, 72.81it/s, loss=0.598, v_num=xqi8, ETH_val_\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:01<00:00, 66.07it/s, loss=0.595, v_num=xqi8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.517\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:01<00:00, 65.15it/s, loss=0.595, v_num=xqi8, ETH_val_\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:01<00:00, 59.84it/s, loss=0.599, v_num=xqi8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:01<00:00, 59.07it/s, loss=0.599, v_num=xqi8, ETH_val_\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:01<00:00, 71.82it/s, loss=0.581, v_num=xqi8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:01<00:00, 71.27it/s, loss=0.581, v_num=xqi8, ETH_val_\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:00<00:00, 85.79it/s, loss=0.584, v_num=xqi8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:00<00:00, 85.04it/s, loss=0.584, v_num=xqi8, ETH_val_\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:00<00:00, 89.36it/s, loss=0.649, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:00<00:00, 88.56it/s, loss=0.649, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:00<00:00, 90.79it/s, loss=0.555, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:00<00:00, 90.00it/s, loss=0.555, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:00<00:00, 87.21it/s, loss=0.571, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.033 >= min_delta = 0.003. New best score: 0.484\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:00<00:00, 86.43it/s, loss=0.571, v_num=xqi8, ETH_val\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:01<00:00, 66.99it/s, loss=0.554, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:01<00:00, 66.38it/s, loss=0.554, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:01<00:00, 68.32it/s, loss=0.574, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:01<00:00, 67.81it/s, loss=0.574, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:00<00:00, 85.39it/s, loss=0.571, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:00<00:00, 84.60it/s, loss=0.571, v_num=xqi8, ETH_val\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.478\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:00<00:00, 86.73it/s, loss=0.565, v_num=xqi8, ETH_val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:00<00:00, 85.89it/s, loss=0.565, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:01<00:00, 76.86it/s, loss=0.567, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:01<00:00, 76.15it/s, loss=0.567, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:00<00:00, 85.43it/s, loss=0.595, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:00<00:00, 84.68it/s, loss=0.595, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:00<00:00, 90.19it/s, loss=0.572, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:00<00:00, 89.45it/s, loss=0.572, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:00<00:00, 90.07it/s, loss=0.607, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:00<00:00, 89.26it/s, loss=0.607, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:00<00:00, 89.82it/s, loss=0.584, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:00<00:00, 89.05it/s, loss=0.584, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:00<00:00, 88.68it/s, loss=0.579, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:00<00:00, 87.90it/s, loss=0.579, v_num=xqi8, ETH_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|â–ˆ| 80/80 [00:00<00:00, 90.38it/s, loss=0.603, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:00<00:00, 89.49it/s, loss=0.603, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:00<00:00, 81.54it/s, loss=0.606, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:00<00:00, 80.86it/s, loss=0.606, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:01<00:00, 70.23it/s, loss=0.555, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:01<00:00, 69.69it/s, loss=0.555, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:01<00:00, 79.14it/s, loss=0.566, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:01<00:00, 78.07it/s, loss=0.566, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 80/80 [00:01<00:00, 76.24it/s, loss=0.565, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 80/80 [00:01<00:00, 75.61it/s, loss=0.565, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:00<00:00, 81.66it/s, loss=0.53, v_num=xqi8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:00<00:00, 81.01it/s, loss=0.53, v_num=xqi8, ETH_val_\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:00<00:00, 88.35it/s, loss=0.553, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:00<00:00, 87.61it/s, loss=0.553, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 80/80 [00:00<00:00, 89.26it/s, loss=0.566, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 80/80 [00:00<00:00, 88.42it/s, loss=0.566, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 80/80 [00:00<00:00, 89.71it/s, loss=0.581, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 80/80 [00:00<00:00, 88.89it/s, loss=0.581, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 80/80 [00:00<00:00, 84.91it/s, loss=0.589, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 80/80 [00:00<00:00, 84.11it/s, loss=0.589, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 80/80 [00:00<00:00, 83.82it/s, loss=0.579, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 80/80 [00:00<00:00, 82.93it/s, loss=0.579, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 80/80 [00:00<00:00, 82.99it/s, loss=0.577, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 80/80 [00:00<00:00, 82.33it/s, loss=0.577, v_num=xqi8, ETH_val\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 80/80 [00:01<00:00, 78.29it/s, loss=0.539, v_num=xqi8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 80/80 [00:01<00:00, 77.60it/s, loss=0.539, v_num=xqi8, ETH_val\u001b[A\n",
      "                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.478. Signaling Trainer to stop.\n",
      "Epoch 35: 100%|â–ˆ| 80/80 [00:01<00:00, 77.35it/s, loss=0.539, v_num=xqi8, ETH_val\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 114.98it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.7419354915618896,\n",
      " 'ETH_test_f1': 0.741647481918335,\n",
      " 'test_loss': 0.5086269378662109}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 66129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011236-3si4xqi8/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011236-3si4xqi8/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.6875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.68627\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.68277\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 35\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2844\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 45\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462401\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.71813\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.70306\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.55649\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.77778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.47895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.74194\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.74165\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.50863\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–…â–…â–…â–„â–â–ƒâ–ƒâ–…â–…â–ƒâ–…â–…â–ƒâ–ƒâ–â–…â–„â–…â–ƒâ–…â–‚â–‚â–ƒâ–„â–ˆâ–†â–„â–…â–â–…â–„â–„â–†â–ƒâ–‡â–…â–…â–ƒâ–ƒâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–…â–…â–…â–„â–â–‚â–ƒâ–„â–…â–â–„â–„â–‚â–â–â–„â–ƒâ–…â–ƒâ–…â–â–‚â–ƒâ–„â–ˆâ–†â–ƒâ–„â–â–…â–„â–„â–†â–ƒâ–‡â–…â–…â–ƒâ–‚â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–ƒâ–ƒâ–‚â–†â–‡â–„â–ˆâ–„â–ƒâ–†â–ƒâ–‚â–…â–„â–‡â–ƒâ–ƒâ–ƒâ–…â–„â–ˆâ–…â–„â–„â–â–‚â–„â–‚â–†â–ƒâ–„â–ƒâ–‚â–ƒâ–â–‚â–‚â–‡â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–…â–†â–‡â–…â–‡â–†â–‡â–‡â–†â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–â–…â–†â–†â–†â–‡â–†â–‡â–‡â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–†â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–â–â–‚â–‚â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–â–â–ƒâ–†â–ƒâ–†â–ƒâ–â–â–â–â–â–ˆâ–â–†â–ˆâ–â–â–â–ƒâ–ƒâ–ƒâ–†â–â–†â–ƒâ–†â–†â–ƒâ–†â–†â–ƒâ–â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–â–â–„â–†â–„â–†â–„â–â–â–â–‚â–‚â–ˆâ–‚â–†â–ˆâ–â–â–‚â–„â–„â–„â–†â–â–†â–„â–†â–†â–„â–†â–†â–„â–â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–‡â–„â–„â–…â–„â–ƒâ–„â–…â–…â–…â–…â–â–ƒâ–ƒâ–â–†â–…â–ƒâ–„â–ƒâ–‚â–‚â–…â–„â–â–â–‚â–ƒâ–‚â–‚â–ƒâ–„â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_single_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3si4xqi8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:13:31.377867: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_single_lstm_loss_weighted_binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/6arfgw3b\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011330-6arfgw3b\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 130   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.7 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 80/81 [00:00<00:00, 90.62it/s, loss=0.703, v_num=gw3b, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.603\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:00<00:00, 87.76it/s, loss=0.703, v_num=gw3b, ETH_val_\n",
      "Epoch 1:  99%|â–‰| 80/81 [00:01<00:00, 79.59it/s, loss=0.709, v_num=gw3b, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:01<00:00, 77.45it/s, loss=0.709, v_num=gw3b, ETH_val_\u001b[A\n",
      "Epoch 2:  99%|â–‰| 80/81 [00:00<00:00, 88.78it/s, loss=0.696, v_num=gw3b, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.038 >= min_delta = 0.003. New best score: 0.565\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:00<00:00, 86.33it/s, loss=0.696, v_num=gw3b, ETH_val_\n",
      "Epoch 3:  99%|â–‰| 80/81 [00:00<00:00, 87.95it/s, loss=0.693, v_num=gw3b, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:00<00:00, 85.21it/s, loss=0.693, v_num=gw3b, ETH_val_\u001b[A\n",
      "Epoch 4:  99%|â–‰| 80/81 [00:00<00:00, 80.35it/s, loss=0.699, v_num=gw3b, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:01<00:00, 78.08it/s, loss=0.699, v_num=gw3b, ETH_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 80/81 [00:00<00:00, 83.82it/s, loss=0.689, v_num=gw3b, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:00<00:00, 81.61it/s, loss=0.689, v_num=gw3b, ETH_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 80/81 [00:01<00:00, 72.93it/s, loss=0.698, v_num=gw3b, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:01<00:00, 70.08it/s, loss=0.698, v_num=gw3b, ETH_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 80/81 [00:01<00:00, 77.20it/s, loss=0.701, v_num=gw3b, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:01<00:00, 75.33it/s, loss=0.701, v_num=gw3b, ETH_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 80/81 [00:00<00:00, 83.69it/s, loss=0.692, v_num=gw3b, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:00<00:00, 81.61it/s, loss=0.692, v_num=gw3b, ETH_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 80/81 [00:00<00:00, 92.33it/s, loss=0.701, v_num=gw3b, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:00<00:00, 89.82it/s, loss=0.701, v_num=gw3b, ETH_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 80/81 [00:00<00:00, 92.13it/s, loss=0.692, v_num=gw3b, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:00<00:00, 89.79it/s, loss=0.692, v_num=gw3b, ETH_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 80/81 [00:00<00:00, 92.76it/s, loss=0.694, v_num=gw3b, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:00<00:00, 90.29it/s, loss=0.694, v_num=gw3b, ETH_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 80/81 [00:00<00:00, 92.13it/s, loss=0.695, v_num=gw3b, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:00<00:00, 89.78it/s, loss=0.695, v_num=gw3b, ETH_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 80/81 [00:00<00:00, 90.13it/s, loss=0.698, v_num=gw3b, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:00<00:00, 87.51it/s, loss=0.698, v_num=gw3b, ETH_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 80/81 [00:00<00:00, 91.94it/s, loss=0.695, v_num=gw3b, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:00<00:00, 89.32it/s, loss=0.695, v_num=gw3b, ETH_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 80/81 [00:00<00:00, 89.68it/s, loss=0.69, v_num=gw3b, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:00<00:00, 82.14it/s, loss=0.69, v_num=gw3b, ETH_val_\u001b[A\n",
      "Epoch 16:  99%|â–‰| 80/81 [00:00<00:00, 82.31it/s, loss=0.693, v_num=gw3b, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:01<00:00, 80.22it/s, loss=0.693, v_num=gw3b, ETH_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 80/81 [00:00<00:00, 88.18it/s, loss=0.695, v_num=gw3b, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:00<00:00, 85.84it/s, loss=0.695, v_num=gw3b, ETH_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 80/81 [00:01<00:00, 78.39it/s, loss=0.702, v_num=gw3b, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:01<00:00, 75.81it/s, loss=0.702, v_num=gw3b, ETH_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 80/81 [00:01<00:00, 75.26it/s, loss=0.699, v_num=gw3b, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:01<00:00, 73.51it/s, loss=0.699, v_num=gw3b, ETH_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 80/81 [00:00<00:00, 85.83it/s, loss=0.695, v_num=gw3b, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:00<00:00, 83.68it/s, loss=0.695, v_num=gw3b, ETH_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 80/81 [00:00<00:00, 90.58it/s, loss=0.696, v_num=gw3b, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:00<00:00, 88.28it/s, loss=0.696, v_num=gw3b, ETH_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 80/81 [00:00<00:00, 89.74it/s, loss=0.694, v_num=gw3b, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.565. Signaling Trainer to stop.\n",
      "Epoch 22: 100%|â–ˆ| 81/81 [00:00<00:00, 87.24it/s, loss=0.694, v_num=gw3b, ETH_val\n",
      "Epoch 22: 100%|â–ˆ| 81/81 [00:00<00:00, 87.00it/s, loss=0.694, v_num=gw3b, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 147.78it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.6129032373428345,\n",
      " 'ETH_test_f1': 0.3793548345565796,\n",
      " 'test_loss': 0.6772454977035522}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 66324\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011330-6arfgw3b/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011330-6arfgw3b/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.4375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.417\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.70312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 22\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1840\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 31\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462441\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 82\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.50906\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.49335\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69379\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.88889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.47059\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.65843\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.37935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.67725\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–ƒâ–ƒâ–ƒâ–„â–„â–‚â–ƒâ–…â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–„â–ƒâ–…â–ƒâ–â–„â–…â–…â–†â–…â–â–„â–…â–ƒâ–ˆâ–†â–…â–†â–ƒâ–…â–ƒâ–„â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–ƒâ–ƒâ–ƒâ–„â–„â–‚â–ƒâ–…â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–…â–‚â–â–„â–…â–…â–†â–…â–â–„â–„â–ƒâ–ˆâ–†â–…â–†â–‚â–…â–ƒâ–„â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–†â–ˆâ–†â–ƒâ–‚â–†â–‚â–‚â–…â–„â–†â–„â–„â–ƒâ–…â–„â–„â–†â–…â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–…â–â–‚â–‚â–ƒâ–‡â–ƒâ–„â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–‚â–â–ˆâ–„â–ƒâ–‡â–„â–„â–„â–…â–†â–„â–…â–†â–ƒâ–ƒâ–†â–†â–ƒâ–‡â–†â–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–‚â–â–…â–‚â–‚â–ˆâ–„â–…â–ƒâ–…â–†â–ƒâ–…â–†â–…â–ƒâ–‡â–ˆâ–‚â–ˆâ–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–‡â–‚â–„â–„â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–â–â–‚â–â–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ƒâ–†â–â–„â–‡â–ƒâ–„â–†â–†â–ƒâ–†â–ˆâ–†â–ˆâ–‡â–†â–‡â–†â–ˆâ–†â–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_single_lstm_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/6arfgw3b\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:14:10.809025: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_single_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2bbz7omm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011409-2bbz7omm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 195   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.8 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 79/80 [00:01<00:00, 74.01it/s, loss=1.11, v_num=7omm, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.082\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:01<00:00, 67.61it/s, loss=1.11, v_num=7omm, ETH_val_a\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 67.99it/s, loss=1.1, v_num=7omm, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 67.48it/s, loss=1.1, v_num=7omm, ETH_val_ac\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:00<00:00, 80.71it/s, loss=1.09, v_num=7omm, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:00<00:00, 80.06it/s, loss=1.09, v_num=7omm, ETH_val_a\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:00<00:00, 89.22it/s, loss=1.04, v_num=7omm, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.023 >= min_delta = 0.003. New best score: 1.060\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:00<00:00, 88.33it/s, loss=1.04, v_num=7omm, ETH_val_a\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:00<00:00, 81.74it/s, loss=0.993, v_num=7omm, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:00<00:00, 80.68it/s, loss=0.993, v_num=7omm, ETH_val_\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:01<00:00, 77.80it/s, loss=1.04, v_num=7omm, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 1.055\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:01<00:00, 77.00it/s, loss=1.04, v_num=7omm, ETH_val_a\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:01<00:00, 77.63it/s, loss=0.978, v_num=7omm, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.019 >= min_delta = 0.003. New best score: 1.036\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:01<00:00, 76.96it/s, loss=0.978, v_num=7omm, ETH_val_\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:01<00:00, 70.77it/s, loss=0.941, v_num=7omm, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:01<00:00, 69.81it/s, loss=0.941, v_num=7omm, ETH_val_\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:01<00:00, 62.06it/s, loss=0.999, v_num=7omm, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:01<00:00, 61.51it/s, loss=0.999, v_num=7omm, ETH_val_\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:01<00:00, 74.38it/s, loss=0.998, v_num=7omm, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:01<00:00, 73.77it/s, loss=0.998, v_num=7omm, ETH_val_\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:00<00:00, 86.21it/s, loss=0.945, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:00<00:00, 85.38it/s, loss=0.945, v_num=7omm, ETH_val\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:00<00:00, 88.23it/s, loss=0.918, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.052 >= min_delta = 0.003. New best score: 0.984\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:00<00:00, 87.47it/s, loss=0.918, v_num=7omm, ETH_val\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:00<00:00, 84.65it/s, loss=0.934, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:00<00:00, 83.90it/s, loss=0.934, v_num=7omm, ETH_val\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:01<00:00, 76.25it/s, loss=0.996, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:01<00:00, 75.66it/s, loss=0.996, v_num=7omm, ETH_val\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:00<00:00, 87.50it/s, loss=0.946, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:00<00:00, 86.67it/s, loss=0.946, v_num=7omm, ETH_val\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:00<00:00, 88.94it/s, loss=0.946, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:00<00:00, 88.13it/s, loss=0.946, v_num=7omm, ETH_val\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:00<00:00, 88.39it/s, loss=0.974, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:00<00:00, 87.49it/s, loss=0.974, v_num=7omm, ETH_val\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:00<00:00, 86.07it/s, loss=0.935, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:00<00:00, 85.33it/s, loss=0.935, v_num=7omm, ETH_val\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:00<00:00, 89.20it/s, loss=0.994, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:00<00:00, 88.44it/s, loss=0.994, v_num=7omm, ETH_val\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:00<00:00, 88.62it/s, loss=0.976, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:00<00:00, 87.85it/s, loss=0.976, v_num=7omm, ETH_val\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:00<00:00, 91.05it/s, loss=0.913, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:00<00:00, 90.19it/s, loss=0.913, v_num=7omm, ETH_val\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:00<00:00, 88.18it/s, loss=0.943, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:00<00:00, 87.36it/s, loss=0.943, v_num=7omm, ETH_val\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:01<00:00, 69.94it/s, loss=0.934, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:01<00:00, 69.37it/s, loss=0.934, v_num=7omm, ETH_val\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:01<00:00, 69.75it/s, loss=0.949, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:01<00:00, 69.26it/s, loss=0.949, v_num=7omm, ETH_val\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:00<00:00, 84.73it/s, loss=0.945, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:00<00:00, 84.03it/s, loss=0.945, v_num=7omm, ETH_val\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:00<00:00, 91.76it/s, loss=0.983, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:00<00:00, 90.93it/s, loss=0.983, v_num=7omm, ETH_val\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:00<00:00, 87.78it/s, loss=0.942, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:00<00:00, 86.99it/s, loss=0.942, v_num=7omm, ETH_val\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 80/80 [00:00<00:00, 90.15it/s, loss=0.892, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 80/80 [00:00<00:00, 89.36it/s, loss=0.892, v_num=7omm, ETH_val\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:00<00:00, 87.03it/s, loss=1, v_num=7omm, ETH_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:00<00:00, 86.26it/s, loss=1, v_num=7omm, ETH_val_acc\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:00<00:00, 89.36it/s, loss=0.944, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:00<00:00, 88.57it/s, loss=0.944, v_num=7omm, ETH_val\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 80/80 [00:00<00:00, 88.20it/s, loss=0.961, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 80/80 [00:00<00:00, 87.42it/s, loss=0.961, v_num=7omm, ETH_val\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 80/80 [00:00<00:00, 86.67it/s, loss=0.898, v_num=7omm, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 80/80 [00:00<00:00, 85.89it/s, loss=0.898, v_num=7omm, ETH_val\u001b[A\n",
      "                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.984. Signaling Trainer to stop.\n",
      "Epoch 31: 100%|â–ˆ| 80/80 [00:00<00:00, 85.60it/s, loss=0.898, v_num=7omm, ETH_val\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 126.58it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.5161290168762207,\n",
      " 'ETH_test_f1': 0.5179212093353271,\n",
      " 'test_loss': 0.7952907681465149}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 66470\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011409-2bbz7omm/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011409-2bbz7omm/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.4375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.419\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.94675\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 31\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2528\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462489\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 114\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.49089\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.45903\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.91355\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.06603\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.51613\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.51792\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.79529\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–„â–„â–…â–ƒâ–…â–„â–…â–…â–‡â–ƒâ–‡â–…â–‚â–„â–‚â–‚â–„â–†â–‡â–ƒâ–„â–‡â–…â–ˆâ–…â–â–†â–„â–„â–‡â–†â–ƒâ–…â–‡â–†â–…â–†â–…â–ˆâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–â–ƒâ–„â–ƒâ–„â–‚â–…â–†â–‡â–‚â–…â–‚â–‚â–‚â–‚â–‚â–‚â–†â–‡â–ƒâ–ƒâ–‡â–ƒâ–ˆâ–…â–â–†â–ƒâ–ƒâ–†â–†â–‚â–„â–‡â–…â–„â–…â–…â–ˆâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–†â–…â–†â–…â–…â–„â–ƒâ–‚â–‚â–„â–„â–…â–ˆâ–„â–‡â–†â–„â–„â–‚â–…â–„â–â–…â–ƒâ–„â–†â–‚â–„â–„â–ƒâ–â–…â–ƒâ–â–‚â–ƒâ–‚â–‚â–‚â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–â–â–ƒâ–„â–„â–…â–†â–†â–‡â–‡â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–â–â–â–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–‡â–†â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–ˆâ–‡â–†â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–ˆâ–â–„â–„â–â–„â–†â–ƒâ–„â–ƒâ–†â–ƒâ–„â–†â–†â–„â–„â–„â–†â–„â–„â–„â–â–„â–†â–„â–†â–„â–„â–â–â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–ˆâ–â–…â–…â–â–…â–‡â–„â–…â–„â–‡â–„â–…â–‡â–‡â–…â–…â–…â–‡â–…â–…â–…â–â–…â–‡â–…â–‡â–…â–…â–â–â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–„â–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‡â–„â–†â–â–‚â–„â–‚â–…â–„â–†â–‡â–†â–‡â–…â–ƒâ–„â–…â–†â–„â–‚â–ˆâ–…â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_single_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2bbz7omm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:15:04.086735: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_single_lstm_loss_weighted_multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/rhannaj0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011502-rhannaj0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 195   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.8 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|â–‰| 80/81 [00:00<00:00, 91.85it/s, loss=1.12, v_num=naj0, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.077\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:00<00:00, 88.96it/s, loss=1.12, v_num=naj0, ETH_val_a\n",
      "Epoch 1:  99%|â–‰| 80/81 [00:01<00:00, 76.68it/s, loss=1.11, v_num=naj0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:01<00:00, 74.77it/s, loss=1.11, v_num=naj0, ETH_val_a\u001b[A\n",
      "Epoch 2:  99%|â–‰| 80/81 [00:00<00:00, 83.68it/s, loss=1.11, v_num=naj0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:00<00:00, 81.80it/s, loss=1.11, v_num=naj0, ETH_val_a\u001b[A\n",
      "Epoch 3:  99%|â–‰| 80/81 [00:00<00:00, 90.50it/s, loss=1.09, v_num=naj0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:00<00:00, 88.08it/s, loss=1.09, v_num=naj0, ETH_val_a\u001b[A\n",
      "Epoch 4:  99%|â–‰| 80/81 [00:00<00:00, 90.75it/s, loss=1.1, v_num=naj0, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:00<00:00, 88.18it/s, loss=1.1, v_num=naj0, ETH_val_ac\u001b[A\n",
      "Epoch 5:  99%|â–‰| 80/81 [00:00<00:00, 92.76it/s, loss=1.1, v_num=naj0, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:00<00:00, 90.15it/s, loss=1.1, v_num=naj0, ETH_val_ac\u001b[A\n",
      "Epoch 6:  99%|â–‰| 80/81 [00:00<00:00, 90.00it/s, loss=1.11, v_num=naj0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:00<00:00, 87.67it/s, loss=1.11, v_num=naj0, ETH_val_a\u001b[A\n",
      "Epoch 7:  99%|â–‰| 80/81 [00:00<00:00, 88.06it/s, loss=1.1, v_num=naj0, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:00<00:00, 85.90it/s, loss=1.1, v_num=naj0, ETH_val_ac\u001b[A\n",
      "Epoch 8:  99%|â–‰| 80/81 [00:00<00:00, 91.54it/s, loss=1.1, v_num=naj0, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:00<00:00, 89.11it/s, loss=1.1, v_num=naj0, ETH_val_ac\u001b[A\n",
      "Epoch 9:  99%|â–‰| 80/81 [00:00<00:00, 92.64it/s, loss=1.1, v_num=naj0, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:00<00:00, 90.01it/s, loss=1.1, v_num=naj0, ETH_val_ac\u001b[A\n",
      "Epoch 10:  99%|â–‰| 80/81 [00:00<00:00, 88.77it/s, loss=1.11, v_num=naj0, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:00<00:00, 86.54it/s, loss=1.11, v_num=naj0, ETH_val_\u001b[A\n",
      "Epoch 11:  99%|â–‰| 80/81 [00:00<00:00, 92.78it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:00<00:00, 90.16it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n",
      "Epoch 12:  99%|â–‰| 80/81 [00:00<00:00, 93.41it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:00<00:00, 90.98it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n",
      "Epoch 13:  99%|â–‰| 80/81 [00:00<00:00, 91.78it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:00<00:00, 89.42it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n",
      "Epoch 14:  99%|â–‰| 80/81 [00:00<00:00, 92.70it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:00<00:00, 90.24it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n",
      "Epoch 15:  99%|â–‰| 80/81 [00:00<00:00, 90.87it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:00<00:00, 82.92it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n",
      "Epoch 16:  99%|â–‰| 80/81 [00:00<00:00, 93.03it/s, loss=1.11, v_num=naj0, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:00<00:00, 90.54it/s, loss=1.11, v_num=naj0, ETH_val_\u001b[A\n",
      "Epoch 17:  99%|â–‰| 80/81 [00:00<00:00, 91.56it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:00<00:00, 87.83it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n",
      "Epoch 18:  99%|â–‰| 80/81 [00:00<00:00, 88.70it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:00<00:00, 86.34it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n",
      "Epoch 19:  99%|â–‰| 80/81 [00:00<00:00, 93.11it/s, loss=1.11, v_num=naj0, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:00<00:00, 90.58it/s, loss=1.11, v_num=naj0, ETH_val_\u001b[A\n",
      "Epoch 20:  99%|â–‰| 80/81 [00:00<00:00, 93.08it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:00<00:00, 90.60it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n",
      "                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.077. Signaling Trainer to stop.\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:00<00:00, 90.27it/s, loss=1.1, v_num=naj0, ETH_val_a\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 141.71it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.4193548262119293,\n",
      " 'ETH_test_f1': 0.19648092985153198,\n",
      " 'test_loss': 1.124062180519104}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 66637\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011502-rhannaj0/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011502-rhannaj0/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.1875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.19024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.11129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1680\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 28\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462530\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.33964\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.30714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09908\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.2381\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.11437\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.41935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.12406\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–†â–…â–„â–‚â–…â–‚â–‚â–†â–â–…â–â–„â–â–â–â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–…â–‚â–ˆâ–ƒâ–ƒâ–„â–‚â–â–‚â–ƒâ–ƒâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–…â–†â–…â–â–†â–ƒâ–…â–ˆâ–‚â–†â–â–†â–â–â–â–…â–ƒâ–ƒâ–‚â–ƒâ–„â–‡â–ƒâ–ˆâ–„â–…â–…â–ƒâ–‚â–„â–ƒâ–„â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–„â–â–ƒâ–†â–†â–†â–‡â–„â–ˆâ–ƒâ–‡â–…â–†â–†â–‡â–„â–…â–…â–…â–‡â–†â–„â–…â–„â–…â–†â–…â–…â–‡â–‡â–†â–‡â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–„â–â–ƒâ–ˆâ–…â–‚â–â–‚â–„â–…â–…â–„â–„â–ƒâ–‚â–…â–…â–…â–…â–‚â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–ƒâ–ƒâ–…â–‡â–ˆâ–ƒâ–â–„â–…â–‡â–‡â–…â–…â–…â–„â–‡â–…â–‡â–ˆâ–ƒâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–…â–„â–â–‚â–ƒâ–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–ˆâ–â–ˆâ–ˆâ–â–â–â–ƒâ–â–â–â–†â–â–â–â–ˆâ–â–â–â–â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–…â–â–…â–ˆâ–â–â–â–„â–â–â–â–†â–â–â–â–…â–â–â–â–â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–â–ˆâ–‚â–„â–…â–…â–†â–„â–…â–…â–„â–ƒâ–…â–…â–…â–„â–†â–…â–…â–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_single_lstm_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/rhannaj0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:15:40.917349: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/27tapgej\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011539-27tapgej\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:01<00:00, 40.03it/s, loss=0.683, v_num=pgej, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.658\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:02<00:00, 39.79it/s, loss=0.683, v_num=pgej, ETH_val_\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 41.82it/s, loss=0.637, v_num=pgej, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.101 >= min_delta = 0.003. New best score: 0.557\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 41.57it/s, loss=0.637, v_num=pgej, ETH_val_\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 40.66it/s, loss=0.595, v_num=pgej, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.548\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 40.42it/s, loss=0.595, v_num=pgej, ETH_val_\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:01<00:00, 40.80it/s, loss=0.581, v_num=pgej, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.030 >= min_delta = 0.003. New best score: 0.518\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:01<00:00, 40.54it/s, loss=0.581, v_num=pgej, ETH_val_\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 40.28it/s, loss=0.611, v_num=pgej, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:02<00:00, 39.98it/s, loss=0.611, v_num=pgej, ETH_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.501\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:02<00:00, 38.84it/s, loss=0.593, v_num=pgej, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:02<00:00, 38.62it/s, loss=0.593, v_num=pgej, ETH_val_\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:02<00:00, 36.97it/s, loss=0.599, v_num=pgej, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.033 >= min_delta = 0.003. New best score: 0.468\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:02<00:00, 36.72it/s, loss=0.599, v_num=pgej, ETH_val_\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:03<00:00, 26.16it/s, loss=0.553, v_num=pgej, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:03<00:00, 26.06it/s, loss=0.553, v_num=pgej, ETH_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 0.465\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:02<00:00, 34.01it/s, loss=0.606, v_num=pgej, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:02<00:00, 33.83it/s, loss=0.606, v_num=pgej, ETH_val_\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:02<00:00, 34.38it/s, loss=0.574, v_num=pgej, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:02<00:00, 34.22it/s, loss=0.574, v_num=pgej, ETH_val_\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:02<00:00, 28.44it/s, loss=0.565, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 0.451\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:02<00:00, 28.30it/s, loss=0.565, v_num=pgej, ETH_val\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:02<00:00, 33.05it/s, loss=0.573, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:02<00:00, 32.82it/s, loss=0.573, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:01<00:00, 40.35it/s, loss=0.571, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:01<00:00, 40.05it/s, loss=0.571, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:01<00:00, 41.59it/s, loss=0.579, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:01<00:00, 41.29it/s, loss=0.579, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:01<00:00, 40.72it/s, loss=0.605, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:01<00:00, 40.39it/s, loss=0.605, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:02<00:00, 30.46it/s, loss=0.56, v_num=pgej, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:02<00:00, 30.33it/s, loss=0.56, v_num=pgej, ETH_val_\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:02<00:00, 28.79it/s, loss=0.583, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:02<00:00, 28.59it/s, loss=0.583, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:02<00:00, 30.13it/s, loss=0.574, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:02<00:00, 29.97it/s, loss=0.574, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:01<00:00, 41.32it/s, loss=0.537, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:01<00:00, 41.04it/s, loss=0.537, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:02<00:00, 29.57it/s, loss=0.613, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:02<00:00, 29.44it/s, loss=0.613, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:02<00:00, 35.90it/s, loss=0.58, v_num=pgej, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:02<00:00, 35.72it/s, loss=0.58, v_num=pgej, ETH_val_\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:02<00:00, 30.07it/s, loss=0.578, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:02<00:00, 29.92it/s, loss=0.578, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:02<00:00, 39.17it/s, loss=0.565, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:02<00:00, 38.94it/s, loss=0.565, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:02<00:00, 38.69it/s, loss=0.532, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:02<00:00, 38.44it/s, loss=0.532, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:02<00:00, 31.93it/s, loss=0.562, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:02<00:00, 31.76it/s, loss=0.562, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:02<00:00, 39.14it/s, loss=0.574, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:02<00:00, 38.93it/s, loss=0.574, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:02<00:00, 29.20it/s, loss=0.553, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:02<00:00, 29.06it/s, loss=0.553, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 80/80 [00:02<00:00, 37.54it/s, loss=0.534, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 80/80 [00:02<00:00, 37.06it/s, loss=0.534, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:02<00:00, 28.82it/s, loss=0.559, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:02<00:00, 28.69it/s, loss=0.559, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:01<00:00, 41.97it/s, loss=0.553, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:01<00:00, 41.61it/s, loss=0.553, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 80/80 [00:02<00:00, 37.06it/s, loss=0.531, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.020 >= min_delta = 0.003. New best score: 0.431\n",
      "Epoch 30: 100%|â–ˆ| 80/80 [00:02<00:00, 36.80it/s, loss=0.531, v_num=pgej, ETH_val\n",
      "Epoch 31: 100%|â–ˆ| 80/80 [00:02<00:00, 32.38it/s, loss=0.592, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.073 >= min_delta = 0.003. New best score: 0.359\n",
      "Epoch 31: 100%|â–ˆ| 80/80 [00:02<00:00, 32.22it/s, loss=0.592, v_num=pgej, ETH_val\n",
      "Epoch 32: 100%|â–ˆ| 80/80 [00:02<00:00, 32.58it/s, loss=0.552, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 80/80 [00:02<00:00, 32.39it/s, loss=0.552, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 80/80 [00:02<00:00, 34.89it/s, loss=0.553, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 80/80 [00:02<00:00, 34.67it/s, loss=0.553, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 80/80 [00:02<00:00, 30.26it/s, loss=0.587, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 80/80 [00:02<00:00, 30.12it/s, loss=0.587, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 80/80 [00:02<00:00, 34.26it/s, loss=0.581, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 80/80 [00:02<00:00, 34.07it/s, loss=0.581, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 80/80 [00:02<00:00, 38.29it/s, loss=0.558, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 80/80 [00:02<00:00, 38.08it/s, loss=0.558, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 37: 100%|â–ˆ| 80/80 [00:02<00:00, 36.69it/s, loss=0.519, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|â–ˆ| 80/80 [00:02<00:00, 36.47it/s, loss=0.519, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 38: 100%|â–ˆ| 80/80 [00:02<00:00, 35.28it/s, loss=0.518, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|â–ˆ| 80/80 [00:02<00:00, 35.10it/s, loss=0.518, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 39: 100%|â–ˆ| 80/80 [00:02<00:00, 30.28it/s, loss=0.575, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|â–ˆ| 80/80 [00:02<00:00, 30.09it/s, loss=0.575, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 40: 100%|â–ˆ| 80/80 [00:02<00:00, 34.20it/s, loss=0.554, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|â–ˆ| 80/80 [00:02<00:00, 33.99it/s, loss=0.554, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 41: 100%|â–ˆ| 80/80 [00:02<00:00, 34.68it/s, loss=0.551, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|â–ˆ| 80/80 [00:02<00:00, 34.47it/s, loss=0.551, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 42: 100%|â–ˆ| 80/80 [00:02<00:00, 36.45it/s, loss=0.558, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|â–ˆ| 80/80 [00:02<00:00, 36.25it/s, loss=0.558, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 43: 100%|â–ˆ| 80/80 [00:02<00:00, 35.17it/s, loss=0.544, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|â–ˆ| 80/80 [00:02<00:00, 34.89it/s, loss=0.544, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 44: 100%|â–ˆ| 80/80 [00:02<00:00, 36.29it/s, loss=0.504, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|â–ˆ| 80/80 [00:02<00:00, 36.09it/s, loss=0.504, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 45: 100%|â–ˆ| 80/80 [00:02<00:00, 39.42it/s, loss=0.537, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|â–ˆ| 80/80 [00:02<00:00, 39.17it/s, loss=0.537, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 46: 100%|â–ˆ| 80/80 [00:02<00:00, 35.24it/s, loss=0.546, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|â–ˆ| 80/80 [00:02<00:00, 35.00it/s, loss=0.546, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 47: 100%|â–ˆ| 80/80 [00:02<00:00, 32.20it/s, loss=0.493, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|â–ˆ| 80/80 [00:02<00:00, 32.03it/s, loss=0.493, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 48: 100%|â–ˆ| 80/80 [00:02<00:00, 34.30it/s, loss=0.438, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|â–ˆ| 80/80 [00:02<00:00, 34.09it/s, loss=0.438, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 49: 100%|â–ˆ| 80/80 [00:02<00:00, 28.12it/s, loss=0.463, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|â–ˆ| 80/80 [00:02<00:00, 28.00it/s, loss=0.463, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 50: 100%|â–ˆ| 80/80 [00:02<00:00, 38.91it/s, loss=0.486, v_num=pgej, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|â–ˆ| 80/80 [00:02<00:00, 38.64it/s, loss=0.486, v_num=pgej, ETH_val\u001b[A\n",
      "Epoch 51: 100%|â–ˆ| 80/80 [00:02<00:00, 32.49it/s, loss=0.47, v_num=pgej, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.359. Signaling Trainer to stop.\n",
      "Epoch 51: 100%|â–ˆ| 80/80 [00:02<00:00, 32.22it/s, loss=0.47, v_num=pgej, ETH_val_\n",
      "Epoch 51: 100%|â–ˆ| 80/80 [00:02<00:00, 32.17it/s, loss=0.47, v_num=pgej, ETH_val_\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 68.84it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.6774193644523621,\n",
      " 'ETH_test_f1': 0.6737711429595947,\n",
      " 'test_loss': 0.5348719358444214}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 66771\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011539-27tapgej/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011539-27tapgej/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.6875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.68627\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.48098\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 51\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4108\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 130\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462669\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 186\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.76089\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.74942\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.48915\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.77778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.57336\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.67742\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.67377\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.53487\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–‚â–„â–†â–…â–…â–‚â–ƒâ–…â–…â–„â–„â–…â–…â–„â–…â–†â–‡â–„â–„â–‡â–…â–†â–…â–‡â–‡â–…â–…â–†â–ˆâ–†â–…â–…â–‡â–…â–‡â–„â–…â–â–‡â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–ƒâ–„â–†â–…â–…â–‚â–ƒâ–…â–…â–„â–„â–…â–…â–ƒâ–…â–†â–‡â–„â–„â–‡â–…â–†â–…â–‡â–‡â–…â–…â–†â–ˆâ–†â–…â–…â–‡â–…â–‡â–„â–…â–â–‡â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–†â–…â–ƒâ–„â–„â–‡â–…â–ƒâ–„â–…â–ˆâ–„â–„â–‡â–„â–ƒâ–ƒâ–†â–„â–‚â–…â–ƒâ–„â–ƒâ–‚â–…â–†â–ƒâ–â–„â–†â–ƒâ–‚â–ƒâ–‚â–…â–ƒâ–‡â–‚â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–ƒâ–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–â–ƒâ–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–‡â–…â–„â–„â–…â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–„â–„â–„â–‡â–…â–‡â–…â–‡â–‡â–…â–‚â–„â–…â–‚â–…â–‡â–„â–‚â–…â–‚â–‚â–â–„â–‡â–‡â–…â–‡â–‚â–â–…â–‡â–ˆâ–‚â–‡â–‚â–‡â–…â–…â–‚â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–„â–„â–„â–‡â–…â–‡â–…â–‡â–‡â–…â–‚â–„â–…â–‚â–…â–‡â–„â–‚â–…â–‚â–‚â–â–„â–‡â–‡â–…â–‡â–‚â–â–…â–‡â–ˆâ–‚â–‡â–‚â–‡â–…â–…â–‚â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–†â–…â–…â–„â–„â–ƒâ–„â–ƒâ–…â–…â–„â–„â–†â–„â–„â–…â–†â–„â–…â–…â–…â–„â–ƒâ–â–ƒâ–‚â–„â–†â–ƒâ–ƒâ–‚â–†â–ƒâ–†â–ƒâ–…â–…â–‡â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/27tapgej\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:18:04.102915: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_loss_weighted_binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2j95iuew\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011802-2j95iuew\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/81 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 80/81 [00:01<00:00, 43.07it/s, loss=0.692, v_num=iuew, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.710\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:01<00:00, 42.40it/s, loss=0.692, v_num=iuew, ETH_val_\n",
      "Epoch 1:  99%|â–‰| 80/81 [00:02<00:00, 36.02it/s, loss=0.71, v_num=iuew, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.018 >= min_delta = 0.003. New best score: 0.692\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:02<00:00, 35.72it/s, loss=0.71, v_num=iuew, ETH_val_a\n",
      "Epoch 2:  99%|â–‰| 80/81 [00:01<00:00, 41.70it/s, loss=0.695, v_num=iuew, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 0.681\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:01<00:00, 41.09it/s, loss=0.695, v_num=iuew, ETH_val_\n",
      "Epoch 3:  99%|â–‰| 80/81 [00:01<00:00, 40.57it/s, loss=0.689, v_num=iuew, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:02<00:00, 40.03it/s, loss=0.689, v_num=iuew, ETH_val_\u001b[A\n",
      "Epoch 4:  99%|â–‰| 80/81 [00:02<00:00, 35.81it/s, loss=0.69, v_num=iuew, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.025 >= min_delta = 0.003. New best score: 0.656\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:02<00:00, 35.59it/s, loss=0.69, v_num=iuew, ETH_val_a\n",
      "Epoch 5:  99%|â–‰| 80/81 [00:02<00:00, 39.07it/s, loss=0.7, v_num=iuew, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:02<00:00, 38.72it/s, loss=0.7, v_num=iuew, ETH_val_ac\u001b[A\n",
      "Epoch 6:  99%|â–‰| 80/81 [00:02<00:00, 39.71it/s, loss=0.697, v_num=iuew, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.027 >= min_delta = 0.003. New best score: 0.629\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:02<00:00, 39.17it/s, loss=0.697, v_num=iuew, ETH_val_\n",
      "Epoch 7:  99%|â–‰| 80/81 [00:02<00:00, 31.61it/s, loss=0.698, v_num=iuew, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:02<00:00, 31.30it/s, loss=0.698, v_num=iuew, ETH_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 80/81 [00:02<00:00, 28.73it/s, loss=0.698, v_num=iuew, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:02<00:00, 28.59it/s, loss=0.698, v_num=iuew, ETH_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 80/81 [00:02<00:00, 33.41it/s, loss=0.698, v_num=iuew, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:02<00:00, 33.26it/s, loss=0.698, v_num=iuew, ETH_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.622\n",
      "Epoch 10:  99%|â–‰| 80/81 [00:02<00:00, 37.30it/s, loss=0.694, v_num=iuew, ETH_val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:02<00:00, 36.88it/s, loss=0.694, v_num=iuew, ETH_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 80/81 [00:02<00:00, 33.53it/s, loss=0.695, v_num=iuew, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:02<00:00, 33.40it/s, loss=0.695, v_num=iuew, ETH_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 80/81 [00:02<00:00, 30.21it/s, loss=0.694, v_num=iuew, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:02<00:00, 30.02it/s, loss=0.694, v_num=iuew, ETH_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 80/81 [00:02<00:00, 34.49it/s, loss=0.697, v_num=iuew, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:02<00:00, 34.16it/s, loss=0.697, v_num=iuew, ETH_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 80/81 [00:02<00:00, 37.93it/s, loss=0.691, v_num=iuew, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:02<00:00, 37.39it/s, loss=0.691, v_num=iuew, ETH_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 80/81 [00:02<00:00, 34.94it/s, loss=0.692, v_num=iuew, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:02<00:00, 33.90it/s, loss=0.692, v_num=iuew, ETH_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 80/81 [00:02<00:00, 39.89it/s, loss=0.697, v_num=iuew, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:02<00:00, 39.58it/s, loss=0.697, v_num=iuew, ETH_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 80/81 [00:02<00:00, 36.76it/s, loss=0.7, v_num=iuew, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:02<00:00, 36.55it/s, loss=0.7, v_num=iuew, ETH_val_a\u001b[A\n",
      "Epoch 18:  99%|â–‰| 80/81 [00:02<00:00, 32.06it/s, loss=0.695, v_num=iuew, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:02<00:00, 31.82it/s, loss=0.695, v_num=iuew, ETH_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 80/81 [00:02<00:00, 38.79it/s, loss=0.693, v_num=iuew, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:02<00:00, 38.38it/s, loss=0.693, v_num=iuew, ETH_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 80/81 [00:02<00:00, 35.50it/s, loss=0.689, v_num=iuew, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:02<00:00, 35.20it/s, loss=0.689, v_num=iuew, ETH_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 80/81 [00:02<00:00, 27.60it/s, loss=0.702, v_num=iuew, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:02<00:00, 27.55it/s, loss=0.702, v_num=iuew, ETH_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 80/81 [00:02<00:00, 38.49it/s, loss=0.697, v_num=iuew, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 81/81 [00:02<00:00, 38.17it/s, loss=0.697, v_num=iuew, ETH_val\u001b[A\n",
      "Epoch 23:  99%|â–‰| 80/81 [00:02<00:00, 28.60it/s, loss=0.698, v_num=iuew, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 81/81 [00:03<00:00, 26.70it/s, loss=0.698, v_num=iuew, ETH_val\u001b[A\n",
      "Epoch 24:  99%|â–‰| 80/81 [00:02<00:00, 31.31it/s, loss=0.692, v_num=iuew, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 81/81 [00:02<00:00, 31.22it/s, loss=0.692, v_num=iuew, ETH_val\u001b[A\n",
      "Epoch 25:  99%|â–‰| 80/81 [00:02<00:00, 35.34it/s, loss=0.694, v_num=iuew, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 81/81 [00:02<00:00, 35.18it/s, loss=0.694, v_num=iuew, ETH_val\u001b[A\n",
      "Epoch 26:  99%|â–‰| 80/81 [00:02<00:00, 26.96it/s, loss=0.692, v_num=iuew, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 81/81 [00:03<00:00, 26.85it/s, loss=0.692, v_num=iuew, ETH_val\u001b[A\n",
      "Epoch 27:  99%|â–‰| 80/81 [00:03<00:00, 26.45it/s, loss=0.694, v_num=iuew, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 81/81 [00:03<00:00, 26.27it/s, loss=0.694, v_num=iuew, ETH_val\u001b[A\n",
      "Epoch 28:  99%|â–‰| 80/81 [00:02<00:00, 27.71it/s, loss=0.69, v_num=iuew, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 81/81 [00:02<00:00, 27.58it/s, loss=0.69, v_num=iuew, ETH_val_\u001b[A\n",
      "Epoch 29:  99%|â–‰| 80/81 [00:02<00:00, 34.90it/s, loss=0.694, v_num=iuew, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 81/81 [00:02<00:00, 34.70it/s, loss=0.694, v_num=iuew, ETH_val\u001b[A\n",
      "                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.622. Signaling Trainer to stop.\n",
      "Epoch 29: 100%|â–ˆ| 81/81 [00:02<00:00, 34.64it/s, loss=0.694, v_num=iuew, ETH_val\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 94.47it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.6129032373428345,\n",
      " 'ETH_test_f1': 0.3793548345565796,\n",
      " 'test_loss': 0.6798999905586243}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 67132\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011802-2j95iuew/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011802-2j95iuew/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.58333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.68506\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2400\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 81\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462763\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 108\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.48385\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.45631\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69572\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.88889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.47059\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.37935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.6799\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–â–…â–„â–…â–„â–†â–ƒâ–ƒâ–…â–†â–…â–…â–†â–ƒâ–…â–…â–…â–‚â–…â–…â–†â–…â–„â–„â–‡â–‚â–†â–ƒâ–‡â–…â–…â–†â–ƒâ–†â–…â–„â–…â–ˆâ–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–â–ƒâ–ƒâ–†â–„â–‡â–‚â–ƒâ–†â–‡â–†â–…â–…â–„â–†â–…â–…â–‚â–„â–ƒâ–‡â–„â–„â–„â–ˆâ–‚â–‡â–„â–ˆâ–†â–„â–‡â–‚â–„â–…â–„â–…â–ˆâ–†â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–ˆâ–…â–†â–ƒâ–†â–„â–„â–…â–„â–„â–ƒâ–ƒâ–â–‡â–‚â–ƒâ–„â–†â–…â–ƒâ–„â–ƒâ–„â–…â–ƒâ–…â–„â–„â–ƒâ–„â–…â–‚â–„â–ƒâ–ƒâ–„â–„â–ƒâ–„â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–†â–‚â–†â–‚â–‡â–„â–…â–ƒâ–ƒâ–…â–†â–…â–„â–ƒâ–„â–‡â–…â–‚â–‚â–ˆâ–†â–„â–„â–„â–ƒâ–ƒâ–ˆâ–„â–…â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–ƒâ–ƒâ–†â–ƒâ–‡â–…â–†â–‚â–†â–ƒâ–…â–…â–…â–‚â–†â–ˆâ–‡â–‚â–‚â–ƒâ–‡â–…â–„â–â–â–‚â–†â–…â–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–…â–„â–„â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–‡â–†â–†â–„â–„â–‚â–…â–ƒâ–â–…â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–…â–…â–†â–†â–†â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2j95iuew\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:19:39.235906: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/38lcyiu0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011937-38lcyiu0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Training: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆ| 80/80 [00:02<00:00, 39.57it/s, loss=1.11, v_num=yiu0, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.092\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:02<00:00, 39.33it/s, loss=1.11, v_num=yiu0, ETH_val_a\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:02<00:00, 39.80it/s, loss=1.11, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.024 >= min_delta = 0.003. New best score: 1.068\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:02<00:00, 39.54it/s, loss=1.11, v_num=yiu0, ETH_val_a\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:02<00:00, 32.97it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:02<00:00, 32.78it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:02<00:00, 38.22it/s, loss=1.11, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:02<00:00, 37.98it/s, loss=1.11, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:02<00:00, 30.70it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:02<00:00, 30.54it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:02<00:00, 31.83it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:02<00:00, 31.69it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:02<00:00, 39.82it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:02<00:00, 39.59it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:02<00:00, 39.56it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:02<00:00, 39.33it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:01<00:00, 40.35it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:01<00:00, 40.11it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:02<00:00, 36.53it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:02<00:00, 36.33it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:02<00:00, 39.97it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:02<00:00, 39.74it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:02<00:00, 33.63it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:02<00:00, 33.42it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:02<00:00, 31.64it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:02<00:00, 31.48it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:02<00:00, 38.15it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:02<00:00, 37.92it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:02<00:00, 38.51it/s, loss=1.11, v_num=yiu0, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:02<00:00, 38.26it/s, loss=1.11, v_num=yiu0, ETH_val_\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:02<00:00, 29.28it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:02<00:00, 29.16it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:02<00:00, 29.67it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:02<00:00, 29.49it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:02<00:00, 35.41it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:02<00:00, 35.19it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:02<00:00, 36.54it/s, loss=1.09, v_num=yiu0, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:02<00:00, 36.34it/s, loss=1.09, v_num=yiu0, ETH_val_\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:02<00:00, 28.98it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:02<00:00, 28.86it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:02<00:00, 26.76it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:03<00:00, 26.63it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:03<00:00, 25.94it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:03<00:00, 25.85it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n",
      "                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.068. Signaling Trainer to stop.\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:03<00:00, 25.81it/s, loss=1.1, v_num=yiu0, ETH_val_a\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 91.08it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.4838709533214569,\n",
      " 'ETH_test_f1': 0.2169238030910492,\n",
      " 'test_loss': 1.1012004613876343}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 67370\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011937-38lcyiu0/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011937-38lcyiu0/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.17172\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.11422\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1738\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 61\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462838\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 78\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.42993\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.29497\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09807\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.26667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.08673\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.48387\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.21692\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.1012\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–â–†â–†â–ˆâ–‡â–‡â–‚â–ƒâ–…â–â–ƒâ–‚â–ƒâ–…â–†â–…â–ƒâ–â–‡â–†â–ƒâ–‚â–…â–ˆâ–ƒâ–‚â–…â–ˆâ–…â–ƒâ–ƒâ–ˆâ–‡â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–â–„â–…â–†â–ˆâ–…â–‚â–…â–„â–‚â–ƒâ–‚â–ƒâ–„â–…â–‚â–…â–â–‡â–…â–„â–ƒâ–‚â–†â–„â–ƒâ–ƒâ–…â–„â–‚â–ƒâ–…â–†â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–ˆâ–â–„â–‡â–ƒâ–ƒâ–†â–†â–…â–‡â–†â–†â–†â–„â–…â–…â–„â–†â–„â–…â–…â–†â–†â–ƒâ–…â–‡â–…â–…â–„â–…â–…â–„â–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–‚â–ƒâ–ƒâ–â–â–ƒâ–…â–„â–‡â–„â–ƒâ–†â–†â–†â–‡â–‡â–†â–ˆâ–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–ƒâ–â–„â–…â–â–ƒâ–‚â–†â–„â–†â–…â–„â–ˆâ–ˆâ–†â–†â–…â–â–ƒâ–†â–ƒâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–‡â–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–â–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–â–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–â–‡â–â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–…â–â–ˆâ–„â–„â–„â–„â–„â–„â–„â–†â–„â–„â–„â–„â–…â–…â–„â–…â–„â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/38lcyiu0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:20:53.070186: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_loss_weighted_multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1n4w9wbk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012051-1n4w9wbk\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 80/81 [00:01<00:00, 41.04it/s, loss=1.11, v_num=9wbk, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.066\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:01<00:00, 40.65it/s, loss=1.11, v_num=9wbk, ETH_val_a\n",
      "Epoch 1:  99%|â–‰| 80/81 [00:01<00:00, 41.54it/s, loss=1.12, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:01<00:00, 41.22it/s, loss=1.12, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 2:  99%|â–‰| 80/81 [00:01<00:00, 41.56it/s, loss=1.12, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:01<00:00, 41.22it/s, loss=1.12, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 3:  99%|â–‰| 80/81 [00:02<00:00, 37.40it/s, loss=1.1, v_num=9wbk, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:02<00:00, 37.15it/s, loss=1.1, v_num=9wbk, ETH_val_ac\u001b[A\n",
      "Epoch 4:  99%|â–‰| 80/81 [00:01<00:00, 41.56it/s, loss=1.1, v_num=9wbk, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:01<00:00, 41.18it/s, loss=1.1, v_num=9wbk, ETH_val_ac\u001b[A\n",
      "Epoch 5:  99%|â–‰| 80/81 [00:02<00:00, 38.52it/s, loss=1.11, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:02<00:00, 38.26it/s, loss=1.11, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 6:  99%|â–‰| 80/81 [00:02<00:00, 35.46it/s, loss=1.1, v_num=9wbk, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:02<00:00, 35.18it/s, loss=1.1, v_num=9wbk, ETH_val_ac\u001b[A\n",
      "Epoch 7:  99%|â–‰| 80/81 [00:01<00:00, 41.75it/s, loss=1.11, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:01<00:00, 41.25it/s, loss=1.11, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 8:  99%|â–‰| 80/81 [00:01<00:00, 42.02it/s, loss=1.11, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:01<00:00, 41.47it/s, loss=1.11, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 9:  99%|â–‰| 80/81 [00:01<00:00, 40.62it/s, loss=1.1, v_num=9wbk, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:02<00:00, 40.09it/s, loss=1.1, v_num=9wbk, ETH_val_ac\u001b[A\n",
      "Epoch 10:  99%|â–‰| 80/81 [00:02<00:00, 27.55it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:02<00:00, 27.48it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 11:  99%|â–‰| 80/81 [00:02<00:00, 31.90it/s, loss=1.09, v_num=9wbk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:02<00:00, 31.76it/s, loss=1.09, v_num=9wbk, ETH_val_\u001b[A\n",
      "Metric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 1.058\n",
      "Epoch 12:  99%|â–‰| 80/81 [00:02<00:00, 34.82it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:02<00:00, 34.58it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 13:  99%|â–‰| 80/81 [00:02<00:00, 31.02it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:02<00:00, 30.80it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 14:  99%|â–‰| 80/81 [00:02<00:00, 33.42it/s, loss=1.11, v_num=9wbk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:02<00:00, 33.14it/s, loss=1.11, v_num=9wbk, ETH_val_\u001b[A\n",
      "Epoch 15:  99%|â–‰| 80/81 [00:02<00:00, 29.12it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:02<00:00, 28.45it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 16:  99%|â–‰| 80/81 [00:02<00:00, 30.45it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:02<00:00, 30.16it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 17:  99%|â–‰| 80/81 [00:01<00:00, 40.14it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:02<00:00, 39.61it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 18:  99%|â–‰| 80/81 [00:02<00:00, 33.19it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:02<00:00, 33.02it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 19:  99%|â–‰| 80/81 [00:02<00:00, 37.66it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:02<00:00, 37.34it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 20:  99%|â–‰| 80/81 [00:02<00:00, 37.04it/s, loss=1.09, v_num=9wbk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:02<00:00, 36.76it/s, loss=1.09, v_num=9wbk, ETH_val_\u001b[A\n",
      "Epoch 21:  99%|â–‰| 80/81 [00:02<00:00, 37.40it/s, loss=1.09, v_num=9wbk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:02<00:00, 37.02it/s, loss=1.09, v_num=9wbk, ETH_val_\u001b[A\n",
      "Epoch 22:  99%|â–‰| 80/81 [00:02<00:00, 38.37it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 81/81 [00:02<00:00, 38.12it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 23:  99%|â–‰| 80/81 [00:03<00:00, 26.52it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 81/81 [00:03<00:00, 26.45it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 24:  99%|â–‰| 80/81 [00:02<00:00, 34.42it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 81/81 [00:02<00:00, 34.25it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 25:  99%|â–‰| 80/81 [00:02<00:00, 37.93it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 81/81 [00:02<00:00, 37.69it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 26:  99%|â–‰| 80/81 [00:02<00:00, 37.65it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 81/81 [00:02<00:00, 37.41it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 27:  99%|â–‰| 80/81 [00:02<00:00, 36.02it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 81/81 [00:02<00:00, 35.53it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 28:  99%|â–‰| 80/81 [00:02<00:00, 36.00it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 81/81 [00:02<00:00, 35.69it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 29:  99%|â–‰| 80/81 [00:02<00:00, 34.26it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 81/81 [00:02<00:00, 33.78it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 30:  99%|â–‰| 80/81 [00:02<00:00, 34.72it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 81/81 [00:02<00:00, 34.41it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Epoch 31:  99%|â–‰| 80/81 [00:02<00:00, 37.70it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 81/81 [00:02<00:00, 37.39it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n",
      "                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.058. Signaling Trainer to stop.\n",
      "Epoch 31: 100%|â–ˆ| 81/81 [00:02<00:00, 37.33it/s, loss=1.1, v_num=9wbk, ETH_val_a\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 91.59it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.4193548262119293,\n",
      " 'ETH_test_f1': 0.19648092985153198,\n",
      " 'test_loss': 1.093916893005371}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 67582\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012051-1n4w9wbk/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012051-1n4w9wbk/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.36667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.11474\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 31\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2560\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 82\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462933\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 115\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.32939\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.30131\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.10063\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.2381\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.09221\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.41935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.09392\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–…â–…â–…â–ƒâ–…â–ƒâ–†â–„â–„â–„â–…â–‚â–†â–†â–…â–…â–„â–ˆâ–â–„â–‚â–„â–„â–…â–ˆâ–†â–‡â–†â–…â–„â–…â–ˆâ–„â–…â–…â–…â–…â–†â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–„â–…â–…â–ƒâ–„â–ƒâ–†â–„â–ƒâ–„â–…â–‚â–ƒâ–†â–…â–…â–„â–ˆâ–â–„â–ƒâ–„â–ƒâ–„â–†â–†â–‡â–‡â–…â–„â–†â–†â–„â–†â–…â–„â–„â–…â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–„â–ˆâ–‡â–„â–†â–ˆâ–„â–‡â–…â–…â–„â–‡â–â–ƒâ–ƒâ–ƒâ–‡â–„â–†â–„â–†â–…â–…â–…â–ƒâ–…â–ƒâ–„â–†â–„â–†â–„â–†â–…â–…â–…â–†â–„â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–…â–„â–„â–„â–ƒâ–„â–‚â–â–…â–ƒâ–‡â–‡â–„â–„â–…â–†â–ƒâ–†â–…â–…â–…â–‡â–ˆâ–†â–…â–‚â–„â–ˆâ–‡â–†â–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–ƒâ–†â–ƒâ–‚â–„â–â–‚â–„â–†â–…â–…â–ˆâ–„â–†â–…â–„â–…â–ˆâ–†â–…â–„â–…â–‡â–†â–†â–ƒâ–…â–†â–ˆâ–ƒâ–ƒâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–†â–…â–…â–„â–„â–ƒâ–„â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–‚â–ƒâ–‚â–ƒâ–â–‚â–‚â–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–ˆâ–â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–ˆâ–â–ˆâ–ˆâ–ˆâ–„â–„â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–ˆâ–â–…â–…â–ˆâ–ˆâ–ˆâ–ˆâ–†â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–ˆâ–â–ˆâ–ˆâ–ˆâ–…â–…â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–‚â–ˆâ–†â–…â–…â–…â–…â–…â–…â–…â–ƒâ–â–„â–„â–„â–„â–„â–„â–„â–…â–…â–ˆâ–†â–„â–…â–„â–…â–…â–…â–„â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1n4w9wbk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:22:29.091323: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_single_lstm__binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/xcvozi1q\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012227-xcvozi1q\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 130   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.7 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/80 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 79/80 [00:01<00:00, 77.46it/s, loss=0.59, v_num=zi1q, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:01<00:00, 70.67it/s, loss=0.59, v_num=zi1q, ETH_val_a\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved. New best score: 0.535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 72.50it/s, loss=0.534, v_num=zi1q, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.048 >= min_delta = 0.003. New best score: 0.487\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 71.90it/s, loss=0.534, v_num=zi1q, ETH_val_\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 71.47it/s, loss=0.562, v_num=zi1q, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 70.77it/s, loss=0.562, v_num=zi1q, ETH_val_\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:01<00:00, 64.50it/s, loss=0.604, v_num=zi1q, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:01<00:00, 63.90it/s, loss=0.604, v_num=zi1q, ETH_val_\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 66.98it/s, loss=0.616, v_num=zi1q, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 66.51it/s, loss=0.616, v_num=zi1q, ETH_val_\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:01<00:00, 75.60it/s, loss=0.558, v_num=zi1q, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:01<00:00, 74.78it/s, loss=0.558, v_num=zi1q, ETH_val_\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:01<00:00, 65.99it/s, loss=0.617, v_num=zi1q, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:01<00:00, 65.26it/s, loss=0.617, v_num=zi1q, ETH_val_\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:01<00:00, 65.08it/s, loss=0.596, v_num=zi1q, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:01<00:00, 64.39it/s, loss=0.596, v_num=zi1q, ETH_val_\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:00<00:00, 80.32it/s, loss=0.535, v_num=zi1q, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:01<00:00, 79.67it/s, loss=0.535, v_num=zi1q, ETH_val_\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:01<00:00, 71.42it/s, loss=0.556, v_num=zi1q, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.026 >= min_delta = 0.003. New best score: 0.461\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:01<00:00, 70.85it/s, loss=0.556, v_num=zi1q, ETH_val_\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:00<00:00, 83.54it/s, loss=0.583, v_num=zi1q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:00<00:00, 82.53it/s, loss=0.583, v_num=zi1q, ETH_val\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:01<00:00, 66.51it/s, loss=0.587, v_num=zi1q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:01<00:00, 66.00it/s, loss=0.587, v_num=zi1q, ETH_val\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:01<00:00, 77.15it/s, loss=0.607, v_num=zi1q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:01<00:00, 76.08it/s, loss=0.607, v_num=zi1q, ETH_val\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:01<00:00, 68.72it/s, loss=0.606, v_num=zi1q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:01<00:00, 68.22it/s, loss=0.606, v_num=zi1q, ETH_val\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:01<00:00, 60.10it/s, loss=0.605, v_num=zi1q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:01<00:00, 59.60it/s, loss=0.605, v_num=zi1q, ETH_val\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:01<00:00, 73.69it/s, loss=0.58, v_num=zi1q, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:01<00:00, 73.09it/s, loss=0.58, v_num=zi1q, ETH_val_\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:01<00:00, 69.56it/s, loss=0.582, v_num=zi1q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:01<00:00, 69.07it/s, loss=0.582, v_num=zi1q, ETH_val\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:00<00:00, 84.38it/s, loss=0.581, v_num=zi1q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:00<00:00, 83.48it/s, loss=0.581, v_num=zi1q, ETH_val\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:01<00:00, 70.61it/s, loss=0.577, v_num=zi1q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:01<00:00, 70.07it/s, loss=0.577, v_num=zi1q, ETH_val\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:00<00:00, 82.36it/s, loss=0.537, v_num=zi1q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:00<00:00, 81.38it/s, loss=0.537, v_num=zi1q, ETH_val\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:01<00:00, 70.70it/s, loss=0.585, v_num=zi1q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:01<00:00, 70.14it/s, loss=0.585, v_num=zi1q, ETH_val\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:00<00:00, 82.81it/s, loss=0.554, v_num=zi1q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:00<00:00, 81.92it/s, loss=0.554, v_num=zi1q, ETH_val\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:01<00:00, 74.54it/s, loss=0.55, v_num=zi1q, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:01<00:00, 73.99it/s, loss=0.55, v_num=zi1q, ETH_val_\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:01<00:00, 78.29it/s, loss=0.582, v_num=zi1q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:01<00:00, 77.54it/s, loss=0.582, v_num=zi1q, ETH_val\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:01<00:00, 64.08it/s, loss=0.549, v_num=zi1q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:01<00:00, 63.47it/s, loss=0.549, v_num=zi1q, ETH_val\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:01<00:00, 67.73it/s, loss=0.574, v_num=zi1q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:01<00:00, 66.99it/s, loss=0.574, v_num=zi1q, ETH_val\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:00<00:00, 83.10it/s, loss=0.536, v_num=zi1q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:00<00:00, 82.42it/s, loss=0.536, v_num=zi1q, ETH_val\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 80/80 [00:01<00:00, 69.91it/s, loss=0.543, v_num=zi1q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 80/80 [00:01<00:00, 69.35it/s, loss=0.543, v_num=zi1q, ETH_val\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:00<00:00, 85.85it/s, loss=0.583, v_num=zi1q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:00<00:00, 85.11it/s, loss=0.583, v_num=zi1q, ETH_val\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:01<00:00, 69.56it/s, loss=0.55, v_num=zi1q, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.461. Signaling Trainer to stop.\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:01<00:00, 69.04it/s, loss=0.55, v_num=zi1q, ETH_val_\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:01<00:00, 68.84it/s, loss=0.55, v_num=zi1q, ETH_val_\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 111.16it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.7096773982048035,\n",
      " 'ETH_test_f1': 0.7085039019584656,\n",
      " 'test_loss': 0.5299782752990723}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 67846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012227-xcvozi1q/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012227-xcvozi1q/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.9375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.92271\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.42581\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2370\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462989\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 107\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.70942\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.69617\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.56198\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.77778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.48858\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.70968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.7085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.52998\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–ƒâ–‡â–ƒâ–ƒâ–†â–‚â–‡â–†â–…â–…â–…â–„â–‚â–…â–â–†â–„â–‚â–„â–ƒâ–„â–ƒâ–†â–‚â–„â–„â–ƒâ–‚â–…â–‚â–‚â–‡â–‡â–ƒâ–…â–‚â–…â–†â–‚â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–„â–‡â–ƒâ–ƒâ–†â–â–‡â–†â–…â–…â–…â–„â–‚â–…â–‚â–†â–„â–‚â–„â–„â–„â–‚â–†â–‚â–„â–„â–‚â–ƒâ–…â–‚â–‚â–‡â–‡â–ƒâ–…â–ƒâ–…â–†â–ƒâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–…â–â–„â–…â–‚â–…â–â–ƒâ–ƒâ–‚â–„â–„â–‡â–ƒâ–…â–„â–„â–…â–„â–ˆâ–„â–‡â–‚â–…â–…â–…â–…â–†â–‚â–„â–†â–â–ƒâ–ˆâ–ƒâ–…â–„â–ƒâ–…â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–†â–…â–†â–†â–†â–†â–‡â–‡â–‡â–†â–‡â–‡â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–â–‡â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–†â–‡â–‡â–†â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–ƒâ–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–…â–†â–…â–†â–…â–â–†â–ƒâ–…â–ˆâ–…â–ˆâ–ˆâ–†â–†â–ˆâ–†â–…â–…â–…â–†â–ˆâ–†â–…â–…â–†â–†â–…â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–…â–†â–…â–†â–…â–â–†â–‚â–…â–ˆâ–…â–ˆâ–ˆâ–†â–†â–ˆâ–†â–…â–…â–…â–†â–ˆâ–†â–…â–…â–†â–†â–…â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–ƒâ–†â–‡â–ˆâ–†â–„â–†â–…â–â–†â–„â–…â–…â–„â–…â–ƒâ–ƒâ–…â–‚â–„â–â–„â–‚â–‚â–ƒâ–…â–„â–ƒâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_single_lstm__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/xcvozi1q\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:23:19.572980: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_single_lstm__binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/10m6n47o\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012318-10m6n47o\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 130   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.7 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 80/81 [00:01<00:00, 68.46it/s, loss=0.691, v_num=n47o, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.950\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:01<00:00, 67.00it/s, loss=0.691, v_num=n47o, ETH_val_\n",
      "Epoch 1:  99%|â–‰| 80/81 [00:01<00:00, 75.97it/s, loss=0.718, v_num=n47o, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.162 >= min_delta = 0.003. New best score: 0.788\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:01<00:00, 73.86it/s, loss=0.718, v_num=n47o, ETH_val_\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:01<00:00, 73.95it/s, loss=0.723, v_num=n47o, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.068 >= min_delta = 0.003. New best score: 0.720\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:01<00:00, 73.30it/s, loss=0.723, v_num=n47o, ETH_val_\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:01<00:00, 65.06it/s, loss=0.7, v_num=n47o, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:01<00:00, 64.37it/s, loss=0.7, v_num=n47o, ETH_val_ac\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:01<00:00, 71.95it/s, loss=0.694, v_num=n47o, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.024 >= min_delta = 0.003. New best score: 0.696\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:01<00:00, 71.11it/s, loss=0.694, v_num=n47o, ETH_val_\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:01<00:00, 68.70it/s, loss=0.698, v_num=n47o, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:01<00:00, 68.19it/s, loss=0.698, v_num=n47o, ETH_val_\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:01<00:00, 74.12it/s, loss=0.692, v_num=n47o, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.025 >= min_delta = 0.003. New best score: 0.671\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:01<00:00, 73.25it/s, loss=0.692, v_num=n47o, ETH_val_\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:01<00:00, 64.97it/s, loss=0.688, v_num=n47o, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 0.660\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:01<00:00, 64.52it/s, loss=0.688, v_num=n47o, ETH_val_\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:01<00:00, 55.59it/s, loss=0.691, v_num=n47o, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:01<00:00, 55.20it/s, loss=0.691, v_num=n47o, ETH_val_\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:01<00:00, 71.43it/s, loss=0.695, v_num=n47o, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:01<00:00, 70.72it/s, loss=0.695, v_num=n47o, ETH_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 80/81 [00:01<00:00, 59.43it/s, loss=0.697, v_num=n47o, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:01<00:00, 58.47it/s, loss=0.697, v_num=n47o, ETH_val\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.049 >= min_delta = 0.003. New best score: 0.611\n",
      "Epoch 11:  99%|â–‰| 80/81 [00:01<00:00, 64.93it/s, loss=0.699, v_num=n47o, ETH_val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:01<00:00, 63.34it/s, loss=0.699, v_num=n47o, ETH_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 80/81 [00:01<00:00, 71.46it/s, loss=0.695, v_num=n47o, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:01<00:00, 69.85it/s, loss=0.695, v_num=n47o, ETH_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 80/81 [00:01<00:00, 54.89it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:01<00:00, 54.23it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 80/81 [00:01<00:00, 78.95it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:01<00:00, 76.58it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 80/81 [00:01<00:00, 78.32it/s, loss=0.691, v_num=n47o, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:01<00:00, 76.42it/s, loss=0.691, v_num=n47o, ETH_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 80/81 [00:01<00:00, 54.16it/s, loss=0.691, v_num=n47o, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:01<00:00, 53.49it/s, loss=0.691, v_num=n47o, ETH_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 80/81 [00:01<00:00, 77.75it/s, loss=0.695, v_num=n47o, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:01<00:00, 75.40it/s, loss=0.695, v_num=n47o, ETH_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 80/81 [00:01<00:00, 60.83it/s, loss=0.691, v_num=n47o, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:01<00:00, 59.88it/s, loss=0.691, v_num=n47o, ETH_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 80/81 [00:01<00:00, 74.63it/s, loss=0.696, v_num=n47o, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:01<00:00, 72.49it/s, loss=0.696, v_num=n47o, ETH_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 80/81 [00:00<00:00, 81.17it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:01<00:00, 79.17it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 80/81 [00:01<00:00, 57.49it/s, loss=0.696, v_num=n47o, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:01<00:00, 56.68it/s, loss=0.696, v_num=n47o, ETH_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 80/81 [00:00<00:00, 87.95it/s, loss=0.692, v_num=n47o, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 81/81 [00:00<00:00, 85.03it/s, loss=0.692, v_num=n47o, ETH_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:  99%|â–‰| 80/81 [00:01<00:00, 57.91it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 81/81 [00:01<00:00, 57.15it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n",
      "Epoch 24:  99%|â–‰| 80/81 [00:01<00:00, 79.18it/s, loss=0.693, v_num=n47o, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 81/81 [00:01<00:00, 76.80it/s, loss=0.693, v_num=n47o, ETH_val\u001b[A\n",
      "Epoch 25:  99%|â–‰| 80/81 [00:01<00:00, 79.61it/s, loss=0.692, v_num=n47o, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 81/81 [00:01<00:00, 77.81it/s, loss=0.692, v_num=n47o, ETH_val\u001b[A\n",
      "Epoch 26:  99%|â–‰| 80/81 [00:01<00:00, 68.33it/s, loss=0.693, v_num=n47o, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 81/81 [00:01<00:00, 66.80it/s, loss=0.693, v_num=n47o, ETH_val\u001b[A\n",
      "Epoch 27:  99%|â–‰| 80/81 [00:01<00:00, 74.08it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 81/81 [00:01<00:00, 71.85it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n",
      "Epoch 28:  99%|â–‰| 80/81 [00:01<00:00, 66.42it/s, loss=0.693, v_num=n47o, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 81/81 [00:01<00:00, 64.99it/s, loss=0.693, v_num=n47o, ETH_val\u001b[A\n",
      "Epoch 29:  99%|â–‰| 80/81 [00:00<00:00, 82.80it/s, loss=0.695, v_num=n47o, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 81/81 [00:01<00:00, 80.24it/s, loss=0.695, v_num=n47o, ETH_val\u001b[A\n",
      "Epoch 30:  99%|â–‰| 80/81 [00:01<00:00, 72.64it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.611. Signaling Trainer to stop.\n",
      "Epoch 30: 100%|â–ˆ| 81/81 [00:01<00:00, 70.96it/s, loss=0.694, v_num=n47o, ETH_val\n",
      "Epoch 30: 100%|â–ˆ| 81/81 [00:01<00:00, 70.76it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 126.79it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.6129032373428345,\n",
      " 'ETH_test_f1': 0.3793548345565796,\n",
      " 'test_loss': 0.6854448914527893}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 68005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012318-10m6n47o/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012318-10m6n47o/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.4375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.30435\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.68893\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2480\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 46\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463044\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.50591\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.39304\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.88889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.47059\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.68576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.37935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.68544\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–ƒâ–ƒâ–„â–ƒâ–†â–ƒâ–ƒâ–…â–„â–„â–†â–†â–ƒâ–ƒâ–„â–„â–ƒâ–…â–…â–ƒâ–ƒâ–‡â–„â–ƒâ–„â–†â–ƒâ–„â–ƒâ–ˆâ–…â–â–ƒâ–ˆâ–„â–†â–„â–†â–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–ƒâ–ƒâ–„â–„â–†â–ƒâ–ƒâ–‚â–„â–„â–„â–‡â–‚â–ƒâ–„â–„â–ƒâ–…â–…â–â–„â–‡â–ƒâ–„â–„â–…â–ƒâ–„â–ƒâ–ˆâ–„â–â–â–†â–„â–ƒâ–‚â–ƒâ–„â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–ˆâ–„â–‚â–…â–„â–†â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–„â–„â–ƒâ–ƒâ–„â–ƒâ–ƒâ–„â–„â–â–„â–„â–„â–ƒâ–„â–„â–„â–‚â–ƒâ–…â–…â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–ˆâ–„â–†â–†â–„â–†â–…â–†â–†â–ˆâ–‡â–‚â–„â–‡â–…â–†â–‡â–†â–ƒâ–†â–…â–…â–†â–…â–ƒâ–‡â–â–‡â–†â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–ˆâ–†â–‡â–†â–‡â–†â–†â–‡â–†â–†â–…â–†â–„â–‡â–†â–‡â–†â–…â–†â–ƒâ–…â–…â–„â–ƒâ–†â–„â–„â–‚â–â–‚â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–…â–ˆâ–†â–„â–ƒâ–ƒâ–ƒâ–â–ƒâ–‚â–‚â–ƒâ–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_single_lstm__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/10m6n47o\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:24:15.590597: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_single_lstm__multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2pc295sf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012412-2pc295sf\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 195   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.8 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|â–‰| 79/80 [00:01<00:00, 66.17it/s, loss=1.08, v_num=95sf, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.977\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:01<00:00, 60.54it/s, loss=1.08, v_num=95sf, ETH_val_a\n",
      "Epoch 1:  99%|â–‰| 79/80 [00:01<00:00, 69.16it/s, loss=1.07, v_num=95sf, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 67.02it/s, loss=1.07, v_num=95sf, ETH_val_a\u001b[A\n",
      "Epoch 2:  99%|â–‰| 79/80 [00:00<00:00, 79.58it/s, loss=1.05, v_num=95sf, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.018 >= min_delta = 0.003. New best score: 0.959\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 77.83it/s, loss=1.05, v_num=95sf, ETH_val_a\n",
      "Epoch 3:  99%|â–‰| 79/80 [00:01<00:00, 76.48it/s, loss=1, v_num=95sf, ETH_val_acc=\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 0.949\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:01<00:00, 73.84it/s, loss=1, v_num=95sf, ETH_val_acc=\n",
      "Epoch 4:  99%|â–‰| 79/80 [00:01<00:00, 64.77it/s, loss=0.955, v_num=95sf, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.034 >= min_delta = 0.003. New best score: 0.915\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 62.99it/s, loss=0.955, v_num=95sf, ETH_val_\n",
      "Epoch 5:  99%|â–‰| 79/80 [00:01<00:00, 62.79it/s, loss=0.952, v_num=95sf, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:01<00:00, 61.50it/s, loss=0.952, v_num=95sf, ETH_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 79/80 [00:01<00:00, 71.56it/s, loss=0.92, v_num=95sf, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.047 >= min_delta = 0.003. New best score: 0.867\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:01<00:00, 69.43it/s, loss=0.92, v_num=95sf, ETH_val_a\n",
      "Epoch 7:  99%|â–‰| 79/80 [00:01<00:00, 72.27it/s, loss=0.965, v_num=95sf, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:01<00:00, 70.37it/s, loss=0.965, v_num=95sf, ETH_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 79/80 [00:01<00:00, 68.47it/s, loss=0.95, v_num=95sf, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:01<00:00, 66.80it/s, loss=0.95, v_num=95sf, ETH_val_a\u001b[A\n",
      "Epoch 9:  99%|â–‰| 79/80 [00:01<00:00, 77.25it/s, loss=0.929, v_num=95sf, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:01<00:00, 74.76it/s, loss=0.929, v_num=95sf, ETH_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 79/80 [00:01<00:00, 63.79it/s, loss=0.928, v_num=95sf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:01<00:00, 62.56it/s, loss=0.928, v_num=95sf, ETH_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 79/80 [00:01<00:00, 71.58it/s, loss=0.935, v_num=95sf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:01<00:00, 69.41it/s, loss=0.935, v_num=95sf, ETH_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 79/80 [00:01<00:00, 73.14it/s, loss=0.92, v_num=95sf, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:01<00:00, 71.19it/s, loss=0.92, v_num=95sf, ETH_val_\u001b[A\n",
      "Epoch 13:  99%|â–‰| 79/80 [00:00<00:00, 81.08it/s, loss=0.908, v_num=95sf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:01<00:00, 78.11it/s, loss=0.908, v_num=95sf, ETH_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 79/80 [00:01<00:00, 70.59it/s, loss=0.912, v_num=95sf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:01<00:00, 69.27it/s, loss=0.912, v_num=95sf, ETH_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 79/80 [00:01<00:00, 76.55it/s, loss=0.944, v_num=95sf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:01<00:00, 74.37it/s, loss=0.944, v_num=95sf, ETH_val\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:01<00:00, 76.25it/s, loss=0.886, v_num=95sf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:01<00:00, 75.67it/s, loss=0.886, v_num=95sf, ETH_val\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:01<00:00, 65.89it/s, loss=0.897, v_num=95sf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:01<00:00, 65.40it/s, loss=0.897, v_num=95sf, ETH_val\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:00<00:00, 84.49it/s, loss=0.9, v_num=95sf, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:00<00:00, 83.40it/s, loss=0.9, v_num=95sf, ETH_val_a\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:01<00:00, 70.54it/s, loss=0.937, v_num=95sf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:01<00:00, 69.98it/s, loss=0.937, v_num=95sf, ETH_val\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:01<00:00, 53.48it/s, loss=0.904, v_num=95sf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:01<00:00, 53.08it/s, loss=0.904, v_num=95sf, ETH_val\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:01<00:00, 71.61it/s, loss=0.911, v_num=95sf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:01<00:00, 71.09it/s, loss=0.911, v_num=95sf, ETH_val\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:01<00:00, 74.16it/s, loss=0.965, v_num=95sf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:01<00:00, 73.35it/s, loss=0.965, v_num=95sf, ETH_val\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:00<00:00, 80.22it/s, loss=0.911, v_num=95sf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:01<00:00, 79.57it/s, loss=0.911, v_num=95sf, ETH_val\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:01<00:00, 72.41it/s, loss=0.933, v_num=95sf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:01<00:00, 71.85it/s, loss=0.933, v_num=95sf, ETH_val\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:01<00:00, 73.96it/s, loss=0.919, v_num=95sf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:01<00:00, 73.33it/s, loss=0.919, v_num=95sf, ETH_val\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:01<00:00, 65.86it/s, loss=0.915, v_num=95sf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:01<00:00, 65.42it/s, loss=0.915, v_num=95sf, ETH_val\u001b[A\n",
      "                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.867. Signaling Trainer to stop.\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:01<00:00, 65.24it/s, loss=0.915, v_num=95sf, ETH_val\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 136.75it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.4838709533214569,\n",
      " 'ETH_test_f1': 0.31950849294662476,\n",
      " 'test_loss': 0.8143577575683594}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 68206\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012412-2pc295sf/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012412-2pc295sf/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.58378\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.78695\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2133\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 41\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463093\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.53523\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.46465\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.91559\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.89928\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.48387\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.31951\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.81436\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–„â–ˆâ–â–…â–‚â–„â–…â–†â–ˆâ–…â–…â–„â–‡â–…â–ˆâ–‚â–ƒâ–…â–„â–†â–…â–†â–‚â–ƒâ–„â–…â–„â–…â–†â–†â–†â–„â–†â–†â–…â–‡â–†â–„â–†â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–‚â–ƒâ–â–‚â–‚â–‚â–„â–ƒâ–ƒâ–„â–„â–„â–…â–„â–†â–‚â–…â–…â–‚â–‡â–‡â–ˆâ–„â–ƒâ–ƒâ–…â–„â–†â–‡â–„â–‡â–„â–…â–ˆâ–„â–ˆâ–‡â–†â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–…â–ƒâ–†â–„â–†â–…â–ƒâ–„â–ƒâ–„â–ƒâ–…â–â–ƒâ–â–ˆâ–ƒâ–ƒâ–„â–ƒâ–‚â–‚â–…â–…â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–ƒâ–‚â–ƒâ–„â–…â–„â–…â–†â–…â–†â–…â–†â–…â–†â–…â–…â–†â–†â–ˆâ–‡â–†â–†â–‡â–‡â–†â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–‚â–‚â–â–‚â–ƒâ–ƒâ–„â–„â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–‡â–‡â–†â–„â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–†â–†â–†â–†â–ƒâ–ƒâ–ˆâ–â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–‚â–‚â–‚â–‚â–â–„â–ˆâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–‡â–‡â–†â–…â–„â–…â–â–ƒâ–…â–…â–ˆâ–„â–…â–‡â–…â–†â–‡â–…â–‚â–‡â–„â–‡â–‚â–„â–‡â–„â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_single_lstm__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2pc295sf\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:25:04.083015: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_single_lstm__multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/tmlbsttl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012502-tmlbsttl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 195   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.8 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/81 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 80/81 [00:01<00:00, 78.69it/s, loss=1.09, v_num=sttl, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.018\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:01<00:00, 76.80it/s, loss=1.09, v_num=sttl, ETH_val_a\n",
      "Epoch 1:  99%|â–‰| 80/81 [00:01<00:00, 70.08it/s, loss=1.13, v_num=sttl, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:01<00:00, 67.98it/s, loss=1.13, v_num=sttl, ETH_val_a\u001b[A\n",
      "Epoch 2:  99%|â–‰| 80/81 [00:00<00:00, 84.11it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:00<00:00, 82.01it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n",
      "Epoch 3:  99%|â–‰| 80/81 [00:01<00:00, 70.29it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:01<00:00, 68.77it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n",
      "Epoch 4:  99%|â–‰| 80/81 [00:00<00:00, 91.64it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:00<00:00, 88.84it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n",
      "Epoch 5:  99%|â–‰| 80/81 [00:01<00:00, 71.59it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:01<00:00, 70.11it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n",
      "Epoch 6:  99%|â–‰| 80/81 [00:00<00:00, 88.50it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:00<00:00, 84.91it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n",
      "Epoch 7:  99%|â–‰| 80/81 [00:01<00:00, 68.63it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:01<00:00, 66.86it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n",
      "Epoch 8:  99%|â–‰| 80/81 [00:01<00:00, 72.60it/s, loss=1.1, v_num=sttl, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:01<00:00, 70.18it/s, loss=1.1, v_num=sttl, ETH_val_ac\u001b[A\n",
      "Epoch 9:  99%|â–‰| 80/81 [00:01<00:00, 77.80it/s, loss=1.1, v_num=sttl, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:01<00:00, 75.76it/s, loss=1.1, v_num=sttl, ETH_val_ac\u001b[A\n",
      "Epoch 10:  99%|â–‰| 80/81 [00:01<00:00, 79.17it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:01<00:00, 76.93it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n",
      "Epoch 11:  99%|â–‰| 80/81 [00:01<00:00, 79.80it/s, loss=1.11, v_num=sttl, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:01<00:00, 77.99it/s, loss=1.11, v_num=sttl, ETH_val_\u001b[A\n",
      "Epoch 12:  99%|â–‰| 80/81 [00:01<00:00, 75.80it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:01<00:00, 73.65it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n",
      "Epoch 13:  99%|â–‰| 80/81 [00:00<00:00, 83.06it/s, loss=1.09, v_num=sttl, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:00<00:00, 81.17it/s, loss=1.09, v_num=sttl, ETH_val_\u001b[A\n",
      "Epoch 14:  99%|â–‰| 80/81 [00:01<00:00, 75.73it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:01<00:00, 73.47it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n",
      "Epoch 15:  99%|â–‰| 80/81 [00:00<00:00, 84.64it/s, loss=1.11, v_num=sttl, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:00<00:00, 82.64it/s, loss=1.11, v_num=sttl, ETH_val_\u001b[A\n",
      "Epoch 16:  99%|â–‰| 80/81 [00:01<00:00, 71.73it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:01<00:00, 70.04it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n",
      "Epoch 17:  99%|â–‰| 80/81 [00:00<00:00, 80.14it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:01<00:00, 77.72it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n",
      "Epoch 18:  99%|â–‰| 80/81 [00:01<00:00, 70.37it/s, loss=1.11, v_num=sttl, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:01<00:00, 68.98it/s, loss=1.11, v_num=sttl, ETH_val_\u001b[A\n",
      "Epoch 19:  99%|â–‰| 80/81 [00:01<00:00, 75.49it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:01<00:00, 73.00it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n",
      "Epoch 20:  99%|â–‰| 80/81 [00:01<00:00, 69.51it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.018. Signaling Trainer to stop.\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:01<00:00, 68.09it/s, loss=1.1, v_num=sttl, ETH_val_a\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:01<00:00, 67.91it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 120.29it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.4193548262119293,\n",
      " 'ETH_test_f1': 0.19648092985153198,\n",
      " 'test_loss': 1.105859398841858}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 68369\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012502-tmlbsttl/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012502-tmlbsttl/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.4375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.08555\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1680\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463134\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.35619\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.24744\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.10005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.29293\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.0909\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.41935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.10586\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–‚â–„â–…â–„â–„â–‚â–‚â–â–‚â–„â–„â–„â–…â–‚â–ƒâ–ƒâ–„â–ƒâ–„â–„â–ƒâ–†â–ƒâ–…â–„â–…â–„â–…â–ƒâ–†â–„â–ˆâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–ƒâ–†â–†â–…â–…â–ƒâ–‚â–â–‚â–…â–†â–…â–‡â–‚â–„â–ƒâ–…â–ƒâ–„â–†â–„â–ˆâ–„â–„â–†â–‡â–„â–†â–„â–†â–†â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–‡â–ƒâ–‚â–„â–„â–…â–…â–ˆâ–„â–„â–„â–ƒâ–ƒâ–„â–„â–…â–ƒâ–„â–ƒâ–ƒâ–„â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–ˆâ–‡â–…â–†â–ƒâ–ƒâ–†â–„â–†â–†â–†â–ˆâ–…â–†â–†â–†â–†â–„â–â–…â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–‡â–ˆâ–‡â–ˆâ–„â–…â–†â–ƒâ–…â–„â–„â–ƒâ–†â–ˆâ–ƒâ–†â–„â–â–‚â–†â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–‡â–„â–‚â–…â–†â–ƒâ–„â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–ƒâ–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–ˆâ–ˆâ–ˆâ–â–†â–ƒâ–ˆâ–„â–ƒâ–ˆâ–„â–ƒâ–„â–„â–ˆâ–„â–ƒâ–„â–„â–ˆâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–…â–…â–…â–â–ˆâ–„â–…â–…â–„â–…â–†â–„â–…â–†â–…â–†â–„â–†â–…â–…â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–â–â–…â–ˆâ–…â–†â–„â–…â–†â–„â–…â–†â–†â–†â–…â–…â–†â–†â–†â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_single_lstm__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/tmlbsttl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:25:43.788125: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm__binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3dhx48n4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012542-3dhx48n4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 79/80 [00:01<00:00, 40.86it/s, loss=0.706, v_num=48n4, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.692\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:02<00:00, 38.85it/s, loss=0.706, v_num=48n4, ETH_val_\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:02<00:00, 30.52it/s, loss=0.655, v_num=48n4, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.085 >= min_delta = 0.003. New best score: 0.607\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:02<00:00, 30.37it/s, loss=0.655, v_num=48n4, ETH_val_\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:02<00:00, 31.95it/s, loss=0.584, v_num=48n4, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.108 >= min_delta = 0.003. New best score: 0.498\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:02<00:00, 31.80it/s, loss=0.584, v_num=48n4, ETH_val_\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:02<00:00, 39.66it/s, loss=0.605, v_num=48n4, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:02<00:00, 39.41it/s, loss=0.605, v_num=48n4, ETH_val_\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:02<00:00, 28.46it/s, loss=0.601, v_num=48n4, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:02<00:00, 28.32it/s, loss=0.601, v_num=48n4, ETH_val_\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:02<00:00, 29.74it/s, loss=0.636, v_num=48n4, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:02<00:00, 29.62it/s, loss=0.636, v_num=48n4, ETH_val_\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:02<00:00, 34.24it/s, loss=0.586, v_num=48n4, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.045 >= min_delta = 0.003. New best score: 0.453\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:02<00:00, 34.07it/s, loss=0.586, v_num=48n4, ETH_val_\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:02<00:00, 34.52it/s, loss=0.589, v_num=48n4, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:02<00:00, 34.35it/s, loss=0.589, v_num=48n4, ETH_val_\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:02<00:00, 32.47it/s, loss=0.599, v_num=48n4, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:02<00:00, 32.28it/s, loss=0.599, v_num=48n4, ETH_val_\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:02<00:00, 26.85it/s, loss=0.541, v_num=48n4, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:02<00:00, 26.75it/s, loss=0.541, v_num=48n4, ETH_val_\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|â–ˆ| 80/80 [00:02<00:00, 31.51it/s, loss=0.589, v_num=48n4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:02<00:00, 31.36it/s, loss=0.589, v_num=48n4, ETH_val\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:03<00:00, 25.12it/s, loss=0.573, v_num=48n4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:03<00:00, 25.03it/s, loss=0.573, v_num=48n4, ETH_val\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:02<00:00, 31.07it/s, loss=0.592, v_num=48n4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:02<00:00, 30.74it/s, loss=0.592, v_num=48n4, ETH_val\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:02<00:00, 27.88it/s, loss=0.56, v_num=48n4, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:02<00:00, 27.74it/s, loss=0.56, v_num=48n4, ETH_val_\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:02<00:00, 28.62it/s, loss=0.578, v_num=48n4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:02<00:00, 28.51it/s, loss=0.578, v_num=48n4, ETH_val\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:02<00:00, 29.95it/s, loss=0.54, v_num=48n4, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:02<00:00, 29.80it/s, loss=0.54, v_num=48n4, ETH_val_\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:02<00:00, 28.39it/s, loss=0.573, v_num=48n4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:02<00:00, 28.28it/s, loss=0.573, v_num=48n4, ETH_val\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:02<00:00, 35.25it/s, loss=0.605, v_num=48n4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:02<00:00, 35.07it/s, loss=0.605, v_num=48n4, ETH_val\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:02<00:00, 38.56it/s, loss=0.543, v_num=48n4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:02<00:00, 38.36it/s, loss=0.543, v_num=48n4, ETH_val\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:02<00:00, 36.81it/s, loss=0.553, v_num=48n4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:02<00:00, 36.61it/s, loss=0.553, v_num=48n4, ETH_val\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:02<00:00, 32.18it/s, loss=0.536, v_num=48n4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:02<00:00, 32.04it/s, loss=0.536, v_num=48n4, ETH_val\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:02<00:00, 36.10it/s, loss=0.547, v_num=48n4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:02<00:00, 35.88it/s, loss=0.547, v_num=48n4, ETH_val\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:02<00:00, 39.58it/s, loss=0.537, v_num=48n4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:02<00:00, 39.35it/s, loss=0.537, v_num=48n4, ETH_val\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:02<00:00, 37.79it/s, loss=0.549, v_num=48n4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:02<00:00, 37.58it/s, loss=0.549, v_num=48n4, ETH_val\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:02<00:00, 37.26it/s, loss=0.55, v_num=48n4, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:02<00:00, 36.98it/s, loss=0.55, v_num=48n4, ETH_val_\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:02<00:00, 29.32it/s, loss=0.584, v_num=48n4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:02<00:00, 29.20it/s, loss=0.584, v_num=48n4, ETH_val\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:02<00:00, 34.70it/s, loss=0.515, v_num=48n4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.453. Signaling Trainer to stop.\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:02<00:00, 34.53it/s, loss=0.515, v_num=48n4, ETH_val\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:02<00:00, 34.48it/s, loss=0.515, v_num=48n4, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 94.54it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.7096773982048035,\n",
      " 'ETH_test_f1': 0.7096773982048035,\n",
      " 'test_loss': 0.5478140115737915}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 68509\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012542-3dhx48n4/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012542-3dhx48n4/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.70909\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.45342\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2133\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 76\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463218\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.72526\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.71134\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.54515\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.64935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.49525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.70968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.70968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.54781\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–†â–â–„â–„â–…â–‚â–‚â–„â–‚â–…â–…â–†â–‡â–‡â–…â–…â–†â–…â–…â–‡â–†â–…â–„â–…â–…â–…â–‡â–…â–†â–…â–…â–†â–‡â–ˆâ–…â–†â–†â–†â–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–†â–â–ƒâ–ƒâ–…â–‚â–‚â–„â–‚â–…â–„â–…â–‡â–‡â–„â–„â–†â–…â–…â–‡â–†â–„â–„â–„â–…â–…â–‡â–„â–†â–…â–…â–†â–‡â–ˆâ–…â–†â–†â–†â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–†â–‡â–„â–†â–…â–‡â–†â–…â–‡â–†â–…â–…â–„â–ƒâ–„â–„â–ƒâ–…â–ƒâ–‚â–ƒâ–†â–…â–ˆâ–ƒâ–„â–â–„â–ƒâ–„â–„â–‚â–ƒâ–â–…â–ƒâ–‚â–ƒâ–â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–‚â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–â–‚â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–‡â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–â–ƒâ–†â–ˆâ–ƒâ–ƒâ–†â–†â–ˆâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–†â–ƒâ–ƒâ–ƒâ–ƒâ–…â–ƒâ–…â–…â–…â–…â–ƒâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–â–ƒâ–‡â–ˆâ–ƒâ–ƒâ–‡â–‡â–ˆâ–ˆâ–ƒâ–ƒâ–„â–ƒâ–‡â–ƒâ–ƒâ–ƒâ–ƒâ–…â–ƒâ–…â–…â–…â–…â–ƒâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–†â–‚â–‚â–„â–ƒâ–â–‚â–‚â–â–‚â–„â–ƒâ–†â–‚â–„â–„â–†â–„â–‚â–…â–‚â–‚â–„â–‚â–†â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3dhx48n4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:27:13.816308: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm__binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2dhg4mr4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012712-2dhg4mr4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 80/81 [00:01<00:00, 41.08it/s, loss=0.731, v_num=4mr4, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.688\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:01<00:00, 40.56it/s, loss=0.731, v_num=4mr4, ETH_val_\n",
      "Epoch 1:  99%|â–‰| 80/81 [00:01<00:00, 42.52it/s, loss=0.697, v_num=4mr4, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 0.667\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:01<00:00, 42.01it/s, loss=0.697, v_num=4mr4, ETH_val_\n",
      "Epoch 2:  99%|â–‰| 80/81 [00:01<00:00, 40.88it/s, loss=0.686, v_num=4mr4, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.048 >= min_delta = 0.003. New best score: 0.620\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:02<00:00, 40.40it/s, loss=0.686, v_num=4mr4, ETH_val_\n",
      "Epoch 3:  99%|â–‰| 80/81 [00:02<00:00, 39.80it/s, loss=0.706, v_num=4mr4, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:02<00:00, 39.29it/s, loss=0.706, v_num=4mr4, ETH_val_\u001b[A\n",
      "Epoch 4:  99%|â–‰| 80/81 [00:01<00:00, 41.21it/s, loss=0.695, v_num=4mr4, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:01<00:00, 40.71it/s, loss=0.695, v_num=4mr4, ETH_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 80/81 [00:02<00:00, 39.19it/s, loss=0.708, v_num=4mr4, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:02<00:00, 38.72it/s, loss=0.708, v_num=4mr4, ETH_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 80/81 [00:01<00:00, 41.40it/s, loss=0.697, v_num=4mr4, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:01<00:00, 40.91it/s, loss=0.697, v_num=4mr4, ETH_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 80/81 [00:02<00:00, 30.87it/s, loss=0.699, v_num=4mr4, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:02<00:00, 30.66it/s, loss=0.699, v_num=4mr4, ETH_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 80/81 [00:02<00:00, 35.48it/s, loss=0.69, v_num=4mr4, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:02<00:00, 35.30it/s, loss=0.69, v_num=4mr4, ETH_val_a\u001b[A\n",
      "Epoch 9:  99%|â–‰| 80/81 [00:01<00:00, 40.39it/s, loss=0.694, v_num=4mr4, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:02<00:00, 40.09it/s, loss=0.694, v_num=4mr4, ETH_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 80/81 [00:01<00:00, 41.28it/s, loss=0.689, v_num=4mr4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:01<00:00, 40.94it/s, loss=0.689, v_num=4mr4, ETH_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 80/81 [00:01<00:00, 40.61it/s, loss=0.698, v_num=4mr4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:02<00:00, 40.28it/s, loss=0.698, v_num=4mr4, ETH_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 80/81 [00:02<00:00, 34.45it/s, loss=0.691, v_num=4mr4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:02<00:00, 34.28it/s, loss=0.691, v_num=4mr4, ETH_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 80/81 [00:01<00:00, 41.14it/s, loss=0.694, v_num=4mr4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:01<00:00, 40.84it/s, loss=0.694, v_num=4mr4, ETH_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 80/81 [00:02<00:00, 38.54it/s, loss=0.691, v_num=4mr4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:02<00:00, 38.24it/s, loss=0.691, v_num=4mr4, ETH_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 80/81 [00:02<00:00, 31.79it/s, loss=0.695, v_num=4mr4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:02<00:00, 30.80it/s, loss=0.695, v_num=4mr4, ETH_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 80/81 [00:01<00:00, 40.39it/s, loss=0.697, v_num=4mr4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:02<00:00, 39.90it/s, loss=0.697, v_num=4mr4, ETH_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 80/81 [00:02<00:00, 33.48it/s, loss=0.694, v_num=4mr4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:02<00:00, 33.28it/s, loss=0.694, v_num=4mr4, ETH_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 80/81 [00:02<00:00, 39.87it/s, loss=0.696, v_num=4mr4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:02<00:00, 39.41it/s, loss=0.696, v_num=4mr4, ETH_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 80/81 [00:02<00:00, 37.67it/s, loss=0.693, v_num=4mr4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:02<00:00, 37.43it/s, loss=0.693, v_num=4mr4, ETH_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 80/81 [00:01<00:00, 40.30it/s, loss=0.694, v_num=4mr4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:02<00:00, 39.97it/s, loss=0.694, v_num=4mr4, ETH_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 80/81 [00:02<00:00, 38.05it/s, loss=0.691, v_num=4mr4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:02<00:00, 37.81it/s, loss=0.691, v_num=4mr4, ETH_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 80/81 [00:01<00:00, 40.95it/s, loss=0.695, v_num=4mr4, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 81/81 [00:01<00:00, 40.58it/s, loss=0.695, v_num=4mr4, ETH_val\u001b[A\n",
      "                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.620. Signaling Trainer to stop.\n",
      "Epoch 22: 100%|â–ˆ| 81/81 [00:02<00:00, 40.50it/s, loss=0.695, v_num=4mr4, ETH_val\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 93.09it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.6129032373428345,\n",
      " 'ETH_test_f1': 0.3793548345565796,\n",
      " 'test_loss': 0.6768962144851685}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 68737\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012712-2dhg4mr4/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012712-2dhg4mr4/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.56364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.673\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 22\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1840\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 58\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463290\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 82\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.50591\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.40438\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.88889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.47059\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.67953\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.37935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.6769\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–„â–ƒâ–‡â–‡â–„â–‚â–…â–â–„â–„â–ƒâ–„â–„â–‚â–‡â–ˆâ–†â–„â–‚â–‚â–†â–…â–„â–†â–‡â–ƒâ–…â–ƒâ–‚â–†â–ƒâ–†â–ƒâ–„â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–„â–ƒâ–‡â–‡â–ƒâ–ƒâ–ƒâ–â–„â–„â–ƒâ–…â–„â–‚â–†â–ˆâ–†â–„â–ƒâ–‚â–…â–…â–„â–†â–„â–ƒâ–„â–ƒâ–‚â–…â–ƒâ–†â–‚â–ƒâ–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–…â–ˆâ–â–ƒâ–…â–†â–ƒâ–†â–„â–„â–…â–ƒâ–…â–„â–ƒâ–„â–„â–„â–†â–„â–ƒâ–„â–„â–…â–ƒâ–…â–…â–„â–…â–„â–„â–„â–„â–„â–„â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–‚â–ˆâ–ˆâ–â–…â–†â–„â–†â–‚â–…â–ƒâ–„â–…â–ˆâ–„â–„â–†â–‚â–„â–„â–‚â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–†â–ˆâ–ˆâ–…â–…â–†â–†â–†â–…â–…â–…â–„â–…â–†â–…â–‚â–‡â–‚â–â–†â–‚â–…â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–ƒâ–ƒâ–†â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–â–â–‚â–â–â–â–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–†â–â–„â–†â–…â–„â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–†â–†â–†â–‚â–†â–‡â–‡â–†â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2dhg4mr4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:28:24.651716: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm__multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/19c4lpom\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012823-19c4lpom\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:02<00:00, 40.00it/s, loss=1.04, v_num=lpom, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.853\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:02<00:00, 39.75it/s, loss=1.04, v_num=lpom, ETH_val_a\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 41.34it/s, loss=1.09, v_num=lpom, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 41.10it/s, loss=1.09, v_num=lpom, ETH_val_a\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 42.11it/s, loss=1.04, v_num=lpom, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 41.80it/s, loss=1.04, v_num=lpom, ETH_val_a\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:02<00:00, 36.40it/s, loss=1.05, v_num=lpom, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:02<00:00, 36.22it/s, loss=1.05, v_num=lpom, ETH_val_a\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 42.03it/s, loss=1.06, v_num=lpom, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 41.77it/s, loss=1.06, v_num=lpom, ETH_val_a\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:02<00:00, 31.32it/s, loss=1.04, v_num=lpom, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:02<00:00, 31.17it/s, loss=1.04, v_num=lpom, ETH_val_a\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:02<00:00, 35.84it/s, loss=1.02, v_num=lpom, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:02<00:00, 35.66it/s, loss=1.02, v_num=lpom, ETH_val_a\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:02<00:00, 30.62it/s, loss=1.03, v_num=lpom, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:02<00:00, 30.46it/s, loss=1.03, v_num=lpom, ETH_val_a\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:02<00:00, 31.25it/s, loss=1.08, v_num=lpom, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:02<00:00, 31.09it/s, loss=1.08, v_num=lpom, ETH_val_a\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:02<00:00, 35.11it/s, loss=1.05, v_num=lpom, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:02<00:00, 34.93it/s, loss=1.05, v_num=lpom, ETH_val_a\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:02<00:00, 35.60it/s, loss=1.03, v_num=lpom, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:02<00:00, 35.42it/s, loss=1.03, v_num=lpom, ETH_val_\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:02<00:00, 26.92it/s, loss=1.02, v_num=lpom, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:02<00:00, 26.79it/s, loss=1.02, v_num=lpom, ETH_val_\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:01<00:00, 40.49it/s, loss=1.05, v_num=lpom, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:01<00:00, 40.19it/s, loss=1.05, v_num=lpom, ETH_val_\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:02<00:00, 33.67it/s, loss=1.03, v_num=lpom, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:02<00:00, 33.50it/s, loss=1.03, v_num=lpom, ETH_val_\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:01<00:00, 41.00it/s, loss=1.05, v_num=lpom, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:01<00:00, 40.75it/s, loss=1.05, v_num=lpom, ETH_val_\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:02<00:00, 38.55it/s, loss=1.05, v_num=lpom, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:02<00:00, 38.32it/s, loss=1.05, v_num=lpom, ETH_val_\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:02<00:00, 28.49it/s, loss=1.06, v_num=lpom, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:02<00:00, 28.38it/s, loss=1.06, v_num=lpom, ETH_val_\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:02<00:00, 29.92it/s, loss=1.03, v_num=lpom, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:02<00:00, 29.75it/s, loss=1.03, v_num=lpom, ETH_val_\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:02<00:00, 30.72it/s, loss=1, v_num=lpom, ETH_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:02<00:00, 30.58it/s, loss=1, v_num=lpom, ETH_val_acc\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:02<00:00, 34.30it/s, loss=1.05, v_num=lpom, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:02<00:00, 33.90it/s, loss=1.05, v_num=lpom, ETH_val_\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:02<00:00, 29.16it/s, loss=1.05, v_num=lpom, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:02<00:00, 29.00it/s, loss=1.05, v_num=lpom, ETH_val_Monitored metric val_loss did not improve in the last 20 records. Best score: 0.853. Signaling Trainer to stop.\n",
      "\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:02<00:00, 28.96it/s, loss=1.05, v_num=lpom, ETH_val_\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 81.14it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.4838709533214569,\n",
      " 'ETH_test_f1': 0.2169238030910492,\n",
      " 'test_loss': 1.0823054313659668}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 68968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012823-19c4lpom/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012823-19c4lpom/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.97029\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1659\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 58\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463361\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.4806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.21312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.03947\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.26667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.93892\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.48387\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.21692\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.08231\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–…â–†â–‡â–…â–†â–†â–‡â–†â–†â–ˆâ–†â–…â–…â–…â–†â–‡â–ˆâ–†â–‚â–…â–†â–‚â–…â–‡â–‡â–ƒâ–ƒâ–…â–…â–‡â–â–ƒâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–†â–‡â–ˆâ–ƒâ–„â–„â–„â–„â–„â–…â–„â–„â–ƒâ–ƒâ–„â–„â–…â–„â–‚â–ƒâ–„â–‚â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–â–ƒâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–„â–…â–ƒâ–†â–‚â–ƒâ–„â–…â–„â–ƒâ–ƒâ–…â–„â–…â–ƒâ–â–‚â–…â–‡â–„â–…â–ƒâ–…â–…â–â–†â–†â–†â–„â–‚â–ˆâ–„â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–…â–†â–…â–‡â–‡â–‡â–‡â–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–ˆâ–ƒâ–‚â–ƒâ–‚â–…â–ƒâ–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–†â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–â–†â–†â–†â–‡â–…â–†â–…â–ˆâ–…â–„â–…â–†â–†â–†â–†â–…â–…â–„â–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/19c4lpom\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:29:36.631732: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm__multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3f5p6x2i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012935-3f5p6x2i\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|â–‰| 80/81 [00:01<00:00, 42.90it/s, loss=1.12, v_num=6x2i, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.023\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:01<00:00, 42.37it/s, loss=1.12, v_num=6x2i, ETH_val_a\n",
      "Epoch 1:  99%|â–‰| 80/81 [00:01<00:00, 41.73it/s, loss=1.09, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:01<00:00, 41.12it/s, loss=1.09, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Epoch 2:  99%|â–‰| 80/81 [00:01<00:00, 43.15it/s, loss=1.09, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:01<00:00, 42.63it/s, loss=1.09, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Epoch 3:  99%|â–‰| 80/81 [00:02<00:00, 35.39it/s, loss=1.13, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:02<00:00, 35.21it/s, loss=1.13, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Epoch 4:  99%|â–‰| 80/81 [00:02<00:00, 34.78it/s, loss=1.11, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:02<00:00, 34.54it/s, loss=1.11, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Epoch 5:  99%|â–‰| 80/81 [00:02<00:00, 36.34it/s, loss=1.1, v_num=6x2i, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:02<00:00, 36.12it/s, loss=1.1, v_num=6x2i, ETH_val_ac\u001b[A\n",
      "Epoch 6:  99%|â–‰| 80/81 [00:02<00:00, 37.94it/s, loss=1.11, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:02<00:00, 37.58it/s, loss=1.11, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Epoch 7:  99%|â–‰| 80/81 [00:02<00:00, 36.70it/s, loss=1.11, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:02<00:00, 36.40it/s, loss=1.11, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Epoch 8:  99%|â–‰| 80/81 [00:01<00:00, 42.11it/s, loss=1.1, v_num=6x2i, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:01<00:00, 41.59it/s, loss=1.1, v_num=6x2i, ETH_val_ac\u001b[A\n",
      "Epoch 9:  99%|â–‰| 80/81 [00:01<00:00, 43.12it/s, loss=1.1, v_num=6x2i, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:01<00:00, 42.64it/s, loss=1.1, v_num=6x2i, ETH_val_ac\u001b[A\n",
      "Epoch 10:  99%|â–‰| 80/81 [00:01<00:00, 42.10it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:01<00:00, 41.71it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Epoch 11:  99%|â–‰| 80/81 [00:01<00:00, 40.45it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:02<00:00, 40.16it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Epoch 12:  99%|â–‰| 80/81 [00:02<00:00, 35.77it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:02<00:00, 35.46it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Epoch 13:  99%|â–‰| 80/81 [00:02<00:00, 31.37it/s, loss=1.12, v_num=6x2i, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:02<00:00, 31.17it/s, loss=1.12, v_num=6x2i, ETH_val_\u001b[A\n",
      "Epoch 14:  99%|â–‰| 80/81 [00:02<00:00, 30.94it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:02<00:00, 30.83it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Epoch 15:  99%|â–‰| 80/81 [00:02<00:00, 35.95it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:02<00:00, 34.84it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Epoch 16:  99%|â–‰| 80/81 [00:01<00:00, 40.18it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:02<00:00, 39.90it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Epoch 17:  99%|â–‰| 80/81 [00:02<00:00, 39.36it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:02<00:00, 39.10it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Epoch 18:  99%|â–‰| 80/81 [00:02<00:00, 29.56it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:02<00:00, 29.40it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Epoch 19:  99%|â–‰| 80/81 [00:02<00:00, 31.29it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:02<00:00, 31.15it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Epoch 20:  99%|â–‰| 80/81 [00:02<00:00, 38.78it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.023. Signaling Trainer to stop.\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:02<00:00, 38.54it/s, loss=1.1, v_num=6x2i, ETH_val_a\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:02<00:00, 38.48it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 92.94it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.4193548262119293,\n",
      " 'ETH_test_f1': 0.19648092985153198,\n",
      " 'test_loss': 1.0810121297836304}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 69160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012935-3f5p6x2i/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012935-3f5p6x2i/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.3125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.23148\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.08112\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1680\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 55\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463430\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.33255\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.24784\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.0983\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.2381\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.09688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.41935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.08101\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–…â–ˆâ–‡â–…â–…â–ƒâ–…â–ƒâ–†â–…â–…â–…â–…â–†â–…â–…â–ƒâ–…â–…â–‡â–…â–„â–†â–ƒâ–ƒâ–„â–…â–‡â–…â–„â–…â–â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–…â–ˆâ–…â–„â–„â–ƒâ–„â–ƒâ–†â–„â–…â–…â–…â–…â–„â–„â–ƒâ–…â–„â–†â–„â–ƒâ–†â–‚â–ƒâ–ƒâ–„â–†â–…â–ƒâ–„â–â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–ˆâ–â–â–â–…â–„â–…â–ˆâ–ƒâ–†â–„â–„â–…â–†â–…â–„â–…â–†â–…â–‚â–†â–…â–ƒâ–„â–…â–…â–„â–„â–„â–„â–…â–‡â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–‡â–‡â–â–†â–ƒâ–†â–ƒâ–ƒâ–ˆâ–…â–ˆâ–…â–…â–…â–…â–„â–‡â–†â–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–‚â–ˆâ–‡â–…â–‡â–…â–…â–…â–…â–†â–…â–†â–†â–…â–†â–…â–‡â–…â–ƒâ–…â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–„â–„â–…â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–â–â–â–ˆâ–ƒâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–‡â–‡â–â–‡â–‡â–‡â–â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–â–â–â–‡â–…â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–â–ƒâ–ˆâ–…â–†â–…â–ˆâ–…â–†â–†â–†â–†â–†â–†â–†â–‡â–†â–†â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3f5p6x2i\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:30:45.690738: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_single_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1wawr371\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013044-1wawr371\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 130   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.7 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 72/73 [00:00<00:00, 74.13it/s, loss=0.604, v_num=r371, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.536\n",
      "Epoch 0: 100%|â–ˆ| 73/73 [00:01<00:00, 68.08it/s, loss=0.604, v_num=r371, LTC_val_\n",
      "Epoch 1:  99%|â–‰| 72/73 [00:00<00:00, 78.55it/s, loss=0.611, v_num=r371, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 73/73 [00:00<00:00, 76.10it/s, loss=0.611, v_num=r371, LTC_val_\u001b[A\n",
      "Epoch 2:  99%|â–‰| 72/73 [00:01<00:00, 71.11it/s, loss=0.575, v_num=r371, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.035 >= min_delta = 0.003. New best score: 0.501\n",
      "Epoch 2: 100%|â–ˆ| 73/73 [00:01<00:00, 69.45it/s, loss=0.575, v_num=r371, LTC_val_\n",
      "Epoch 3:  99%|â–‰| 72/73 [00:00<00:00, 77.76it/s, loss=0.601, v_num=r371, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 73/73 [00:00<00:00, 75.44it/s, loss=0.601, v_num=r371, LTC_val_\u001b[A\n",
      "Epoch 4:  99%|â–‰| 72/73 [00:01<00:00, 70.76it/s, loss=0.569, v_num=r371, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 73/73 [00:01<00:00, 69.14it/s, loss=0.569, v_num=r371, LTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 72/73 [00:00<00:00, 79.18it/s, loss=0.589, v_num=r371, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 73/73 [00:00<00:00, 76.97it/s, loss=0.589, v_num=r371, LTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 72/73 [00:00<00:00, 73.55it/s, loss=0.557, v_num=r371, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 73/73 [00:01<00:00, 72.13it/s, loss=0.557, v_num=r371, LTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 72/73 [00:00<00:00, 86.58it/s, loss=0.582, v_num=r371, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 73/73 [00:00<00:00, 83.90it/s, loss=0.582, v_num=r371, LTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 72/73 [00:00<00:00, 73.65it/s, loss=0.552, v_num=r371, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 73/73 [00:01<00:00, 72.26it/s, loss=0.552, v_num=r371, LTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 72/73 [00:00<00:00, 89.44it/s, loss=0.537, v_num=r371, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 73/73 [00:00<00:00, 86.97it/s, loss=0.537, v_num=r371, LTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 72/73 [00:01<00:00, 69.43it/s, loss=0.566, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 73/73 [00:01<00:00, 67.96it/s, loss=0.566, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 72/73 [00:00<00:00, 85.54it/s, loss=0.573, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 73/73 [00:00<00:00, 83.34it/s, loss=0.573, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 72/73 [00:00<00:00, 72.03it/s, loss=0.581, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 73/73 [00:01<00:00, 70.22it/s, loss=0.581, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 72/73 [00:00<00:00, 87.69it/s, loss=0.591, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 73/73 [00:00<00:00, 85.26it/s, loss=0.591, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 72/73 [00:01<00:00, 68.77it/s, loss=0.546, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 73/73 [00:01<00:00, 66.93it/s, loss=0.546, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 72/73 [00:00<00:00, 82.91it/s, loss=0.526, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 73/73 [00:00<00:00, 80.88it/s, loss=0.526, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 72/73 [00:00<00:00, 74.56it/s, loss=0.567, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 0.487\n",
      "Epoch 16: 100%|â–ˆ| 73/73 [00:01<00:00, 72.74it/s, loss=0.567, v_num=r371, LTC_val\n",
      "Epoch 17:  99%|â–‰| 72/73 [00:00<00:00, 80.42it/s, loss=0.616, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 73/73 [00:00<00:00, 78.18it/s, loss=0.616, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 72/73 [00:01<00:00, 67.85it/s, loss=0.591, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 73/73 [00:01<00:00, 66.12it/s, loss=0.591, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 72/73 [00:00<00:00, 81.44it/s, loss=0.533, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 73/73 [00:00<00:00, 79.59it/s, loss=0.533, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 72/73 [00:00<00:00, 74.71it/s, loss=0.588, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 73/73 [00:01<00:00, 72.67it/s, loss=0.588, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 72/73 [00:00<00:00, 83.72it/s, loss=0.564, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 73/73 [00:00<00:00, 81.58it/s, loss=0.564, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 72/73 [00:00<00:00, 75.70it/s, loss=0.565, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 73/73 [00:00<00:00, 73.62it/s, loss=0.565, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 23:  99%|â–‰| 72/73 [00:01<00:00, 71.04it/s, loss=0.503, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 73/73 [00:01<00:00, 69.60it/s, loss=0.503, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 24:  99%|â–‰| 72/73 [00:00<00:00, 76.09it/s, loss=0.555, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 73/73 [00:00<00:00, 73.71it/s, loss=0.555, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 25:  99%|â–‰| 72/73 [00:00<00:00, 84.71it/s, loss=0.552, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 73/73 [00:00<00:00, 82.60it/s, loss=0.552, v_num=r371, LTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26:  99%|â–‰| 72/73 [00:00<00:00, 79.44it/s, loss=0.566, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 73/73 [00:00<00:00, 77.14it/s, loss=0.566, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 27:  99%|â–‰| 72/73 [00:01<00:00, 70.92it/s, loss=0.582, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 73/73 [00:01<00:00, 69.65it/s, loss=0.582, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 28:  99%|â–‰| 72/73 [00:01<00:00, 55.03it/s, loss=0.519, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 73/73 [00:01<00:00, 54.13it/s, loss=0.519, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 29:  99%|â–‰| 72/73 [00:00<00:00, 88.10it/s, loss=0.566, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 73/73 [00:00<00:00, 85.80it/s, loss=0.566, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 30:  99%|â–‰| 72/73 [00:01<00:00, 71.10it/s, loss=0.571, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 73/73 [00:01<00:00, 69.30it/s, loss=0.571, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 31:  99%|â–‰| 72/73 [00:00<00:00, 87.30it/s, loss=0.539, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 73/73 [00:00<00:00, 84.82it/s, loss=0.539, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 32:  99%|â–‰| 72/73 [00:00<00:00, 75.52it/s, loss=0.543, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 73/73 [00:00<00:00, 73.57it/s, loss=0.543, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 33:  99%|â–‰| 72/73 [00:00<00:00, 84.75it/s, loss=0.565, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 73/73 [00:00<00:00, 82.79it/s, loss=0.565, v_num=r371, LTC_valMetric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 0.472\n",
      "\n",
      "Epoch 34:  99%|â–‰| 72/73 [00:00<00:00, 74.48it/s, loss=0.579, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 73/73 [00:01<00:00, 72.38it/s, loss=0.579, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 35:  99%|â–‰| 72/73 [00:01<00:00, 71.04it/s, loss=0.581, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 73/73 [00:01<00:00, 69.35it/s, loss=0.581, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 36:  99%|â–‰| 72/73 [00:00<00:00, 72.22it/s, loss=0.577, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 73/73 [00:01<00:00, 70.29it/s, loss=0.577, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 37:  99%|â–‰| 72/73 [00:00<00:00, 84.09it/s, loss=0.556, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|â–ˆ| 73/73 [00:00<00:00, 82.01it/s, loss=0.556, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 38:  99%|â–‰| 72/73 [00:01<00:00, 63.62it/s, loss=0.545, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.035 >= min_delta = 0.003. New best score: 0.438\n",
      "Epoch 38: 100%|â–ˆ| 73/73 [00:01<00:00, 62.17it/s, loss=0.545, v_num=r371, LTC_val\n",
      "Epoch 39:  99%|â–‰| 72/73 [00:00<00:00, 83.28it/s, loss=0.603, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|â–ˆ| 73/73 [00:00<00:00, 80.70it/s, loss=0.603, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 40:  99%|â–‰| 72/73 [00:01<00:00, 65.26it/s, loss=0.534, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|â–ˆ| 73/73 [00:01<00:00, 63.49it/s, loss=0.534, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 41:  99%|â–‰| 72/73 [00:00<00:00, 80.19it/s, loss=0.536, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|â–ˆ| 73/73 [00:00<00:00, 78.31it/s, loss=0.536, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 42:  99%|â–‰| 72/73 [00:00<00:00, 73.43it/s, loss=0.572, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|â–ˆ| 73/73 [00:01<00:00, 71.60it/s, loss=0.572, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 43:  99%|â–‰| 72/73 [00:00<00:00, 86.01it/s, loss=0.596, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.430\n",
      "Epoch 43: 100%|â–ˆ| 73/73 [00:00<00:00, 83.25it/s, loss=0.596, v_num=r371, LTC_val\n",
      "Epoch 44:  99%|â–‰| 72/73 [00:01<00:00, 66.78it/s, loss=0.533, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|â–ˆ| 73/73 [00:01<00:00, 64.93it/s, loss=0.533, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 45:  99%|â–‰| 72/73 [00:00<00:00, 76.22it/s, loss=0.557, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|â–ˆ| 73/73 [00:00<00:00, 74.26it/s, loss=0.557, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 46:  99%|â–‰| 72/73 [00:01<00:00, 58.71it/s, loss=0.526, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|â–ˆ| 73/73 [00:01<00:00, 57.57it/s, loss=0.526, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 47:  99%|â–‰| 72/73 [00:00<00:00, 82.58it/s, loss=0.54, v_num=r371, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|â–ˆ| 73/73 [00:00<00:00, 79.93it/s, loss=0.54, v_num=r371, LTC_val_\u001b[A\n",
      "Epoch 48:  99%|â–‰| 72/73 [00:01<00:00, 70.15it/s, loss=0.572, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|â–ˆ| 73/73 [00:01<00:00, 68.82it/s, loss=0.572, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 49:  99%|â–‰| 72/73 [00:00<00:00, 77.99it/s, loss=0.579, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|â–ˆ| 73/73 [00:00<00:00, 75.83it/s, loss=0.579, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 50:  99%|â–‰| 72/73 [00:01<00:00, 71.23it/s, loss=0.552, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|â–ˆ| 73/73 [00:01<00:00, 69.92it/s, loss=0.552, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 51:  99%|â–‰| 72/73 [00:00<00:00, 89.57it/s, loss=0.581, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 0.420\n",
      "Epoch 51: 100%|â–ˆ| 73/73 [00:00<00:00, 87.17it/s, loss=0.581, v_num=r371, LTC_val\n",
      "Epoch 52:  99%|â–‰| 72/73 [00:01<00:00, 52.62it/s, loss=0.558, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|â–ˆ| 73/73 [00:01<00:00, 51.78it/s, loss=0.558, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 53:  99%|â–‰| 72/73 [00:01<00:00, 59.87it/s, loss=0.564, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|â–ˆ| 73/73 [00:01<00:00, 58.78it/s, loss=0.564, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 54:  99%|â–‰| 72/73 [00:00<00:00, 82.72it/s, loss=0.538, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|â–ˆ| 73/73 [00:00<00:00, 80.61it/s, loss=0.538, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 55:  99%|â–‰| 72/73 [00:00<00:00, 79.78it/s, loss=0.529, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|â–ˆ| 73/73 [00:00<00:00, 77.52it/s, loss=0.529, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 56:  99%|â–‰| 72/73 [00:01<00:00, 61.18it/s, loss=0.556, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|â–ˆ| 73/73 [00:01<00:00, 60.19it/s, loss=0.556, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 57:  99%|â–‰| 72/73 [00:01<00:00, 69.40it/s, loss=0.557, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|â–ˆ| 73/73 [00:01<00:00, 67.85it/s, loss=0.557, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 58:  99%|â–‰| 72/73 [00:00<00:00, 81.24it/s, loss=0.585, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|â–ˆ| 73/73 [00:00<00:00, 78.88it/s, loss=0.585, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 59:  99%|â–‰| 72/73 [00:00<00:00, 74.17it/s, loss=0.54, v_num=r371, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|â–ˆ| 73/73 [00:01<00:00, 72.28it/s, loss=0.54, v_num=r371, LTC_val_\u001b[A\n",
      "Epoch 60:  99%|â–‰| 72/73 [00:00<00:00, 74.51it/s, loss=0.563, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 60: 100%|â–ˆ| 73/73 [00:01<00:00, 72.95it/s, loss=0.563, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 61:  99%|â–‰| 72/73 [00:01<00:00, 54.24it/s, loss=0.569, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.415\n",
      "Epoch 61: 100%|â–ˆ| 73/73 [00:01<00:00, 53.53it/s, loss=0.569, v_num=r371, LTC_val\n",
      "Epoch 62:  99%|â–‰| 72/73 [00:00<00:00, 87.76it/s, loss=0.513, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 62: 100%|â–ˆ| 73/73 [00:00<00:00, 84.86it/s, loss=0.513, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 63:  99%|â–‰| 72/73 [00:01<00:00, 59.10it/s, loss=0.532, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 63: 100%|â–ˆ| 73/73 [00:01<00:00, 57.94it/s, loss=0.532, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 64:  99%|â–‰| 72/73 [00:00<00:00, 85.63it/s, loss=0.522, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 64: 100%|â–ˆ| 73/73 [00:00<00:00, 83.02it/s, loss=0.522, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 65:  99%|â–‰| 72/73 [00:01<00:00, 64.04it/s, loss=0.528, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 65: 100%|â–ˆ| 73/73 [00:01<00:00, 62.99it/s, loss=0.528, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 66:  99%|â–‰| 72/73 [00:00<00:00, 85.46it/s, loss=0.545, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 66: 100%|â–ˆ| 73/73 [00:00<00:00, 82.66it/s, loss=0.545, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 67:  99%|â–‰| 72/73 [00:01<00:00, 64.80it/s, loss=0.559, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 67: 100%|â–ˆ| 73/73 [00:01<00:00, 63.73it/s, loss=0.559, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 68:  99%|â–‰| 72/73 [00:00<00:00, 84.99it/s, loss=0.542, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 68: 100%|â–ˆ| 73/73 [00:00<00:00, 82.48it/s, loss=0.542, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 69:  99%|â–‰| 72/73 [00:01<00:00, 69.36it/s, loss=0.503, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 69: 100%|â–ˆ| 73/73 [00:01<00:00, 68.06it/s, loss=0.503, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 70:  99%|â–‰| 72/73 [00:00<00:00, 72.78it/s, loss=0.53, v_num=r371, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.410\n",
      "Epoch 70: 100%|â–ˆ| 73/73 [00:01<00:00, 70.83it/s, loss=0.53, v_num=r371, LTC_val_\n",
      "Epoch 71:  99%|â–‰| 72/73 [00:01<00:00, 69.90it/s, loss=0.543, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 71: 100%|â–ˆ| 73/73 [00:01<00:00, 68.17it/s, loss=0.543, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 72:  99%|â–‰| 72/73 [00:01<00:00, 62.15it/s, loss=0.589, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 72: 100%|â–ˆ| 73/73 [00:01<00:00, 60.68it/s, loss=0.589, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 73:  99%|â–‰| 72/73 [00:01<00:00, 70.30it/s, loss=0.536, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 73: 100%|â–ˆ| 73/73 [00:01<00:00, 68.97it/s, loss=0.536, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 74:  99%|â–‰| 72/73 [00:01<00:00, 70.32it/s, loss=0.525, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 74: 100%|â–ˆ| 73/73 [00:01<00:00, 68.20it/s, loss=0.525, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 75:  99%|â–‰| 72/73 [00:00<00:00, 81.44it/s, loss=0.529, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 75: 100%|â–ˆ| 73/73 [00:00<00:00, 79.46it/s, loss=0.529, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 76:  99%|â–‰| 72/73 [00:01<00:00, 64.59it/s, loss=0.558, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 76: 100%|â–ˆ| 73/73 [00:01<00:00, 63.23it/s, loss=0.558, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 77:  99%|â–‰| 72/73 [00:00<00:00, 85.76it/s, loss=0.55, v_num=r371, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 77: 100%|â–ˆ| 73/73 [00:00<00:00, 83.61it/s, loss=0.55, v_num=r371, LTC_val_\u001b[A\n",
      "Epoch 78:  99%|â–‰| 72/73 [00:01<00:00, 69.59it/s, loss=0.553, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 78: 100%|â–ˆ| 73/73 [00:01<00:00, 67.80it/s, loss=0.553, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 79:  99%|â–‰| 72/73 [00:00<00:00, 74.95it/s, loss=0.565, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 79: 100%|â–ˆ| 73/73 [00:00<00:00, 73.30it/s, loss=0.565, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 80:  99%|â–‰| 72/73 [00:00<00:00, 72.70it/s, loss=0.532, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 80: 100%|â–ˆ| 73/73 [00:01<00:00, 70.67it/s, loss=0.532, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 81:  99%|â–‰| 72/73 [00:00<00:00, 83.41it/s, loss=0.543, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 81: 100%|â–ˆ| 73/73 [00:00<00:00, 81.35it/s, loss=0.543, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 82:  99%|â–‰| 72/73 [00:01<00:00, 68.04it/s, loss=0.478, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 82: 100%|â–ˆ| 73/73 [00:01<00:00, 66.23it/s, loss=0.478, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 83:  99%|â–‰| 72/73 [00:00<00:00, 80.02it/s, loss=0.545, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 83: 100%|â–ˆ| 73/73 [00:00<00:00, 77.07it/s, loss=0.545, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 84:  99%|â–‰| 72/73 [00:01<00:00, 45.86it/s, loss=0.521, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 84: 100%|â–ˆ| 73/73 [00:01<00:00, 45.50it/s, loss=0.521, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 85:  99%|â–‰| 72/73 [00:00<00:00, 81.03it/s, loss=0.538, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 85: 100%|â–ˆ| 73/73 [00:00<00:00, 78.36it/s, loss=0.538, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 86:  99%|â–‰| 72/73 [00:01<00:00, 70.81it/s, loss=0.576, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 86: 100%|â–ˆ| 73/73 [00:01<00:00, 69.53it/s, loss=0.576, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 87:  99%|â–‰| 72/73 [00:00<00:00, 83.97it/s, loss=0.52, v_num=r371, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 87: 100%|â–ˆ| 73/73 [00:00<00:00, 81.21it/s, loss=0.52, v_num=r371, LTC_val_\u001b[A\n",
      "Epoch 88:  99%|â–‰| 72/73 [00:01<00:00, 68.58it/s, loss=0.497, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 88: 100%|â–ˆ| 73/73 [00:01<00:00, 67.38it/s, loss=0.497, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 89:  99%|â–‰| 72/73 [00:01<00:00, 69.58it/s, loss=0.506, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 89: 100%|â–ˆ| 73/73 [00:01<00:00, 68.04it/s, loss=0.506, v_num=r371, LTC_val\u001b[A\n",
      "Epoch 90:  99%|â–‰| 72/73 [00:00<00:00, 78.12it/s, loss=0.542, v_num=r371, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.410. Signaling Trainer to stop.\n",
      "Epoch 90: 100%|â–ˆ| 73/73 [00:00<00:00, 76.42it/s, loss=0.542, v_num=r371, LTC_val\n",
      "Epoch 90: 100%|â–ˆ| 73/73 [00:00<00:00, 76.19it/s, loss=0.542, v_num=r371, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 130.61it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.75,\n",
      " 'LTC_test_f1': 0.7414525747299194,\n",
      " 'test_loss': 0.43861886858940125}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 69362\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013044-1wawr371/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013044-1wawr371/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.26427\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 6552\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 102\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463546\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 313\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.74826\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.73695\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.51494\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.61905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.42779\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.74145\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.43862\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–…â–…â–ƒâ–ˆâ–…â–‚â–†â–‡â–…â–…â–ƒâ–†â–„â–ˆâ–…â–…â–„â–…â–…â–…â–‚â–…â–ˆâ–†â–â–ˆâ–„â–…â–…â–†â–‡â–†â–…â–…â–…â–„â–†â–ƒâ–…â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–„â–…â–ƒâ–ˆâ–…â–â–†â–‡â–…â–…â–ƒâ–†â–ƒâ–ˆâ–…â–„â–„â–„â–„â–…â–‚â–„â–ˆâ–†â–â–ˆâ–„â–…â–…â–†â–‡â–†â–…â–…â–…â–ƒâ–†â–ƒâ–„â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–†â–…â–†â–â–†â–…â–‚â–ƒâ–„â–„â–…â–ƒâ–†â–â–†â–‡â–…â–„â–‡â–ƒâ–…â–ƒâ–‚â–‚â–ˆâ–â–„â–†â–ƒâ–„â–ƒâ–ƒâ–„â–„â–ƒâ–…â–‚â–ˆâ–…â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–†â–†â–†â–‡â–†â–‡â–†â–‡â–†â–†â–†â–†â–‡â–‡â–‡â–†â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–â–…â–…â–†â–‡â–†â–†â–†â–†â–‡â–†â–†â–†â–‡â–†â–‡â–†â–‡â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–„â–„â–„â–ƒâ–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–ƒâ–†â–†â–â–â–†â–ƒâ–†â–ƒâ–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–†â–ƒâ–†â–†â–†â–†â–†â–†â–†â–†â–ƒâ–†â–†â–ˆâ–†â–†â–†â–†â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–ƒâ–…â–…â–â–â–…â–ƒâ–…â–ƒâ–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–…â–ƒâ–…â–…â–†â–†â–†â–†â–†â–†â–ƒâ–†â–†â–ˆâ–…â–†â–†â–…â–…â–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–‡â–…â–†â–ˆâ–‡â–…â–…â–„â–†â–‡â–†â–†â–‡â–…â–†â–…â–†â–„â–‚â–â–„â–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–„â–â–‚â–â–‚â–â–â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_single_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1wawr371\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:32:36.867484: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_single_lstm_loss_weighted_binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2ub8061i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013235-2ub8061i\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 130   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.7 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 73/74 [00:00<00:00, 74.80it/s, loss=0.72, v_num=061i, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.697\n",
      "Epoch 0: 100%|â–ˆ| 74/74 [00:01<00:00, 72.68it/s, loss=0.72, v_num=061i, LTC_val_a\n",
      "Epoch 1:  99%|â–‰| 73/74 [00:00<00:00, 73.39it/s, loss=0.704, v_num=061i, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 74/74 [00:01<00:00, 71.15it/s, loss=0.704, v_num=061i, LTC_val_\u001b[A\n",
      "Epoch 2:  99%|â–‰| 73/74 [00:00<00:00, 75.93it/s, loss=0.719, v_num=061i, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 74/74 [00:01<00:00, 73.50it/s, loss=0.719, v_num=061i, LTC_val_\u001b[A\n",
      "Epoch 3:  99%|â–‰| 73/74 [00:00<00:00, 78.17it/s, loss=0.693, v_num=061i, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 74/74 [00:00<00:00, 76.12it/s, loss=0.693, v_num=061i, LTC_val_\u001b[A\n",
      "Epoch 4:  99%|â–‰| 73/74 [00:00<00:00, 75.44it/s, loss=0.693, v_num=061i, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 74/74 [00:01<00:00, 72.50it/s, loss=0.693, v_num=061i, LTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 73/74 [00:00<00:00, 75.71it/s, loss=0.703, v_num=061i, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 0.693\n",
      "Epoch 5: 100%|â–ˆ| 74/74 [00:01<00:00, 73.91it/s, loss=0.703, v_num=061i, LTC_val_\n",
      "Epoch 6:  99%|â–‰| 73/74 [00:00<00:00, 74.50it/s, loss=0.697, v_num=061i, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 74/74 [00:01<00:00, 72.35it/s, loss=0.697, v_num=061i, LTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 73/74 [00:01<00:00, 62.44it/s, loss=0.708, v_num=061i, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 74/74 [00:01<00:00, 61.11it/s, loss=0.708, v_num=061i, LTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 73/74 [00:01<00:00, 72.45it/s, loss=0.691, v_num=061i, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 74/74 [00:01<00:00, 70.02it/s, loss=0.691, v_num=061i, LTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 73/74 [00:00<00:00, 84.47it/s, loss=0.698, v_num=061i, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 74/74 [00:00<00:00, 82.35it/s, loss=0.698, v_num=061i, LTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 73/74 [00:00<00:00, 75.72it/s, loss=0.694, v_num=061i, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 74/74 [00:01<00:00, 73.41it/s, loss=0.694, v_num=061i, LTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 73/74 [00:00<00:00, 73.62it/s, loss=0.686, v_num=061i, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 74/74 [00:01<00:00, 71.87it/s, loss=0.686, v_num=061i, LTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 73/74 [00:01<00:00, 71.73it/s, loss=0.694, v_num=061i, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 74/74 [00:01<00:00, 69.82it/s, loss=0.694, v_num=061i, LTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 73/74 [00:00<00:00, 85.38it/s, loss=0.693, v_num=061i, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 74/74 [00:00<00:00, 83.28it/s, loss=0.693, v_num=061i, LTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 73/74 [00:01<00:00, 67.15it/s, loss=0.691, v_num=061i, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 74/74 [00:01<00:00, 65.61it/s, loss=0.691, v_num=061i, LTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 73/74 [00:00<00:00, 78.58it/s, loss=0.696, v_num=061i, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 74/74 [00:00<00:00, 76.89it/s, loss=0.696, v_num=061i, LTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 73/74 [00:01<00:00, 71.45it/s, loss=0.698, v_num=061i, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 74/74 [00:01<00:00, 69.78it/s, loss=0.698, v_num=061i, LTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 73/74 [00:00<00:00, 85.67it/s, loss=0.691, v_num=061i, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 74/74 [00:00<00:00, 83.51it/s, loss=0.691, v_num=061i, LTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 73/74 [00:00<00:00, 76.22it/s, loss=0.693, v_num=061i, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 74/74 [00:01<00:00, 73.76it/s, loss=0.693, v_num=061i, LTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 73/74 [00:00<00:00, 82.19it/s, loss=0.69, v_num=061i, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 74/74 [00:00<00:00, 80.20it/s, loss=0.69, v_num=061i, LTC_val_\u001b[A\n",
      "Epoch 20:  99%|â–‰| 73/74 [00:01<00:00, 71.44it/s, loss=0.691, v_num=061i, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:01<00:00, 69.80it/s, loss=0.691, v_num=061i, LTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 73/74 [00:00<00:00, 79.22it/s, loss=0.695, v_num=061i, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 74/74 [00:00<00:00, 77.37it/s, loss=0.695, v_num=061i, LTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 73/74 [00:00<00:00, 75.96it/s, loss=0.69, v_num=061i, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 74/74 [00:01<00:00, 73.95it/s, loss=0.69, v_num=061i, LTC_val_\u001b[A\n",
      "Epoch 23:  99%|â–‰| 73/74 [00:00<00:00, 81.23it/s, loss=0.697, v_num=061i, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 74/74 [00:00<00:00, 79.32it/s, loss=0.697, v_num=061i, LTC_val\u001b[A\n",
      "Epoch 24:  99%|â–‰| 73/74 [00:00<00:00, 79.29it/s, loss=0.69, v_num=061i, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 74/74 [00:00<00:00, 76.86it/s, loss=0.69, v_num=061i, LTC_val_\u001b[A\n",
      "Epoch 25:  99%|â–‰| 73/74 [00:01<00:00, 68.97it/s, loss=0.691, v_num=061i, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.693. Signaling Trainer to stop.\n",
      "Epoch 25: 100%|â–ˆ| 74/74 [00:01<00:00, 67.71it/s, loss=0.691, v_num=061i, LTC_val\n",
      "Epoch 25: 100%|â–ˆ| 74/74 [00:01<00:00, 67.53it/s, loss=0.691, v_num=061i, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 148.05it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.5,\n",
      " 'LTC_test_f1': 0.3272727429866791,\n",
      " 'test_loss': 0.6927607655525208}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 69633\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013235-2ub8061i/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013235-2ub8061i/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.73333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.6755\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1898\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 35\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463590\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 89\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.53282\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.51489\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69464\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.69479\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.32727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.69276\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–ƒâ–„â–„â–…â–‡â–ˆâ–„â–â–…â–ƒâ–â–†â–‡â–ƒâ–†â–â–ƒâ–„â–‚â–‚â–†â–â–‚â–ˆâ–â–„â–…â–‡â–ƒâ–‚â–‚â–‚â–†â–â–…â–†â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–ƒâ–„â–ƒâ–…â–‡â–ƒâ–„â–‚â–…â–ƒâ–‚â–†â–†â–ƒâ–†â–‚â–â–„â–â–â–…â–‚â–ƒâ–ˆâ–â–ƒâ–…â–‡â–ƒâ–ƒâ–‚â–ƒâ–†â–‚â–…â–†â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–…â–‡â–…â–†â–ƒâ–â–„â–‡â–…â–„â–‡â–„â–„â–…â–„â–†â–…â–…â–†â–†â–„â–†â–ˆâ–‚â–‡â–†â–„â–ƒâ–…â–„â–…â–†â–„â–…â–„â–„â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–ƒâ–†â–â–ƒâ–†â–ƒâ–…â–…â–„â–†â–…â–…â–„â–„â–ˆâ–†â–…â–…â–†â–ƒâ–…â–‚â–…â–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–„â–†â–â–â–‚â–„â–†â–„â–…â–†â–†â–…â–…â–‚â–‡â–†â–â–„â–…â–„â–…â–ƒâ–†â–†â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–…â–‡â–…â–‚â–„â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‡â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–â–ƒâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ƒâ–„â–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–â–â–‚â–…â–‚â–„â–‚â–‡â–‚â–â–„â–ˆâ–‚â–†â–ƒâ–ƒâ–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_single_lstm_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2ub8061i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:33:21.378120: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_single_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3glxbifx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013319-3glxbifx\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 195   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.8 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 72/73 [00:01<00:00, 71.32it/s, loss=1.14, v_num=bifx, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.047\n",
      "Epoch 0: 100%|â–ˆ| 73/73 [00:01<00:00, 65.20it/s, loss=1.14, v_num=bifx, LTC_val_a\n",
      "Epoch 1:  99%|â–‰| 72/73 [00:00<00:00, 78.04it/s, loss=1.1, v_num=bifx, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 73/73 [00:00<00:00, 76.08it/s, loss=1.1, v_num=bifx, LTC_val_ac\u001b[A\n",
      "Epoch 2:  99%|â–‰| 72/73 [00:01<00:00, 71.59it/s, loss=1.09, v_num=bifx, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 73/73 [00:01<00:00, 69.75it/s, loss=1.09, v_num=bifx, LTC_val_a\u001b[A\n",
      "Epoch 3:  99%|â–‰| 72/73 [00:00<00:00, 86.59it/s, loss=1.08, v_num=bifx, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.067 >= min_delta = 0.003. New best score: 0.979\n",
      "Epoch 3: 100%|â–ˆ| 73/73 [00:00<00:00, 84.16it/s, loss=1.08, v_num=bifx, LTC_val_a\n",
      "Epoch 4:  99%|â–‰| 72/73 [00:01<00:00, 71.15it/s, loss=1.05, v_num=bifx, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 73/73 [00:01<00:00, 69.41it/s, loss=1.05, v_num=bifx, LTC_val_a\u001b[A\n",
      "Epoch 5:  99%|â–‰| 72/73 [00:00<00:00, 73.06it/s, loss=0.988, v_num=bifx, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 73/73 [00:01<00:00, 71.41it/s, loss=0.988, v_num=bifx, LTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 72/73 [00:01<00:00, 66.68it/s, loss=1, v_num=bifx, LTC_val_acc=\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.976\n",
      "Epoch 6: 100%|â–ˆ| 73/73 [00:01<00:00, 65.17it/s, loss=1, v_num=bifx, LTC_val_acc=\n",
      "Epoch 7:  99%|â–‰| 72/73 [00:00<00:00, 78.32it/s, loss=0.972, v_num=bifx, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 73/73 [00:00<00:00, 76.19it/s, loss=0.972, v_num=bifx, LTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 72/73 [00:01<00:00, 67.60it/s, loss=0.991, v_num=bifx, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 73/73 [00:01<00:00, 66.04it/s, loss=0.991, v_num=bifx, LTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 72/73 [00:00<00:00, 82.67it/s, loss=0.998, v_num=bifx, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 73/73 [00:00<00:00, 80.73it/s, loss=0.998, v_num=bifx, LTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 72/73 [00:01<00:00, 71.26it/s, loss=0.979, v_num=bifx, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 73/73 [00:01<00:00, 69.70it/s, loss=0.979, v_num=bifx, LTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 72/73 [00:00<00:00, 87.68it/s, loss=0.988, v_num=bifx, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 73/73 [00:00<00:00, 85.45it/s, loss=0.988, v_num=bifx, LTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 72/73 [00:00<00:00, 74.85it/s, loss=0.99, v_num=bifx, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.036 >= min_delta = 0.003. New best score: 0.939\n",
      "Epoch 12: 100%|â–ˆ| 73/73 [00:01<00:00, 72.89it/s, loss=0.99, v_num=bifx, LTC_val_\n",
      "Epoch 13:  99%|â–‰| 72/73 [00:00<00:00, 84.04it/s, loss=1, v_num=bifx, LTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 73/73 [00:00<00:00, 81.45it/s, loss=1, v_num=bifx, LTC_val_acc\u001b[A\n",
      "Epoch 14:  99%|â–‰| 72/73 [00:01<00:00, 67.88it/s, loss=0.969, v_num=bifx, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 73/73 [00:01<00:00, 66.29it/s, loss=0.969, v_num=bifx, LTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 72/73 [00:00<00:00, 84.96it/s, loss=0.938, v_num=bifx, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 73/73 [00:00<00:00, 81.70it/s, loss=0.938, v_num=bifx, LTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 72/73 [00:00<00:00, 75.53it/s, loss=0.96, v_num=bifx, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 73/73 [00:00<00:00, 73.03it/s, loss=0.96, v_num=bifx, LTC_val_\u001b[A\n",
      "Epoch 17:  99%|â–‰| 72/73 [00:00<00:00, 74.30it/s, loss=0.961, v_num=bifx, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 73/73 [00:01<00:00, 72.88it/s, loss=0.961, v_num=bifx, LTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 72/73 [00:00<00:00, 74.19it/s, loss=0.935, v_num=bifx, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 73/73 [00:01<00:00, 72.35it/s, loss=0.935, v_num=bifx, LTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 72/73 [00:00<00:00, 85.18it/s, loss=0.96, v_num=bifx, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 73/73 [00:00<00:00, 83.17it/s, loss=0.96, v_num=bifx, LTC_val_\u001b[A\n",
      "Epoch 20:  99%|â–‰| 72/73 [00:01<00:00, 64.97it/s, loss=0.947, v_num=bifx, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 73/73 [00:01<00:00, 63.61it/s, loss=0.947, v_num=bifx, LTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 72/73 [00:00<00:00, 85.59it/s, loss=0.957, v_num=bifx, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 73/73 [00:00<00:00, 83.55it/s, loss=0.957, v_num=bifx, LTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 72/73 [00:00<00:00, 73.61it/s, loss=0.997, v_num=bifx, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 73/73 [00:01<00:00, 71.69it/s, loss=0.997, v_num=bifx, LTC_val\u001b[A\n",
      "Epoch 23:  99%|â–‰| 72/73 [00:00<00:00, 84.36it/s, loss=0.958, v_num=bifx, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 73/73 [00:00<00:00, 82.22it/s, loss=0.958, v_num=bifx, LTC_val\u001b[A\n",
      "Epoch 24:  99%|â–‰| 72/73 [00:00<00:00, 72.83it/s, loss=0.939, v_num=bifx, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 73/73 [00:01<00:00, 70.93it/s, loss=0.939, v_num=bifx, LTC_val\u001b[A\n",
      "Epoch 25:  99%|â–‰| 72/73 [00:00<00:00, 79.81it/s, loss=0.966, v_num=bifx, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 73/73 [00:00<00:00, 77.73it/s, loss=0.966, v_num=bifx, LTC_val\u001b[A\n",
      "Epoch 26:  99%|â–‰| 72/73 [00:00<00:00, 76.77it/s, loss=0.96, v_num=bifx, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 73/73 [00:00<00:00, 74.65it/s, loss=0.96, v_num=bifx, LTC_val_\u001b[A\n",
      "Epoch 27:  99%|â–‰| 72/73 [00:00<00:00, 75.01it/s, loss=1.01, v_num=bifx, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 73/73 [00:00<00:00, 73.28it/s, loss=1.01, v_num=bifx, LTC_val_\u001b[A\n",
      "Epoch 28:  99%|â–‰| 72/73 [00:00<00:00, 78.08it/s, loss=0.961, v_num=bifx, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 73/73 [00:00<00:00, 75.77it/s, loss=0.961, v_num=bifx, LTC_val\u001b[A\n",
      "Epoch 29:  99%|â–‰| 72/73 [00:01<00:00, 66.73it/s, loss=0.931, v_num=bifx, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 73/73 [00:01<00:00, 65.49it/s, loss=0.931, v_num=bifx, LTC_val\u001b[A\n",
      "Epoch 30:  99%|â–‰| 72/73 [00:00<00:00, 75.28it/s, loss=0.997, v_num=bifx, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 73/73 [00:01<00:00, 72.82it/s, loss=0.997, v_num=bifx, LTC_val\u001b[A\n",
      "Epoch 31:  99%|â–‰| 72/73 [00:00<00:00, 78.56it/s, loss=0.912, v_num=bifx, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 73/73 [00:00<00:00, 76.55it/s, loss=0.912, v_num=bifx, LTC_val\u001b[A\n",
      "Epoch 32:  99%|â–‰| 72/73 [00:00<00:00, 74.95it/s, loss=1, v_num=bifx, LTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.939. Signaling Trainer to stop.\n",
      "Epoch 32: 100%|â–ˆ| 73/73 [00:01<00:00, 72.92it/s, loss=1, v_num=bifx, LTC_val_acc\n",
      "Epoch 32: 100%|â–ˆ| 73/73 [00:01<00:00, 72.64it/s, loss=1, v_num=bifx, LTC_val_acc\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 105.73it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.6785714030265808,\n",
      " 'LTC_test_f1': 0.6618326306343079,\n",
      " 'test_loss': 0.8116836547851562}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 69817\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013319-3glxbifx/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013319-3glxbifx/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.62222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.75376\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2376\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463641\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 113\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.47049\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.44906\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.96406\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.39524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.04748\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.67857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.66183\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.81168\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–…â–…â–‚â–â–‚â–‚â–‚â–†â–„â–„â–‚â–‚â–„â–…â–‚â–ƒâ–‡â–‚â–„â–…â–„â–ƒâ–„â–‚â–…â–†â–‚â–…â–†â–‡â–ˆâ–‡â–ƒâ–†â–…â–„â–„â–„â–‚â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–„â–…â–‚â–â–ƒâ–‚â–ƒâ–†â–„â–„â–ƒâ–ƒâ–ƒâ–…â–‚â–ƒâ–‡â–ƒâ–„â–„â–ƒâ–ƒâ–„â–ƒâ–…â–†â–‚â–†â–†â–ˆâ–ˆâ–‡â–ƒâ–‡â–…â–„â–„â–„â–ƒâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–„â–„â–…â–†â–†â–…â–…â–ƒâ–„â–„â–…â–…â–ƒâ–ƒâ–…â–„â–â–†â–‚â–â–„â–…â–ˆâ–†â–ƒâ–â–…â–â–ƒâ–â–‚â–‚â–…â–â–ƒâ–†â–‚â–…â–„â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–ƒâ–ƒâ–â–ƒâ–ƒâ–„â–„â–„â–…â–†â–†â–†â–†â–…â–‡â–‡â–‡â–‡â–†â–‡â–†â–‡â–‡â–†â–‡â–‡â–‡â–†â–‡â–‡â–‡â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–‚â–â–â–ƒâ–„â–„â–…â–„â–…â–†â–†â–†â–‡â–†â–‡â–‡â–‡â–‡â–†â–ˆâ–‡â–ˆâ–ˆâ–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–‡â–‡â–‡â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–†â–…â–ƒâ–ˆâ–…â–†â–ˆâ–…â–…â–â–ˆâ–…â–ˆâ–†â–ƒâ–ˆâ–†â–…â–ˆâ–†â–…â–ˆâ–ƒâ–†â–â–†â–ƒâ–ƒâ–…â–ˆâ–…â–ˆâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–…â–„â–ƒâ–ˆâ–„â–†â–ˆâ–…â–„â–â–ˆâ–„â–ˆâ–†â–ƒâ–ˆâ–†â–…â–ˆâ–†â–…â–ˆâ–ƒâ–†â–â–†â–ƒâ–ƒâ–„â–ˆâ–„â–ˆâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–„â–…â–…â–‚â–ƒâ–‚â–‚â–†â–ˆâ–…â–ƒâ–…â–â–‚â–‡â–…â–„â–…â–†â–„â–†â–†â–‡â–„â–†â–„â–ˆâ–„â–ƒâ–†â–‡â–‚â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_single_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3glxbifx\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:34:11.075424: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_single_lstm_loss_weighted_multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2xwo69me\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013409-2xwo69me\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 195   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.8 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/74 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 73/74 [00:00<00:00, 79.62it/s, loss=1.12, v_num=69me, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.112\n",
      "Epoch 0: 100%|â–ˆ| 74/74 [00:00<00:00, 76.98it/s, loss=1.12, v_num=69me, LTC_val_a\n",
      "Epoch 1:  99%|â–‰| 73/74 [00:00<00:00, 74.11it/s, loss=1.13, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 1.095\n",
      "Epoch 1: 100%|â–ˆ| 74/74 [00:01<00:00, 71.69it/s, loss=1.13, v_num=69me, LTC_val_a\n",
      "Epoch 2:  99%|â–‰| 73/74 [00:00<00:00, 74.04it/s, loss=1.12, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 1.091\n",
      "Epoch 2: 100%|â–ˆ| 74/74 [00:01<00:00, 71.92it/s, loss=1.12, v_num=69me, LTC_val_a\n",
      "Epoch 3:  99%|â–‰| 73/74 [00:01<00:00, 72.47it/s, loss=1.11, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 74/74 [00:01<00:00, 70.83it/s, loss=1.11, v_num=69me, LTC_val_a\u001b[A\n",
      "Epoch 4:  99%|â–‰| 73/74 [00:01<00:00, 69.23it/s, loss=1.1, v_num=69me, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 1.081\n",
      "Epoch 4: 100%|â–ˆ| 74/74 [00:01<00:00, 67.50it/s, loss=1.1, v_num=69me, LTC_val_ac\n",
      "Epoch 5:  99%|â–‰| 73/74 [00:00<00:00, 76.90it/s, loss=1.11, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 74/74 [00:00<00:00, 74.99it/s, loss=1.11, v_num=69me, LTC_val_a\u001b[A\n",
      "Epoch 6:  99%|â–‰| 73/74 [00:01<00:00, 67.28it/s, loss=1.11, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 74/74 [00:01<00:00, 65.53it/s, loss=1.11, v_num=69me, LTC_val_a\u001b[A\n",
      "Epoch 7:  99%|â–‰| 73/74 [00:00<00:00, 83.66it/s, loss=1.1, v_num=69me, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 74/74 [00:00<00:00, 80.09it/s, loss=1.1, v_num=69me, LTC_val_ac\u001b[A\n",
      "Epoch 8:  99%|â–‰| 73/74 [00:01<00:00, 65.61it/s, loss=1.11, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 74/74 [00:01<00:00, 64.13it/s, loss=1.11, v_num=69me, LTC_val_a\u001b[A\n",
      "Epoch 9:  99%|â–‰| 73/74 [00:01<00:00, 71.77it/s, loss=1.1, v_num=69me, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 74/74 [00:01<00:00, 70.05it/s, loss=1.1, v_num=69me, LTC_val_ac\u001b[A\n",
      "Epoch 10:  99%|â–‰| 73/74 [00:01<00:00, 70.65it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 74/74 [00:01<00:00, 69.07it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Epoch 11:  99%|â–‰| 73/74 [00:00<00:00, 87.37it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 74/74 [00:00<00:00, 85.14it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Epoch 12:  99%|â–‰| 73/74 [00:01<00:00, 72.00it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 74/74 [00:01<00:00, 70.11it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Epoch 13:  99%|â–‰| 73/74 [00:00<00:00, 88.27it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 74/74 [00:00<00:00, 85.89it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Epoch 14:  99%|â–‰| 73/74 [00:01<00:00, 72.65it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 74/74 [00:01<00:00, 71.02it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Epoch 15:  99%|â–‰| 73/74 [00:00<00:00, 79.06it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 74/74 [00:00<00:00, 77.26it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Epoch 16:  99%|â–‰| 73/74 [00:01<00:00, 60.05it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 74/74 [00:01<00:00, 59.17it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Epoch 17:  99%|â–‰| 73/74 [00:00<00:00, 88.39it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 74/74 [00:00<00:00, 86.16it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Epoch 18:  99%|â–‰| 73/74 [00:01<00:00, 69.46it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 74/74 [00:01<00:00, 68.02it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Epoch 19:  99%|â–‰| 73/74 [00:00<00:00, 79.44it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 74/74 [00:00<00:00, 77.45it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Epoch 20:  99%|â–‰| 73/74 [00:01<00:00, 70.83it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:01<00:00, 69.37it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Epoch 21:  99%|â–‰| 73/74 [00:00<00:00, 88.80it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 74/74 [00:00<00:00, 86.57it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Epoch 22:  99%|â–‰| 73/74 [00:01<00:00, 69.28it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 74/74 [00:01<00:00, 67.77it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Epoch 23:  99%|â–‰| 73/74 [00:00<00:00, 89.03it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 74/74 [00:00<00:00, 86.63it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Epoch 24:  99%|â–‰| 73/74 [00:01<00:00, 71.87it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.081. Signaling Trainer to stop.\n",
      "Epoch 24: 100%|â–ˆ| 74/74 [00:01<00:00, 69.92it/s, loss=1.1, v_num=69me, LTC_val_a\n",
      "Epoch 24: 100%|â–ˆ| 74/74 [00:01<00:00, 69.71it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 109.56it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.25,\n",
      " 'LTC_test_f1': 0.13151928782463074,\n",
      " 'test_loss': 1.1005421876907349}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 69978\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013409-2xwo69me/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013409-2xwo69me/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.25877\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.09018\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 34\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 86\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.32211\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.27431\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09972\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.07407\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.10292\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.13152\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.10054\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–†â–ˆâ–†â–‚â–…â–†â–…â–‡â–ƒâ–„â–…â–…â–†â–„â–â–†â–…â–…â–‚â–…â–ˆâ–†â–„â–‡â–…â–…â–…â–†â–…â–…â–ƒâ–ˆâ–†â–‡â–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–†â–ˆâ–†â–‚â–…â–†â–„â–‡â–ƒâ–„â–…â–†â–†â–ƒâ–â–†â–„â–…â–‚â–„â–‡â–…â–„â–‡â–„â–…â–†â–†â–…â–†â–ƒâ–ˆâ–‡â–‡â–†â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–…â–â–„â–ˆâ–„â–ƒâ–…â–„â–ƒâ–„â–ƒâ–„â–ƒâ–„â–†â–ƒâ–„â–„â–…â–„â–„â–„â–„â–„â–ƒâ–„â–„â–„â–„â–ƒâ–„â–„â–„â–„â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–…â–†â–…â–‡â–…â–†â–‚â–…â–…â–„â–…â–ˆâ–ƒâ–„â–†â–„â–ƒâ–„â–‚â–…â–…â–ˆâ–â–‡â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–„â–†â–…â–…â–…â–…â–‚â–†â–…â–„â–…â–‡â–ƒâ–„â–‡â–„â–ƒâ–ƒâ–ƒâ–…â–…â–ˆâ–â–†â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–…â–…â–ƒâ–ƒâ–‚â–„â–‚â–‚â–‚â–ƒâ–â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–†â–†â–†â–†â–ˆâ–†â–â–†â–†â–†â–ƒâ–†â–†â–†â–†â–†â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–„â–„â–„â–„â–ˆâ–„â–â–„â–„â–„â–ƒâ–„â–„â–„â–„â–„â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–„â–ƒâ–…â–â–„â–…â–†â–‡â–†â–…â–†â–…â–†â–†â–†â–†â–‡â–†â–†â–†â–†â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_single_lstm_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2xwo69me\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:34:54.039441: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/233c0o7p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013452-233c0o7p\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|â–ˆ| 73/73 [00:01<00:00, 40.04it/s, loss=0.681, v_num=0o7p, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.618\n",
      "Epoch 0: 100%|â–ˆ| 73/73 [00:01<00:00, 39.77it/s, loss=0.681, v_num=0o7p, LTC_val_\n",
      "Epoch 1:  99%|â–‰| 72/73 [00:01<00:00, 42.28it/s, loss=0.646, v_num=0o7p, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 73/73 [00:01<00:00, 41.75it/s, loss=0.646, v_num=0o7p, LTC_val_\u001b[A\n",
      "Epoch 2:  99%|â–‰| 72/73 [00:02<00:00, 34.73it/s, loss=0.631, v_num=0o7p, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 73/73 [00:02<00:00, 34.50it/s, loss=0.631, v_num=0o7p, LTC_val_\u001b[A\n",
      "Epoch 3:  99%|â–‰| 72/73 [00:01<00:00, 42.84it/s, loss=0.608, v_num=0o7p, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.089 >= min_delta = 0.003. New best score: 0.529\n",
      "Epoch 3: 100%|â–ˆ| 73/73 [00:01<00:00, 42.54it/s, loss=0.608, v_num=0o7p, LTC_val_\n",
      "Epoch 4:  99%|â–‰| 72/73 [00:01<00:00, 40.13it/s, loss=0.619, v_num=0o7p, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 73/73 [00:01<00:00, 39.79it/s, loss=0.619, v_num=0o7p, LTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 72/73 [00:02<00:00, 32.42it/s, loss=0.555, v_num=0o7p, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 73/73 [00:02<00:00, 32.24it/s, loss=0.555, v_num=0o7p, LTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 72/73 [00:01<00:00, 42.65it/s, loss=0.591, v_num=0o7p, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 73/73 [00:01<00:00, 42.25it/s, loss=0.591, v_num=0o7p, LTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 72/73 [00:01<00:00, 40.99it/s, loss=0.606, v_num=0o7p, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 73/73 [00:01<00:00, 40.73it/s, loss=0.606, v_num=0o7p, LTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 72/73 [00:01<00:00, 40.90it/s, loss=0.581, v_num=0o7p, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 73/73 [00:01<00:00, 40.62it/s, loss=0.581, v_num=0o7p, LTC_val_\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  99%|â–‰| 72/73 [00:01<00:00, 41.02it/s, loss=0.548, v_num=0o7p, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 73/73 [00:01<00:00, 40.74it/s, loss=0.548, v_num=0o7p, LTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 72/73 [00:01<00:00, 39.72it/s, loss=0.601, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 73/73 [00:01<00:00, 39.45it/s, loss=0.601, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 72/73 [00:02<00:00, 35.54it/s, loss=0.584, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 73/73 [00:02<00:00, 35.41it/s, loss=0.584, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 72/73 [00:01<00:00, 37.51it/s, loss=0.574, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 73/73 [00:01<00:00, 37.32it/s, loss=0.574, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 72/73 [00:01<00:00, 41.33it/s, loss=0.596, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.516\n",
      "Epoch 13: 100%|â–ˆ| 73/73 [00:01<00:00, 40.92it/s, loss=0.596, v_num=0o7p, LTC_val\n",
      "Epoch 14:  99%|â–‰| 72/73 [00:01<00:00, 40.18it/s, loss=0.549, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 73/73 [00:01<00:00, 39.74it/s, loss=0.549, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 72/73 [00:02<00:00, 29.34it/s, loss=0.571, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 73/73 [00:02<00:00, 29.31it/s, loss=0.571, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 72/73 [00:01<00:00, 40.35it/s, loss=0.59, v_num=0o7p, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 73/73 [00:01<00:00, 40.01it/s, loss=0.59, v_num=0o7p, LTC_val_\u001b[A\n",
      "Epoch 17:  99%|â–‰| 72/73 [00:01<00:00, 37.88it/s, loss=0.585, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 73/73 [00:01<00:00, 37.53it/s, loss=0.585, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 72/73 [00:02<00:00, 30.81it/s, loss=0.567, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 73/73 [00:02<00:00, 30.40it/s, loss=0.567, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 72/73 [00:02<00:00, 27.75it/s, loss=0.541, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 73/73 [00:02<00:00, 27.69it/s, loss=0.541, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 72/73 [00:02<00:00, 29.81it/s, loss=0.633, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 73/73 [00:02<00:00, 27.14it/s, loss=0.633, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 72/73 [00:02<00:00, 28.81it/s, loss=0.583, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 73/73 [00:02<00:00, 28.77it/s, loss=0.583, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 72/73 [00:02<00:00, 26.71it/s, loss=0.54, v_num=0o7p, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 0.495\n",
      "Epoch 22: 100%|â–ˆ| 73/73 [00:02<00:00, 26.59it/s, loss=0.54, v_num=0o7p, LTC_val_\n",
      "Epoch 23:  99%|â–‰| 72/73 [00:02<00:00, 28.62it/s, loss=0.531, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 73/73 [00:02<00:00, 28.52it/s, loss=0.531, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 24:  99%|â–‰| 72/73 [00:02<00:00, 29.95it/s, loss=0.589, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 73/73 [00:02<00:00, 29.86it/s, loss=0.589, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 25:  99%|â–‰| 72/73 [00:02<00:00, 27.17it/s, loss=0.507, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 73/73 [00:02<00:00, 27.15it/s, loss=0.507, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 26:  99%|â–‰| 72/73 [00:01<00:00, 39.75it/s, loss=0.545, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 73/73 [00:01<00:00, 39.51it/s, loss=0.545, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 27:  99%|â–‰| 72/73 [00:01<00:00, 37.84it/s, loss=0.598, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 73/73 [00:01<00:00, 37.65it/s, loss=0.598, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 28:  99%|â–‰| 72/73 [00:01<00:00, 38.26it/s, loss=0.589, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 73/73 [00:01<00:00, 38.03it/s, loss=0.589, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 29:  99%|â–‰| 72/73 [00:01<00:00, 37.93it/s, loss=0.516, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 73/73 [00:01<00:00, 37.66it/s, loss=0.516, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 30:  99%|â–‰| 72/73 [00:01<00:00, 37.92it/s, loss=0.558, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 73/73 [00:01<00:00, 37.74it/s, loss=0.558, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 31:  99%|â–‰| 72/73 [00:02<00:00, 30.31it/s, loss=0.537, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 73/73 [00:02<00:00, 30.25it/s, loss=0.537, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 32:  99%|â–‰| 72/73 [00:02<00:00, 25.10it/s, loss=0.527, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 73/73 [00:02<00:00, 25.05it/s, loss=0.527, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 33:  99%|â–‰| 72/73 [00:02<00:00, 29.59it/s, loss=0.536, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 0.481\n",
      "Epoch 33: 100%|â–ˆ| 73/73 [00:02<00:00, 29.57it/s, loss=0.536, v_num=0o7p, LTC_val\n",
      "Epoch 34:  99%|â–‰| 72/73 [00:02<00:00, 28.99it/s, loss=0.53, v_num=0o7p, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 73/73 [00:02<00:00, 28.74it/s, loss=0.53, v_num=0o7p, LTC_val_\u001b[A\n",
      "Epoch 35:  99%|â–‰| 72/73 [00:02<00:00, 30.45it/s, loss=0.551, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 73/73 [00:02<00:00, 30.32it/s, loss=0.551, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 36:  99%|â–‰| 72/73 [00:02<00:00, 35.46it/s, loss=0.587, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 73/73 [00:02<00:00, 35.11it/s, loss=0.587, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 37:  99%|â–‰| 72/73 [00:01<00:00, 38.74it/s, loss=0.534, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|â–ˆ| 73/73 [00:01<00:00, 38.40it/s, loss=0.534, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 38:  99%|â–‰| 72/73 [00:02<00:00, 33.51it/s, loss=0.531, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|â–ˆ| 73/73 [00:02<00:00, 33.32it/s, loss=0.531, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 39:  99%|â–‰| 72/73 [00:02<00:00, 35.64it/s, loss=0.575, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|â–ˆ| 73/73 [00:02<00:00, 35.19it/s, loss=0.575, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 40:  99%|â–‰| 72/73 [00:02<00:00, 33.32it/s, loss=0.498, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.047 >= min_delta = 0.003. New best score: 0.434\n",
      "Epoch 40: 100%|â–ˆ| 73/73 [00:02<00:00, 33.05it/s, loss=0.498, v_num=0o7p, LTC_val\n",
      "Epoch 41:  99%|â–‰| 72/73 [00:02<00:00, 31.59it/s, loss=0.56, v_num=0o7p, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|â–ˆ| 73/73 [00:02<00:00, 31.45it/s, loss=0.56, v_num=0o7p, LTC_val_\u001b[A\n",
      "Epoch 42:  99%|â–‰| 72/73 [00:02<00:00, 26.75it/s, loss=0.51, v_num=0o7p, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|â–ˆ| 73/73 [00:02<00:00, 26.73it/s, loss=0.51, v_num=0o7p, LTC_val_\u001b[A\n",
      "Epoch 43:  99%|â–‰| 72/73 [00:02<00:00, 33.29it/s, loss=0.527, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|â–ˆ| 73/73 [00:02<00:00, 33.17it/s, loss=0.527, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 44:  99%|â–‰| 72/73 [00:02<00:00, 33.97it/s, loss=0.551, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.038 >= min_delta = 0.003. New best score: 0.396\n",
      "Epoch 44: 100%|â–ˆ| 73/73 [00:02<00:00, 33.81it/s, loss=0.551, v_num=0o7p, LTC_val\n",
      "Epoch 45:  99%|â–‰| 72/73 [00:02<00:00, 28.14it/s, loss=0.539, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|â–ˆ| 73/73 [00:02<00:00, 27.13it/s, loss=0.539, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 46:  99%|â–‰| 72/73 [00:02<00:00, 31.22it/s, loss=0.488, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|â–ˆ| 73/73 [00:02<00:00, 31.17it/s, loss=0.488, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 47:  99%|â–‰| 72/73 [00:01<00:00, 39.46it/s, loss=0.512, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|â–ˆ| 73/73 [00:01<00:00, 38.91it/s, loss=0.512, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 48:  99%|â–‰| 72/73 [00:01<00:00, 39.79it/s, loss=0.53, v_num=0o7p, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|â–ˆ| 73/73 [00:01<00:00, 39.43it/s, loss=0.53, v_num=0o7p, LTC_val_\u001b[A\n",
      "Epoch 49:  99%|â–‰| 72/73 [00:01<00:00, 40.49it/s, loss=0.542, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|â–ˆ| 73/73 [00:01<00:00, 40.00it/s, loss=0.542, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 50:  99%|â–‰| 72/73 [00:01<00:00, 40.31it/s, loss=0.473, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|â–ˆ| 73/73 [00:01<00:00, 39.90it/s, loss=0.473, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 51:  99%|â–‰| 72/73 [00:01<00:00, 40.53it/s, loss=0.473, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|â–ˆ| 73/73 [00:01<00:00, 40.12it/s, loss=0.473, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 52:  99%|â–‰| 72/73 [00:01<00:00, 39.66it/s, loss=0.538, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|â–ˆ| 73/73 [00:01<00:00, 39.29it/s, loss=0.538, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 53:  99%|â–‰| 72/73 [00:01<00:00, 40.63it/s, loss=0.494, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|â–ˆ| 73/73 [00:01<00:00, 40.25it/s, loss=0.494, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 54:  99%|â–‰| 72/73 [00:02<00:00, 35.93it/s, loss=0.51, v_num=0o7p, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|â–ˆ| 73/73 [00:02<00:00, 35.63it/s, loss=0.51, v_num=0o7p, LTC_val_\u001b[A\n",
      "Epoch 55:  99%|â–‰| 72/73 [00:02<00:00, 30.88it/s, loss=0.503, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|â–ˆ| 73/73 [00:02<00:00, 30.73it/s, loss=0.503, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 56:  99%|â–‰| 72/73 [00:02<00:00, 35.20it/s, loss=0.486, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|â–ˆ| 73/73 [00:02<00:00, 32.62it/s, loss=0.486, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 57:  99%|â–‰| 72/73 [00:02<00:00, 28.08it/s, loss=0.487, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.392\n",
      "Epoch 57: 100%|â–ˆ| 73/73 [00:02<00:00, 27.94it/s, loss=0.487, v_num=0o7p, LTC_val\n",
      "Epoch 58:  99%|â–‰| 72/73 [00:02<00:00, 29.70it/s, loss=0.508, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|â–ˆ| 73/73 [00:02<00:00, 29.46it/s, loss=0.508, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 59:  99%|â–‰| 72/73 [00:01<00:00, 40.09it/s, loss=0.446, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|â–ˆ| 73/73 [00:01<00:00, 39.83it/s, loss=0.446, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 60:  99%|â–‰| 72/73 [00:02<00:00, 31.90it/s, loss=0.481, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 60: 100%|â–ˆ| 73/73 [00:02<00:00, 31.79it/s, loss=0.481, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 61:  99%|â–‰| 72/73 [00:02<00:00, 32.69it/s, loss=0.54, v_num=0o7p, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|â–ˆ| 73/73 [00:02<00:00, 30.99it/s, loss=0.54, v_num=0o7p, LTC_val_\u001b[A\n",
      "Epoch 62:  99%|â–‰| 72/73 [00:02<00:00, 30.64it/s, loss=0.458, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.388\n",
      "Epoch 62: 100%|â–ˆ| 73/73 [00:02<00:00, 30.50it/s, loss=0.458, v_num=0o7p, LTC_val\n",
      "Epoch 63:  99%|â–‰| 72/73 [00:02<00:00, 32.82it/s, loss=0.436, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 63: 100%|â–ˆ| 73/73 [00:02<00:00, 32.63it/s, loss=0.436, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 64:  99%|â–‰| 72/73 [00:02<00:00, 33.40it/s, loss=0.385, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.033 >= min_delta = 0.003. New best score: 0.355\n",
      "Epoch 64: 100%|â–ˆ| 73/73 [00:02<00:00, 33.31it/s, loss=0.385, v_num=0o7p, LTC_val\n",
      "Epoch 65:  99%|â–‰| 72/73 [00:03<00:00, 22.93it/s, loss=0.496, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 65: 100%|â–ˆ| 73/73 [00:03<00:00, 22.94it/s, loss=0.496, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 66:  99%|â–‰| 72/73 [00:01<00:00, 40.08it/s, loss=0.486, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 66: 100%|â–ˆ| 73/73 [00:01<00:00, 39.64it/s, loss=0.486, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 67:  99%|â–‰| 72/73 [00:01<00:00, 37.19it/s, loss=0.395, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 67: 100%|â–ˆ| 73/73 [00:01<00:00, 36.93it/s, loss=0.395, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 68:  99%|â–‰| 72/73 [00:02<00:00, 33.11it/s, loss=0.461, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 68: 100%|â–ˆ| 73/73 [00:02<00:00, 32.90it/s, loss=0.461, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 69:  99%|â–‰| 72/73 [00:02<00:00, 31.43it/s, loss=0.384, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 69: 100%|â–ˆ| 73/73 [00:02<00:00, 31.23it/s, loss=0.384, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 70:  99%|â–‰| 72/73 [00:02<00:00, 30.85it/s, loss=0.408, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 70: 100%|â–ˆ| 73/73 [00:02<00:00, 30.76it/s, loss=0.408, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 71:  99%|â–‰| 72/73 [00:01<00:00, 39.27it/s, loss=0.429, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 71: 100%|â–ˆ| 73/73 [00:01<00:00, 38.91it/s, loss=0.429, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 72:  99%|â–‰| 72/73 [00:01<00:00, 39.36it/s, loss=0.466, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 72: 100%|â–ˆ| 73/73 [00:01<00:00, 38.88it/s, loss=0.466, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 73:  99%|â–‰| 72/73 [00:01<00:00, 40.72it/s, loss=0.393, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 73: 100%|â–ˆ| 73/73 [00:01<00:00, 40.28it/s, loss=0.393, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 74:  99%|â–‰| 72/73 [00:01<00:00, 40.01it/s, loss=0.356, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 74: 100%|â–ˆ| 73/73 [00:01<00:00, 39.44it/s, loss=0.356, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 75:  99%|â–‰| 72/73 [00:01<00:00, 40.25it/s, loss=0.396, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 75: 100%|â–ˆ| 73/73 [00:01<00:00, 39.85it/s, loss=0.396, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 76:  99%|â–‰| 72/73 [00:01<00:00, 40.03it/s, loss=0.384, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 76: 100%|â–ˆ| 73/73 [00:01<00:00, 39.52it/s, loss=0.384, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 77:  99%|â–‰| 72/73 [00:01<00:00, 40.59it/s, loss=0.376, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 77: 100%|â–ˆ| 73/73 [00:01<00:00, 40.17it/s, loss=0.376, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 78:  99%|â–‰| 72/73 [00:01<00:00, 40.08it/s, loss=0.356, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 78: 100%|â–ˆ| 73/73 [00:01<00:00, 39.79it/s, loss=0.356, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 79:  99%|â–‰| 72/73 [00:01<00:00, 39.86it/s, loss=0.368, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 79: 100%|â–ˆ| 73/73 [00:01<00:00, 39.56it/s, loss=0.368, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 80:  99%|â–‰| 72/73 [00:01<00:00, 39.61it/s, loss=0.381, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 80: 100%|â–ˆ| 73/73 [00:01<00:00, 39.26it/s, loss=0.381, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 81:  99%|â–‰| 72/73 [00:01<00:00, 39.22it/s, loss=0.371, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 81: 100%|â–ˆ| 73/73 [00:01<00:00, 38.94it/s, loss=0.371, v_num=0o7p, LTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82:  99%|â–‰| 72/73 [00:01<00:00, 39.32it/s, loss=0.387, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 82: 100%|â–ˆ| 73/73 [00:01<00:00, 39.03it/s, loss=0.387, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 83:  99%|â–‰| 72/73 [00:01<00:00, 40.13it/s, loss=0.367, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 83: 100%|â–ˆ| 73/73 [00:01<00:00, 38.64it/s, loss=0.367, v_num=0o7p, LTC_val\u001b[A\n",
      "Epoch 84:  99%|â–‰| 72/73 [00:01<00:00, 39.46it/s, loss=0.369, v_num=0o7p, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 84: 100%|â–ˆ| 73/73 [00:01<00:00, 39.16it/s, loss=0.369, v_num=0o7p, LTC_val\u001b[A\n",
      "                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.355. Signaling Trainer to stop.\n",
      "Epoch 84: 100%|â–ˆ| 73/73 [00:01<00:00, 39.09it/s, loss=0.369, v_num=0o7p, LTC_val\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 82.68it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.75,\n",
      " 'LTC_test_f1': 0.7463489174842834,\n",
      " 'test_loss': 0.48873159289360046}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 70139\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013452-233c0o7p/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013452-233c0o7p/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.16663\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 84\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 6120\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 189\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463881\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 292\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.81337\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.80411\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.3506\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.42625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.74635\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.48873\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–ƒâ–‚â–„â–‚â–ƒâ–ƒâ–†â–„â–„â–ƒâ–‚â–‚â–„â–„â–ƒâ–†â–†â–ƒâ–‚â–ƒâ–…â–†â–â–„â–‚â–„â–„â–„â–‡â–‚â–‡â–„â–†â–‚â–…â–†â–„â–†â–„â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–ƒâ–‚â–„â–‚â–ƒâ–‚â–†â–„â–„â–ƒâ–‚â–‚â–„â–„â–ƒâ–†â–†â–ƒâ–‚â–ƒâ–…â–†â–â–ƒâ–‚â–„â–„â–„â–‡â–‚â–‡â–ƒâ–†â–â–„â–†â–„â–†â–„â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–†â–ˆâ–…â–‡â–†â–†â–ƒâ–†â–…â–…â–†â–ˆâ–…â–…â–…â–„â–„â–„â–…â–…â–†â–„â–†â–†â–„â–„â–…â–†â–‚â–…â–…â–„â–‚â–‡â–ƒâ–ƒâ–…â–ƒâ–ƒâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–â–…â–…â–†â–…â–…â–†â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–â–…â–…â–†â–…â–…â–†â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–„â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–ˆâ–â–ˆâ–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–ˆâ–…â–…â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–ˆâ–â–ˆâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ˆâ–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–ˆâ–†â–†â–†â–†â–†â–…â–…â–‡â–…â–†â–…â–…â–…â–…â–†â–…â–†â–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–„â–‚â–‚â–ƒâ–„â–„â–„â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/233c0o7p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:38:17.138439: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_loss_weighted_binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/6fbrw9ap\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013815-6fbrw9ap\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 73/74 [00:01<00:00, 41.35it/s, loss=0.708, v_num=w9ap, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.703\n",
      "Epoch 0: 100%|â–ˆ| 74/74 [00:01<00:00, 40.76it/s, loss=0.708, v_num=w9ap, LTC_val_\n",
      "Epoch 1:  99%|â–‰| 73/74 [00:01<00:00, 41.28it/s, loss=0.702, v_num=w9ap, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 0.692\n",
      "Epoch 1: 100%|â–ˆ| 74/74 [00:01<00:00, 41.02it/s, loss=0.702, v_num=w9ap, LTC_val_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  99%|â–‰| 73/74 [00:01<00:00, 37.53it/s, loss=0.703, v_num=w9ap, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 74/74 [00:01<00:00, 37.22it/s, loss=0.703, v_num=w9ap, LTC_val_\u001b[A\n",
      "Epoch 3:  99%|â–‰| 73/74 [00:01<00:00, 38.73it/s, loss=0.698, v_num=w9ap, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 74/74 [00:01<00:00, 38.46it/s, loss=0.698, v_num=w9ap, LTC_val_\u001b[A\n",
      "Epoch 4:  99%|â–‰| 73/74 [00:01<00:00, 38.58it/s, loss=0.707, v_num=w9ap, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 74/74 [00:01<00:00, 38.35it/s, loss=0.707, v_num=w9ap, LTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 73/74 [00:02<00:00, 31.64it/s, loss=0.692, v_num=w9ap, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 74/74 [00:02<00:00, 31.58it/s, loss=0.692, v_num=w9ap, LTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 73/74 [00:02<00:00, 27.48it/s, loss=0.699, v_num=w9ap, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 74/74 [00:02<00:00, 27.43it/s, loss=0.699, v_num=w9ap, LTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 73/74 [00:01<00:00, 40.83it/s, loss=0.704, v_num=w9ap, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 74/74 [00:01<00:00, 40.51it/s, loss=0.704, v_num=w9ap, LTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 73/74 [00:01<00:00, 41.02it/s, loss=0.698, v_num=w9ap, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 74/74 [00:01<00:00, 40.75it/s, loss=0.698, v_num=w9ap, LTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 73/74 [00:01<00:00, 38.94it/s, loss=0.693, v_num=w9ap, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 74/74 [00:01<00:00, 38.72it/s, loss=0.693, v_num=w9ap, LTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 73/74 [00:01<00:00, 40.22it/s, loss=0.701, v_num=w9ap, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 74/74 [00:01<00:00, 39.65it/s, loss=0.701, v_num=w9ap, LTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 73/74 [00:02<00:00, 27.13it/s, loss=0.693, v_num=w9ap, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 74/74 [00:02<00:00, 27.06it/s, loss=0.693, v_num=w9ap, LTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 73/74 [00:02<00:00, 35.35it/s, loss=0.695, v_num=w9ap, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 74/74 [00:02<00:00, 35.05it/s, loss=0.695, v_num=w9ap, LTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 73/74 [00:01<00:00, 41.23it/s, loss=0.692, v_num=w9ap, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 74/74 [00:01<00:00, 40.79it/s, loss=0.692, v_num=w9ap, LTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 73/74 [00:01<00:00, 38.12it/s, loss=0.692, v_num=w9ap, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 74/74 [00:01<00:00, 37.80it/s, loss=0.692, v_num=w9ap, LTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 73/74 [00:01<00:00, 39.79it/s, loss=0.699, v_num=w9ap, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 74/74 [00:01<00:00, 38.08it/s, loss=0.699, v_num=w9ap, LTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 73/74 [00:02<00:00, 26.05it/s, loss=0.697, v_num=w9ap, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 74/74 [00:02<00:00, 26.03it/s, loss=0.697, v_num=w9ap, LTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 73/74 [00:01<00:00, 42.67it/s, loss=0.699, v_num=w9ap, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 74/74 [00:01<00:00, 42.14it/s, loss=0.699, v_num=w9ap, LTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 73/74 [00:01<00:00, 42.88it/s, loss=0.698, v_num=w9ap, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 74/74 [00:01<00:00, 42.40it/s, loss=0.698, v_num=w9ap, LTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 73/74 [00:01<00:00, 42.15it/s, loss=0.697, v_num=w9ap, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 74/74 [00:01<00:00, 41.58it/s, loss=0.697, v_num=w9ap, LTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 73/74 [00:01<00:00, 42.58it/s, loss=0.694, v_num=w9ap, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:01<00:00, 42.05it/s, loss=0.694, v_num=w9ap, LTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 73/74 [00:01<00:00, 39.49it/s, loss=0.694, v_num=w9ap, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.692. Signaling Trainer to stop.\n",
      "Epoch 21: 100%|â–ˆ| 74/74 [00:01<00:00, 39.22it/s, loss=0.694, v_num=w9ap, LTC_val\n",
      "Epoch 21: 100%|â–ˆ| 74/74 [00:01<00:00, 39.15it/s, loss=0.694, v_num=w9ap, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 72.16it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.5,\n",
      " 'LTC_test_f1': 0.3272727429866791,\n",
      " 'test_loss': 0.6933034062385559}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 70606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013815-6fbrw9ap/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013815-6fbrw9ap/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.5625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.54656\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.68455\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463948\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 76\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.48359\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.47013\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69423\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.69308\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.32727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.6933\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–†â–â–†â–†â–ƒâ–â–…â–‚â–‚â–‚â–ˆâ–‡â–‡â–ƒâ–…â–‚â–ƒâ–†â–…â–…â–‚â–‚â–…â–‡â–‚â–ƒâ–ƒâ–…â–â–…â–ˆâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–…â–â–…â–…â–â–â–„â–‚â–‚â–‚â–‡â–†â–‡â–ƒâ–„â–‚â–‚â–„â–…â–„â–‚â–‚â–„â–†â–‚â–â–ƒâ–„â–â–„â–ˆâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–â–ˆâ–ƒâ–„â–„â–†â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–„â–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–†â–„â–†â–â–…â–ˆâ–‚â–†â–†â–‚â–„â–†â–„â–‡â–…â–…â–…â–…â–†â–…â–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–†â–…â–…â–â–â–†â–ƒâ–ˆâ–‡â–„â–„â–†â–„â–‡â–†â–†â–…â–„â–…â–…â–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–†â–„â–„â–ƒâ–â–„â–‚â–‚â–ƒâ–‚â–â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–‚â–â–â–â–‚â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/6fbrw9ap\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:39:26.143843: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2i1habeu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013924-2i1habeu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/73 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 72/73 [00:01<00:00, 43.13it/s, loss=1.13, v_num=abeu, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.054\n",
      "Epoch 0: 100%|â–ˆ| 73/73 [00:01<00:00, 40.55it/s, loss=1.13, v_num=abeu, LTC_val_a\n",
      "Epoch 1:  99%|â–‰| 72/73 [00:01<00:00, 43.70it/s, loss=1.1, v_num=abeu, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 73/73 [00:01<00:00, 43.19it/s, loss=1.1, v_num=abeu, LTC_val_ac\u001b[A\n",
      "Epoch 2:  99%|â–‰| 72/73 [00:01<00:00, 42.48it/s, loss=1.1, v_num=abeu, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 73/73 [00:01<00:00, 42.19it/s, loss=1.1, v_num=abeu, LTC_val_ac\u001b[A\n",
      "Epoch 3:  99%|â–‰| 72/73 [00:02<00:00, 35.75it/s, loss=1.11, v_num=abeu, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 73/73 [00:02<00:00, 35.60it/s, loss=1.11, v_num=abeu, LTC_val_a\u001b[A\n",
      "Epoch 4:  99%|â–‰| 72/73 [00:01<00:00, 42.13it/s, loss=1.11, v_num=abeu, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 73/73 [00:01<00:00, 41.82it/s, loss=1.11, v_num=abeu, LTC_val_a\u001b[A\n",
      "Epoch 5:  99%|â–‰| 72/73 [00:01<00:00, 41.50it/s, loss=1.11, v_num=abeu, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 73/73 [00:01<00:00, 41.24it/s, loss=1.11, v_num=abeu, LTC_val_a\u001b[A\n",
      "Epoch 6:  99%|â–‰| 72/73 [00:02<00:00, 35.66it/s, loss=1.11, v_num=abeu, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 73/73 [00:02<00:00, 35.54it/s, loss=1.11, v_num=abeu, LTC_val_a\u001b[A\n",
      "Epoch 7:  99%|â–‰| 72/73 [00:01<00:00, 41.72it/s, loss=1.1, v_num=abeu, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 73/73 [00:01<00:00, 41.47it/s, loss=1.1, v_num=abeu, LTC_val_ac\u001b[A\n",
      "Epoch 8:  99%|â–‰| 72/73 [00:01<00:00, 38.60it/s, loss=1.1, v_num=abeu, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 73/73 [00:01<00:00, 38.26it/s, loss=1.1, v_num=abeu, LTC_val_ac\u001b[A\n",
      "Epoch 9:  99%|â–‰| 72/73 [00:02<00:00, 35.16it/s, loss=1.1, v_num=abeu, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 73/73 [00:02<00:00, 35.04it/s, loss=1.1, v_num=abeu, LTC_val_ac\u001b[A\n",
      "Epoch 10:  99%|â–‰| 72/73 [00:01<00:00, 41.66it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 73/73 [00:01<00:00, 41.34it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n",
      "Epoch 11:  99%|â–‰| 72/73 [00:01<00:00, 38.22it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 73/73 [00:01<00:00, 37.78it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n",
      "Epoch 12:  99%|â–‰| 72/73 [00:01<00:00, 41.50it/s, loss=1.11, v_num=abeu, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 73/73 [00:01<00:00, 40.98it/s, loss=1.11, v_num=abeu, LTC_val_\u001b[A\n",
      "Epoch 13:  99%|â–‰| 72/73 [00:02<00:00, 34.21it/s, loss=1.11, v_num=abeu, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 73/73 [00:02<00:00, 34.02it/s, loss=1.11, v_num=abeu, LTC_val_\u001b[A\n",
      "Epoch 14:  99%|â–‰| 72/73 [00:01<00:00, 39.22it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 73/73 [00:01<00:00, 38.88it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n",
      "Epoch 15:  99%|â–‰| 72/73 [00:02<00:00, 35.61it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 73/73 [00:02<00:00, 35.30it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n",
      "Epoch 16:  99%|â–‰| 72/73 [00:01<00:00, 39.02it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 73/73 [00:01<00:00, 38.68it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n",
      "Epoch 17:  99%|â–‰| 72/73 [00:01<00:00, 38.72it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 73/73 [00:01<00:00, 38.34it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n",
      "Epoch 18:  99%|â–‰| 72/73 [00:01<00:00, 39.25it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 73/73 [00:01<00:00, 38.90it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n",
      "Epoch 19:  99%|â–‰| 72/73 [00:01<00:00, 41.94it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 73/73 [00:01<00:00, 41.42it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n",
      "Epoch 20:  99%|â–‰| 72/73 [00:01<00:00, 41.65it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.054. Signaling Trainer to stop.\n",
      "Epoch 20: 100%|â–ˆ| 73/73 [00:01<00:00, 41.17it/s, loss=1.1, v_num=abeu, LTC_val_a\n",
      "Epoch 20: 100%|â–ˆ| 73/73 [00:01<00:00, 41.09it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 77.21it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.1785714328289032,\n",
      " 'LTC_test_f1': 0.0981685072183609,\n",
      " 'test_loss': 1.1191810369491577}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 70789\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013924-2i1habeu/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013924-2i1habeu/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.5625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.3619\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.107\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 48\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464012\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.40538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.29441\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.0996\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.18182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.10387\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.17857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.09817\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.11918\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–ƒâ–â–ƒâ–†â–ƒâ–‚â–ƒâ–…â–…â–„â–ƒâ–‚â–‚â–…â–„â–„â–…â–ˆâ–ƒâ–†â–‚â–…â–…â–‡â–…â–…â–‚â–†â–ƒâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–ƒâ–â–„â–†â–ƒâ–â–ƒâ–„â–†â–ƒâ–ƒâ–ƒâ–‚â–„â–‚â–ƒâ–„â–ˆâ–„â–„â–‚â–…â–„â–ƒâ–‚â–…â–‚â–…â–‚â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–ˆâ–ˆâ–…â–…â–ƒâ–„â–‡â–„â–‚â–„â–†â–…â–…â–‡â–â–ƒâ–ƒâ–‚â–„â–ƒâ–…â–„â–ƒâ–‚â–„â–ƒâ–„â–ƒâ–…â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–ƒâ–ƒâ–…â–â–„â–…â–…â–„â–„â–†â–†â–…â–†â–…â–‡â–†â–ˆâ–ˆâ–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–‚â–„â–„â–â–ƒâ–…â–…â–ƒâ–„â–†â–…â–…â–…â–†â–„â–„â–ƒâ–‚â–ˆâ–†â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–…â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–â–†â–†â–ˆâ–…â–†â–…â–†â–‡â–‡â–†â–‡â–†â–‡â–†â–‡â–†â–†â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2i1habeu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:40:28.177213: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_loss_weighted_multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/xlclrwnp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014026-xlclrwnp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|â–ˆ| 74/74 [00:01<00:00, 42.13it/s, loss=1.13, v_num=rwnp, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.034\n",
      "Epoch 0: 100%|â–ˆ| 74/74 [00:01<00:00, 41.87it/s, loss=1.13, v_num=rwnp, LTC_val_a\n",
      "Epoch 1:  99%|â–‰| 73/74 [00:01<00:00, 43.59it/s, loss=1.11, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 74/74 [00:01<00:00, 42.93it/s, loss=1.11, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Epoch 2:  99%|â–‰| 73/74 [00:01<00:00, 42.17it/s, loss=1.11, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 74/74 [00:01<00:00, 41.86it/s, loss=1.11, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Epoch 3:  99%|â–‰| 73/74 [00:01<00:00, 39.44it/s, loss=1.11, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 74/74 [00:01<00:00, 38.94it/s, loss=1.11, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Epoch 4:  99%|â–‰| 73/74 [00:02<00:00, 35.10it/s, loss=1.1, v_num=rwnp, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 74/74 [00:02<00:00, 34.94it/s, loss=1.1, v_num=rwnp, LTC_val_ac\u001b[A\n",
      "Epoch 5:  99%|â–‰| 73/74 [00:01<00:00, 41.94it/s, loss=1.11, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 74/74 [00:01<00:00, 41.65it/s, loss=1.11, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Epoch 6:  99%|â–‰| 73/74 [00:01<00:00, 41.57it/s, loss=1.1, v_num=rwnp, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 74/74 [00:01<00:00, 41.30it/s, loss=1.1, v_num=rwnp, LTC_val_ac\u001b[A\n",
      "Epoch 7:  99%|â–‰| 73/74 [00:02<00:00, 35.49it/s, loss=1.1, v_num=rwnp, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 74/74 [00:02<00:00, 35.34it/s, loss=1.1, v_num=rwnp, LTC_val_ac\u001b[A\n",
      "Epoch 8:  99%|â–‰| 73/74 [00:01<00:00, 41.45it/s, loss=1.1, v_num=rwnp, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 74/74 [00:01<00:00, 41.18it/s, loss=1.1, v_num=rwnp, LTC_val_ac\u001b[A\n",
      "Epoch 9:  99%|â–‰| 73/74 [00:01<00:00, 40.62it/s, loss=1.1, v_num=rwnp, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 74/74 [00:01<00:00, 40.39it/s, loss=1.1, v_num=rwnp, LTC_val_ac\u001b[A\n",
      "Epoch 10:  99%|â–‰| 73/74 [00:02<00:00, 31.56it/s, loss=1.09, v_num=rwnp, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 74/74 [00:02<00:00, 31.32it/s, loss=1.09, v_num=rwnp, LTC_val_\u001b[A\n",
      "Epoch 11:  99%|â–‰| 73/74 [00:02<00:00, 28.89it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 74/74 [00:02<00:00, 28.79it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  99%|â–‰| 73/74 [00:02<00:00, 30.11it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 74/74 [00:02<00:00, 29.98it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Epoch 13:  99%|â–‰| 73/74 [00:01<00:00, 41.06it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 74/74 [00:01<00:00, 40.62it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Epoch 14:  99%|â–‰| 73/74 [00:01<00:00, 40.74it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 74/74 [00:01<00:00, 40.33it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Epoch 15:  99%|â–‰| 73/74 [00:01<00:00, 41.46it/s, loss=1.09, v_num=rwnp, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 74/74 [00:01<00:00, 39.70it/s, loss=1.09, v_num=rwnp, LTC_val_\u001b[A\n",
      "Epoch 16:  99%|â–‰| 73/74 [00:02<00:00, 34.73it/s, loss=1.11, v_num=rwnp, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 74/74 [00:02<00:00, 34.46it/s, loss=1.11, v_num=rwnp, LTC_val_\u001b[A\n",
      "Epoch 17:  99%|â–‰| 73/74 [00:02<00:00, 29.16it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 74/74 [00:02<00:00, 29.06it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Epoch 18:  99%|â–‰| 73/74 [00:01<00:00, 41.86it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 74/74 [00:01<00:00, 41.38it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Epoch 19:  99%|â–‰| 73/74 [00:01<00:00, 37.26it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 74/74 [00:02<00:00, 36.94it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Epoch 20:  99%|â–‰| 73/74 [00:01<00:00, 39.28it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.034. Signaling Trainer to stop.\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:01<00:00, 38.88it/s, loss=1.1, v_num=rwnp, LTC_val_a\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:01<00:00, 38.80it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 73.71it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.25,\n",
      " 'LTC_test_f1': 0.13151928782463074,\n",
      " 'test_loss': 1.1264402866363525}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 70978\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014026-xlclrwnp/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014026-xlclrwnp/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.3125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.31111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.11554\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 51\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464077\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.35406\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.32887\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09848\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.22222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.08551\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.13152\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.12644\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–â–‚â–…â–â–‚â–…â–…â–‡â–„â–ƒâ–ƒâ–‚â–„â–â–„â–ˆâ–ƒâ–‚â–â–„â–‚â–…â–„â–â–â–ƒâ–…â–…â–‚â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–â–‚â–†â–â–‚â–…â–…â–‡â–ƒâ–ƒâ–ƒâ–‚â–„â–â–ƒâ–ˆâ–‚â–‚â–â–ƒâ–‚â–„â–„â–â–‚â–ƒâ–„â–…â–‚â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–ˆâ–‚â–‚â–„â–…â–ƒâ–â–â–ƒâ–„â–†â–…â–ƒâ–…â–„â–‚â–„â–…â–†â–„â–…â–ƒâ–„â–„â–…â–…â–…â–ƒâ–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–„â–…â–†â–â–ˆâ–†â–†â–ˆâ–…â–†â–ƒâ–…â–„â–„â–†â–„â–…â–„â–‡â–†â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–â–„â–…â–â–ˆâ–†â–†â–‡â–…â–‡â–ƒâ–ƒâ–„â–‚â–†â–„â–„â–ƒâ–„â–…â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–…â–„â–„â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–â–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–â–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–â–…â–…â–†â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/xlclrwnp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:41:32.689629: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_single_lstm__binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2cbnmwbm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014131-2cbnmwbm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 130   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.7 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|â–‰| 72/73 [00:00<00:00, 85.38it/s, loss=0.619, v_num=mwbm, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.511\n",
      "Epoch 0: 100%|â–ˆ| 73/73 [00:00<00:00, 82.29it/s, loss=0.619, v_num=mwbm, LTC_val_\n",
      "Epoch 1:  99%|â–‰| 72/73 [00:01<00:00, 63.90it/s, loss=0.625, v_num=mwbm, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 73/73 [00:01<00:00, 62.62it/s, loss=0.625, v_num=mwbm, LTC_val_\u001b[A\n",
      "Epoch 2:  99%|â–‰| 72/73 [00:00<00:00, 84.71it/s, loss=0.627, v_num=mwbm, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 73/73 [00:00<00:00, 81.97it/s, loss=0.627, v_num=mwbm, LTC_val_\u001b[A\n",
      "Epoch 3:  99%|â–‰| 72/73 [00:00<00:00, 75.05it/s, loss=0.555, v_num=mwbm, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 73/73 [00:00<00:00, 73.42it/s, loss=0.555, v_num=mwbm, LTC_val_\u001b[A\n",
      "Epoch 4:  99%|â–‰| 72/73 [00:00<00:00, 91.00it/s, loss=0.596, v_num=mwbm, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 73/73 [00:00<00:00, 88.61it/s, loss=0.596, v_num=mwbm, LTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 72/73 [00:00<00:00, 76.03it/s, loss=0.59, v_num=mwbm, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 73/73 [00:00<00:00, 74.17it/s, loss=0.59, v_num=mwbm, LTC_val_a\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.494\n",
      "Epoch 6:  99%|â–‰| 72/73 [00:00<00:00, 78.68it/s, loss=0.553, v_num=mwbm, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 73/73 [00:00<00:00, 75.00it/s, loss=0.553, v_num=mwbm, LTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 72/73 [00:01<00:00, 66.43it/s, loss=0.589, v_num=mwbm, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 73/73 [00:01<00:00, 65.10it/s, loss=0.589, v_num=mwbm, LTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 72/73 [00:00<00:00, 87.27it/s, loss=0.614, v_num=mwbm, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 73/73 [00:00<00:00, 84.92it/s, loss=0.614, v_num=mwbm, LTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 72/73 [00:00<00:00, 73.57it/s, loss=0.582, v_num=mwbm, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 73/73 [00:01<00:00, 71.82it/s, loss=0.582, v_num=mwbm, LTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 72/73 [00:00<00:00, 88.98it/s, loss=0.565, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 73/73 [00:00<00:00, 86.65it/s, loss=0.565, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 72/73 [00:00<00:00, 74.64it/s, loss=0.546, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 73/73 [00:01<00:00, 72.83it/s, loss=0.546, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 72/73 [00:00<00:00, 84.99it/s, loss=0.592, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 73/73 [00:00<00:00, 82.70it/s, loss=0.592, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 72/73 [00:00<00:00, 76.55it/s, loss=0.543, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 73/73 [00:00<00:00, 74.12it/s, loss=0.543, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 72/73 [00:00<00:00, 82.26it/s, loss=0.541, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 73/73 [00:00<00:00, 80.23it/s, loss=0.541, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 72/73 [00:00<00:00, 79.67it/s, loss=0.575, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 73/73 [00:00<00:00, 77.14it/s, loss=0.575, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 72/73 [00:00<00:00, 77.99it/s, loss=0.561, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 73/73 [00:00<00:00, 76.21it/s, loss=0.561, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 72/73 [00:01<00:00, 70.12it/s, loss=0.53, v_num=mwbm, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.477\n",
      "Epoch 17: 100%|â–ˆ| 73/73 [00:01<00:00, 67.27it/s, loss=0.53, v_num=mwbm, LTC_val_\n",
      "Epoch 18:  99%|â–‰| 72/73 [00:01<00:00, 55.47it/s, loss=0.575, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 73/73 [00:01<00:00, 54.35it/s, loss=0.575, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 72/73 [00:01<00:00, 64.82it/s, loss=0.575, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 73/73 [00:01<00:00, 63.61it/s, loss=0.575, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 72/73 [00:00<00:00, 86.67it/s, loss=0.561, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.473\n",
      "Epoch 20: 100%|â–ˆ| 73/73 [00:00<00:00, 84.48it/s, loss=0.561, v_num=mwbm, LTC_val\n",
      "Epoch 21:  99%|â–‰| 72/73 [00:01<00:00, 69.70it/s, loss=0.588, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 73/73 [00:01<00:00, 68.24it/s, loss=0.588, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 72/73 [00:00<00:00, 79.50it/s, loss=0.558, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 73/73 [00:00<00:00, 77.28it/s, loss=0.558, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 23:  99%|â–‰| 72/73 [00:01<00:00, 69.91it/s, loss=0.59, v_num=mwbm, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 73/73 [00:01<00:00, 68.51it/s, loss=0.59, v_num=mwbm, LTC_val_\u001b[A\n",
      "Epoch 24:  99%|â–‰| 72/73 [00:00<00:00, 87.79it/s, loss=0.54, v_num=mwbm, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 73/73 [00:00<00:00, 85.44it/s, loss=0.54, v_num=mwbm, LTC_val_\u001b[A\n",
      "Epoch 25:  99%|â–‰| 72/73 [00:00<00:00, 72.41it/s, loss=0.572, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 73/73 [00:01<00:00, 70.73it/s, loss=0.572, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 26:  99%|â–‰| 72/73 [00:00<00:00, 82.68it/s, loss=0.588, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 73/73 [00:00<00:00, 80.83it/s, loss=0.588, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 27:  99%|â–‰| 72/73 [00:00<00:00, 74.38it/s, loss=0.57, v_num=mwbm, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 73/73 [00:01<00:00, 72.44it/s, loss=0.57, v_num=mwbm, LTC_val_\u001b[A\n",
      "Epoch 28:  99%|â–‰| 72/73 [00:00<00:00, 84.96it/s, loss=0.563, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 0.463\n",
      "Epoch 28: 100%|â–ˆ| 73/73 [00:00<00:00, 82.72it/s, loss=0.563, v_num=mwbm, LTC_val\n",
      "Epoch 29:  99%|â–‰| 72/73 [00:00<00:00, 72.45it/s, loss=0.579, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 73/73 [00:01<00:00, 68.97it/s, loss=0.579, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 30:  99%|â–‰| 72/73 [00:00<00:00, 75.56it/s, loss=0.574, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 73/73 [00:00<00:00, 73.75it/s, loss=0.574, v_num=mwbm, LTC_val\u001b[A\n",
      "                                                                                Metric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.450\n",
      "Epoch 31:  99%|â–‰| 72/73 [00:01<00:00, 60.05it/s, loss=0.567, v_num=mwbm, LTC_val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 73/73 [00:01<00:00, 58.97it/s, loss=0.567, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 32:  99%|â–‰| 72/73 [00:00<00:00, 78.41it/s, loss=0.585, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 73/73 [00:01<00:00, 69.05it/s, loss=0.585, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 33:  99%|â–‰| 72/73 [00:01<00:00, 55.29it/s, loss=0.584, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 73/73 [00:01<00:00, 54.60it/s, loss=0.584, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 34:  99%|â–‰| 72/73 [00:00<00:00, 81.14it/s, loss=0.554, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 73/73 [00:00<00:00, 78.83it/s, loss=0.554, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 35:  99%|â–‰| 72/73 [00:00<00:00, 75.58it/s, loss=0.535, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 73/73 [00:00<00:00, 73.95it/s, loss=0.535, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 36:  99%|â–‰| 72/73 [00:00<00:00, 84.32it/s, loss=0.565, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 73/73 [00:00<00:00, 81.39it/s, loss=0.565, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 37:  99%|â–‰| 72/73 [00:01<00:00, 67.50it/s, loss=0.587, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|â–ˆ| 73/73 [00:01<00:00, 66.01it/s, loss=0.587, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 38:  99%|â–‰| 72/73 [00:01<00:00, 58.33it/s, loss=0.588, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.038 >= min_delta = 0.003. New best score: 0.412\n",
      "Epoch 38: 100%|â–ˆ| 73/73 [00:01<00:00, 57.24it/s, loss=0.588, v_num=mwbm, LTC_val\n",
      "Epoch 39:  99%|â–‰| 72/73 [00:01<00:00, 66.98it/s, loss=0.594, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|â–ˆ| 73/73 [00:01<00:00, 65.52it/s, loss=0.594, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 40:  99%|â–‰| 72/73 [00:01<00:00, 68.04it/s, loss=0.564, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|â–ˆ| 73/73 [00:01<00:00, 66.04it/s, loss=0.564, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 41:  99%|â–‰| 72/73 [00:00<00:00, 82.95it/s, loss=0.554, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|â–ˆ| 73/73 [00:00<00:00, 80.85it/s, loss=0.554, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 42:  99%|â–‰| 72/73 [00:01<00:00, 54.32it/s, loss=0.566, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|â–ˆ| 73/73 [00:01<00:00, 53.74it/s, loss=0.566, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 43:  99%|â–‰| 72/73 [00:00<00:00, 89.01it/s, loss=0.618, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|â–ˆ| 73/73 [00:00<00:00, 86.49it/s, loss=0.618, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 44:  99%|â–‰| 72/73 [00:01<00:00, 67.33it/s, loss=0.589, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|â–ˆ| 73/73 [00:01<00:00, 64.87it/s, loss=0.589, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 45:  99%|â–‰| 72/73 [00:01<00:00, 70.16it/s, loss=0.57, v_num=mwbm, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|â–ˆ| 73/73 [00:01<00:00, 68.62it/s, loss=0.57, v_num=mwbm, LTC_val_\u001b[A\n",
      "Epoch 46:  99%|â–‰| 72/73 [00:01<00:00, 60.27it/s, loss=0.536, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|â–ˆ| 73/73 [00:01<00:00, 59.40it/s, loss=0.536, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 47:  99%|â–‰| 72/73 [00:00<00:00, 80.72it/s, loss=0.56, v_num=mwbm, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|â–ˆ| 73/73 [00:00<00:00, 78.17it/s, loss=0.56, v_num=mwbm, LTC_val_\u001b[A\n",
      "Epoch 48:  99%|â–‰| 72/73 [00:01<00:00, 68.17it/s, loss=0.554, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|â–ˆ| 73/73 [00:01<00:00, 66.98it/s, loss=0.554, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 49:  99%|â–‰| 72/73 [00:01<00:00, 67.96it/s, loss=0.52, v_num=mwbm, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|â–ˆ| 73/73 [00:01<00:00, 66.18it/s, loss=0.52, v_num=mwbm, LTC_val_\u001b[A\n",
      "Epoch 50:  99%|â–‰| 72/73 [00:01<00:00, 63.48it/s, loss=0.57, v_num=mwbm, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|â–ˆ| 73/73 [00:01<00:00, 62.59it/s, loss=0.57, v_num=mwbm, LTC_val_\u001b[A\n",
      "Epoch 51:  99%|â–‰| 72/73 [00:01<00:00, 62.96it/s, loss=0.526, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|â–ˆ| 73/73 [00:01<00:00, 61.67it/s, loss=0.526, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 52:  99%|â–‰| 72/73 [00:00<00:00, 88.48it/s, loss=0.54, v_num=mwbm, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|â–ˆ| 73/73 [00:00<00:00, 86.28it/s, loss=0.54, v_num=mwbm, LTC_val_\u001b[A\n",
      "Epoch 53:  99%|â–‰| 72/73 [00:00<00:00, 74.84it/s, loss=0.606, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|â–ˆ| 73/73 [00:01<00:00, 72.81it/s, loss=0.606, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 54:  99%|â–‰| 72/73 [00:00<00:00, 85.54it/s, loss=0.522, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|â–ˆ| 73/73 [00:00<00:00, 83.43it/s, loss=0.522, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 55:  99%|â–‰| 72/73 [00:01<00:00, 66.53it/s, loss=0.534, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|â–ˆ| 73/73 [00:01<00:00, 65.16it/s, loss=0.534, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 56:  99%|â–‰| 72/73 [00:00<00:00, 75.31it/s, loss=0.569, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|â–ˆ| 73/73 [00:00<00:00, 73.76it/s, loss=0.569, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 57:  99%|â–‰| 72/73 [00:01<00:00, 68.96it/s, loss=0.549, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.408\n",
      "Epoch 57: 100%|â–ˆ| 73/73 [00:01<00:00, 67.12it/s, loss=0.549, v_num=mwbm, LTC_val\n",
      "Epoch 58:  99%|â–‰| 72/73 [00:00<00:00, 77.94it/s, loss=0.533, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 0.404\n",
      "Epoch 58: 100%|â–ˆ| 73/73 [00:00<00:00, 75.99it/s, loss=0.533, v_num=mwbm, LTC_val\n",
      "Epoch 59:  99%|â–‰| 72/73 [00:01<00:00, 63.28it/s, loss=0.512, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 0.392\n",
      "Epoch 59: 100%|â–ˆ| 73/73 [00:01<00:00, 61.93it/s, loss=0.512, v_num=mwbm, LTC_val\n",
      "Epoch 60:  99%|â–‰| 72/73 [00:00<00:00, 75.58it/s, loss=0.539, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 60: 100%|â–ˆ| 73/73 [00:00<00:00, 73.13it/s, loss=0.539, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 61:  99%|â–‰| 72/73 [00:01<00:00, 65.34it/s, loss=0.516, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|â–ˆ| 73/73 [00:01<00:00, 63.90it/s, loss=0.516, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 62:  99%|â–‰| 72/73 [00:00<00:00, 75.75it/s, loss=0.542, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 62: 100%|â–ˆ| 73/73 [00:00<00:00, 73.84it/s, loss=0.542, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 63:  99%|â–‰| 72/73 [00:01<00:00, 71.57it/s, loss=0.566, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 63: 100%|â–ˆ| 73/73 [00:01<00:00, 69.83it/s, loss=0.566, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 64:  99%|â–‰| 72/73 [00:00<00:00, 79.95it/s, loss=0.55, v_num=mwbm, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 64: 100%|â–ˆ| 73/73 [00:00<00:00, 77.47it/s, loss=0.55, v_num=mwbm, LTC_val_\u001b[A\n",
      "Epoch 65:  99%|â–‰| 72/73 [00:01<00:00, 64.71it/s, loss=0.516, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 65: 100%|â–ˆ| 73/73 [00:01<00:00, 63.60it/s, loss=0.516, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 66:  99%|â–‰| 72/73 [00:00<00:00, 83.84it/s, loss=0.555, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 66: 100%|â–ˆ| 73/73 [00:00<00:00, 81.29it/s, loss=0.555, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 67:  99%|â–‰| 72/73 [00:01<00:00, 65.94it/s, loss=0.512, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 67: 100%|â–ˆ| 73/73 [00:01<00:00, 64.92it/s, loss=0.512, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 68:  99%|â–‰| 72/73 [00:00<00:00, 85.58it/s, loss=0.519, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.388\n",
      "Epoch 68: 100%|â–ˆ| 73/73 [00:00<00:00, 82.56it/s, loss=0.519, v_num=mwbm, LTC_val\n",
      "Epoch 69:  99%|â–‰| 72/73 [00:01<00:00, 67.86it/s, loss=0.535, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 69: 100%|â–ˆ| 73/73 [00:01<00:00, 66.34it/s, loss=0.535, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 70:  99%|â–‰| 72/73 [00:00<00:00, 72.28it/s, loss=0.519, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 70: 100%|â–ˆ| 73/73 [00:01<00:00, 69.94it/s, loss=0.519, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 71:  99%|â–‰| 72/73 [00:01<00:00, 67.12it/s, loss=0.543, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 71: 100%|â–ˆ| 73/73 [00:01<00:00, 65.92it/s, loss=0.543, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 72:  99%|â–‰| 72/73 [00:00<00:00, 81.23it/s, loss=0.518, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 72: 100%|â–ˆ| 73/73 [00:00<00:00, 78.83it/s, loss=0.518, v_num=mwbm, LTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73:  99%|â–‰| 72/73 [00:00<00:00, 77.22it/s, loss=0.499, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 73: 100%|â–ˆ| 73/73 [00:00<00:00, 75.68it/s, loss=0.499, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 74:  99%|â–‰| 72/73 [00:00<00:00, 83.13it/s, loss=0.537, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 74: 100%|â–ˆ| 73/73 [00:00<00:00, 74.82it/s, loss=0.537, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 75:  99%|â–‰| 72/73 [00:00<00:00, 74.44it/s, loss=0.565, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 75: 100%|â–ˆ| 73/73 [00:01<00:00, 72.87it/s, loss=0.565, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 76:  99%|â–‰| 72/73 [00:01<00:00, 71.41it/s, loss=0.511, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 76: 100%|â–ˆ| 73/73 [00:01<00:00, 69.63it/s, loss=0.511, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 77:  99%|â–‰| 72/73 [00:00<00:00, 72.68it/s, loss=0.531, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 77: 100%|â–ˆ| 73/73 [00:01<00:00, 71.38it/s, loss=0.531, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 78:  99%|â–‰| 72/73 [00:00<00:00, 81.70it/s, loss=0.604, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 78: 100%|â–ˆ| 73/73 [00:00<00:00, 79.31it/s, loss=0.604, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 79:  99%|â–‰| 72/73 [00:01<00:00, 60.49it/s, loss=0.52, v_num=mwbm, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 79: 100%|â–ˆ| 73/73 [00:01<00:00, 59.51it/s, loss=0.52, v_num=mwbm, LTC_val_\u001b[A\n",
      "Epoch 80:  99%|â–‰| 72/73 [00:01<00:00, 71.06it/s, loss=0.546, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 80: 100%|â–ˆ| 73/73 [00:01<00:00, 69.22it/s, loss=0.546, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 81:  99%|â–‰| 72/73 [00:01<00:00, 64.25it/s, loss=0.495, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.381\n",
      "Epoch 81: 100%|â–ˆ| 73/73 [00:01<00:00, 63.20it/s, loss=0.495, v_num=mwbm, LTC_val\n",
      "Epoch 82:  99%|â–‰| 72/73 [00:01<00:00, 70.50it/s, loss=0.554, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 82: 100%|â–ˆ| 73/73 [00:01<00:00, 62.39it/s, loss=0.554, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 83:  99%|â–‰| 72/73 [00:01<00:00, 66.91it/s, loss=0.531, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 83: 100%|â–ˆ| 73/73 [00:01<00:00, 65.16it/s, loss=0.531, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 84:  99%|â–‰| 72/73 [00:01<00:00, 64.25it/s, loss=0.477, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 84: 100%|â–ˆ| 73/73 [00:01<00:00, 63.25it/s, loss=0.477, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 85:  99%|â–‰| 72/73 [00:00<00:00, 83.68it/s, loss=0.543, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 85: 100%|â–ˆ| 73/73 [00:00<00:00, 81.12it/s, loss=0.543, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 86:  99%|â–‰| 72/73 [00:00<00:00, 73.34it/s, loss=0.523, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 86: 100%|â–ˆ| 73/73 [00:01<00:00, 71.95it/s, loss=0.523, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 87:  99%|â–‰| 72/73 [00:00<00:00, 72.69it/s, loss=0.542, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 87: 100%|â–ˆ| 73/73 [00:01<00:00, 70.85it/s, loss=0.542, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 88:  99%|â–‰| 72/73 [00:01<00:00, 58.60it/s, loss=0.558, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 88: 100%|â–ˆ| 73/73 [00:01<00:00, 57.82it/s, loss=0.558, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 89:  99%|â–‰| 72/73 [00:00<00:00, 79.97it/s, loss=0.519, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 89: 100%|â–ˆ| 73/73 [00:00<00:00, 77.39it/s, loss=0.519, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 90:  99%|â–‰| 72/73 [00:00<00:00, 72.03it/s, loss=0.555, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 90: 100%|â–ˆ| 73/73 [00:01<00:00, 70.56it/s, loss=0.555, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 91:  99%|â–‰| 72/73 [00:00<00:00, 81.84it/s, loss=0.506, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 91: 100%|â–ˆ| 73/73 [00:00<00:00, 79.33it/s, loss=0.506, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 92:  99%|â–‰| 72/73 [00:01<00:00, 71.28it/s, loss=0.483, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 92: 100%|â–ˆ| 73/73 [00:01<00:00, 69.93it/s, loss=0.483, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 93:  99%|â–‰| 72/73 [00:00<00:00, 80.82it/s, loss=0.536, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 93: 100%|â–ˆ| 73/73 [00:00<00:00, 78.60it/s, loss=0.536, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 94:  99%|â–‰| 72/73 [00:01<00:00, 57.82it/s, loss=0.513, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 94: 100%|â–ˆ| 73/73 [00:01<00:00, 56.95it/s, loss=0.513, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 95:  99%|â–‰| 72/73 [00:01<00:00, 65.95it/s, loss=0.44, v_num=mwbm, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 95: 100%|â–ˆ| 73/73 [00:01<00:00, 64.58it/s, loss=0.44, v_num=mwbm, LTC_val_\u001b[A\n",
      "Epoch 96:  99%|â–‰| 72/73 [00:00<00:00, 75.89it/s, loss=0.536, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 96: 100%|â–ˆ| 73/73 [00:00<00:00, 74.31it/s, loss=0.536, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 97:  99%|â–‰| 72/73 [00:00<00:00, 73.13it/s, loss=0.536, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 97: 100%|â–ˆ| 73/73 [00:01<00:00, 70.69it/s, loss=0.536, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 98:  99%|â–‰| 72/73 [00:00<00:00, 80.51it/s, loss=0.518, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 98: 100%|â–ˆ| 73/73 [00:00<00:00, 78.76it/s, loss=0.518, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 99:  99%|â–‰| 72/73 [00:00<00:00, 74.15it/s, loss=0.527, v_num=mwbm, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 99: 100%|â–ˆ| 73/73 [00:01<00:00, 72.11it/s, loss=0.527, v_num=mwbm, LTC_val\u001b[A\n",
      "Epoch 100:  99%|â–‰| 72/73 [00:00<00:00, 82.51it/s, loss=0.488, v_num=mwbm, LTC_va\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 100: 100%|â–ˆ| 73/73 [00:00<00:00, 80.53it/s, loss=0.488, v_num=mwbm, LTC_va\u001b[A\n",
      "Epoch 101:  99%|â–‰| 72/73 [00:01<00:00, 63.38it/s, loss=0.489, v_num=mwbm, LTC_va\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.381. Signaling Trainer to stop.\n",
      "Epoch 101: 100%|â–ˆ| 73/73 [00:01<00:00, 61.85it/s, loss=0.489, v_num=mwbm, LTC_va\n",
      "Epoch 101: 100%|â–ˆ| 73/73 [00:01<00:00, 61.68it/s, loss=0.489, v_num=mwbm, LTC_va\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 109.54it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.75,\n",
      " 'LTC_test_f1': 0.7414525747299194,\n",
      " 'test_loss': 0.46684202551841736}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 71155\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014131-2cbnmwbm/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014131-2cbnmwbm/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.43983\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 101\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 7344\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 114\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464205\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 350\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.74826\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.73441\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.49645\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.87302\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.39826\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.74145\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.46684\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–†â–„â–‡â–†â–…â–„â–†â–†â–ˆâ–‡â–†â–†â–†â–†â–ƒâ–‡â–†â–†â–‡â–…â–„â–†â–„â–‡â–†â–ƒâ–…â–†â–„â–â–†â–ƒâ–…â–ˆâ–…â–„â–†â–†â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–†â–ƒâ–‡â–…â–…â–„â–†â–†â–ˆâ–‡â–†â–†â–†â–†â–ƒâ–‡â–†â–†â–‡â–…â–„â–†â–„â–‡â–†â–ƒâ–…â–†â–ƒâ–â–†â–ƒâ–…â–ˆâ–…â–„â–†â–†â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–ƒâ–„â–ƒâ–ƒâ–„â–„â–…â–ƒâ–‚â–ƒâ–ƒâ–„â–ƒâ–‚â–‡â–â–‚â–ƒâ–‚â–…â–…â–ƒâ–„â–ƒâ–ƒâ–…â–„â–ƒâ–„â–‡â–ƒâ–ˆâ–„â–â–†â–ƒâ–ƒâ–‚â–ƒâ–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–â–…â–…â–…â–…â–†â–†â–†â–†â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–†â–†â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–†â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–†â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ƒâ–†â–†â–†â–ƒâ–†â–†â–ˆâ–†â–ˆâ–†â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ƒâ–†â–…â–…â–ƒâ–†â–†â–ˆâ–†â–ˆâ–…â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–†â–ˆâ–…â–†â–†â–†â–†â–†â–„â–†â–…â–„â–†â–†â–…â–‚â–„â–…â–‚â–ƒâ–ƒâ–‚â–ƒâ–â–„â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_single_lstm__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2cbnmwbm\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:43:36.390804: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_single_lstm__binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2e4itpvr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014334-2e4itpvr\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 130   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.7 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 73/74 [00:00<00:00, 86.99it/s, loss=0.697, v_num=tpvr, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.715\n",
      "Epoch 0: 100%|â–ˆ| 74/74 [00:00<00:00, 84.64it/s, loss=0.697, v_num=tpvr, LTC_val_\n",
      "Epoch 1:  99%|â–‰| 73/74 [00:01<00:00, 69.42it/s, loss=0.698, v_num=tpvr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.708\n",
      "Epoch 1: 100%|â–ˆ| 74/74 [00:01<00:00, 67.77it/s, loss=0.698, v_num=tpvr, LTC_val_\n",
      "Epoch 2:  99%|â–‰| 73/74 [00:00<00:00, 80.08it/s, loss=0.701, v_num=tpvr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 0.694\n",
      "Epoch 2: 100%|â–ˆ| 74/74 [00:00<00:00, 77.86it/s, loss=0.701, v_num=tpvr, LTC_val_\n",
      "Epoch 3:  99%|â–‰| 73/74 [00:01<00:00, 69.03it/s, loss=0.7, v_num=tpvr, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 74/74 [00:01<00:00, 66.76it/s, loss=0.7, v_num=tpvr, LTC_val_ac\u001b[A\n",
      "Epoch 4:  99%|â–‰| 73/74 [00:00<00:00, 82.14it/s, loss=0.701, v_num=tpvr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 74/74 [00:00<00:00, 79.92it/s, loss=0.701, v_num=tpvr, LTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 73/74 [00:01<00:00, 69.80it/s, loss=0.697, v_num=tpvr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 74/74 [00:01<00:00, 68.30it/s, loss=0.697, v_num=tpvr, LTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 73/74 [00:00<00:00, 87.91it/s, loss=0.693, v_num=tpvr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 74/74 [00:00<00:00, 85.29it/s, loss=0.693, v_num=tpvr, LTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 73/74 [00:01<00:00, 65.65it/s, loss=0.694, v_num=tpvr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 74/74 [00:01<00:00, 64.06it/s, loss=0.694, v_num=tpvr, LTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 73/74 [00:00<00:00, 80.84it/s, loss=0.699, v_num=tpvr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 74/74 [00:00<00:00, 78.58it/s, loss=0.699, v_num=tpvr, LTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 73/74 [00:01<00:00, 66.63it/s, loss=0.696, v_num=tpvr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 74/74 [00:01<00:00, 65.17it/s, loss=0.696, v_num=tpvr, LTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 73/74 [00:00<00:00, 75.64it/s, loss=0.694, v_num=tpvr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 74/74 [00:01<00:00, 73.73it/s, loss=0.694, v_num=tpvr, LTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 73/74 [00:01<00:00, 68.19it/s, loss=0.696, v_num=tpvr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 74/74 [00:01<00:00, 66.68it/s, loss=0.696, v_num=tpvr, LTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 73/74 [00:00<00:00, 79.95it/s, loss=0.692, v_num=tpvr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 74/74 [00:00<00:00, 77.61it/s, loss=0.692, v_num=tpvr, LTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 73/74 [00:01<00:00, 65.15it/s, loss=0.692, v_num=tpvr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 74/74 [00:01<00:00, 64.15it/s, loss=0.692, v_num=tpvr, LTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 73/74 [00:00<00:00, 84.12it/s, loss=0.694, v_num=tpvr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 74/74 [00:00<00:00, 81.65it/s, loss=0.694, v_num=tpvr, LTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 73/74 [00:00<00:00, 74.91it/s, loss=0.696, v_num=tpvr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 74/74 [00:01<00:00, 73.34it/s, loss=0.696, v_num=tpvr, LTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 73/74 [00:00<00:00, 78.57it/s, loss=0.696, v_num=tpvr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 74/74 [00:00<00:00, 76.24it/s, loss=0.696, v_num=tpvr, LTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 73/74 [00:01<00:00, 57.40it/s, loss=0.694, v_num=tpvr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 74/74 [00:01<00:00, 56.37it/s, loss=0.694, v_num=tpvr, LTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 73/74 [00:01<00:00, 68.39it/s, loss=0.691, v_num=tpvr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 74/74 [00:01<00:00, 66.61it/s, loss=0.691, v_num=tpvr, LTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 73/74 [00:00<00:00, 78.04it/s, loss=0.698, v_num=tpvr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 74/74 [00:00<00:00, 76.29it/s, loss=0.698, v_num=tpvr, LTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 73/74 [00:01<00:00, 53.89it/s, loss=0.696, v_num=tpvr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:01<00:00, 53.30it/s, loss=0.696, v_num=tpvr, LTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 73/74 [00:00<00:00, 89.61it/s, loss=0.691, v_num=tpvr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 74/74 [00:00<00:00, 87.09it/s, loss=0.691, v_num=tpvr, LTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 73/74 [00:01<00:00, 54.58it/s, loss=0.691, v_num=tpvr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.694. Signaling Trainer to stop.\n",
      "Epoch 22: 100%|â–ˆ| 74/74 [00:01<00:00, 53.98it/s, loss=0.691, v_num=tpvr, LTC_val\n",
      "Epoch 22: 100%|â–ˆ| 74/74 [00:01<00:00, 53.84it/s, loss=0.691, v_num=tpvr, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 147.95it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.5,\n",
      " 'LTC_test_f1': 0.32692310214042664,\n",
      " 'test_loss': 0.6933276057243347}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 71483\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014334-2e4itpvr/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014334-2e4itpvr/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.4375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.37662\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.69875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 22\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1679\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 34\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464248\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 79\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.51554\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.48434\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.6934\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.69318\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.32692\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.69333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–ƒâ–†â–‚â–†â–ˆâ–ƒâ–…â–…â–ƒâ–‚â–…â–…â–â–‡â–‡â–…â–†â–‡â–†â–„â–ƒâ–ƒâ–ˆâ–‡â–„â–…â–„â–‡â–ƒâ–ƒâ–ˆâ–…â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–‚â–†â–‚â–…â–ˆâ–ƒâ–„â–…â–ƒâ–‚â–…â–…â–â–‡â–†â–…â–†â–…â–†â–„â–ƒâ–‚â–‡â–‡â–„â–…â–„â–‡â–ƒâ–ƒâ–ˆâ–…â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–ˆâ–ƒâ–„â–‚â–â–‡â–ƒâ–ƒâ–ƒâ–„â–‚â–‚â–„â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–„â–„â–…â–ƒâ–„â–‚â–â–„â–â–ƒâ–†â–„â–‚â–‚â–â–ƒâ–…â–‚â–ˆâ–‚â–…â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–…â–†â–‡â–…â–ƒâ–†â–…â–†â–‚â–…â–ƒâ–‡â–…â–„â–‚â–â–‡â–‚â–ˆâ–†â–‚â–…â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–‡â–„â–…â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–†â–‚â–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_single_lstm__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2e4itpvr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:44:22.809361: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_single_lstm__multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/53y9o27n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014421-53y9o27n\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 195   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.8 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 72/73 [00:00<00:00, 81.71it/s, loss=1.05, v_num=o27n, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 73/73 [00:00<00:00, 73.82it/s, loss=1.05, v_num=o27n, LTC_val_a\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved. New best score: 1.132\n",
      "Epoch 1:  99%|â–‰| 72/73 [00:01<00:00, 70.81it/s, loss=1.07, v_num=o27n, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.015 >= min_delta = 0.003. New best score: 1.117\n",
      "Epoch 1: 100%|â–ˆ| 73/73 [00:01<00:00, 68.95it/s, loss=1.07, v_num=o27n, LTC_val_a\n",
      "Epoch 2:  99%|â–‰| 72/73 [00:00<00:00, 79.40it/s, loss=1.03, v_num=o27n, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 73/73 [00:00<00:00, 76.75it/s, loss=1.03, v_num=o27n, LTC_val_a\u001b[A\n",
      "Epoch 3:  99%|â–‰| 72/73 [00:01<00:00, 70.23it/s, loss=1.02, v_num=o27n, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 1.111\n",
      "Epoch 3: 100%|â–ˆ| 73/73 [00:01<00:00, 68.52it/s, loss=1.02, v_num=o27n, LTC_val_a\n",
      "Epoch 4:  99%|â–‰| 72/73 [00:00<00:00, 78.39it/s, loss=1.04, v_num=o27n, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 73/73 [00:00<00:00, 76.27it/s, loss=1.04, v_num=o27n, LTC_val_a\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.029 >= min_delta = 0.003. New best score: 1.082\n",
      "Epoch 5:  99%|â–‰| 72/73 [00:01<00:00, 70.80it/s, loss=1, v_num=o27n, LTC_val_acc=\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.050 >= min_delta = 0.003. New best score: 1.032\n",
      "Epoch 5: 100%|â–ˆ| 73/73 [00:01<00:00, 69.10it/s, loss=1, v_num=o27n, LTC_val_acc=\n",
      "Epoch 6:  99%|â–‰| 72/73 [00:00<00:00, 73.13it/s, loss=0.985, v_num=o27n, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 73/73 [00:01<00:00, 71.23it/s, loss=0.985, v_num=o27n, LTC_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 1.023\n",
      "Epoch 7:  99%|â–‰| 72/73 [00:01<00:00, 70.54it/s, loss=0.963, v_num=o27n, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 73/73 [00:01<00:00, 68.99it/s, loss=0.963, v_num=o27n, LTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 72/73 [00:01<00:00, 70.40it/s, loss=0.961, v_num=o27n, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 1.002\n",
      "Epoch 8: 100%|â–ˆ| 73/73 [00:01<00:00, 68.64it/s, loss=0.961, v_num=o27n, LTC_val_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  99%|â–‰| 72/73 [00:00<00:00, 74.94it/s, loss=0.976, v_num=o27n, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 73/73 [00:00<00:00, 73.13it/s, loss=0.976, v_num=o27n, LTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 72/73 [00:01<00:00, 68.89it/s, loss=0.91, v_num=o27n, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 73/73 [00:01<00:00, 66.56it/s, loss=0.91, v_num=o27n, LTC_val_\u001b[A\n",
      "Epoch 11:  99%|â–‰| 72/73 [00:00<00:00, 74.55it/s, loss=0.945, v_num=o27n, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 73/73 [00:00<00:00, 73.07it/s, loss=0.945, v_num=o27n, LTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 72/73 [00:00<00:00, 75.98it/s, loss=0.957, v_num=o27n, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 73/73 [00:00<00:00, 73.67it/s, loss=0.957, v_num=o27n, LTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 72/73 [00:00<00:00, 76.48it/s, loss=0.952, v_num=o27n, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 73/73 [00:00<00:00, 74.77it/s, loss=0.952, v_num=o27n, LTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 72/73 [00:00<00:00, 77.94it/s, loss=0.927, v_num=o27n, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 73/73 [00:01<00:00, 63.54it/s, loss=0.927, v_num=o27n, LTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 72/73 [00:00<00:00, 74.50it/s, loss=0.939, v_num=o27n, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 73/73 [00:01<00:00, 73.00it/s, loss=0.939, v_num=o27n, LTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 72/73 [00:00<00:00, 73.42it/s, loss=0.968, v_num=o27n, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 73/73 [00:01<00:00, 71.13it/s, loss=0.968, v_num=o27n, LTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 72/73 [00:00<00:00, 84.79it/s, loss=0.924, v_num=o27n, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 73/73 [00:00<00:00, 82.79it/s, loss=0.924, v_num=o27n, LTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 72/73 [00:00<00:00, 76.29it/s, loss=0.931, v_num=o27n, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 73/73 [00:00<00:00, 74.25it/s, loss=0.931, v_num=o27n, LTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 72/73 [00:00<00:00, 80.89it/s, loss=0.944, v_num=o27n, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 73/73 [00:00<00:00, 78.96it/s, loss=0.944, v_num=o27n, LTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 72/73 [00:00<00:00, 77.21it/s, loss=0.935, v_num=o27n, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 73/73 [00:00<00:00, 75.08it/s, loss=0.935, v_num=o27n, LTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 72/73 [00:01<00:00, 69.42it/s, loss=0.934, v_num=o27n, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 73/73 [00:01<00:00, 68.17it/s, loss=0.934, v_num=o27n, LTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 72/73 [00:00<00:00, 75.19it/s, loss=0.907, v_num=o27n, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 73/73 [00:01<00:00, 72.89it/s, loss=0.907, v_num=o27n, LTC_val\u001b[A\n",
      "Epoch 23:  99%|â–‰| 72/73 [00:00<00:00, 80.34it/s, loss=0.991, v_num=o27n, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 73/73 [00:00<00:00, 78.53it/s, loss=0.991, v_num=o27n, LTC_val\u001b[A\n",
      "Epoch 24:  99%|â–‰| 72/73 [00:00<00:00, 76.97it/s, loss=0.946, v_num=o27n, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 73/73 [00:00<00:00, 74.72it/s, loss=0.946, v_num=o27n, LTC_val\u001b[A\n",
      "Epoch 25:  99%|â–‰| 72/73 [00:00<00:00, 80.09it/s, loss=0.924, v_num=o27n, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 73/73 [00:00<00:00, 78.32it/s, loss=0.924, v_num=o27n, LTC_val\u001b[A\n",
      "Epoch 26:  99%|â–‰| 72/73 [00:01<00:00, 66.41it/s, loss=0.937, v_num=o27n, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 73/73 [00:01<00:00, 64.87it/s, loss=0.937, v_num=o27n, LTC_val\u001b[A\n",
      "Epoch 27:  99%|â–‰| 72/73 [00:00<00:00, 80.42it/s, loss=0.914, v_num=o27n, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 73/73 [00:00<00:00, 78.52it/s, loss=0.914, v_num=o27n, LTC_val\u001b[A\n",
      "Epoch 28:  99%|â–‰| 72/73 [00:00<00:00, 77.88it/s, loss=0.919, v_num=o27n, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.002. Signaling Trainer to stop.\n",
      "Epoch 28: 100%|â–ˆ| 73/73 [00:00<00:00, 75.61it/s, loss=0.919, v_num=o27n, LTC_val\n",
      "Epoch 28: 100%|â–ˆ| 73/73 [00:00<00:00, 75.15it/s, loss=0.919, v_num=o27n, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 104.40it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.5,\n",
      " 'LTC_test_f1': 0.3047619163990021,\n",
      " 'test_loss': 0.8802793622016907}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 71654\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014421-53y9o27n/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014421-53y9o27n/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.34444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.90984\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 28\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2088\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 38\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464299\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.5191\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.39949\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.92335\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.13992\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.30476\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.88028\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–…â–…â–„â–†â–†â–ˆâ–…â–…â–„â–…â–…â–â–…â–…â–„â–‡â–„â–…â–…â–„â–†â–†â–…â–ƒâ–†â–…â–…â–ƒâ–…â–†â–†â–…â–„â–ƒâ–…â–ƒâ–†â–‡â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–„â–„â–ƒâ–„â–„â–…â–„â–‚â–ƒâ–‚â–„â–â–‚â–„â–ƒâ–„â–ƒâ–…â–„â–‚â–…â–†â–…â–‚â–…â–ƒâ–…â–ƒâ–ƒâ–ˆâ–…â–‚â–ƒâ–ƒâ–‡â–‚â–…â–†â–ƒâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–…â–ˆâ–‡â–„â–†â–„â–‡â–…â–‡â–…â–†â–‡â–„â–…â–ˆâ–ƒâ–‡â–„â–…â–ˆâ–†â–„â–…â–†â–…â–…â–†â–…â–ƒâ–ˆâ–‚â–„â–†â–ˆâ–†â–†â–ƒâ–â–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–â–ƒâ–„â–…â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–ˆâ–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–†â–ˆâ–ˆâ–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–‚â–‚â–‚â–â–‚â–â–â–‚â–„â–„â–„â–…â–†â–…â–†â–†â–‡â–†â–†â–†â–ˆâ–†â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–‡â–‡â–‡â–‡â–†â–…â–…â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–â–â–ƒâ–â–â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–…â–…â–…â–…â–…â–…â–…â–â–ˆâ–…â–ˆâ–…â–…â–…â–â–…â–â–…â–â–…â–…â–ˆâ–…â–ˆâ–ˆâ–…â–â–â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‡â–…â–ˆâ–…â–…â–…â–ƒâ–…â–ƒâ–…â–ƒâ–…â–…â–ˆâ–…â–ˆâ–ˆâ–†â–ƒâ–ƒâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–…â–„â–…â–„â–ƒâ–‚â–‚â–‚â–â–ƒâ–ƒâ–„â–„â–ƒâ–…â–‚â–†â–‚â–„â–ƒâ–‚â–„â–ƒâ–‚â–‚â–‚â–ˆâ–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_single_lstm__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/53y9o27n\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:45:10.316975: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_single_lstm__multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/23me2100\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014507-23me2100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 195   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.8 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 73/74 [00:00<00:00, 79.10it/s, loss=1.11, v_num=2100, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.083\n",
      "Epoch 0: 100%|â–ˆ| 74/74 [00:00<00:00, 77.01it/s, loss=1.11, v_num=2100, LTC_val_a\n",
      "Epoch 1:  99%|â–‰| 73/74 [00:01<00:00, 70.31it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 1.077\n",
      "Epoch 1: 100%|â–ˆ| 74/74 [00:01<00:00, 68.13it/s, loss=1.11, v_num=2100, LTC_val_a\n",
      "Epoch 2:  99%|â–‰| 73/74 [00:00<00:00, 81.50it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 74/74 [00:00<00:00, 79.50it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n",
      "Epoch 3:  99%|â–‰| 73/74 [00:01<00:00, 71.55it/s, loss=1.1, v_num=2100, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 74/74 [00:01<00:00, 69.67it/s, loss=1.1, v_num=2100, LTC_val_ac\u001b[A\n",
      "Epoch 4:  99%|â–‰| 73/74 [00:00<00:00, 79.51it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 74/74 [00:00<00:00, 77.66it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n",
      "Epoch 5:  99%|â–‰| 73/74 [00:00<00:00, 74.76it/s, loss=1.1, v_num=2100, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 74/74 [00:01<00:00, 72.69it/s, loss=1.1, v_num=2100, LTC_val_ac\u001b[A\n",
      "Epoch 6:  99%|â–‰| 73/74 [00:00<00:00, 84.97it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 74/74 [00:00<00:00, 82.58it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n",
      "Epoch 7:  99%|â–‰| 73/74 [00:00<00:00, 73.95it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 74/74 [00:01<00:00, 71.90it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n",
      "Epoch 8:  99%|â–‰| 73/74 [00:00<00:00, 81.74it/s, loss=1.09, v_num=2100, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 74/74 [00:00<00:00, 79.67it/s, loss=1.09, v_num=2100, LTC_val_a\u001b[A\n",
      "Epoch 9:  99%|â–‰| 73/74 [00:00<00:00, 79.06it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 74/74 [00:00<00:00, 76.38it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n",
      "Epoch 10:  99%|â–‰| 73/74 [00:00<00:00, 75.50it/s, loss=1.11, v_num=2100, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 74/74 [00:01<00:00, 73.71it/s, loss=1.11, v_num=2100, LTC_val_\u001b[A\n",
      "Epoch 11:  99%|â–‰| 73/74 [00:00<00:00, 78.46it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 74/74 [00:00<00:00, 75.85it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n",
      "Epoch 12:  99%|â–‰| 73/74 [00:00<00:00, 82.20it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 74/74 [00:00<00:00, 80.32it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n",
      "Epoch 13:  99%|â–‰| 73/74 [00:00<00:00, 83.20it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 74/74 [00:00<00:00, 80.74it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n",
      "Epoch 14:  99%|â–‰| 73/74 [00:00<00:00, 77.90it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 74/74 [00:00<00:00, 76.20it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n",
      "Epoch 15:  99%|â–‰| 73/74 [00:00<00:00, 82.64it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 74/74 [00:00<00:00, 80.06it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n",
      "Epoch 16:  99%|â–‰| 73/74 [00:01<00:00, 71.16it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 74/74 [00:01<00:00, 69.78it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n",
      "Epoch 17:  99%|â–‰| 73/74 [00:00<00:00, 83.62it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 74/74 [00:00<00:00, 81.10it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n",
      "Epoch 18:  99%|â–‰| 73/74 [00:00<00:00, 76.01it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 74/74 [00:00<00:00, 74.09it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n",
      "Epoch 19:  99%|â–‰| 73/74 [00:00<00:00, 74.95it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 74/74 [00:01<00:00, 72.69it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n",
      "Epoch 20:  99%|â–‰| 73/74 [00:01<00:00, 70.85it/s, loss=1.09, v_num=2100, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:01<00:00, 69.38it/s, loss=1.09, v_num=2100, LTC_val_\u001b[A\n",
      "Epoch 21:  99%|â–‰| 73/74 [00:00<00:00, 82.53it/s, loss=1.09, v_num=2100, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.077. Signaling Trainer to stop.\n",
      "Epoch 21: 100%|â–ˆ| 74/74 [00:00<00:00, 79.92it/s, loss=1.09, v_num=2100, LTC_val_\n",
      "Epoch 21: 100%|â–ˆ| 74/74 [00:00<00:00, 79.59it/s, loss=1.09, v_num=2100, LTC_val_\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 107.11it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.25,\n",
      " 'LTC_test_f1': 0.13151928782463074,\n",
      " 'test_loss': 1.1257140636444092}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 71818\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014507-23me2100/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014507-23me2100/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.49697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.07673\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464339\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 76\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.35579\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.32371\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.18182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.09773\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.13152\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.12571\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–„â–ƒâ–ˆâ–…â–„â–†â–…â–…â–â–â–„â–„â–‚â–ƒâ–†â–ƒâ–‚â–„â–…â–‚â–†â–‡â–ƒâ–‚â–ƒâ–†â–ƒâ–…â–…â–ƒâ–â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–„â–ƒâ–ˆâ–…â–„â–„â–„â–„â–‚â–â–…â–„â–ƒâ–ƒâ–†â–ƒâ–‚â–„â–†â–‚â–†â–ˆâ–ƒâ–‚â–ƒâ–…â–ƒâ–„â–†â–‚â–â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–„â–„â–â–â–…â–‚â–†â–â–ˆâ–†â–…â–…â–‡â–„â–…â–†â–„â–„â–ƒâ–…â–‚â–„â–ˆâ–„â–„â–ƒâ–„â–„â–„â–„â–‡â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–â–â–ƒâ–‚â–…â–†â–‚â–„â–†â–ƒâ–†â–‡â–†â–…â–ˆâ–†â–†â–‚â–„â–…â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–â–â–ƒâ–„â–…â–†â–â–„â–‡â–†â–‡â–…â–†â–…â–ˆâ–‡â–„â–â–‚â–â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–‡â–…â–„â–…â–ƒâ–„â–„â–‚â–ƒâ–„â–‚â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–â–â–â–â–â–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–‚â–â–…â–â–‡â–†â–ˆâ–†â–„â–…â–ƒâ–„â–…â–‚â–ƒâ–‚â–‚â–‚â–„â–‚â–„â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_single_lstm__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/23me2100\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:45:49.023290: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm__binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3kuodfw5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014547-3kuodfw5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 72/73 [00:01<00:00, 41.98it/s, loss=0.689, v_num=dfw5, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.641\n",
      "Epoch 0: 100%|â–ˆ| 73/73 [00:01<00:00, 39.96it/s, loss=0.689, v_num=dfw5, LTC_val_\n",
      "Epoch 1:  99%|â–‰| 72/73 [00:01<00:00, 40.23it/s, loss=0.647, v_num=dfw5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.632\n",
      "Epoch 1: 100%|â–ˆ| 73/73 [00:01<00:00, 40.00it/s, loss=0.647, v_num=dfw5, LTC_val_\n",
      "Epoch 2:  99%|â–‰| 72/73 [00:01<00:00, 37.78it/s, loss=0.59, v_num=dfw5, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.067 >= min_delta = 0.003. New best score: 0.564\n",
      "Epoch 2: 100%|â–ˆ| 73/73 [00:01<00:00, 37.53it/s, loss=0.59, v_num=dfw5, LTC_val_a\n",
      "Epoch 3:  99%|â–‰| 72/73 [00:01<00:00, 38.81it/s, loss=0.613, v_num=dfw5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.035 >= min_delta = 0.003. New best score: 0.529\n",
      "Epoch 3: 100%|â–ˆ| 73/73 [00:01<00:00, 38.43it/s, loss=0.613, v_num=dfw5, LTC_val_\n",
      "Epoch 4:  99%|â–‰| 72/73 [00:02<00:00, 34.31it/s, loss=0.587, v_num=dfw5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 73/73 [00:02<00:00, 34.11it/s, loss=0.587, v_num=dfw5, LTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 72/73 [00:01<00:00, 41.48it/s, loss=0.636, v_num=dfw5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 73/73 [00:01<00:00, 40.97it/s, loss=0.636, v_num=dfw5, LTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 72/73 [00:01<00:00, 42.46it/s, loss=0.57, v_num=dfw5, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 73/73 [00:01<00:00, 41.96it/s, loss=0.57, v_num=dfw5, LTC_val_a\u001b[A\n",
      "Epoch 7:  99%|â–‰| 72/73 [00:01<00:00, 42.70it/s, loss=0.593, v_num=dfw5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 73/73 [00:01<00:00, 42.29it/s, loss=0.593, v_num=dfw5, LTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 72/73 [00:01<00:00, 39.41it/s, loss=0.568, v_num=dfw5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 73/73 [00:01<00:00, 39.01it/s, loss=0.568, v_num=dfw5, LTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 72/73 [00:02<00:00, 34.52it/s, loss=0.58, v_num=dfw5, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆ| 73/73 [00:02<00:00, 34.32it/s, loss=0.58, v_num=dfw5, LTC_val_a\u001b[A\n",
      "Epoch 10:  99%|â–‰| 72/73 [00:01<00:00, 40.30it/s, loss=0.639, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 73/73 [00:01<00:00, 39.91it/s, loss=0.639, v_num=dfw5, LTC_val\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 0.508\n",
      "Epoch 11:  99%|â–‰| 72/73 [00:02<00:00, 28.45it/s, loss=0.604, v_num=dfw5, LTC_val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 73/73 [00:02<00:00, 28.36it/s, loss=0.604, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 72/73 [00:02<00:00, 28.59it/s, loss=0.602, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 73/73 [00:02<00:00, 28.46it/s, loss=0.602, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 72/73 [00:01<00:00, 41.04it/s, loss=0.599, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 73/73 [00:01<00:00, 40.80it/s, loss=0.599, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 72/73 [00:01<00:00, 40.44it/s, loss=0.586, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 73/73 [00:01<00:00, 40.08it/s, loss=0.586, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 72/73 [00:01<00:00, 38.17it/s, loss=0.608, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 73/73 [00:01<00:00, 37.89it/s, loss=0.608, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 72/73 [00:01<00:00, 41.84it/s, loss=0.564, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 73/73 [00:01<00:00, 41.28it/s, loss=0.564, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 72/73 [00:01<00:00, 42.61it/s, loss=0.625, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 73/73 [00:01<00:00, 42.05it/s, loss=0.625, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 72/73 [00:01<00:00, 37.48it/s, loss=0.577, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.503\n",
      "Epoch 18: 100%|â–ˆ| 73/73 [00:01<00:00, 36.92it/s, loss=0.577, v_num=dfw5, LTC_val\n",
      "Epoch 19:  99%|â–‰| 72/73 [00:01<00:00, 36.61it/s, loss=0.571, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 73/73 [00:02<00:00, 36.24it/s, loss=0.571, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 72/73 [00:02<00:00, 30.82it/s, loss=0.541, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 73/73 [00:02<00:00, 30.64it/s, loss=0.541, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 72/73 [00:02<00:00, 29.73it/s, loss=0.555, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 73/73 [00:02<00:00, 29.65it/s, loss=0.555, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 72/73 [00:02<00:00, 30.10it/s, loss=0.524, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 73/73 [00:02<00:00, 30.04it/s, loss=0.524, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 23:  99%|â–‰| 72/73 [00:02<00:00, 35.32it/s, loss=0.568, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 73/73 [00:02<00:00, 35.07it/s, loss=0.568, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 24:  99%|â–‰| 72/73 [00:01<00:00, 40.15it/s, loss=0.616, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.499\n",
      "Epoch 24: 100%|â–ˆ| 73/73 [00:01<00:00, 39.86it/s, loss=0.616, v_num=dfw5, LTC_val\n",
      "Epoch 25:  99%|â–‰| 72/73 [00:02<00:00, 35.03it/s, loss=0.519, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.019 >= min_delta = 0.003. New best score: 0.480\n",
      "Epoch 25: 100%|â–ˆ| 73/73 [00:02<00:00, 34.68it/s, loss=0.519, v_num=dfw5, LTC_val\n",
      "Epoch 26:  99%|â–‰| 72/73 [00:01<00:00, 36.20it/s, loss=0.592, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.028 >= min_delta = 0.003. New best score: 0.452\n",
      "Epoch 26: 100%|â–ˆ| 73/73 [00:02<00:00, 35.80it/s, loss=0.592, v_num=dfw5, LTC_val\n",
      "Epoch 27:  99%|â–‰| 72/73 [00:02<00:00, 31.66it/s, loss=0.557, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 73/73 [00:02<00:00, 31.34it/s, loss=0.557, v_num=dfw5, LTC_val\u001b[A\n",
      "Metric val_loss improved by 0.015 >= min_delta = 0.003. New best score: 0.437\n",
      "Epoch 28:  99%|â–‰| 72/73 [00:02<00:00, 32.06it/s, loss=0.541, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 73/73 [00:02<00:00, 31.89it/s, loss=0.541, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 29:  99%|â–‰| 72/73 [00:02<00:00, 32.16it/s, loss=0.546, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 73/73 [00:02<00:00, 31.98it/s, loss=0.546, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 30:  99%|â–‰| 72/73 [00:01<00:00, 38.18it/s, loss=0.551, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.419\n",
      "Epoch 30: 100%|â–ˆ| 73/73 [00:01<00:00, 37.79it/s, loss=0.551, v_num=dfw5, LTC_val\n",
      "Epoch 31:  99%|â–‰| 72/73 [00:02<00:00, 34.20it/s, loss=0.556, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 73/73 [00:02<00:00, 33.94it/s, loss=0.556, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 32:  99%|â–‰| 72/73 [00:02<00:00, 35.16it/s, loss=0.541, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 73/73 [00:02<00:00, 35.01it/s, loss=0.541, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 33:  99%|â–‰| 72/73 [00:02<00:00, 35.83it/s, loss=0.576, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 73/73 [00:02<00:00, 35.64it/s, loss=0.576, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 34:  99%|â–‰| 72/73 [00:02<00:00, 32.74it/s, loss=0.58, v_num=dfw5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 73/73 [00:02<00:00, 32.65it/s, loss=0.58, v_num=dfw5, LTC_val_\u001b[A\n",
      "Epoch 35:  99%|â–‰| 72/73 [00:01<00:00, 37.65it/s, loss=0.565, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 73/73 [00:01<00:00, 37.38it/s, loss=0.565, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 36:  99%|â–‰| 72/73 [00:02<00:00, 35.00it/s, loss=0.583, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 73/73 [00:02<00:00, 34.82it/s, loss=0.583, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 37:  99%|â–‰| 72/73 [00:01<00:00, 36.03it/s, loss=0.508, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|â–ˆ| 73/73 [00:02<00:00, 35.85it/s, loss=0.508, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 38:  99%|â–‰| 72/73 [00:02<00:00, 32.25it/s, loss=0.54, v_num=dfw5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|â–ˆ| 73/73 [00:02<00:00, 32.14it/s, loss=0.54, v_num=dfw5, LTC_val_\u001b[A\n",
      "Epoch 39:  99%|â–‰| 72/73 [00:02<00:00, 35.15it/s, loss=0.558, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|â–ˆ| 73/73 [00:02<00:00, 34.98it/s, loss=0.558, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 40:  99%|â–‰| 72/73 [00:02<00:00, 32.67it/s, loss=0.549, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|â–ˆ| 73/73 [00:02<00:00, 32.49it/s, loss=0.549, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 41:  99%|â–‰| 72/73 [00:01<00:00, 37.67it/s, loss=0.502, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|â–ˆ| 73/73 [00:01<00:00, 37.25it/s, loss=0.502, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 42:  99%|â–‰| 72/73 [00:02<00:00, 35.22it/s, loss=0.555, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|â–ˆ| 73/73 [00:02<00:00, 34.88it/s, loss=0.555, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 43:  99%|â–‰| 72/73 [00:02<00:00, 35.17it/s, loss=0.491, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|â–ˆ| 73/73 [00:02<00:00, 34.92it/s, loss=0.491, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 44:  99%|â–‰| 72/73 [00:01<00:00, 36.17it/s, loss=0.558, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|â–ˆ| 73/73 [00:02<00:00, 35.92it/s, loss=0.558, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 45:  99%|â–‰| 72/73 [00:02<00:00, 32.39it/s, loss=0.566, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|â–ˆ| 73/73 [00:02<00:00, 32.23it/s, loss=0.566, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 46:  99%|â–‰| 72/73 [00:02<00:00, 34.53it/s, loss=0.512, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|â–ˆ| 73/73 [00:02<00:00, 34.31it/s, loss=0.512, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 47:  99%|â–‰| 72/73 [00:02<00:00, 34.21it/s, loss=0.566, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|â–ˆ| 73/73 [00:02<00:00, 33.85it/s, loss=0.566, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 48:  99%|â–‰| 72/73 [00:01<00:00, 39.87it/s, loss=0.529, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|â–ˆ| 73/73 [00:01<00:00, 39.53it/s, loss=0.529, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 49:  99%|â–‰| 72/73 [00:02<00:00, 27.68it/s, loss=0.485, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.034 >= min_delta = 0.003. New best score: 0.385\n",
      "Epoch 49: 100%|â–ˆ| 73/73 [00:02<00:00, 27.62it/s, loss=0.485, v_num=dfw5, LTC_val\n",
      "Epoch 50:  99%|â–‰| 72/73 [00:01<00:00, 37.15it/s, loss=0.544, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|â–ˆ| 73/73 [00:01<00:00, 36.63it/s, loss=0.544, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 51:  99%|â–‰| 72/73 [00:01<00:00, 37.01it/s, loss=0.512, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|â–ˆ| 73/73 [00:01<00:00, 36.78it/s, loss=0.512, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 52:  99%|â–‰| 72/73 [00:01<00:00, 37.93it/s, loss=0.51, v_num=dfw5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|â–ˆ| 73/73 [00:01<00:00, 37.65it/s, loss=0.51, v_num=dfw5, LTC_val_\u001b[A\n",
      "Epoch 53:  99%|â–‰| 72/73 [00:01<00:00, 38.01it/s, loss=0.526, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|â–ˆ| 73/73 [00:01<00:00, 37.68it/s, loss=0.526, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 54:  99%|â–‰| 72/73 [00:01<00:00, 37.46it/s, loss=0.467, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|â–ˆ| 73/73 [00:01<00:00, 37.21it/s, loss=0.467, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 55:  99%|â–‰| 72/73 [00:02<00:00, 35.15it/s, loss=0.49, v_num=dfw5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|â–ˆ| 73/73 [00:02<00:00, 34.96it/s, loss=0.49, v_num=dfw5, LTC_val_\u001b[A\n",
      "Epoch 56:  99%|â–‰| 72/73 [00:01<00:00, 37.14it/s, loss=0.469, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.015 >= min_delta = 0.003. New best score: 0.370\n",
      "Epoch 56: 100%|â–ˆ| 73/73 [00:01<00:00, 36.87it/s, loss=0.469, v_num=dfw5, LTC_val\n",
      "Epoch 57:  99%|â–‰| 72/73 [00:02<00:00, 32.16it/s, loss=0.463, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|â–ˆ| 73/73 [00:02<00:00, 31.99it/s, loss=0.463, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 58:  99%|â–‰| 72/73 [00:02<00:00, 32.53it/s, loss=0.502, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|â–ˆ| 73/73 [00:02<00:00, 32.42it/s, loss=0.502, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 59:  99%|â–‰| 72/73 [00:02<00:00, 35.54it/s, loss=0.493, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|â–ˆ| 73/73 [00:02<00:00, 35.39it/s, loss=0.493, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 60:  99%|â–‰| 72/73 [00:02<00:00, 29.82it/s, loss=0.45, v_num=dfw5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 60: 100%|â–ˆ| 73/73 [00:02<00:00, 29.72it/s, loss=0.45, v_num=dfw5, LTC_val_\u001b[A\n",
      "Epoch 61:  99%|â–‰| 72/73 [00:01<00:00, 36.48it/s, loss=0.47, v_num=dfw5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|â–ˆ| 73/73 [00:02<00:00, 36.22it/s, loss=0.47, v_num=dfw5, LTC_val_\u001b[A\n",
      "Epoch 62:  99%|â–‰| 72/73 [00:02<00:00, 31.77it/s, loss=0.467, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 62: 100%|â–ˆ| 73/73 [00:02<00:00, 31.50it/s, loss=0.467, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 63:  99%|â–‰| 72/73 [00:01<00:00, 37.18it/s, loss=0.441, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 63: 100%|â–ˆ| 73/73 [00:01<00:00, 36.82it/s, loss=0.441, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 64:  99%|â–‰| 72/73 [00:01<00:00, 39.78it/s, loss=0.443, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 64: 100%|â–ˆ| 73/73 [00:01<00:00, 39.27it/s, loss=0.443, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 65:  99%|â–‰| 72/73 [00:01<00:00, 36.73it/s, loss=0.47, v_num=dfw5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 65: 100%|â–ˆ| 73/73 [00:02<00:00, 36.44it/s, loss=0.47, v_num=dfw5, LTC_val_\u001b[A\n",
      "Epoch 66:  99%|â–‰| 72/73 [00:02<00:00, 35.97it/s, loss=0.457, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 66: 100%|â–ˆ| 73/73 [00:02<00:00, 35.63it/s, loss=0.457, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 67:  99%|â–‰| 72/73 [00:02<00:00, 32.56it/s, loss=0.371, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 67: 100%|â–ˆ| 73/73 [00:02<00:00, 32.39it/s, loss=0.371, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 68:  99%|â–‰| 72/73 [00:02<00:00, 30.80it/s, loss=0.407, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 68: 100%|â–ˆ| 73/73 [00:02<00:00, 30.69it/s, loss=0.407, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 69:  99%|â–‰| 72/73 [00:02<00:00, 32.86it/s, loss=0.386, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 69: 100%|â–ˆ| 73/73 [00:02<00:00, 32.74it/s, loss=0.386, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 70:  99%|â–‰| 72/73 [00:02<00:00, 33.28it/s, loss=0.372, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 70: 100%|â–ˆ| 73/73 [00:02<00:00, 33.19it/s, loss=0.372, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 71:  99%|â–‰| 72/73 [00:02<00:00, 29.67it/s, loss=0.357, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 71: 100%|â–ˆ| 73/73 [00:02<00:00, 27.73it/s, loss=0.357, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 72:  99%|â–‰| 72/73 [00:02<00:00, 28.64it/s, loss=0.368, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 72: 100%|â–ˆ| 73/73 [00:02<00:00, 28.58it/s, loss=0.368, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 73:  99%|â–‰| 72/73 [00:01<00:00, 40.66it/s, loss=0.368, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 73: 100%|â–ˆ| 73/73 [00:01<00:00, 40.30it/s, loss=0.368, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 74:  99%|â–‰| 72/73 [00:01<00:00, 40.40it/s, loss=0.385, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 74: 100%|â–ˆ| 73/73 [00:01<00:00, 39.97it/s, loss=0.385, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 75:  99%|â–‰| 72/73 [00:01<00:00, 40.29it/s, loss=0.357, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 75: 100%|â–ˆ| 73/73 [00:01<00:00, 39.84it/s, loss=0.357, v_num=dfw5, LTC_val\u001b[A\n",
      "Epoch 76:  99%|â–‰| 72/73 [00:02<00:00, 34.76it/s, loss=0.279, v_num=dfw5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 76: 100%|â–ˆ| 73/73 [00:02<00:00, 34.22it/s, loss=0.279, v_num=dfw5, LTC_val\u001b[A\n",
      "                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.370. Signaling Trainer to stop.\n",
      "Epoch 76: 100%|â–ˆ| 73/73 [00:02<00:00, 34.16it/s, loss=0.279, v_num=dfw5, LTC_val\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 51.36it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.7142857313156128,\n",
      " 'LTC_test_f1': 0.7088435888290405,\n",
      " 'test_loss': 0.5559489130973816}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 71970\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014547-3kuodfw5/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014547-3kuodfw5/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.6875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.68627\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.62474\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 76\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 5544\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 170\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464517\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 264\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.82465\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.81569\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.36146\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.43753\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.71429\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.70884\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.55595\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–â–ƒâ–…â–‡â–…â–‡â–‡â–…â–„â–…â–…â–…â–†â–…â–†â–‡â–†â–„â–†â–‡â–†â–…â–‡â–…â–‡â–…â–…â–†â–…â–†â–‡â–…â–ˆâ–…â–†â–‡â–ƒâ–†â–ˆâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–â–‚â–…â–‡â–…â–‡â–‡â–„â–ƒâ–„â–…â–…â–†â–„â–†â–†â–†â–„â–†â–‡â–†â–…â–‡â–…â–‡â–…â–…â–†â–…â–†â–‡â–…â–ˆâ–„â–†â–‡â–ƒâ–†â–ˆâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–ˆâ–†â–…â–ƒâ–…â–„â–ƒâ–…â–†â–„â–„â–†â–„â–‡â–„â–‚â–‚â–†â–„â–‚â–„â–ƒâ–ƒâ–…â–ƒâ–„â–†â–„â–…â–„â–‚â–…â–â–„â–ƒâ–‚â–‡â–ƒâ–â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–â–‚â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–â–‚â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–‡â–†â–†â–†â–†â–†â–…â–†â–†â–…â–…â–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–…â–…â–â–…â–…â–â–…â–…â–…â–ˆâ–…â–…â–…â–…â–…â–…â–ˆâ–â–…â–…â–â–â–…â–…â–…â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–â–„â–ˆâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–†â–†â–„â–†â–†â–„â–†â–†â–†â–ˆâ–†â–†â–†â–†â–†â–†â–ˆâ–„â–†â–†â–„â–„â–†â–†â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–ˆâ–…â–‡â–‡â–ˆâ–†â–…â–†â–…â–„â–„â–…â–„â–ƒâ–…â–ƒâ–‚â–„â–ƒâ–„â–ƒâ–ƒâ–…â–‚â–‚â–…â–ƒâ–ƒâ–â–ƒâ–‚â–‚â–…â–ƒâ–‚â–‚â–ƒâ–„â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3kuodfw5\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:48:52.815743: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm__binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/23jfnnf5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014851-23jfnnf5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 73/74 [00:01<00:00, 41.76it/s, loss=0.722, v_num=nnf5, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.702\n",
      "Epoch 0: 100%|â–ˆ| 74/74 [00:01<00:00, 41.08it/s, loss=0.722, v_num=nnf5, LTC_val_\n",
      "Epoch 1:  99%|â–‰| 73/74 [00:02<00:00, 33.42it/s, loss=0.706, v_num=nnf5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 74/74 [00:02<00:00, 33.30it/s, loss=0.706, v_num=nnf5, LTC_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.694\n",
      "Epoch 2:  99%|â–‰| 73/74 [00:02<00:00, 30.33it/s, loss=0.695, v_num=nnf5, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 74/74 [00:02<00:00, 30.32it/s, loss=0.695, v_num=nnf5, LTC_val_\u001b[A\n",
      "Epoch 3:  99%|â–‰| 73/74 [00:02<00:00, 31.25it/s, loss=0.714, v_num=nnf5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 74/74 [00:02<00:00, 31.08it/s, loss=0.714, v_num=nnf5, LTC_val_\u001b[A\n",
      "Epoch 4:  99%|â–‰| 73/74 [00:01<00:00, 40.90it/s, loss=0.699, v_num=nnf5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 74/74 [00:01<00:00, 40.39it/s, loss=0.699, v_num=nnf5, LTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 73/74 [00:01<00:00, 41.90it/s, loss=0.693, v_num=nnf5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 74/74 [00:01<00:00, 41.63it/s, loss=0.693, v_num=nnf5, LTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 73/74 [00:01<00:00, 38.73it/s, loss=0.705, v_num=nnf5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 74/74 [00:01<00:00, 38.49it/s, loss=0.705, v_num=nnf5, LTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 73/74 [00:02<00:00, 35.18it/s, loss=0.693, v_num=nnf5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 74/74 [00:02<00:00, 35.06it/s, loss=0.693, v_num=nnf5, LTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 73/74 [00:02<00:00, 27.78it/s, loss=0.697, v_num=nnf5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 74/74 [00:02<00:00, 27.56it/s, loss=0.697, v_num=nnf5, LTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 73/74 [00:02<00:00, 28.94it/s, loss=0.696, v_num=nnf5, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 74/74 [00:02<00:00, 28.86it/s, loss=0.696, v_num=nnf5, LTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 73/74 [00:01<00:00, 38.01it/s, loss=0.697, v_num=nnf5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 74/74 [00:01<00:00, 37.76it/s, loss=0.697, v_num=nnf5, LTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 73/74 [00:01<00:00, 41.52it/s, loss=0.694, v_num=nnf5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 74/74 [00:01<00:00, 41.05it/s, loss=0.694, v_num=nnf5, LTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 73/74 [00:01<00:00, 39.14it/s, loss=0.688, v_num=nnf5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 74/74 [00:01<00:00, 38.76it/s, loss=0.688, v_num=nnf5, LTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 73/74 [00:01<00:00, 38.47it/s, loss=0.694, v_num=nnf5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 74/74 [00:01<00:00, 38.15it/s, loss=0.694, v_num=nnf5, LTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 73/74 [00:02<00:00, 30.78it/s, loss=0.698, v_num=nnf5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 74/74 [00:02<00:00, 30.58it/s, loss=0.698, v_num=nnf5, LTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 73/74 [00:02<00:00, 27.86it/s, loss=0.695, v_num=nnf5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 74/74 [00:02<00:00, 27.27it/s, loss=0.695, v_num=nnf5, LTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 73/74 [00:01<00:00, 38.69it/s, loss=0.696, v_num=nnf5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 74/74 [00:01<00:00, 38.32it/s, loss=0.696, v_num=nnf5, LTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 73/74 [00:02<00:00, 34.38it/s, loss=0.698, v_num=nnf5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 74/74 [00:02<00:00, 34.18it/s, loss=0.698, v_num=nnf5, LTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 73/74 [00:01<00:00, 39.67it/s, loss=0.692, v_num=nnf5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 74/74 [00:01<00:00, 39.43it/s, loss=0.692, v_num=nnf5, LTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 73/74 [00:02<00:00, 35.30it/s, loss=0.691, v_num=nnf5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 74/74 [00:02<00:00, 35.11it/s, loss=0.691, v_num=nnf5, LTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 73/74 [00:02<00:00, 33.51it/s, loss=0.699, v_num=nnf5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:02<00:00, 33.24it/s, loss=0.699, v_num=nnf5, LTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 73/74 [00:02<00:00, 36.43it/s, loss=0.695, v_num=nnf5, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.694. Signaling Trainer to stop.\n",
      "Epoch 21: 100%|â–ˆ| 74/74 [00:02<00:00, 36.11it/s, loss=0.695, v_num=nnf5, LTC_val\n",
      "Epoch 21: 100%|â–ˆ| 74/74 [00:02<00:00, 36.06it/s, loss=0.695, v_num=nnf5, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 74.06it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.5,\n",
      " 'LTC_test_f1': 0.32692310214042664,\n",
      " 'test_loss': 0.6930055618286133}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 72428\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014851-23jfnnf5/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014851-23jfnnf5/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.73672\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 56\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464587\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 76\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.51123\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.4794\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69236\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.69448\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.32692\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.69301\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–‡â–‡â–‚â–‡â–†â–…â–‚â–†â–†â–ƒâ–†â–†â–…â–†â–‡â–ƒâ–†â–‡â–‡â–„â–â–â–„â–ƒâ–ˆâ–…â–…â–‡â–…â–…â–ˆâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–‡â–‡â–‚â–‡â–…â–„â–‚â–†â–†â–‚â–†â–ƒâ–…â–†â–‡â–ƒâ–†â–‡â–„â–„â–‚â–‚â–„â–ƒâ–ˆâ–…â–…â–‡â–…â–…â–ˆâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–â–‚â–†â–†â–†â–„â–ˆâ–„â–…â–…â–…â–„â–…â–…â–…â–…â–…â–„â–ƒâ–…â–†â–‡â–…â–†â–„â–†â–…â–ƒâ–…â–…â–…â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–‡â–‡â–‚â–„â–„â–†â–…â–†â–ƒâ–â–†â–ˆâ–‚â–‡â–†â–‚â–„â–ˆâ–…â–…â–ƒâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–†â–†â–â–‚â–†â–…â–…â–…â–â–ƒâ–…â–†â–ƒâ–ƒâ–‡â–â–‚â–ˆâ–…â–†â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–‡â–„â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–â–â–‚â–‚â–â–â–‚â–â–â–â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–â–‚â–‚â–ƒâ–„â–‚â–‚â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/23jfnnf5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:50:01.874256: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm__multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2dvmmjmf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015000-2dvmmjmf\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 72/73 [00:01<00:00, 43.30it/s, loss=1.05, v_num=mjmf, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.280\n",
      "Epoch 0: 100%|â–ˆ| 73/73 [00:01<00:00, 41.01it/s, loss=1.05, v_num=mjmf, LTC_val_a\n",
      "Epoch 1:  99%|â–‰| 72/73 [00:01<00:00, 42.64it/s, loss=1.09, v_num=mjmf, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.134 >= min_delta = 0.003. New best score: 1.146\n",
      "Epoch 1: 100%|â–ˆ| 73/73 [00:01<00:00, 42.35it/s, loss=1.09, v_num=mjmf, LTC_val_a\n",
      "Epoch 2:  99%|â–‰| 72/73 [00:01<00:00, 39.77it/s, loss=1.04, v_num=mjmf, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 73/73 [00:01<00:00, 39.42it/s, loss=1.04, v_num=mjmf, LTC_val_a\u001b[A\n",
      "Epoch 3:  99%|â–‰| 72/73 [00:01<00:00, 36.10it/s, loss=1.04, v_num=mjmf, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 73/73 [00:02<00:00, 35.99it/s, loss=1.04, v_num=mjmf, LTC_val_a\u001b[A\n",
      "Epoch 4:  99%|â–‰| 72/73 [00:02<00:00, 29.69it/s, loss=1.03, v_num=mjmf, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 73/73 [00:02<00:00, 29.56it/s, loss=1.03, v_num=mjmf, LTC_val_a\u001b[A\n",
      "Epoch 5:  99%|â–‰| 72/73 [00:02<00:00, 35.90it/s, loss=1.05, v_num=mjmf, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 1.134\n",
      "Epoch 5: 100%|â–ˆ| 73/73 [00:02<00:00, 35.61it/s, loss=1.05, v_num=mjmf, LTC_val_a\n",
      "Epoch 6:  99%|â–‰| 72/73 [00:01<00:00, 41.46it/s, loss=1.06, v_num=mjmf, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 73/73 [00:01<00:00, 41.03it/s, loss=1.06, v_num=mjmf, LTC_val_a\u001b[A\n",
      "Epoch 7:  99%|â–‰| 72/73 [00:02<00:00, 28.31it/s, loss=1.03, v_num=mjmf, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 73/73 [00:02<00:00, 28.22it/s, loss=1.03, v_num=mjmf, LTC_val_a\u001b[A\n",
      "Epoch 8:  99%|â–‰| 72/73 [00:02<00:00, 34.80it/s, loss=1.05, v_num=mjmf, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 73/73 [00:02<00:00, 34.62it/s, loss=1.05, v_num=mjmf, LTC_val_a\u001b[A\n",
      "Epoch 9:  99%|â–‰| 72/73 [00:01<00:00, 37.76it/s, loss=1.05, v_num=mjmf, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 73/73 [00:01<00:00, 37.37it/s, loss=1.05, v_num=mjmf, LTC_val_a\u001b[A\n",
      "                                                                                Metric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 1.128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  99%|â–‰| 72/73 [00:01<00:00, 39.14it/s, loss=1.04, v_num=mjmf, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 73/73 [00:01<00:00, 38.72it/s, loss=1.04, v_num=mjmf, LTC_val_\u001b[A\n",
      "Epoch 11:  99%|â–‰| 72/73 [00:01<00:00, 37.51it/s, loss=1.01, v_num=mjmf, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 73/73 [00:01<00:00, 37.16it/s, loss=1.01, v_num=mjmf, LTC_val_\u001b[A\n",
      "Epoch 12:  99%|â–‰| 72/73 [00:02<00:00, 33.25it/s, loss=1, v_num=mjmf, LTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 73/73 [00:02<00:00, 33.11it/s, loss=1, v_num=mjmf, LTC_val_acc\u001b[A\n",
      "Epoch 13:  99%|â–‰| 72/73 [00:01<00:00, 42.05it/s, loss=1.02, v_num=mjmf, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 73/73 [00:01<00:00, 41.47it/s, loss=1.02, v_num=mjmf, LTC_val_\u001b[A\n",
      "Epoch 14:  99%|â–‰| 72/73 [00:01<00:00, 42.34it/s, loss=1.04, v_num=mjmf, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 73/73 [00:01<00:00, 41.88it/s, loss=1.04, v_num=mjmf, LTC_val_\u001b[A\n",
      "Epoch 15:  99%|â–‰| 72/73 [00:02<00:00, 35.92it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 73/73 [00:02<00:00, 33.30it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n",
      "Epoch 16:  99%|â–‰| 72/73 [00:02<00:00, 27.20it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 73/73 [00:02<00:00, 27.19it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n",
      "Epoch 17:  99%|â–‰| 72/73 [00:02<00:00, 30.56it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 73/73 [00:02<00:00, 30.52it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n",
      "Epoch 18:  99%|â–‰| 72/73 [00:02<00:00, 25.85it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 73/73 [00:02<00:00, 25.80it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n",
      "Epoch 19:  99%|â–‰| 72/73 [00:02<00:00, 29.54it/s, loss=1.05, v_num=mjmf, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 73/73 [00:02<00:00, 29.44it/s, loss=1.05, v_num=mjmf, LTC_val_\u001b[A\n",
      "Epoch 20:  99%|â–‰| 72/73 [00:01<00:00, 38.50it/s, loss=1.04, v_num=mjmf, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 73/73 [00:01<00:00, 38.12it/s, loss=1.04, v_num=mjmf, LTC_val_\u001b[A\n",
      "Epoch 21:  99%|â–‰| 72/73 [00:01<00:00, 41.36it/s, loss=1.06, v_num=mjmf, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 73/73 [00:01<00:00, 40.95it/s, loss=1.06, v_num=mjmf, LTC_val_\u001b[A\n",
      "Epoch 22:  99%|â–‰| 72/73 [00:01<00:00, 41.08it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 73/73 [00:01<00:00, 40.71it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n",
      "Epoch 23:  99%|â–‰| 72/73 [00:02<00:00, 31.78it/s, loss=1.01, v_num=mjmf, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 73/73 [00:02<00:00, 31.59it/s, loss=1.01, v_num=mjmf, LTC_val_\u001b[A\n",
      "Epoch 24:  99%|â–‰| 72/73 [00:01<00:00, 37.88it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 73/73 [00:01<00:00, 37.55it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n",
      "Epoch 25:  99%|â–‰| 72/73 [00:02<00:00, 30.06it/s, loss=1.05, v_num=mjmf, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 73/73 [00:02<00:00, 30.04it/s, loss=1.05, v_num=mjmf, LTC_val_\u001b[A\n",
      "Epoch 26:  99%|â–‰| 72/73 [00:02<00:00, 30.39it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 73/73 [00:02<00:00, 30.34it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n",
      "Epoch 27:  99%|â–‰| 72/73 [00:01<00:00, 39.53it/s, loss=1.02, v_num=mjmf, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 73/73 [00:01<00:00, 39.30it/s, loss=1.02, v_num=mjmf, LTC_val_\u001b[A\n",
      "Epoch 28:  99%|â–‰| 72/73 [00:02<00:00, 30.15it/s, loss=1.05, v_num=mjmf, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 73/73 [00:02<00:00, 30.12it/s, loss=1.05, v_num=mjmf, LTC_val_\u001b[A\n",
      "Epoch 29:  99%|â–‰| 72/73 [00:02<00:00, 28.63it/s, loss=1.04, v_num=mjmf, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.128. Signaling Trainer to stop.\n",
      "Epoch 29: 100%|â–ˆ| 73/73 [00:02<00:00, 28.52it/s, loss=1.04, v_num=mjmf, LTC_val_\n",
      "Epoch 29: 100%|â–ˆ| 73/73 [00:02<00:00, 28.48it/s, loss=1.04, v_num=mjmf, LTC_val_\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 71.17it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.4642857015132904,\n",
      " 'LTC_test_f1': 0.21118013560771942,\n",
      " 'test_loss': 1.0434070825576782}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 72619\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015000-2dvmmjmf/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015000-2dvmmjmf/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.5625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.90547\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 73\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464673\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.48872\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.21657\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.03112\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.18182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.14876\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.46429\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.21118\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.04341\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–‚â–„â–ƒâ–‚â–„â–ƒâ–…â–‚â–„â–‚â–„â–ƒâ–…â–â–„â–…â–„â–…â–…â–‚â–…â–ƒâ–ˆâ–‚â–‚â–„â–…â–„â–ƒâ–„â–ƒâ–…â–…â–†â–‡â–…â–‚â–ƒâ–‚â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–ƒâ–…â–‡â–‚â–…â–†â–†â–ƒâ–…â–‚â–…â–„â–…â–â–…â–…â–…â–†â–…â–ƒâ–†â–„â–ˆâ–ƒâ–ƒâ–…â–…â–…â–„â–…â–„â–…â–†â–†â–‡â–†â–ƒâ–„â–‚â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–†â–‚â–…â–†â–„â–†â–†â–…â–…â–†â–†â–…â–„â–ˆâ–„â–…â–…â–‚â–„â–…â–…â–…â–â–‡â–†â–…â–†â–†â–†â–†â–…â–…â–…â–…â–ƒâ–„â–†â–…â–†â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–â–†â–„â–†â–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–ˆâ–‡â–…â–„â–…â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–„â–ƒâ–„â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–â–‚â–â–‚â–â–â–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–‚â–ƒâ–„â–‚â–â–â–‚â–‚â–â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2dvmmjmf\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:51:27.832319: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm__multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2oujlg93\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015126-2oujlg93\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 73/74 [00:01<00:00, 41.87it/s, loss=1.11, v_num=lg93, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.035\n",
      "Epoch 0: 100%|â–ˆ| 74/74 [00:01<00:00, 41.35it/s, loss=1.11, v_num=lg93, LTC_val_a\n",
      "Epoch 1:  99%|â–‰| 73/74 [00:01<00:00, 41.27it/s, loss=1.13, v_num=lg93, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 74/74 [00:01<00:00, 40.99it/s, loss=1.13, v_num=lg93, LTC_val_a\u001b[A\n",
      "Epoch 2:  99%|â–‰| 73/74 [00:01<00:00, 41.52it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 74/74 [00:01<00:00, 41.24it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n",
      "Epoch 3:  99%|â–‰| 73/74 [00:01<00:00, 36.97it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 74/74 [00:02<00:00, 36.79it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n",
      "Epoch 4:  99%|â–‰| 73/74 [00:01<00:00, 41.23it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 74/74 [00:01<00:00, 40.95it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n",
      "Epoch 5:  99%|â–‰| 73/74 [00:02<00:00, 31.78it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 74/74 [00:02<00:00, 31.73it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n",
      "Epoch 6:  99%|â–‰| 73/74 [00:01<00:00, 40.87it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 74/74 [00:01<00:00, 40.62it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n",
      "Epoch 7:  99%|â–‰| 73/74 [00:01<00:00, 40.92it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 74/74 [00:01<00:00, 40.61it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n",
      "Epoch 8:  99%|â–‰| 73/74 [00:01<00:00, 42.08it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 74/74 [00:01<00:00, 41.76it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n",
      "Epoch 9:  99%|â–‰| 73/74 [00:01<00:00, 40.68it/s, loss=1.11, v_num=lg93, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 74/74 [00:01<00:00, 40.42it/s, loss=1.11, v_num=lg93, LTC_val_a\u001b[A\n",
      "Epoch 10:  99%|â–‰| 73/74 [00:01<00:00, 41.92it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 74/74 [00:01<00:00, 41.43it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n",
      "Epoch 11:  99%|â–‰| 73/74 [00:01<00:00, 39.51it/s, loss=1.12, v_num=lg93, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 74/74 [00:01<00:00, 39.12it/s, loss=1.12, v_num=lg93, LTC_val_\u001b[A\n",
      "Epoch 12:  99%|â–‰| 73/74 [00:02<00:00, 30.43it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 74/74 [00:02<00:00, 30.34it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n",
      "Epoch 13:  99%|â–‰| 73/74 [00:01<00:00, 39.72it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 74/74 [00:01<00:00, 39.49it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n",
      "Epoch 14:  99%|â–‰| 73/74 [00:01<00:00, 40.95it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 74/74 [00:01<00:00, 40.53it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n",
      "Epoch 15:  99%|â–‰| 73/74 [00:01<00:00, 40.55it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 74/74 [00:01<00:00, 38.96it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n",
      "Epoch 16:  99%|â–‰| 73/74 [00:01<00:00, 37.77it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 74/74 [00:01<00:00, 37.41it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n",
      "Epoch 17:  99%|â–‰| 73/74 [00:01<00:00, 40.89it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 74/74 [00:01<00:00, 40.49it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n",
      "Epoch 18:  99%|â–‰| 73/74 [00:02<00:00, 35.70it/s, loss=1.11, v_num=lg93, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 74/74 [00:02<00:00, 35.45it/s, loss=1.11, v_num=lg93, LTC_val_\u001b[A\n",
      "Epoch 19:  99%|â–‰| 73/74 [00:01<00:00, 40.34it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 74/74 [00:01<00:00, 39.98it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n",
      "Epoch 20:  99%|â–‰| 73/74 [00:01<00:00, 40.18it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.035. Signaling Trainer to stop.\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:01<00:00, 39.76it/s, loss=1.1, v_num=lg93, LTC_val_a\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:01<00:00, 39.69it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 72.61it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.25,\n",
      " 'LTC_test_f1': 0.13151928782463074,\n",
      " 'test_loss': 1.1257113218307495}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 72830\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015126-2oujlg93/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015126-2oujlg93/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.58053\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.0785\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 49\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464735\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.33074\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.27981\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.10022\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.07407\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.11085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.13152\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.12571\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–„â–…â–…â–…â–…â–…â–„â–„â–…â–…â–‚â–„â–„â–„â–„â–†â–ˆâ–„â–„â–â–„â–…â–„â–ƒâ–„â–„â–…â–…â–‚â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–ƒâ–…â–ƒâ–…â–…â–…â–ƒâ–„â–…â–…â–‚â–„â–„â–ƒâ–…â–†â–ˆâ–ƒâ–„â–â–„â–„â–„â–ƒâ–ƒâ–„â–…â–†â–ƒâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–ƒâ–â–ƒâ–…â–„â–ƒâ–…â–‚â–…â–‚â–‡â–ˆâ–…â–„â–…â–‚â–â–„â–†â–‡â–„â–ƒâ–ƒâ–…â–†â–…â–‚â–„â–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–‡â–†â–†â–ƒâ–ƒâ–‚â–…â–ƒâ–ˆâ–‚â–„â–ƒâ–„â–„â–…â–ƒâ–â–†â–…â–â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–…â–†â–†â–„â–ƒâ–…â–†â–„â–ˆâ–…â–†â–ƒâ–„â–…â–…â–‚â–…â–â–†â–‚â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–†â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–ˆâ–ˆâ–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–‡â–‡â–‚â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–â–†â–ˆâ–‡â–†â–†â–‡â–†â–†â–‡â–‡â–‡â–†â–‡â–‡â–†â–†â–†â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2oujlg93\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:52:30.980778: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_single_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/yh1l3les\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015229-yh1l3les\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 130   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.7 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 79/80 [00:01<00:00, 42.00it/s, loss=0.589, v_num=3les, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.553\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:01<00:00, 41.15it/s, loss=0.589, v_num=3les, BTC_val_\n",
      "Epoch 1:  99%|â–‰| 79/80 [00:01<00:00, 43.65it/s, loss=0.646, v_num=3les, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 42.87it/s, loss=0.646, v_num=3les, BTC_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.546\n",
      "Epoch 2:  99%|â–‰| 79/80 [00:01<00:00, 43.90it/s, loss=0.605, v_num=3les, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 42.87it/s, loss=0.605, v_num=3les, BTC_val_\u001b[A\n",
      "Epoch 3:  99%|â–‰| 79/80 [00:01<00:00, 42.12it/s, loss=0.55, v_num=3les, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.023 >= min_delta = 0.003. New best score: 0.523\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:01<00:00, 41.14it/s, loss=0.55, v_num=3les, BTC_val_a\n",
      "Epoch 4:  99%|â–‰| 79/80 [00:01<00:00, 41.32it/s, loss=0.573, v_num=3les, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 40.44it/s, loss=0.573, v_num=3les, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 79/80 [00:01<00:00, 44.53it/s, loss=0.581, v_num=3les, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:01<00:00, 43.46it/s, loss=0.581, v_num=3les, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 79/80 [00:02<00:00, 39.03it/s, loss=0.56, v_num=3les, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.506\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:02<00:00, 37.38it/s, loss=0.56, v_num=3les, BTC_val_a\n",
      "Epoch 7:  99%|â–‰| 79/80 [00:02<00:00, 38.93it/s, loss=0.579, v_num=3les, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:02<00:00, 38.12it/s, loss=0.579, v_num=3les, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 79/80 [00:01<00:00, 43.43it/s, loss=0.592, v_num=3les, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:01<00:00, 42.54it/s, loss=0.592, v_num=3les, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 79/80 [00:01<00:00, 44.00it/s, loss=0.562, v_num=3les, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:01<00:00, 42.94it/s, loss=0.562, v_num=3les, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 79/80 [00:02<00:00, 35.52it/s, loss=0.581, v_num=3les, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|â–ˆ| 80/80 [00:02<00:00, 35.06it/s, loss=0.581, v_num=3les, BTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 79/80 [00:01<00:00, 43.26it/s, loss=0.582, v_num=3les, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:01<00:00, 42.42it/s, loss=0.582, v_num=3les, BTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 79/80 [00:01<00:00, 41.87it/s, loss=0.559, v_num=3les, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:01<00:00, 41.14it/s, loss=0.559, v_num=3les, BTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 79/80 [00:02<00:00, 37.40it/s, loss=0.605, v_num=3les, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:02<00:00, 36.89it/s, loss=0.605, v_num=3les, BTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 79/80 [00:01<00:00, 45.44it/s, loss=0.603, v_num=3les, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:01<00:00, 44.59it/s, loss=0.603, v_num=3les, BTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 79/80 [00:01<00:00, 45.39it/s, loss=0.53, v_num=3les, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:01<00:00, 44.22it/s, loss=0.53, v_num=3les, BTC_val_\u001b[A\n",
      "Epoch 16:  99%|â–‰| 79/80 [00:01<00:00, 42.51it/s, loss=0.574, v_num=3les, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:01<00:00, 41.48it/s, loss=0.574, v_num=3les, BTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 79/80 [00:02<00:00, 37.58it/s, loss=0.59, v_num=3les, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:02<00:00, 36.88it/s, loss=0.59, v_num=3les, BTC_val_\u001b[A\n",
      "Epoch 18:  99%|â–‰| 79/80 [00:02<00:00, 36.27it/s, loss=0.57, v_num=3les, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:02<00:00, 35.80it/s, loss=0.57, v_num=3les, BTC_val_\u001b[A\n",
      "Epoch 19:  99%|â–‰| 79/80 [00:01<00:00, 44.23it/s, loss=0.533, v_num=3les, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:01<00:00, 43.36it/s, loss=0.533, v_num=3les, BTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 79/80 [00:02<00:00, 36.52it/s, loss=0.593, v_num=3les, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:02<00:00, 36.02it/s, loss=0.593, v_num=3les, BTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 79/80 [00:02<00:00, 36.00it/s, loss=0.581, v_num=3les, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:02<00:00, 35.30it/s, loss=0.581, v_num=3les, BTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 79/80 [00:02<00:00, 36.85it/s, loss=0.596, v_num=3les, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:02<00:00, 36.31it/s, loss=0.596, v_num=3les, BTC_val\u001b[A\n",
      "Epoch 23:  99%|â–‰| 79/80 [00:01<00:00, 43.85it/s, loss=0.567, v_num=3les, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:01<00:00, 43.07it/s, loss=0.567, v_num=3les, BTC_val\u001b[A\n",
      "Epoch 24:  99%|â–‰| 79/80 [00:01<00:00, 40.51it/s, loss=0.559, v_num=3les, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:02<00:00, 39.89it/s, loss=0.559, v_num=3les, BTC_val\u001b[A\n",
      "Epoch 25:  99%|â–‰| 79/80 [00:02<00:00, 37.66it/s, loss=0.606, v_num=3les, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:02<00:00, 37.15it/s, loss=0.606, v_num=3les, BTC_val\u001b[A\n",
      "Epoch 26:  99%|â–‰| 79/80 [00:02<00:00, 37.29it/s, loss=0.586, v_num=3les, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.506. Signaling Trainer to stop.\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:02<00:00, 36.61it/s, loss=0.586, v_num=3les, BTC_val\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:02<00:00, 36.52it/s, loss=0.586, v_num=3les, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 60.19it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6774193644523621,\n",
      " 'BTC_test_f1': 0.6673067212104797,\n",
      " 'ETH_test_acc': 0.7096773982048035,\n",
      " 'ETH_test_f1': 0.7085039019584656,\n",
      " 'test_loss': 0.5560200214385986}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 73060\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015229-yh1l3les/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015229-yh1l3les/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.54656\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.61175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2133\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 63\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.72842\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.7141\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.71101\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.69858\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.56446\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.64935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.54846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.67742\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.66731\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.70968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.7085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.55602\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–‚â–„â–ƒâ–„â–…â–…â–„â–„â–‡â–…â–‡â–„â–„â–‚â–‡â–„â–„â–ˆâ–ƒâ–ƒâ–â–†â–†â–ƒâ–†â–„â–†â–‚â–…â–‚â–‚â–ˆâ–…â–„â–†â–ˆâ–†â–†â–‚â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–‚â–„â–ƒâ–„â–…â–…â–„â–„â–‡â–…â–‡â–„â–„â–‚â–‡â–‚â–„â–ˆâ–‚â–ƒâ–â–†â–†â–ƒâ–†â–„â–†â–‚â–…â–‚â–‚â–ˆâ–…â–„â–†â–ˆâ–†â–†â–‚â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–ˆâ–†â–„â–†â–â–…â–…â–…â–…â–…â–‡â–…â–†â–…â–„â–…â–…â–…â–â–†â–„â–†â–‡â–„â–„â–„â–…â–„â–…â–„â–…â–…â–†â–†â–…â–†â–†â–‡â–…â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–ˆâ–†â–…â–†â–‚â–…â–…â–†â–„â–…â–‡â–†â–†â–…â–„â–…â–†â–„â–â–†â–„â–†â–‡â–…â–„â–…â–†â–„â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–…â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–…â–†â–†â–†â–…â–„â–„â–„â–„â–…â–â–ƒâ–ƒâ–ˆâ–„â–„â–„â–ƒâ–ˆâ–„â–‡â–‚â–ƒâ–†â–…â–„â–„â–†â–„â–…â–†â–ƒâ–„â–ƒâ–„â–ƒâ–„â–ƒâ–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–…â–‡â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–â–…â–†â–„â–…â–†â–†â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–…â–†â–…â–†â–†â–†â–‡â–†â–‡â–†â–‡â–†â–‡â–ˆâ–ˆâ–†â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–â–…â–‡â–†â–†â–†â–†â–‡â–†â–‡â–†â–‡â–†â–‡â–ˆâ–ˆâ–†â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–ˆâ–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–ˆâ–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–â–â–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–â–â–â–ƒâ–ˆâ–†â–â–â–â–ƒâ–ƒâ–â–†â–ƒâ–â–ƒâ–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–â–â–„â–„â–â–„â–„â–„â–„â–â–‚â–â–„â–ˆâ–†â–â–â–â–„â–„â–â–†â–„â–â–„â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–‡â–‡â–„â–…â–ƒâ–â–…â–„â–„â–ƒâ–ƒâ–‚â–†â–„â–„â–„â–‡â–„â–‚â–…â–„â–…â–‡â–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_single_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/yh1l3les\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:53:42.577030: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_single_lstm_loss_weighted_binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/dwrvgzys\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015341-dwrvgzys\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 130   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.7 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 80/81 [00:01<00:00, 44.80it/s, loss=0.688, v_num=gzys, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.666\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:01<00:00, 43.45it/s, loss=0.688, v_num=gzys, BTC_val_\n",
      "Epoch 1:  99%|â–‰| 80/81 [00:01<00:00, 45.00it/s, loss=0.703, v_num=gzys, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:01<00:00, 43.77it/s, loss=0.703, v_num=gzys, BTC_val_\u001b[A\n",
      "Epoch 2:  99%|â–‰| 80/81 [00:01<00:00, 45.36it/s, loss=0.699, v_num=gzys, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:01<00:00, 43.93it/s, loss=0.699, v_num=gzys, BTC_val_\u001b[A\n",
      "Epoch 3:  99%|â–‰| 80/81 [00:01<00:00, 45.58it/s, loss=0.7, v_num=gzys, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:01<00:00, 44.45it/s, loss=0.7, v_num=gzys, BTC_val_ac\u001b[A\n",
      "Epoch 4:  99%|â–‰| 80/81 [00:01<00:00, 46.79it/s, loss=0.696, v_num=gzys, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:01<00:00, 45.51it/s, loss=0.696, v_num=gzys, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 80/81 [00:01<00:00, 45.54it/s, loss=0.695, v_num=gzys, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:01<00:00, 44.43it/s, loss=0.695, v_num=gzys, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 80/81 [00:01<00:00, 44.04it/s, loss=0.696, v_num=gzys, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:01<00:00, 43.10it/s, loss=0.696, v_num=gzys, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 80/81 [00:01<00:00, 45.08it/s, loss=0.702, v_num=gzys, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:01<00:00, 44.27it/s, loss=0.702, v_num=gzys, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 80/81 [00:01<00:00, 44.77it/s, loss=0.697, v_num=gzys, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:01<00:00, 43.93it/s, loss=0.697, v_num=gzys, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 80/81 [00:01<00:00, 44.40it/s, loss=0.692, v_num=gzys, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:01<00:00, 43.48it/s, loss=0.692, v_num=gzys, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 80/81 [00:01<00:00, 43.31it/s, loss=0.691, v_num=gzys, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:01<00:00, 42.44it/s, loss=0.691, v_num=gzys, BTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 80/81 [00:02<00:00, 38.15it/s, loss=0.694, v_num=gzys, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:02<00:00, 37.64it/s, loss=0.694, v_num=gzys, BTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 80/81 [00:01<00:00, 45.02it/s, loss=0.695, v_num=gzys, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:01<00:00, 44.20it/s, loss=0.695, v_num=gzys, BTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 80/81 [00:01<00:00, 41.81it/s, loss=0.695, v_num=gzys, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:01<00:00, 41.12it/s, loss=0.695, v_num=gzys, BTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 80/81 [00:01<00:00, 44.42it/s, loss=0.693, v_num=gzys, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:01<00:00, 43.55it/s, loss=0.693, v_num=gzys, BTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 80/81 [00:01<00:00, 44.91it/s, loss=0.694, v_num=gzys, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:01<00:00, 44.05it/s, loss=0.694, v_num=gzys, BTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 80/81 [00:02<00:00, 38.22it/s, loss=0.696, v_num=gzys, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:02<00:00, 37.70it/s, loss=0.696, v_num=gzys, BTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 80/81 [00:01<00:00, 44.84it/s, loss=0.694, v_num=gzys, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:01<00:00, 44.03it/s, loss=0.694, v_num=gzys, BTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 80/81 [00:01<00:00, 45.33it/s, loss=0.694, v_num=gzys, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:01<00:00, 43.81it/s, loss=0.694, v_num=gzys, BTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 80/81 [00:01<00:00, 41.88it/s, loss=0.694, v_num=gzys, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:01<00:00, 40.87it/s, loss=0.694, v_num=gzys, BTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 80/81 [00:02<00:00, 35.20it/s, loss=0.695, v_num=gzys, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.666. Signaling Trainer to stop.\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:02<00:00, 34.74it/s, loss=0.695, v_num=gzys, BTC_val\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:02<00:00, 34.69it/s, loss=0.695, v_num=gzys, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 78.67it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.4193548262119293,\n",
      " 'BTC_test_f1': 0.29533159732818604,\n",
      " 'ETH_test_acc': 0.6129032373428345,\n",
      " 'ETH_test_f1': 0.3793548345565796,\n",
      " 'test_loss': 0.6961109638214111}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 73286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015341-dwrvgzys/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015341-dwrvgzys/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.4375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.37662\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.4375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.43529\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.69785\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1680\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 49\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464870\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.52955\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.47844\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.50512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.45697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69439\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.35714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.88889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.47059\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.67389\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.41935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.29533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.37935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.69611\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–‚â–…â–ƒâ–…â–„â–†â–…â–ˆâ–…â–„â–…â–†â–‡â–‡â–ˆâ–†â–ƒâ–â–…â–…â–‚â–ƒâ–…â–ƒâ–…â–…â–‚â–„â–„â–ƒâ–†â–†â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–‚â–…â–ƒâ–…â–ƒâ–†â–…â–ˆâ–…â–„â–…â–†â–†â–†â–ˆâ–†â–‚â–â–ƒâ–„â–‚â–ƒâ–ƒâ–‚â–…â–„â–‚â–„â–„â–ƒâ–†â–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–†â–…â–†â–†â–†â–…â–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–…â–†â–…â–ƒâ–ƒâ–ƒâ–…â–‚â–„â–ƒâ–ƒâ–ƒâ–…â–â–â–ˆâ–‚â–„â–†â–…â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–†â–…â–‡â–ƒâ–‡â–…â–ƒâ–‚â–„â–„â–„â–‚â–…â–…â–…â–‚â–ƒâ–ƒâ–ƒâ–‚â–„â–ƒâ–ƒâ–‚â–…â–â–‚â–ˆâ–‚â–„â–†â–ƒâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–„â–‡â–…â–â–ƒâ–ƒâ–…â–‚â–†â–†â–„â–„â–ƒâ–‚â–‚â–„â–‡â–ˆâ–„â–…â–…â–…â–„â–„â–ƒâ–…â–†â–„â–…â–„â–ƒâ–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–…â–ƒâ–â–ˆâ–„â–†â–…â–‚â–„â–†â–ˆâ–‚â–‡â–ƒâ–ˆâ–â–‚â–†â–…â–„â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–‚â–„â–ƒâ–ˆâ–ƒâ–ˆâ–‡â–ƒâ–…â–†â–‚â–â–‡â–ƒâ–†â–‚â–‚â–‡â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–‡â–…â–„â–…â–â–†â–…â–„â–…â–„â–…â–…â–…â–‚â–„â–‡â–â–ˆâ–…â–‡â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–„â–‡â–†â–‡â–ƒâ–‡â–‡â–„â–…â–…â–â–ƒâ–†â–‚â–„â–†â–ƒâ–ˆâ–†â–‡â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–â–â–â–‚â–â–â–‚â–â–â–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–…â–ˆâ–…â–â–â–â–ˆâ–…â–ˆâ–…â–ˆâ–…â–ˆâ–ˆâ–ˆâ–…â–…â–…â–…â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–‚â–ˆâ–‚â–â–â–â–ˆâ–‚â–ˆâ–‚â–ˆâ–‚â–ˆâ–ˆâ–ˆâ–‚â–‚â–‚â–‚â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–ˆâ–ˆâ–ˆâ–â–â–â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–ˆâ–ˆâ–ˆâ–â–â–â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–â–‚â–‚â–ˆâ–ˆâ–„â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_single_lstm_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/dwrvgzys\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:54:40.438672: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_single_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/c6dpd4yb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015438-c6dpd4yb\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 195   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.8 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/80 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 79/80 [00:01<00:00, 43.22it/s, loss=1.11, v_num=d4yb, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:01<00:00, 41.95it/s, loss=1.11, v_num=d4yb, BTC_val_aMetric val_loss improved. New best score: 1.086\n",
      "\n",
      "Epoch 1:  99%|â–‰| 79/80 [00:01<00:00, 44.06it/s, loss=1.08, v_num=d4yb, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 43.06it/s, loss=1.08, v_num=d4yb, BTC_val_a\u001b[A\n",
      "Epoch 2:  99%|â–‰| 79/80 [00:01<00:00, 45.87it/s, loss=1.06, v_num=d4yb, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.024 >= min_delta = 0.003. New best score: 1.062\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 44.67it/s, loss=1.06, v_num=d4yb, BTC_val_a\n",
      "Epoch 3:  99%|â–‰| 79/80 [00:01<00:00, 40.04it/s, loss=0.999, v_num=d4yb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:02<00:00, 39.11it/s, loss=0.999, v_num=d4yb, BTC_val_\u001b[A\n",
      "Epoch 4:  99%|â–‰| 79/80 [00:01<00:00, 42.70it/s, loss=0.994, v_num=d4yb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 40.83it/s, loss=0.994, v_num=d4yb, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 79/80 [00:01<00:00, 44.96it/s, loss=0.999, v_num=d4yb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:01<00:00, 43.83it/s, loss=0.999, v_num=d4yb, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 79/80 [00:01<00:00, 45.32it/s, loss=0.992, v_num=d4yb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 1.058\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:01<00:00, 44.18it/s, loss=0.992, v_num=d4yb, BTC_val_\n",
      "Epoch 7:  99%|â–‰| 79/80 [00:01<00:00, 40.15it/s, loss=0.991, v_num=d4yb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆ| 80/80 [00:02<00:00, 39.19it/s, loss=0.991, v_num=d4yb, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 79/80 [00:02<00:00, 38.51it/s, loss=0.955, v_num=d4yb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:02<00:00, 37.76it/s, loss=0.955, v_num=d4yb, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 79/80 [00:01<00:00, 43.12it/s, loss=0.979, v_num=d4yb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:01<00:00, 42.05it/s, loss=0.979, v_num=d4yb, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 79/80 [00:02<00:00, 39.35it/s, loss=0.939, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:02<00:00, 38.54it/s, loss=0.939, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 79/80 [00:01<00:00, 45.72it/s, loss=1.01, v_num=d4yb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:01<00:00, 44.57it/s, loss=1.01, v_num=d4yb, BTC_val_\u001b[A\n",
      "Epoch 12:  99%|â–‰| 79/80 [00:01<00:00, 42.44it/s, loss=0.949, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:01<00:00, 41.48it/s, loss=0.949, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 79/80 [00:02<00:00, 35.08it/s, loss=0.969, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:02<00:00, 34.42it/s, loss=0.969, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 79/80 [00:01<00:00, 45.59it/s, loss=0.95, v_num=d4yb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:01<00:00, 44.50it/s, loss=0.95, v_num=d4yb, BTC_val_\u001b[A\n",
      "Epoch 15:  99%|â–‰| 79/80 [00:02<00:00, 37.58it/s, loss=0.93, v_num=d4yb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:02<00:00, 36.80it/s, loss=0.93, v_num=d4yb, BTC_val_\u001b[A\n",
      "Epoch 16:  99%|â–‰| 79/80 [00:01<00:00, 45.53it/s, loss=1.01, v_num=d4yb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:01<00:00, 44.31it/s, loss=1.01, v_num=d4yb, BTC_val_\u001b[A\n",
      "Epoch 17:  99%|â–‰| 79/80 [00:01<00:00, 44.77it/s, loss=0.964, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:01<00:00, 43.75it/s, loss=0.964, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 79/80 [00:02<00:00, 35.99it/s, loss=0.968, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:02<00:00, 35.34it/s, loss=0.968, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 79/80 [00:01<00:00, 40.07it/s, loss=0.934, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:02<00:00, 39.17it/s, loss=0.934, v_num=d4yb, BTC_val\u001b[A\n",
      "                                                                                Metric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 1.046\n",
      "Epoch 20:  99%|â–‰| 79/80 [00:02<00:00, 39.13it/s, loss=0.984, v_num=d4yb, BTC_val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:02<00:00, 38.21it/s, loss=0.984, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 79/80 [00:02<00:00, 36.89it/s, loss=0.918, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:02<00:00, 36.25it/s, loss=0.918, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 79/80 [00:02<00:00, 37.17it/s, loss=0.93, v_num=d4yb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:02<00:00, 36.47it/s, loss=0.93, v_num=d4yb, BTC_val_\u001b[A\n",
      "Epoch 23:  99%|â–‰| 79/80 [00:01<00:00, 44.19it/s, loss=0.922, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:01<00:00, 43.16it/s, loss=0.922, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 24:  99%|â–‰| 79/80 [00:02<00:00, 34.44it/s, loss=0.938, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:02<00:00, 34.03it/s, loss=0.938, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 25:  99%|â–‰| 79/80 [00:01<00:00, 42.69it/s, loss=0.952, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:01<00:00, 41.75it/s, loss=0.952, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 26:  99%|â–‰| 79/80 [00:02<00:00, 35.10it/s, loss=0.902, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:02<00:00, 34.69it/s, loss=0.902, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 27:  99%|â–‰| 79/80 [00:02<00:00, 36.44it/s, loss=0.926, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 80/80 [00:02<00:00, 35.96it/s, loss=0.926, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 28:  99%|â–‰| 79/80 [00:02<00:00, 34.66it/s, loss=0.908, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:02<00:00, 34.25it/s, loss=0.908, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 29:  99%|â–‰| 79/80 [00:02<00:00, 38.19it/s, loss=0.946, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:02<00:00, 37.58it/s, loss=0.946, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 30:  99%|â–‰| 79/80 [00:02<00:00, 37.68it/s, loss=0.927, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 80/80 [00:02<00:00, 36.94it/s, loss=0.927, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 31:  99%|â–‰| 79/80 [00:01<00:00, 44.79it/s, loss=0.916, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 80/80 [00:01<00:00, 43.79it/s, loss=0.916, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 32:  99%|â–‰| 79/80 [00:02<00:00, 34.84it/s, loss=0.915, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 80/80 [00:02<00:00, 34.21it/s, loss=0.915, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 33:  99%|â–‰| 79/80 [00:02<00:00, 37.60it/s, loss=0.902, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 80/80 [00:02<00:00, 36.81it/s, loss=0.902, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 34:  99%|â–‰| 79/80 [00:01<00:00, 42.71it/s, loss=0.931, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 80/80 [00:01<00:00, 41.66it/s, loss=0.931, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 35:  99%|â–‰| 79/80 [00:01<00:00, 39.96it/s, loss=0.912, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 80/80 [00:02<00:00, 39.15it/s, loss=0.912, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 36:  99%|â–‰| 79/80 [00:02<00:00, 38.62it/s, loss=0.9, v_num=d4yb, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 80/80 [00:02<00:00, 37.87it/s, loss=0.9, v_num=d4yb, BTC_val_a\u001b[A\n",
      "Epoch 37:  99%|â–‰| 79/80 [00:02<00:00, 31.25it/s, loss=0.916, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|â–ˆ| 80/80 [00:02<00:00, 30.93it/s, loss=0.916, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 38:  99%|â–‰| 79/80 [00:02<00:00, 35.07it/s, loss=0.935, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|â–ˆ| 80/80 [00:02<00:00, 34.61it/s, loss=0.935, v_num=d4yb, BTC_val\u001b[A\n",
      "Epoch 39:  99%|â–‰| 79/80 [00:02<00:00, 37.99it/s, loss=0.919, v_num=d4yb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.046. Signaling Trainer to stop.\n",
      "Epoch 39: 100%|â–ˆ| 80/80 [00:02<00:00, 37.34it/s, loss=0.919, v_num=d4yb, BTC_val\n",
      "Epoch 39: 100%|â–ˆ| 80/80 [00:02<00:00, 37.20it/s, loss=0.919, v_num=d4yb, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 76.29it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.4516128897666931,\n",
      " 'BTC_test_f1': 0.40244993567466736,\n",
      " 'ETH_test_acc': 0.6451612710952759,\n",
      " 'ETH_test_f1': 0.6586883068084717,\n",
      " 'test_loss': 0.8453616499900818}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 73466\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015438-c6dpd4yb/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015438-c6dpd4yb/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.39044\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.50672\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.94113\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 39\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464970\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.4901\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.46269\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.49089\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.45836\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.92495\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.33968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.07729\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.45161\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.40245\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.64516\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.65869\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.84536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–†â–…â–†â–†â–„â–†â–ƒâ–ƒâ–…â–‡â–…â–†â–„â–‡â–„â–…â–†â–†â–ƒâ–„â–…â–„â–‡â–†â–…â–†â–ˆâ–†â–…â–„â–‡â–…â–†â–ˆâ–â–ˆâ–…â–†â–…â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–„â–…â–†â–†â–…â–†â–„â–â–…â–‡â–„â–†â–„â–†â–…â–…â–†â–†â–„â–ƒâ–…â–„â–‡â–„â–…â–†â–ˆâ–†â–…â–„â–†â–…â–‡â–ˆâ–‚â–ˆâ–…â–…â–…â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–„â–†â–‡â–ƒâ–‚â–„â–‚â–ƒâ–…â–†â–…â–„â–„â–‡â–‡â–ƒâ–†â–†â–‡â–†â–‡â–â–â–ƒâ–„â–ˆâ–‚â–‡â–†â–†â–‚â–†â–†â–„â–‚â–â–…â–ˆâ–‡â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–„â–†â–„â–ƒâ–‚â–„â–‚â–‚â–…â–†â–…â–„â–„â–‡â–†â–‚â–†â–…â–†â–…â–†â–â–â–„â–„â–ˆâ–‚â–‡â–…â–†â–ƒâ–†â–†â–„â–‚â–‚â–…â–ˆâ–‡â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–‡â–†â–†â–†â–†â–†â–‡â–†â–…â–†â–„â–…â–†â–â–„â–ƒâ–‚â–‚â–„â–„â–„â–†â–„â–…â–ƒâ–â–ˆâ–‚â–„â–„â–ƒâ–„â–†â–ƒâ–†â–ƒâ–ƒâ–ƒâ–„â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–„â–ƒâ–„â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–â–ƒâ–ƒâ–„â–†â–†â–†â–†â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–‚â–‚â–…â–…â–†â–†â–†â–†â–‡â–†â–†â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–†â–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–â–â–‚â–†â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–‡â–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–â–†â–â–†â–ƒâ–†â–ƒâ–ƒâ–ƒâ–ƒâ–†â–†â–†â–ƒâ–†â–ˆâ–ƒâ–â–†â–â–ƒâ–†â–ˆâ–ƒâ–â–ƒâ–ƒâ–â–ƒâ–†â–†â–â–ƒâ–â–ƒâ–â–ƒâ–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–â–†â–â–†â–„â–†â–„â–„â–„â–ƒâ–†â–†â–†â–„â–†â–ˆâ–„â–â–†â–‚â–„â–†â–ˆâ–„â–‚â–„â–„â–â–„â–†â–†â–â–„â–â–„â–â–„â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–†â–ƒâ–†â–†â–ƒâ–†â–ˆâ–ƒâ–ˆâ–â–†â–ˆâ–†â–ˆâ–ˆâ–ˆâ–†â–ˆâ–†â–ˆâ–†â–†â–†â–†â–†â–ƒâ–†â–ˆâ–ˆâ–†â–†â–†â–ƒâ–†â–ƒâ–†â–ƒâ–†â–†â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–†â–ƒâ–†â–†â–„â–†â–ˆâ–„â–ˆâ–â–†â–ˆâ–†â–ˆâ–ˆâ–ˆâ–†â–ˆâ–†â–ˆâ–†â–†â–†â–†â–†â–„â–†â–ˆâ–ˆâ–†â–†â–†â–„â–†â–„â–†â–„â–†â–†â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–„â–„â–‚â–‚â–„â–„â–‚â–‚â–ƒâ–…â–ƒâ–‚â–ƒâ–…â–‚â–…â–…â–ƒâ–†â–â–ƒâ–‚â–…â–„â–‚â–…â–‚â–„â–„â–†â–‡â–ˆâ–„â–„â–„â–‚â–„â–…â–ˆâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_single_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/c6dpd4yb\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:56:19.989003: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_single_lstm_loss_weighted_multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2edskyvr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015618-2edskyvr\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 195   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.8 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/81 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 80/81 [00:01<00:00, 42.70it/s, loss=1.12, v_num=kyvr, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.161\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:01<00:00, 41.41it/s, loss=1.12, v_num=kyvr, BTC_val_a\n",
      "Epoch 1:  99%|â–‰| 80/81 [00:01<00:00, 45.85it/s, loss=1.11, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.059 >= min_delta = 0.003. New best score: 1.102\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:01<00:00, 44.69it/s, loss=1.11, v_num=kyvr, BTC_val_a\n",
      "Epoch 2:  99%|â–‰| 80/81 [00:01<00:00, 40.21it/s, loss=1.1, v_num=kyvr, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:02<00:00, 39.09it/s, loss=1.1, v_num=kyvr, BTC_val_ac\u001b[A\n",
      "Epoch 3:  99%|â–‰| 80/81 [00:01<00:00, 43.91it/s, loss=1.09, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:01<00:00, 42.79it/s, loss=1.09, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Epoch 4:  99%|â–‰| 80/81 [00:01<00:00, 45.70it/s, loss=1.11, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:01<00:00, 44.47it/s, loss=1.11, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Epoch 5:  99%|â–‰| 80/81 [00:01<00:00, 43.78it/s, loss=1.1, v_num=kyvr, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:01<00:00, 42.71it/s, loss=1.1, v_num=kyvr, BTC_val_ac\u001b[A\n",
      "Epoch 6:  99%|â–‰| 80/81 [00:01<00:00, 42.88it/s, loss=1.1, v_num=kyvr, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:01<00:00, 41.91it/s, loss=1.1, v_num=kyvr, BTC_val_ac\u001b[A\n",
      "Epoch 7:  99%|â–‰| 80/81 [00:02<00:00, 37.75it/s, loss=1.1, v_num=kyvr, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:02<00:00, 37.05it/s, loss=1.1, v_num=kyvr, BTC_val_ac\u001b[A\n",
      "Epoch 8:  99%|â–‰| 80/81 [00:01<00:00, 43.73it/s, loss=1.1, v_num=kyvr, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:01<00:00, 42.73it/s, loss=1.1, v_num=kyvr, BTC_val_ac\u001b[A\n",
      "Epoch 9:  99%|â–‰| 80/81 [00:01<00:00, 44.45it/s, loss=1.11, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:01<00:00, 43.62it/s, loss=1.11, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Epoch 10:  99%|â–‰| 80/81 [00:01<00:00, 44.64it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:01<00:00, 43.84it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Epoch 11:  99%|â–‰| 80/81 [00:01<00:00, 44.52it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:01<00:00, 43.65it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Epoch 12:  99%|â–‰| 80/81 [00:02<00:00, 37.84it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:02<00:00, 37.32it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Epoch 13:  99%|â–‰| 80/81 [00:01<00:00, 43.99it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:01<00:00, 43.23it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Epoch 14:  99%|â–‰| 80/81 [00:01<00:00, 43.31it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:01<00:00, 42.56it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Epoch 15:  99%|â–‰| 80/81 [00:02<00:00, 37.49it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:02<00:00, 36.88it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Epoch 16:  99%|â–‰| 80/81 [00:02<00:00, 35.26it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:02<00:00, 34.67it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Epoch 17:  99%|â–‰| 80/81 [00:01<00:00, 42.14it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:01<00:00, 41.20it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Epoch 18:  99%|â–‰| 80/81 [00:01<00:00, 44.62it/s, loss=1.09, v_num=kyvr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:01<00:00, 43.64it/s, loss=1.09, v_num=kyvr, BTC_val_\u001b[A\n",
      "Epoch 19:  99%|â–‰| 80/81 [00:01<00:00, 44.05it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:01<00:00, 43.25it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n",
      "Epoch 20:  99%|â–‰| 80/81 [00:02<00:00, 34.87it/s, loss=1.11, v_num=kyvr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:02<00:00, 34.33it/s, loss=1.11, v_num=kyvr, BTC_val_\u001b[A\n",
      "Epoch 21:  99%|â–‰| 80/81 [00:02<00:00, 36.31it/s, loss=1.09, v_num=kyvr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.102. Signaling Trainer to stop.\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:02<00:00, 35.63it/s, loss=1.09, v_num=kyvr, BTC_val_\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:02<00:00, 35.57it/s, loss=1.09, v_num=kyvr, BTC_val_\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 57.06it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.25806450843811035,\n",
      " 'BTC_test_f1': 0.1367289274930954,\n",
      " 'ETH_test_acc': 0.4193548262119293,\n",
      " 'ETH_test_f1': 0.19648092985153198,\n",
      " 'test_loss': 1.1303094625473022}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 73739\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015618-2edskyvr/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015618-2edskyvr/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.4375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.46801\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.3125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.26573\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.07848\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1760\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621465031\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 79\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.36013\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.32367\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.34831\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.30734\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09644\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.14163\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.25806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.13673\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.41935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.13031\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–„â–…â–‡â–‚â–‡â–…â–…â–…â–‡â–…â–…â–„â–…â–‡â–‡â–ƒâ–‡â–„â–†â–†â–†â–„â–„â–ˆâ–„â–…â–…â–ˆâ–†â–‚â–†â–â–‡â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–„â–„â–‡â–‚â–†â–…â–„â–„â–…â–ƒâ–„â–ƒâ–„â–…â–†â–ƒâ–†â–‚â–…â–„â–…â–„â–ƒâ–ˆâ–„â–„â–ƒâ–ˆâ–„â–‚â–†â–â–†â–„â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–…â–…â–…â–ƒâ–…â–…â–…â–â–†â–‡â–ƒâ–…â–†â–…â–†â–â–ƒâ–…â–‡â–‚â–…â–ƒâ–„â–â–…â–ƒâ–…â–…â–†â–‡â–†â–ˆâ–†â–ˆâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–…â–„â–…â–ƒâ–…â–…â–…â–â–…â–…â–ƒâ–…â–†â–„â–…â–â–ƒâ–„â–†â–‚â–„â–ƒâ–ƒâ–â–…â–ƒâ–„â–…â–„â–‡â–†â–ˆâ–…â–‡â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–ƒâ–â–‚â–„â–ƒâ–„â–‚â–‡â–‚â–‚â–„â–ƒâ–ƒâ–‚â–ƒâ–ˆâ–ƒâ–„â–â–„â–â–ƒâ–‚â–„â–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–„â–…â–â–…â–†â–ƒâ–ˆâ–ƒâ–…â–„â–„â–…â–„â–…â–„â–†â–‡â–†â–…â–‡â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–„â–ˆâ–â–ˆâ–‡â–‚â–ˆâ–…â–‡â–‡â–‡â–‡â–ƒâ–…â–†â–†â–‡â–ˆâ–…â–†â–†â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–‚â–„â–â–‡â–†â–ˆâ–†â–ƒâ–„â–ƒâ–†â–ˆâ–†â–ƒâ–„â–ƒâ–ˆâ–…â–…â–ˆâ–„â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–‚â–„â–â–‡â–†â–ˆâ–ƒâ–…â–…â–„â–ˆâ–ˆâ–…â–ƒâ–…â–ƒâ–‡â–†â–„â–ƒâ–ƒâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–„â–„â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–†â–†â–†â–†â–†â–†â–†â–†â–†â–â–â–†â–â–â–ˆâ–†â–†â–â–†â–†â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–â–ƒâ–â–â–ˆâ–ƒâ–ƒâ–â–ƒâ–ƒâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–‡â–‡â–‡â–‡â–‡â–â–‡â–‚â–‡â–â–â–‡â–â–‚â–ˆâ–‡â–‡â–â–‡â–…â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–„â–„â–„â–„â–„â–â–„â–ƒâ–„â–â–â–„â–â–ƒâ–ˆâ–„â–„â–â–„â–ƒâ–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–…â–â–†â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–…â–ƒâ–…â–…â–ˆâ–‚â–„â–„â–„â–„â–ƒâ–ƒâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_single_lstm_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2edskyvr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 01:57:20.915505: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1prx6act\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015719-1prx6act\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 79/80 [00:03<00:00, 21.45it/s, loss=0.664, v_num=6act, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.680\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:03<00:00, 21.37it/s, loss=0.664, v_num=6act, BTC_val_\n",
      "Epoch 1:  99%|â–‰| 79/80 [00:04<00:00, 19.57it/s, loss=0.606, v_num=6act, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.153 >= min_delta = 0.003. New best score: 0.527\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:04<00:00, 19.49it/s, loss=0.606, v_num=6act, BTC_val_\n",
      "Epoch 2:  99%|â–‰| 79/80 [00:04<00:00, 17.75it/s, loss=0.607, v_num=6act, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:04<00:00, 17.71it/s, loss=0.607, v_num=6act, BTC_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.520\n",
      "Epoch 3:  99%|â–‰| 79/80 [00:04<00:00, 18.13it/s, loss=0.593, v_num=6act, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:04<00:00, 18.04it/s, loss=0.593, v_num=6act, BTC_val_\u001b[A\n",
      "Epoch 4:  99%|â–‰| 79/80 [00:03<00:00, 20.94it/s, loss=0.584, v_num=6act, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:03<00:00, 20.79it/s, loss=0.584, v_num=6act, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 79/80 [00:03<00:00, 20.13it/s, loss=0.608, v_num=6act, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.515\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:03<00:00, 20.00it/s, loss=0.608, v_num=6act, BTC_val_\n",
      "Epoch 6:  99%|â–‰| 79/80 [00:03<00:00, 20.41it/s, loss=0.602, v_num=6act, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:03<00:00, 20.27it/s, loss=0.602, v_num=6act, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 79/80 [00:04<00:00, 18.92it/s, loss=0.561, v_num=6act, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.508\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:04<00:00, 18.80it/s, loss=0.561, v_num=6act, BTC_val_\n",
      "Epoch 8:  99%|â–‰| 79/80 [00:04<00:00, 18.98it/s, loss=0.563, v_num=6act, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:04<00:00, 18.87it/s, loss=0.563, v_num=6act, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 79/80 [00:04<00:00, 18.87it/s, loss=0.585, v_num=6act, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:04<00:00, 18.74it/s, loss=0.585, v_num=6act, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 79/80 [00:05<00:00, 15.29it/s, loss=0.551, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:05<00:00, 15.26it/s, loss=0.551, v_num=6act, BTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 79/80 [00:04<00:00, 18.63it/s, loss=0.589, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:04<00:00, 18.53it/s, loss=0.589, v_num=6act, BTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 79/80 [00:03<00:00, 21.18it/s, loss=0.547, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:03<00:00, 21.02it/s, loss=0.547, v_num=6act, BTC_val\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 0.495\n",
      "Epoch 13:  99%|â–‰| 79/80 [00:04<00:00, 15.87it/s, loss=0.55, v_num=6act, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.030 >= min_delta = 0.003. New best score: 0.465\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:05<00:00, 15.84it/s, loss=0.55, v_num=6act, BTC_val_\n",
      "Epoch 14:  99%|â–‰| 79/80 [00:04<00:00, 18.84it/s, loss=0.574, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:04<00:00, 18.78it/s, loss=0.574, v_num=6act, BTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 79/80 [00:03<00:00, 21.02it/s, loss=0.564, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:03<00:00, 20.91it/s, loss=0.564, v_num=6act, BTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 79/80 [00:04<00:00, 19.40it/s, loss=0.616, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:04<00:00, 19.34it/s, loss=0.616, v_num=6act, BTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 79/80 [00:04<00:00, 18.02it/s, loss=0.587, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:04<00:00, 17.93it/s, loss=0.587, v_num=6act, BTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 79/80 [00:04<00:00, 16.17it/s, loss=0.565, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:04<00:00, 16.15it/s, loss=0.565, v_num=6act, BTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 79/80 [00:04<00:00, 17.01it/s, loss=0.543, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:04<00:00, 17.00it/s, loss=0.543, v_num=6act, BTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 79/80 [00:04<00:00, 18.63it/s, loss=0.55, v_num=6act, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:04<00:00, 18.52it/s, loss=0.55, v_num=6act, BTC_val_\u001b[A\n",
      "Epoch 21:  99%|â–‰| 79/80 [00:04<00:00, 17.39it/s, loss=0.56, v_num=6act, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:04<00:00, 17.33it/s, loss=0.56, v_num=6act, BTC_val_\u001b[A\n",
      "Epoch 22:  99%|â–‰| 79/80 [00:04<00:00, 18.09it/s, loss=0.537, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:04<00:00, 18.05it/s, loss=0.537, v_num=6act, BTC_val\u001b[A\n",
      "Epoch 23:  99%|â–‰| 79/80 [00:04<00:00, 17.99it/s, loss=0.574, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:04<00:00, 17.95it/s, loss=0.574, v_num=6act, BTC_val\u001b[A\n",
      "Epoch 24:  99%|â–‰| 79/80 [00:05<00:00, 15.10it/s, loss=0.561, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:05<00:00, 15.08it/s, loss=0.561, v_num=6act, BTC_val\u001b[A\n",
      "Epoch 25:  99%|â–‰| 79/80 [00:04<00:00, 17.62it/s, loss=0.563, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:04<00:00, 17.58it/s, loss=0.563, v_num=6act, BTC_val\u001b[A\n",
      "Epoch 26:  99%|â–‰| 79/80 [00:04<00:00, 18.11it/s, loss=0.549, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:04<00:00, 18.06it/s, loss=0.549, v_num=6act, BTC_val\u001b[A\n",
      "Epoch 27:  99%|â–‰| 79/80 [00:04<00:00, 17.02it/s, loss=0.549, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 80/80 [00:04<00:00, 16.97it/s, loss=0.549, v_num=6act, BTC_val\u001b[A\n",
      "Epoch 28:  99%|â–‰| 79/80 [00:04<00:00, 16.67it/s, loss=0.559, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:04<00:00, 16.64it/s, loss=0.559, v_num=6act, BTC_val\u001b[A\n",
      "Epoch 29:  99%|â–‰| 79/80 [00:04<00:00, 17.65it/s, loss=0.534, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:04<00:00, 17.60it/s, loss=0.534, v_num=6act, BTC_val\u001b[A\n",
      "Epoch 30:  99%|â–‰| 79/80 [00:04<00:00, 18.01it/s, loss=0.547, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 80/80 [00:04<00:00, 17.93it/s, loss=0.547, v_num=6act, BTC_val\u001b[A\n",
      "Epoch 31:  99%|â–‰| 79/80 [00:04<00:00, 16.83it/s, loss=0.551, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 80/80 [00:04<00:00, 16.81it/s, loss=0.551, v_num=6act, BTC_val\u001b[A\n",
      "Epoch 32:  99%|â–‰| 79/80 [00:04<00:00, 17.78it/s, loss=0.526, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 80/80 [00:04<00:00, 17.67it/s, loss=0.526, v_num=6act, BTC_val\u001b[A\n",
      "Epoch 33:  99%|â–‰| 79/80 [00:04<00:00, 18.58it/s, loss=0.529, v_num=6act, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.465. Signaling Trainer to stop.\n",
      "Epoch 33: 100%|â–ˆ| 80/80 [00:04<00:00, 18.47it/s, loss=0.529, v_num=6act, BTC_val\n",
      "Epoch 33: 100%|â–ˆ| 80/80 [00:04<00:00, 18.44it/s, loss=0.529, v_num=6act, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 37.91it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6451612710952759,\n",
      " 'BTC_test_f1': 0.6390261650085449,\n",
      " 'ETH_test_acc': 0.6774193644523621,\n",
      " 'ETH_test_f1': 0.6768433451652527,\n",
      " 'test_loss': 0.6043744683265686}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 73918\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015719-1prx6act/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015719-1prx6act/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.6875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.67611\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.87302\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.52106\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2686\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 159\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621465198\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 121\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.73317\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.72289\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.73317\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.71942\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.5358\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.77778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.48338\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.64516\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.63903\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.67742\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.67684\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.60437\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–‚â–„â–†â–†â–‚â–‚â–‡â–…â–ƒâ–…â–…â–ƒâ–‚â–ƒâ–ƒâ–…â–…â–†â–‡â–ƒâ–ƒâ–„â–…â–…â–†â–‡â–„â–…â–ˆâ–ƒâ–â–„â–…â–†â–‡â–…â–…â–‡â–ƒâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–‚â–„â–†â–†â–‚â–‚â–‡â–…â–ƒâ–„â–†â–‚â–‚â–ƒâ–‚â–…â–…â–†â–‡â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–„â–…â–ˆâ–‚â–â–„â–…â–†â–‡â–…â–…â–†â–ƒâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–â–‡â–†â–†â–‚â–…â–†â–…â–…â–†â–ƒâ–„â–â–„â–„â–…â–‚â–…â–†â–„â–…â–ƒâ–‡â–…â–…â–†â–„â–†â–ˆâ–ƒâ–ƒâ–‚â–…â–‡â–„â–ƒâ–…â–„â–‚â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–‚â–‡â–†â–†â–ƒâ–†â–†â–…â–†â–†â–„â–„â–‚â–…â–â–…â–ƒâ–†â–…â–„â–†â–„â–‡â–†â–…â–†â–„â–†â–ˆâ–„â–„â–‚â–†â–‡â–…â–„â–†â–„â–ƒâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–‡â–…â–…â–„â–ˆâ–…â–ƒâ–…â–†â–„â–…â–‡â–†â–‡â–†â–…â–†â–„â–ƒâ–†â–…â–†â–ƒâ–ƒâ–„â–‚â–…â–ƒâ–â–†â–…â–‡â–ƒâ–â–„â–…â–„â–„â–‡â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–â–…â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–â–†â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–…â–ˆâ–ˆâ–…â–ˆâ–…â–…â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–†â–ˆâ–ˆâ–†â–ˆâ–†â–†â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–â–ƒâ–â–â–â–ˆâ–â–†â–â–†â–†â–â–†â–ˆâ–â–â–†â–â–ˆâ–†â–â–ƒâ–ˆâ–ˆâ–ƒâ–†â–ˆâ–ƒâ–â–ˆâ–â–ˆâ–†â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–â–„â–â–â–â–ˆâ–â–†â–â–†â–†â–â–†â–ˆâ–â–â–†â–â–ˆâ–†â–â–„â–ˆâ–ˆâ–„â–†â–ˆâ–„â–â–ˆâ–â–ˆâ–†â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–†â–‚â–„â–‚â–ƒâ–…â–‚â–â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–â–„â–‚â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1prx6act\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:00:18.030344: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_loss_weighted_binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2bqw3y4i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020016-2bqw3y4i\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 80/81 [00:03<00:00, 22.72it/s, loss=0.713, v_num=3y4i, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.688\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:03<00:00, 22.48it/s, loss=0.713, v_num=3y4i, BTC_val_\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:03<00:00, 22.03it/s, loss=0.703, v_num=3y4i, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:03<00:00, 21.90it/s, loss=0.703, v_num=3y4i, BTC_val_\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:04<00:00, 20.13it/s, loss=0.696, v_num=3y4i, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:04<00:00, 20.03it/s, loss=0.696, v_num=3y4i, BTC_val_\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:03<00:00, 21.61it/s, loss=0.691, v_num=3y4i, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:03<00:00, 21.50it/s, loss=0.691, v_num=3y4i, BTC_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.681\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:04<00:00, 19.28it/s, loss=0.696, v_num=3y4i, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:04<00:00, 19.19it/s, loss=0.696, v_num=3y4i, BTC_val_\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:03<00:00, 21.03it/s, loss=0.7, v_num=3y4i, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:03<00:00, 20.92it/s, loss=0.7, v_num=3y4i, BTC_val_ac\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:04<00:00, 16.66it/s, loss=0.695, v_num=3y4i, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:04<00:00, 16.57it/s, loss=0.695, v_num=3y4i, BTC_val_\u001b[A\n",
      "Metric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.676\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:04<00:00, 20.22it/s, loss=0.693, v_num=3y4i, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:04<00:00, 20.10it/s, loss=0.693, v_num=3y4i, BTC_val_\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:04<00:00, 20.05it/s, loss=0.697, v_num=3y4i, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:04<00:00, 19.93it/s, loss=0.697, v_num=3y4i, BTC_val_\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:04<00:00, 19.47it/s, loss=0.698, v_num=3y4i, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:04<00:00, 19.36it/s, loss=0.698, v_num=3y4i, BTC_val_\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:04<00:00, 19.95it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:04<00:00, 19.86it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:04<00:00, 19.42it/s, loss=0.696, v_num=3y4i, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:04<00:00, 19.21it/s, loss=0.696, v_num=3y4i, BTC_val\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:04<00:00, 19.12it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:04<00:00, 19.02it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:03<00:00, 20.53it/s, loss=0.697, v_num=3y4i, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:03<00:00, 20.43it/s, loss=0.697, v_num=3y4i, BTC_val\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:04<00:00, 18.46it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:04<00:00, 18.37it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:04<00:00, 18.61it/s, loss=0.693, v_num=3y4i, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:04<00:00, 18.49it/s, loss=0.693, v_num=3y4i, BTC_val\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:05<00:00, 14.80it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:05<00:00, 14.75it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:04<00:00, 16.90it/s, loss=0.692, v_num=3y4i, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:05<00:00, 15.80it/s, loss=0.692, v_num=3y4i, BTC_val\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:05<00:00, 16.16it/s, loss=0.693, v_num=3y4i, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:05<00:00, 16.09it/s, loss=0.693, v_num=3y4i, BTC_val\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:04<00:00, 17.02it/s, loss=0.693, v_num=3y4i, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:04<00:00, 16.94it/s, loss=0.693, v_num=3y4i, BTC_val\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:04<00:00, 18.65it/s, loss=0.696, v_num=3y4i, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:04<00:00, 18.56it/s, loss=0.696, v_num=3y4i, BTC_val\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:04<00:00, 18.75it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:04<00:00, 18.66it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 81/81 [00:04<00:00, 19.42it/s, loss=0.691, v_num=3y4i, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 81/81 [00:04<00:00, 19.33it/s, loss=0.691, v_num=3y4i, BTC_val\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 81/81 [00:04<00:00, 17.49it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 81/81 [00:04<00:00, 17.41it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 81/81 [00:04<00:00, 17.67it/s, loss=0.692, v_num=3y4i, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 81/81 [00:04<00:00, 17.55it/s, loss=0.692, v_num=3y4i, BTC_val\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 81/81 [00:04<00:00, 18.99it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 81/81 [00:04<00:00, 18.90it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 81/81 [00:04<00:00, 17.13it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.676. Signaling Trainer to stop.\n",
      "Epoch 26: 100%|â–ˆ| 81/81 [00:04<00:00, 17.04it/s, loss=0.694, v_num=3y4i, BTC_val\n",
      "Epoch 26: 100%|â–ˆ| 81/81 [00:04<00:00, 17.02it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 38.63it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.4193548262119293,\n",
      " 'BTC_test_f1': 0.29533159732818604,\n",
      " 'ETH_test_acc': 0.6129032373428345,\n",
      " 'ETH_test_f1': 0.3793548345565796,\n",
      " 'test_loss': 0.6955219507217407}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 74356\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020016-2bqw3y4i/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020016-2bqw3y4i/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.51515\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.69733\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 126\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621465342\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 97\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.52955\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.41539\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.49961\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.38942\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69413\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.35714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.88889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.47059\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.68941\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.41935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.29533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.37935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.69552\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–…â–„â–…â–…â–ƒâ–ƒâ–„â–ƒâ–†â–ƒâ–„â–†â–ƒâ–…â–ˆâ–â–„â–„â–ƒâ–‚â–ƒâ–„â–ˆâ–„â–…â–ƒâ–ƒâ–‚â–„â–†â–ƒâ–ƒâ–ƒâ–…â–„â–„â–ƒâ–†â–ƒâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–„â–„â–…â–ƒâ–‚â–ƒâ–„â–ƒâ–†â–‚â–ƒâ–†â–ƒâ–…â–ˆâ–â–„â–„â–‚â–‚â–ƒâ–„â–ˆâ–„â–…â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–…â–„â–„â–ƒâ–†â–‚â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–„â–„â–„â–‚â–…â–…â–ƒâ–â–‚â–‡â–„â–„â–…â–‡â–ƒâ–‚â–„â–„â–„â–ƒâ–ƒâ–…â–â–„â–„â–‡â–„â–„â–‡â–ˆâ–„â–‡â–ƒâ–…â–„â–‡â–ƒâ–†â–„â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–„â–ƒâ–…â–ƒâ–ƒâ–†â–ƒâ–â–‚â–†â–…â–…â–…â–ˆâ–„â–‚â–…â–„â–„â–ƒâ–ƒâ–…â–â–…â–„â–‡â–…â–…â–‡â–ˆâ–…â–‡â–ƒâ–…â–„â–‡â–ƒâ–ƒâ–…â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–„â–†â–„â–…â–‡â–„â–‡â–ˆâ–…â–„â–†â–„â–†â–ƒâ–â–ˆâ–†â–…â–‡â–†â–†â–…â–†â–…â–„â–…â–†â–…â–„â–ƒâ–†â–…â–…â–…â–†â–…â–…â–ƒâ–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–„â–…â–‡â–„â–‚â–‚â–…â–‡â–â–„â–ˆâ–â–ˆâ–…â–ƒâ–„â–„â–…â–â–‡â–„â–…â–‚â–„â–‚â–â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–…â–†â–ˆâ–…â–…â–„â–â–„â–„â–†â–‡â–…â–„â–ˆâ–†â–†â–…â–‡â–…â–ƒâ–„â–‡â–…â–…â–ƒâ–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–†â–ƒâ–‡â–…â–†â–…â–„â–‡â–ˆâ–…â–â–ˆâ–†â–‚â–…â–ˆâ–‡â–†â–ƒâ–†â–ˆâ–„â–‡â–‡â–„â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–†â–„â–‡â–…â–‡â–†â–‚â–„â–ˆâ–‡â–ƒâ–ˆâ–„â–…â–†â–ˆâ–‡â–‡â–†â–ƒâ–‡â–†â–‡â–†â–ƒâ–…â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–„â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–â–â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–â–â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–â–â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–â–â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–…â–†â–‡â–ƒâ–†â–ƒâ–â–†â–…â–„â–…â–…â–†â–†â–†â–†â–†â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2bqw3y4i\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:02:41.235833: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/11yjrbef\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020239-11yjrbef\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/80 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|â–‰| 79/80 [00:03<00:00, 21.45it/s, loss=1.11, v_num=rbef, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.098\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:03<00:00, 21.36it/s, loss=1.11, v_num=rbef, BTC_val_a\n",
      "Epoch 1:  99%|â–‰| 79/80 [00:03<00:00, 20.09it/s, loss=1.11, v_num=rbef, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:03<00:00, 20.03it/s, loss=1.11, v_num=rbef, BTC_val_a\u001b[A\n",
      "Epoch 2:  99%|â–‰| 79/80 [00:03<00:00, 21.84it/s, loss=1.11, v_num=rbef, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:03<00:00, 21.75it/s, loss=1.11, v_num=rbef, BTC_val_a\u001b[A\n",
      "Epoch 3:  99%|â–‰| 79/80 [00:04<00:00, 17.64it/s, loss=1.11, v_num=rbef, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:04<00:00, 17.60it/s, loss=1.11, v_num=rbef, BTC_val_a\u001b[A\n",
      "Epoch 4:  99%|â–‰| 79/80 [00:04<00:00, 18.58it/s, loss=1.1, v_num=rbef, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:04<00:00, 18.44it/s, loss=1.1, v_num=rbef, BTC_val_ac\u001b[A\n",
      "Epoch 5:  99%|â–‰| 79/80 [00:04<00:00, 16.59it/s, loss=1.1, v_num=rbef, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 1.095\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:04<00:00, 16.53it/s, loss=1.1, v_num=rbef, BTC_val_ac\n",
      "Epoch 6:  99%|â–‰| 79/80 [00:03<00:00, 20.49it/s, loss=1.11, v_num=rbef, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:03<00:00, 20.33it/s, loss=1.11, v_num=rbef, BTC_val_a\u001b[A\n",
      "Epoch 7:  99%|â–‰| 79/80 [00:05<00:00, 14.70it/s, loss=1.1, v_num=rbef, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:05<00:00, 14.68it/s, loss=1.1, v_num=rbef, BTC_val_ac\u001b[A\n",
      "Epoch 8:  99%|â–‰| 79/80 [00:04<00:00, 18.48it/s, loss=1.1, v_num=rbef, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:04<00:00, 18.40it/s, loss=1.1, v_num=rbef, BTC_val_ac\u001b[A\n",
      "Epoch 9:  99%|â–‰| 79/80 [00:04<00:00, 18.13it/s, loss=1.1, v_num=rbef, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:04<00:00, 18.08it/s, loss=1.1, v_num=rbef, BTC_val_ac\u001b[A\n",
      "Epoch 10:  99%|â–‰| 79/80 [00:05<00:00, 14.90it/s, loss=1.11, v_num=rbef, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:05<00:00, 14.87it/s, loss=1.11, v_num=rbef, BTC_val_\u001b[A\n",
      "Epoch 11:  99%|â–‰| 79/80 [00:05<00:00, 15.18it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:05<00:00, 15.17it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Epoch 12:  99%|â–‰| 79/80 [00:05<00:00, 15.45it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:05<00:00, 15.42it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Epoch 13:  99%|â–‰| 79/80 [00:04<00:00, 15.83it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 1.092\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:05<00:00, 15.83it/s, loss=1.1, v_num=rbef, BTC_val_a\n",
      "Epoch 14:  99%|â–‰| 79/80 [00:05<00:00, 15.08it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:05<00:00, 15.08it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Epoch 15:  99%|â–‰| 79/80 [00:04<00:00, 17.56it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:04<00:00, 17.52it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Epoch 16:  99%|â–‰| 79/80 [00:04<00:00, 19.13it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:04<00:00, 19.03it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Epoch 17:  99%|â–‰| 79/80 [00:04<00:00, 17.62it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:04<00:00, 17.56it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Epoch 18:  99%|â–‰| 79/80 [00:03<00:00, 20.84it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:03<00:00, 20.71it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Epoch 19:  99%|â–‰| 79/80 [00:04<00:00, 15.94it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:05<00:00, 15.92it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Epoch 20:  99%|â–‰| 79/80 [00:04<00:00, 19.16it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:04<00:00, 19.09it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Epoch 21:  99%|â–‰| 79/80 [00:04<00:00, 17.32it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:04<00:00, 17.24it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Epoch 22:  99%|â–‰| 79/80 [00:04<00:00, 18.37it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:04<00:00, 18.28it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Epoch 23:  99%|â–‰| 79/80 [00:04<00:00, 17.27it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:04<00:00, 17.17it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Epoch 24:  99%|â–‰| 79/80 [00:04<00:00, 18.41it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:04<00:00, 18.34it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n",
      "Epoch 25:  99%|â–‰| 79/80 [00:04<00:00, 18.54it/s, loss=1.09, v_num=rbef, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:04<00:00, 18.27it/s, loss=1.09, v_num=rbef, BTC_val_\u001b[A\n",
      "Epoch 26:  99%|â–‰| 79/80 [00:04<00:00, 17.84it/s, loss=1.07, v_num=rbef, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:04<00:00, 17.76it/s, loss=1.07, v_num=rbef, BTC_val_\u001b[A\n",
      "Epoch 27:  99%|â–‰| 79/80 [00:03<00:00, 20.87it/s, loss=1.02, v_num=rbef, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 1.071\n",
      "Epoch 27: 100%|â–ˆ| 80/80 [00:03<00:00, 20.71it/s, loss=1.02, v_num=rbef, BTC_val_\n",
      "Epoch 28:  99%|â–‰| 79/80 [00:03<00:00, 20.06it/s, loss=1.03, v_num=rbef, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:04<00:00, 19.86it/s, loss=1.03, v_num=rbef, BTC_val_\u001b[A\n",
      "Epoch 29:  99%|â–‰| 79/80 [00:03<00:00, 20.66it/s, loss=1.02, v_num=rbef, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:03<00:00, 20.47it/s, loss=1.02, v_num=rbef, BTC_val_\u001b[A\n",
      "Epoch 30:  99%|â–‰| 79/80 [00:03<00:00, 20.92it/s, loss=1, v_num=rbef, BTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 80/80 [00:03<00:00, 20.74it/s, loss=1, v_num=rbef, BTC_val_acc\u001b[A\n",
      "Epoch 31:  99%|â–‰| 79/80 [00:04<00:00, 19.23it/s, loss=0.976, v_num=rbef, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 80/80 [00:04<00:00, 19.11it/s, loss=0.976, v_num=rbef, BTC_val\u001b[A\n",
      "Epoch 32:  99%|â–‰| 79/80 [00:04<00:00, 17.54it/s, loss=0.965, v_num=rbef, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 80/80 [00:04<00:00, 17.48it/s, loss=0.965, v_num=rbef, BTC_val\u001b[A\n",
      "Epoch 33:  99%|â–‰| 79/80 [00:03<00:00, 20.26it/s, loss=0.986, v_num=rbef, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 80/80 [00:03<00:00, 20.16it/s, loss=0.986, v_num=rbef, BTC_val\u001b[A\n",
      "Epoch 34:  99%|â–‰| 79/80 [00:04<00:00, 17.89it/s, loss=0.983, v_num=rbef, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 80/80 [00:04<00:00, 17.73it/s, loss=0.983, v_num=rbef, BTC_val\u001b[A\n",
      "Epoch 35:  99%|â–‰| 79/80 [00:05<00:00, 14.54it/s, loss=0.987, v_num=rbef, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 80/80 [00:05<00:00, 14.54it/s, loss=0.987, v_num=rbef, BTC_val\u001b[A\n",
      "Epoch 36:  99%|â–‰| 79/80 [00:04<00:00, 16.50it/s, loss=0.98, v_num=rbef, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 80/80 [00:04<00:00, 16.46it/s, loss=0.98, v_num=rbef, BTC_val_\u001b[A\n",
      "Epoch 37:  99%|â–‰| 79/80 [00:03<00:00, 21.18it/s, loss=0.973, v_num=rbef, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|â–ˆ| 80/80 [00:03<00:00, 21.08it/s, loss=0.973, v_num=rbef, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38:  99%|â–‰| 79/80 [00:04<00:00, 18.79it/s, loss=1.01, v_num=rbef, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|â–ˆ| 80/80 [00:04<00:00, 18.69it/s, loss=1.01, v_num=rbef, BTC_val_\u001b[A\n",
      "Epoch 39:  99%|â–‰| 79/80 [00:03<00:00, 19.91it/s, loss=0.933, v_num=rbef, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|â–ˆ| 80/80 [00:04<00:00, 18.10it/s, loss=0.933, v_num=rbef, BTC_val\u001b[A\n",
      "Epoch 40:  99%|â–‰| 79/80 [00:04<00:00, 18.03it/s, loss=0.971, v_num=rbef, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|â–ˆ| 80/80 [00:04<00:00, 17.94it/s, loss=0.971, v_num=rbef, BTC_val\u001b[A\n",
      "Epoch 41:  99%|â–‰| 79/80 [00:04<00:00, 18.41it/s, loss=0.928, v_num=rbef, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|â–ˆ| 80/80 [00:04<00:00, 18.22it/s, loss=0.928, v_num=rbef, BTC_val\u001b[A\n",
      "Epoch 42:  99%|â–‰| 79/80 [00:04<00:00, 17.85it/s, loss=0.947, v_num=rbef, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|â–ˆ| 80/80 [00:04<00:00, 17.82it/s, loss=0.947, v_num=rbef, BTC_val\u001b[A\n",
      "Epoch 43:  99%|â–‰| 79/80 [00:04<00:00, 16.35it/s, loss=0.906, v_num=rbef, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|â–ˆ| 80/80 [00:04<00:00, 16.29it/s, loss=0.906, v_num=rbef, BTC_val\u001b[A\n",
      "Epoch 44:  99%|â–‰| 79/80 [00:05<00:00, 15.73it/s, loss=0.922, v_num=rbef, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|â–ˆ| 80/80 [00:05<00:00, 15.71it/s, loss=0.922, v_num=rbef, BTC_val\u001b[A\n",
      "Epoch 45:  99%|â–‰| 79/80 [00:04<00:00, 17.13it/s, loss=0.877, v_num=rbef, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|â–ˆ| 80/80 [00:04<00:00, 17.06it/s, loss=0.877, v_num=rbef, BTC_val\u001b[A\n",
      "Epoch 46:  99%|â–‰| 79/80 [00:04<00:00, 17.45it/s, loss=0.991, v_num=rbef, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|â–ˆ| 80/80 [00:04<00:00, 17.39it/s, loss=0.991, v_num=rbef, BTC_val\u001b[A\n",
      "Epoch 47:  99%|â–‰| 79/80 [00:04<00:00, 17.37it/s, loss=0.914, v_num=rbef, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.071. Signaling Trainer to stop.\n",
      "Epoch 47: 100%|â–ˆ| 80/80 [00:04<00:00, 17.33it/s, loss=0.914, v_num=rbef, BTC_val\n",
      "Epoch 47: 100%|â–ˆ| 80/80 [00:04<00:00, 17.31it/s, loss=0.914, v_num=rbef, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 49.09it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.35483869910240173,\n",
      " 'BTC_test_f1': 0.27419355511665344,\n",
      " 'ETH_test_acc': 0.4516128897666931,\n",
      " 'ETH_test_f1': 0.41279301047325134,\n",
      " 'test_loss': 0.935441792011261}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 74693\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020239-11yjrbef/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020239-11yjrbef/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.4375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.39048\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.47381\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.89546\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 47\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3792\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 225\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621465584\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 171\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.46397\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.43822\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.50515\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.48137\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.90426\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.22222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.19048\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.37408\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.35484\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.27419\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.45161\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.41279\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.93544\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–†â–†â–ƒâ–†â–†â–†â–…â–„â–ˆâ–…â–ƒâ–†â–ƒâ–„â–ƒâ–…â–ƒâ–…â–†â–„â–„â–â–…â–ƒâ–‚â–…â–„â–ƒâ–ƒâ–„â–‚â–„â–„â–…â–„â–…â–‡â–ˆâ–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–„â–„â–ƒâ–†â–„â–‡â–…â–ƒâ–ˆâ–…â–ƒâ–„â–‚â–„â–ƒâ–„â–ƒâ–ƒâ–„â–ƒâ–ƒâ–â–…â–„â–‚â–…â–„â–„â–‚â–…â–ƒâ–„â–„â–„â–„â–…â–‡â–‡â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–‡â–…â–„â–…â–„â–…â–…â–„â–‚â–…â–ƒâ–„â–ƒâ–…â–â–â–„â–†â–†â–„â–„â–ƒâ–‚â–…â–ƒâ–…â–…â–ƒâ–†â–…â–„â–…â–ˆâ–„â–‡â–…â–‡â–„â–‚â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–‡â–…â–„â–…â–ƒâ–…â–ƒâ–‚â–‚â–„â–ƒâ–‚â–‚â–„â–â–â–ƒâ–„â–…â–ƒâ–ƒâ–‚â–‚â–„â–ƒâ–…â–…â–ƒâ–…â–†â–„â–…â–ˆâ–ƒâ–‡â–…â–…â–ƒâ–‚â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–‡â–†â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–†â–‡â–…â–‡â–ƒâ–†â–‚â–…â–…â–„â–†â–„â–„â–â–†â–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–„â–„â–…â–†â–†â–„â–ƒâ–„â–ƒâ–…â–…â–…â–†â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–„â–„â–…â–…â–…â–†â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–‚â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–…â–…â–‚â–‚â–ƒâ–„â–…â–…â–…â–„â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–„â–„â–…â–…â–…â–…â–…â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–…â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–†â–†â–ƒâ–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–â–„â–„â–„â–ƒâ–„â–ƒâ–„â–ƒâ–ƒâ–„â–„â–†â–„â–„â–„â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–„â–„â–„â–†â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–â–…â–…â–…â–ƒâ–†â–ƒâ–…â–ƒâ–ƒâ–†â–†â–ˆâ–†â–†â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–„â–„â–„â–ˆâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–â–„â–â–â–„â–„â–â–â–â–„â–„â–„â–ˆâ–„â–ˆâ–ˆâ–ˆâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–‚â–‚â–„â–‡â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–†â–â–‚â–…â–…â–â–‚â–â–†â–…â–…â–ˆâ–…â–ˆâ–ˆâ–ˆâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–ƒâ–…â–†â–†â–„â–†â–†â–…â–‡â–„â–‡â–†â–†â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/11yjrbef\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:06:44.179161: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_loss_weighted_multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/77a6g8bz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020642-77a6g8bz\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:03<00:00, 22.32it/s, loss=1.11, v_num=g8bz, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.117\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:03<00:00, 22.17it/s, loss=1.11, v_num=g8bz, BTC_val_a\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:04<00:00, 18.71it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.028 >= min_delta = 0.003. New best score: 1.089\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:04<00:00, 18.61it/s, loss=1.1, v_num=g8bz, BTC_val_ac\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:04<00:00, 16.70it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:04<00:00, 16.63it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:04<00:00, 20.07it/s, loss=1.11, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:04<00:00, 19.97it/s, loss=1.11, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:03<00:00, 21.06it/s, loss=1.11, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:03<00:00, 20.95it/s, loss=1.11, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:03<00:00, 21.71it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:03<00:00, 21.60it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:03<00:00, 21.43it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:03<00:00, 21.32it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:03<00:00, 21.60it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:03<00:00, 21.47it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:04<00:00, 19.14it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:04<00:00, 19.06it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:04<00:00, 19.45it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:04<00:00, 19.35it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:04<00:00, 19.57it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:04<00:00, 19.48it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:04<00:00, 20.09it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:04<00:00, 19.98it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:04<00:00, 18.94it/s, loss=1.11, v_num=g8bz, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:04<00:00, 18.85it/s, loss=1.11, v_num=g8bz, BTC_val_\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:05<00:00, 15.71it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:05<00:00, 15.63it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:04<00:00, 18.11it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:04<00:00, 17.89it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:04<00:00, 17.69it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:04<00:00, 17.60it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:04<00:00, 19.36it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:04<00:00, 19.25it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:04<00:00, 17.08it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:04<00:00, 17.01it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:04<00:00, 18.32it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:04<00:00, 18.23it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:03<00:00, 20.28it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:04<00:00, 20.17it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:04<00:00, 16.85it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:04<00:00, 16.75it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:04<00:00, 19.81it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.089. Signaling Trainer to stop.\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:04<00:00, 19.69it/s, loss=1.1, v_num=g8bz, BTC_val_a\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:04<00:00, 19.67it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 40.85it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.25806450843811035,\n",
      " 'BTC_test_f1': 0.1367289274930954,\n",
      " 'ETH_test_acc': 0.4193548262119293,\n",
      " 'ETH_test_f1': 0.19648092985153198,\n",
      " 'test_loss': 1.1399511098861694}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 75229\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020642-77a6g8bz/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020642-77a6g8bz/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.1875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.18413\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.1875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.16239\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.11604\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1760\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621465705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 79\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.35382\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.31259\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.34043\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.30833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09645\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.22222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.13333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.22222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.18519\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.11259\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.25806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.13673\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.41935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.13995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–ƒâ–â–„â–…â–ˆâ–†â–‚â–â–‚â–ƒâ–…â–„â–ƒâ–‚â–‚â–„â–ƒâ–…â–ƒâ–‚â–…â–‚â–ƒâ–„â–‡â–‡â–ƒâ–‚â–†â–‚â–†â–ˆâ–…â–ƒâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–ƒâ–â–ƒâ–…â–ˆâ–†â–ƒâ–â–â–ƒâ–…â–„â–ƒâ–‚â–‚â–ƒâ–‚â–…â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–„â–‡â–‡â–ƒâ–‚â–„â–‚â–…â–…â–„â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–ˆâ–ƒâ–…â–†â–…â–…â–„â–ƒâ–…â–‚â–„â–…â–‚â–ƒâ–‚â–…â–‡â–…â–…â–…â–†â–‡â–‡â–ˆâ–…â–‡â–…â–ƒâ–„â–‚â–…â–â–…â–‡â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–ˆâ–ƒâ–„â–…â–„â–…â–ƒâ–ƒâ–„â–‚â–ƒâ–…â–‚â–‚â–‚â–„â–‡â–„â–…â–…â–…â–†â–‡â–„â–…â–‡â–„â–ƒâ–ƒâ–‚â–„â–â–…â–…â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–â–ˆâ–…â–ƒâ–†â–ƒâ–†â–†â–†â–‡â–‡â–…â–‡â–†â–‡â–‡â–‡â–†â–…â–…â–„â–‡â–ƒâ–ˆâ–…â–„â–…â–†â–†â–‡â–„â–…â–…â–†â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–†â–„â–‡â–†â–‚â–ƒâ–„â–†â–…â–â–ˆâ–†â–†â–…â–‡â–ƒâ–„â–†â–†â–‚â–ƒâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–…â–„â–ˆâ–‡â–‚â–„â–„â–†â–†â–ƒâ–ˆâ–†â–‡â–…â–ˆâ–â–…â–†â–„â–â–„â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–‚â–‚â–ˆâ–„â–„â–„â–â–…â–‚â–‚â–†â–‡â–…â–ƒâ–ƒâ–‚â–ƒâ–…â–…â–…â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–‚â–‚â–ˆâ–…â–…â–…â–‚â–„â–ƒâ–ƒâ–‡â–‡â–„â–‚â–„â–â–ƒâ–„â–…â–„â–‡â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–…â–‚â–„â–„â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–â–â–‚â–‚â–‚â–â–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–ˆâ–ˆâ–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–ˆâ–ˆâ–â–ˆâ–â–â–â–â–‚â–â–â–â–â–â–â–â–ˆâ–‚â–â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–â–ˆâ–ˆâ–â–ˆâ–â–â–â–â–ƒâ–â–â–â–â–â–â–â–†â–â–â–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–â–ˆâ–ˆâ–â–ˆâ–â–â–â–â–†â–â–â–â–â–â–â–â–‡â–â–â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–†â–â–„â–…â–ƒâ–†â–‡â–…â–†â–…â–†â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–„â–‡â–‡â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/77a6g8bz\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:08:40.399661: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_single_lstm__binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2io82qdl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020838-2io82qdl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 130   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.7 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 79/80 [00:01<00:00, 42.72it/s, loss=0.597, v_num=2qdl, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.539\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:01<00:00, 41.00it/s, loss=0.597, v_num=2qdl, BTC_val_\n",
      "Epoch 1:  99%|â–‰| 79/80 [00:01<00:00, 45.81it/s, loss=0.58, v_num=2qdl, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.522\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 44.69it/s, loss=0.58, v_num=2qdl, BTC_val_a\n",
      "Epoch 2:  99%|â–‰| 79/80 [00:01<00:00, 44.08it/s, loss=0.575, v_num=2qdl, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 42.75it/s, loss=0.575, v_num=2qdl, BTC_val_\u001b[A\n",
      "Epoch 3:  99%|â–‰| 79/80 [00:01<00:00, 44.15it/s, loss=0.601, v_num=2qdl, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:01<00:00, 43.12it/s, loss=0.601, v_num=2qdl, BTC_val_\u001b[A\n",
      "Epoch 4:  99%|â–‰| 79/80 [00:01<00:00, 43.87it/s, loss=0.562, v_num=2qdl, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 42.87it/s, loss=0.562, v_num=2qdl, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 79/80 [00:01<00:00, 45.80it/s, loss=0.556, v_num=2qdl, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:01<00:00, 44.60it/s, loss=0.556, v_num=2qdl, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 79/80 [00:01<00:00, 43.33it/s, loss=0.596, v_num=2qdl, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:01<00:00, 42.57it/s, loss=0.596, v_num=2qdl, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 79/80 [00:01<00:00, 39.92it/s, loss=0.591, v_num=2qdl, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:02<00:00, 39.33it/s, loss=0.591, v_num=2qdl, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 79/80 [00:02<00:00, 37.77it/s, loss=0.603, v_num=2qdl, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:02<00:00, 37.07it/s, loss=0.603, v_num=2qdl, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 79/80 [00:02<00:00, 39.48it/s, loss=0.607, v_num=2qdl, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:02<00:00, 38.68it/s, loss=0.607, v_num=2qdl, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 79/80 [00:02<00:00, 36.79it/s, loss=0.595, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:02<00:00, 36.14it/s, loss=0.595, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 79/80 [00:01<00:00, 46.63it/s, loss=0.588, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:01<00:00, 45.42it/s, loss=0.588, v_num=2qdl, BTC_valMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.515\n",
      "\n",
      "Epoch 12:  99%|â–‰| 79/80 [00:02<00:00, 35.20it/s, loss=0.559, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:02<00:00, 34.48it/s, loss=0.559, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 79/80 [00:02<00:00, 38.99it/s, loss=0.606, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:02<00:00, 38.25it/s, loss=0.606, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 79/80 [00:01<00:00, 43.34it/s, loss=0.567, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:01<00:00, 42.33it/s, loss=0.567, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 79/80 [00:01<00:00, 46.27it/s, loss=0.545, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:01<00:00, 45.09it/s, loss=0.545, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 79/80 [00:02<00:00, 33.30it/s, loss=0.574, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:02<00:00, 30.43it/s, loss=0.574, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 79/80 [00:02<00:00, 37.23it/s, loss=0.579, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:02<00:00, 36.45it/s, loss=0.579, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 79/80 [00:01<00:00, 41.62it/s, loss=0.549, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:01<00:00, 40.90it/s, loss=0.549, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 79/80 [00:02<00:00, 36.05it/s, loss=0.544, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.510\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:02<00:00, 35.53it/s, loss=0.544, v_num=2qdl, BTC_val\n",
      "Epoch 20:  99%|â–‰| 79/80 [00:02<00:00, 38.63it/s, loss=0.556, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:02<00:00, 37.65it/s, loss=0.556, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 79/80 [00:01<00:00, 41.36it/s, loss=0.605, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:01<00:00, 40.71it/s, loss=0.605, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 79/80 [00:02<00:00, 35.77it/s, loss=0.55, v_num=2qdl, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:02<00:00, 35.36it/s, loss=0.55, v_num=2qdl, BTC_val_\u001b[A\n",
      "Epoch 23:  99%|â–‰| 79/80 [00:01<00:00, 42.23it/s, loss=0.559, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:01<00:00, 41.35it/s, loss=0.559, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 24:  99%|â–‰| 79/80 [00:02<00:00, 34.34it/s, loss=0.577, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:02<00:00, 33.89it/s, loss=0.577, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 25:  99%|â–‰| 79/80 [00:01<00:00, 42.73it/s, loss=0.581, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:01<00:00, 41.96it/s, loss=0.581, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 26:  99%|â–‰| 79/80 [00:01<00:00, 42.20it/s, loss=0.587, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:01<00:00, 41.48it/s, loss=0.587, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 27:  99%|â–‰| 79/80 [00:01<00:00, 40.45it/s, loss=0.551, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 80/80 [00:02<00:00, 39.87it/s, loss=0.551, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 28:  99%|â–‰| 79/80 [00:01<00:00, 39.56it/s, loss=0.577, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:02<00:00, 38.90it/s, loss=0.577, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 29:  99%|â–‰| 79/80 [00:02<00:00, 33.47it/s, loss=0.563, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.502\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:02<00:00, 32.94it/s, loss=0.563, v_num=2qdl, BTC_val\n",
      "Epoch 30:  99%|â–‰| 79/80 [00:02<00:00, 38.70it/s, loss=0.582, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 80/80 [00:02<00:00, 37.79it/s, loss=0.582, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 31:  99%|â–‰| 79/80 [00:01<00:00, 45.10it/s, loss=0.557, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 80/80 [00:01<00:00, 43.87it/s, loss=0.557, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 32:  99%|â–‰| 79/80 [00:01<00:00, 41.05it/s, loss=0.563, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 80/80 [00:01<00:00, 40.07it/s, loss=0.563, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 33:  99%|â–‰| 79/80 [00:01<00:00, 39.79it/s, loss=0.545, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.497\n",
      "Epoch 33: 100%|â–ˆ| 80/80 [00:02<00:00, 38.65it/s, loss=0.545, v_num=2qdl, BTC_val\n",
      "Epoch 34:  99%|â–‰| 79/80 [00:02<00:00, 33.30it/s, loss=0.561, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 80/80 [00:02<00:00, 32.74it/s, loss=0.561, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 35:  99%|â–‰| 79/80 [00:02<00:00, 33.76it/s, loss=0.572, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 80/80 [00:02<00:00, 33.39it/s, loss=0.572, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 36:  99%|â–‰| 79/80 [00:02<00:00, 37.88it/s, loss=0.572, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 80/80 [00:02<00:00, 37.08it/s, loss=0.572, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 37:  99%|â–‰| 79/80 [00:02<00:00, 33.73it/s, loss=0.556, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|â–ˆ| 80/80 [00:02<00:00, 33.37it/s, loss=0.556, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 38:  99%|â–‰| 79/80 [00:01<00:00, 39.72it/s, loss=0.566, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|â–ˆ| 80/80 [00:02<00:00, 39.06it/s, loss=0.566, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 39:  99%|â–‰| 79/80 [00:01<00:00, 39.65it/s, loss=0.584, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|â–ˆ| 80/80 [00:02<00:00, 39.06it/s, loss=0.584, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 40:  99%|â–‰| 79/80 [00:02<00:00, 38.07it/s, loss=0.613, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|â–ˆ| 80/80 [00:02<00:00, 37.40it/s, loss=0.613, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 41:  99%|â–‰| 79/80 [00:02<00:00, 37.78it/s, loss=0.556, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|â–ˆ| 80/80 [00:02<00:00, 37.10it/s, loss=0.556, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 42:  99%|â–‰| 79/80 [00:01<00:00, 42.46it/s, loss=0.553, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|â–ˆ| 80/80 [00:01<00:00, 41.50it/s, loss=0.553, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 43:  99%|â–‰| 79/80 [00:02<00:00, 30.94it/s, loss=0.555, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|â–ˆ| 80/80 [00:02<00:00, 30.58it/s, loss=0.555, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 44:  99%|â–‰| 79/80 [00:01<00:00, 43.28it/s, loss=0.585, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|â–ˆ| 80/80 [00:01<00:00, 42.24it/s, loss=0.585, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 45:  99%|â–‰| 79/80 [00:02<00:00, 35.70it/s, loss=0.596, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|â–ˆ| 80/80 [00:02<00:00, 35.01it/s, loss=0.596, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 46:  99%|â–‰| 79/80 [00:02<00:00, 35.76it/s, loss=0.567, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|â–ˆ| 80/80 [00:02<00:00, 35.17it/s, loss=0.567, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 47:  99%|â–‰| 79/80 [00:02<00:00, 38.49it/s, loss=0.561, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|â–ˆ| 80/80 [00:02<00:00, 37.88it/s, loss=0.561, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 48:  99%|â–‰| 79/80 [00:01<00:00, 39.82it/s, loss=0.569, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|â–ˆ| 80/80 [00:02<00:00, 39.00it/s, loss=0.569, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 49:  99%|â–‰| 79/80 [00:02<00:00, 37.25it/s, loss=0.52, v_num=2qdl, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|â–ˆ| 80/80 [00:02<00:00, 36.69it/s, loss=0.52, v_num=2qdl, BTC_val_\u001b[A\n",
      "Epoch 50:  99%|â–‰| 79/80 [00:02<00:00, 39.02it/s, loss=0.547, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|â–ˆ| 80/80 [00:02<00:00, 38.40it/s, loss=0.547, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 51:  99%|â–‰| 79/80 [00:02<00:00, 35.26it/s, loss=0.551, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|â–ˆ| 80/80 [00:02<00:00, 34.77it/s, loss=0.551, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 52:  99%|â–‰| 79/80 [00:02<00:00, 35.21it/s, loss=0.589, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|â–ˆ| 80/80 [00:02<00:00, 34.60it/s, loss=0.589, v_num=2qdl, BTC_val\u001b[A\n",
      "Epoch 53:  99%|â–‰| 79/80 [00:02<00:00, 35.35it/s, loss=0.591, v_num=2qdl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.497. Signaling Trainer to stop.\n",
      "Epoch 53: 100%|â–ˆ| 80/80 [00:02<00:00, 34.72it/s, loss=0.591, v_num=2qdl, BTC_val\n",
      "Epoch 53: 100%|â–ˆ| 80/80 [00:02<00:00, 34.66it/s, loss=0.591, v_num=2qdl, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 59.40it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6774193644523621,\n",
      " 'BTC_test_f1': 0.6673067212104797,\n",
      " 'ETH_test_acc': 0.7419354915618896,\n",
      " 'ETH_test_f1': 0.741647481918335,\n",
      " 'test_loss': 0.5464009642601013}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 75536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020838-2io82qdl/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020838-2io82qdl/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.52438\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4266\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 123\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621465841\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 193\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.73159\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.71597\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.71971\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.70688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.55512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.52501\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.67742\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.66731\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.74194\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.74165\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.5464\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–ƒâ–ƒâ–…â–ƒâ–„â–†â–„â–†â–„â–…â–„â–ƒâ–‡â–ƒâ–‚â–…â–†â–„â–†â–†â–†â–†â–ˆâ–„â–ƒâ–…â–ƒâ–…â–ƒâ–â–ƒâ–ƒâ–†â–†â–‚â–…â–…â–†â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–ƒâ–ƒâ–…â–ƒâ–„â–…â–„â–†â–„â–…â–„â–‚â–‡â–ƒâ–‚â–…â–†â–„â–†â–†â–†â–†â–ˆâ–„â–ƒâ–…â–ƒâ–…â–ƒâ–â–ƒâ–ƒâ–†â–†â–â–…â–…â–†â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–â–ƒâ–ƒâ–â–…â–â–ƒâ–†â–â–†â–†â–†â–ˆâ–†â–…â–ƒâ–ˆâ–â–ƒâ–ˆâ–…â–…â–ˆâ–ƒâ–†â–ˆâ–†â–…â–…â–…â–ƒâ–ƒâ–†â–ˆâ–…â–ƒâ–ƒâ–…â–ƒâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–‚â–„â–„â–‚â–…â–‚â–„â–†â–â–†â–†â–…â–ˆâ–†â–…â–ƒâ–ˆâ–‚â–„â–ˆâ–…â–…â–ˆâ–„â–‡â–ˆâ–†â–…â–„â–‚â–„â–„â–‡â–ˆâ–…â–„â–„â–…â–„â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–…â–ˆâ–†â–…â–„â–„â–ƒâ–ƒâ–†â–…â–„â–„â–â–…â–„â–„â–„â–†â–ƒâ–ƒâ–ƒâ–„â–â–†â–†â–†â–…â–ƒâ–…â–†â–…â–†â–‚â–ƒâ–…â–„â–„â–„â–ƒâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–…â–†â–†â–ˆâ–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–†â–‡â–ˆâ–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–â–†â–†â–†â–‡â–‡â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–†â–†â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–ˆâ–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–ˆâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–ˆâ–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–ˆâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–â–â–…â–…â–ƒâ–ˆâ–…â–ƒâ–†â–†â–…â–ƒâ–ƒâ–…â–…â–†â–…â–†â–†â–…â–…â–…â–…â–…â–ƒâ–†â–ˆâ–†â–†â–…â–…â–…â–…â–†â–ƒâ–†â–ƒâ–ƒâ–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–â–â–…â–…â–‚â–ˆâ–…â–‚â–†â–†â–„â–‚â–‚â–„â–…â–†â–…â–†â–†â–„â–„â–„â–„â–„â–‚â–†â–ˆâ–†â–†â–„â–„â–„â–„â–†â–‚â–†â–‚â–‚â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–…â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–ƒâ–„â–„â–ƒâ–„â–‚â–ƒâ–„â–ƒâ–ƒâ–…â–„â–„â–â–ƒâ–‡â–â–ƒâ–…â–„â–ƒâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ˆâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_single_lstm__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2io82qdl\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:10:51.617210: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_single_lstm__binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2rcpddlf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021050-2rcpddlf\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 130   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.7 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 80/81 [00:01<00:00, 45.40it/s, loss=0.701, v_num=ddlf, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.608\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:01<00:00, 44.32it/s, loss=0.701, v_num=ddlf, BTC_val_\n",
      "Epoch 1:  99%|â–‰| 80/81 [00:01<00:00, 43.87it/s, loss=0.717, v_num=ddlf, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:01<00:00, 43.07it/s, loss=0.717, v_num=ddlf, BTC_val_\u001b[A\n",
      "Epoch 2:  99%|â–‰| 80/81 [00:01<00:00, 44.58it/s, loss=0.693, v_num=ddlf, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:01<00:00, 43.73it/s, loss=0.693, v_num=ddlf, BTC_val_\u001b[A\n",
      "Epoch 3:  99%|â–‰| 80/81 [00:01<00:00, 44.17it/s, loss=0.69, v_num=ddlf, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:01<00:00, 43.34it/s, loss=0.69, v_num=ddlf, BTC_val_a\u001b[A\n",
      "Epoch 4:  99%|â–‰| 80/81 [00:01<00:00, 44.54it/s, loss=0.697, v_num=ddlf, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:01<00:00, 43.67it/s, loss=0.697, v_num=ddlf, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 80/81 [00:02<00:00, 38.53it/s, loss=0.697, v_num=ddlf, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:02<00:00, 38.00it/s, loss=0.697, v_num=ddlf, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 80/81 [00:01<00:00, 44.14it/s, loss=0.698, v_num=ddlf, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:01<00:00, 43.35it/s, loss=0.698, v_num=ddlf, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 80/81 [00:01<00:00, 44.46it/s, loss=0.698, v_num=ddlf, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:01<00:00, 43.66it/s, loss=0.698, v_num=ddlf, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 80/81 [00:02<00:00, 38.03it/s, loss=0.698, v_num=ddlf, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:02<00:00, 37.45it/s, loss=0.698, v_num=ddlf, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 80/81 [00:02<00:00, 36.94it/s, loss=0.694, v_num=ddlf, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:02<00:00, 36.43it/s, loss=0.694, v_num=ddlf, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 80/81 [00:02<00:00, 37.72it/s, loss=0.69, v_num=ddlf, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:02<00:00, 37.19it/s, loss=0.69, v_num=ddlf, BTC_val_\u001b[A\n",
      "Epoch 11:  99%|â–‰| 80/81 [00:02<00:00, 39.39it/s, loss=0.692, v_num=ddlf, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:02<00:00, 38.76it/s, loss=0.692, v_num=ddlf, BTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 80/81 [00:01<00:00, 43.88it/s, loss=0.694, v_num=ddlf, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:01<00:00, 43.03it/s, loss=0.694, v_num=ddlf, BTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 80/81 [00:01<00:00, 43.55it/s, loss=0.694, v_num=ddlf, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:01<00:00, 42.81it/s, loss=0.694, v_num=ddlf, BTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 80/81 [00:01<00:00, 43.10it/s, loss=0.692, v_num=ddlf, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:01<00:00, 42.27it/s, loss=0.692, v_num=ddlf, BTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 80/81 [00:02<00:00, 37.69it/s, loss=0.69, v_num=ddlf, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:02<00:00, 37.12it/s, loss=0.69, v_num=ddlf, BTC_val_\u001b[A\n",
      "Epoch 16:  99%|â–‰| 80/81 [00:01<00:00, 41.45it/s, loss=0.694, v_num=ddlf, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:01<00:00, 40.80it/s, loss=0.694, v_num=ddlf, BTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 80/81 [00:02<00:00, 39.48it/s, loss=0.692, v_num=ddlf, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:02<00:00, 38.89it/s, loss=0.692, v_num=ddlf, BTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 80/81 [00:02<00:00, 37.52it/s, loss=0.687, v_num=ddlf, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:02<00:00, 36.85it/s, loss=0.687, v_num=ddlf, BTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 80/81 [00:02<00:00, 38.53it/s, loss=0.694, v_num=ddlf, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:02<00:00, 37.71it/s, loss=0.694, v_num=ddlf, BTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 80/81 [00:01<00:00, 41.28it/s, loss=0.692, v_num=ddlf, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.608. Signaling Trainer to stop.\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:02<00:00, 40.39it/s, loss=0.692, v_num=ddlf, BTC_val\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:02<00:00, 40.28it/s, loss=0.692, v_num=ddlf, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 63.17it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.4193548262119293,\n",
      " 'BTC_test_f1': 0.29533159732818604,\n",
      " 'ETH_test_acc': 0.6129032373428345,\n",
      " 'ETH_test_f1': 0.3793548345565796,\n",
      " 'test_loss': 0.7264933586120605}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 75845\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021050-2rcpddlf/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021050-2rcpddlf/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.27273\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.41818\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.69804\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1680\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 51\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621465901\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.53349\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.37274\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.50355\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.34409\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69267\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.35714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.88889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.47059\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.67662\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.41935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.29533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.37935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.72649\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–†â–†â–â–…â–ƒâ–„â–ƒâ–ˆâ–„â–ƒâ–†â–„â–‡â–ƒâ–…â–ƒâ–â–…â–†â–ƒâ–‚â–†â–ƒâ–…â–ƒâ–‡â–ƒâ–‡â–ƒâ–ƒâ–„â–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–‡â–‡â–â–†â–ƒâ–ƒâ–ƒâ–…â–„â–‚â–ˆâ–ƒâ–ˆâ–„â–ƒâ–‚â–â–ƒâ–ˆâ–‚â–â–†â–‚â–„â–‚â–„â–„â–„â–‚â–‚â–„â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–ƒâ–†â–„â–ƒâ–‚â–†â–ƒâ–â–ˆâ–†â–†â–ƒâ–†â–†â–…â–„â–„â–†â–†â–‚â–…â–„â–„â–†â–…â–‡â–„â–…â–ƒâ–†â–„â–â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–ƒâ–†â–„â–ƒâ–‚â–„â–„â–â–ˆâ–†â–‡â–‚â–‡â–‡â–„â–ƒâ–„â–„â–…â–‚â–„â–„â–ƒâ–„â–ƒâ–„â–„â–ƒâ–ƒâ–„â–ƒâ–â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–…â–â–ˆâ–…â–„â–ƒâ–„â–‚â–ƒâ–ƒâ–ƒâ–„â–‚â–„â–ƒâ–…â–…â–ƒâ–„â–…â–„â–„â–…â–‚â–„â–â–„â–„â–…â–„â–„â–†â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–‚â–„â–‚â–ƒâ–â–‚â–†â–ƒâ–ˆâ–‡â–„â–†â–ƒâ–‡â–†â–†â–…â–†â–‡â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–‡â–ˆâ–‡â–†â–‡â–…â–„â–„â–‡â–†â–…â–ƒâ–„â–‚â–â–‚â–‚â–‚â–„â–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–ˆâ–â–ƒâ–…â–ƒâ–â–†â–‡â–ƒâ–„â–„â–†â–ƒâ–ƒâ–†â–…â–…â–„â–†â–†â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–ˆâ–†â–†â–†â–†â–„â–…â–„â–†â–…â–ƒâ–ƒâ–ƒâ–â–â–â–‚â–‚â–„â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–‡â–ˆâ–†â–„â–…â–ƒâ–‚â–â–ƒâ–â–‚â–â–‚â–‚â–â–â–â–‚â–â–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–â–„â–ƒâ–…â–†â–…â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_single_lstm__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2rcpddlf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:11:51.429283: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_single_lstm__multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2ophbjwc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021149-2ophbjwc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 195   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.8 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:02<00:00, 37.22it/s, loss=1.06, v_num=bjwc, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.007\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:02<00:00, 36.77it/s, loss=1.06, v_num=bjwc, BTC_val_a\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:01<00:00, 40.38it/s, loss=1.02, v_num=bjwc, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:02<00:00, 39.96it/s, loss=1.02, v_num=bjwc, BTC_val_a\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.027 >= min_delta = 0.003. New best score: 0.979\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 42.14it/s, loss=1.02, v_num=bjwc, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:01<00:00, 41.78it/s, loss=1.02, v_num=bjwc, BTC_val_a\u001b[A\n",
      "Epoch 3:  99%|â–‰| 79/80 [00:01<00:00, 41.88it/s, loss=0.96, v_num=bjwc, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:01<00:00, 41.17it/s, loss=0.96, v_num=bjwc, BTC_val_a\u001b[A\n",
      "Epoch 4:  99%|â–‰| 79/80 [00:01<00:00, 41.70it/s, loss=0.947, v_num=bjwc, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:01<00:00, 40.95it/s, loss=0.947, v_num=bjwc, BTC_val_\u001b[A\n",
      "Metric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.971\n",
      "Epoch 5:  99%|â–‰| 79/80 [00:01<00:00, 41.09it/s, loss=0.978, v_num=bjwc, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:01<00:00, 40.06it/s, loss=0.978, v_num=bjwc, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 79/80 [00:02<00:00, 35.28it/s, loss=0.96, v_num=bjwc, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:02<00:00, 34.68it/s, loss=0.96, v_num=bjwc, BTC_val_a\u001b[A\n",
      "Epoch 7:  99%|â–‰| 79/80 [00:01<00:00, 44.70it/s, loss=0.911, v_num=bjwc, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:01<00:00, 43.68it/s, loss=0.911, v_num=bjwc, BTC_val_\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  99%|â–‰| 79/80 [00:02<00:00, 37.31it/s, loss=0.876, v_num=bjwc, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:02<00:00, 36.61it/s, loss=0.876, v_num=bjwc, BTC_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.016 >= min_delta = 0.003. New best score: 0.954\n",
      "Epoch 9:  99%|â–‰| 79/80 [00:02<00:00, 29.87it/s, loss=0.973, v_num=bjwc, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:02<00:00, 29.50it/s, loss=0.973, v_num=bjwc, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 79/80 [00:02<00:00, 37.60it/s, loss=0.93, v_num=bjwc, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:02<00:00, 37.01it/s, loss=0.93, v_num=bjwc, BTC_val_\u001b[A\n",
      "Epoch 11:  99%|â–‰| 79/80 [00:02<00:00, 36.54it/s, loss=0.955, v_num=bjwc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:02<00:00, 36.07it/s, loss=0.955, v_num=bjwc, BTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 79/80 [00:02<00:00, 37.14it/s, loss=0.936, v_num=bjwc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:02<00:00, 36.62it/s, loss=0.936, v_num=bjwc, BTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 79/80 [00:01<00:00, 41.31it/s, loss=0.897, v_num=bjwc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:01<00:00, 40.62it/s, loss=0.897, v_num=bjwc, BTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 79/80 [00:01<00:00, 42.43it/s, loss=0.927, v_num=bjwc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:01<00:00, 41.64it/s, loss=0.927, v_num=bjwc, BTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 79/80 [00:02<00:00, 35.38it/s, loss=0.914, v_num=bjwc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:02<00:00, 34.92it/s, loss=0.914, v_num=bjwc, BTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 79/80 [00:02<00:00, 35.96it/s, loss=0.947, v_num=bjwc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:02<00:00, 35.44it/s, loss=0.947, v_num=bjwc, BTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 79/80 [00:02<00:00, 34.46it/s, loss=0.939, v_num=bjwc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:02<00:00, 33.88it/s, loss=0.939, v_num=bjwc, BTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 79/80 [00:01<00:00, 42.50it/s, loss=0.928, v_num=bjwc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:01<00:00, 41.49it/s, loss=0.928, v_num=bjwc, BTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 79/80 [00:02<00:00, 38.49it/s, loss=0.946, v_num=bjwc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:02<00:00, 37.76it/s, loss=0.946, v_num=bjwc, BTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 79/80 [00:01<00:00, 40.41it/s, loss=0.941, v_num=bjwc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:02<00:00, 39.56it/s, loss=0.941, v_num=bjwc, BTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 79/80 [00:02<00:00, 35.92it/s, loss=0.933, v_num=bjwc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:02<00:00, 35.21it/s, loss=0.933, v_num=bjwc, BTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 79/80 [00:02<00:00, 30.70it/s, loss=0.932, v_num=bjwc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:02<00:00, 30.38it/s, loss=0.932, v_num=bjwc, BTC_val\u001b[A\n",
      "Epoch 23:  99%|â–‰| 79/80 [00:02<00:00, 35.51it/s, loss=0.913, v_num=bjwc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:02<00:00, 35.08it/s, loss=0.913, v_num=bjwc, BTC_val\u001b[A\n",
      "Epoch 24:  99%|â–‰| 79/80 [00:01<00:00, 42.64it/s, loss=0.906, v_num=bjwc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:01<00:00, 41.86it/s, loss=0.906, v_num=bjwc, BTC_val\u001b[A\n",
      "Epoch 25:  99%|â–‰| 79/80 [00:01<00:00, 42.28it/s, loss=0.911, v_num=bjwc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:01<00:00, 41.55it/s, loss=0.911, v_num=bjwc, BTC_val\u001b[A\n",
      "Epoch 26:  99%|â–‰| 79/80 [00:02<00:00, 36.15it/s, loss=0.882, v_num=bjwc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:02<00:00, 35.66it/s, loss=0.882, v_num=bjwc, BTC_val\u001b[A\n",
      "Epoch 27:  99%|â–‰| 79/80 [00:02<00:00, 37.11it/s, loss=0.911, v_num=bjwc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 80/80 [00:02<00:00, 36.55it/s, loss=0.911, v_num=bjwc, BTC_val\u001b[A\n",
      "Epoch 28:  99%|â–‰| 79/80 [00:02<00:00, 38.66it/s, loss=0.922, v_num=bjwc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.954. Signaling Trainer to stop.\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:02<00:00, 37.96it/s, loss=0.922, v_num=bjwc, BTC_val\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:02<00:00, 37.88it/s, loss=0.922, v_num=bjwc, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 63.23it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5806451439857483,\n",
      " 'BTC_test_f1': 0.4767591059207916,\n",
      " 'ETH_test_acc': 0.6129032373428345,\n",
      " 'ETH_test_f1': 0.480448454618454,\n",
      " 'test_loss': 0.8642573356628418}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 76018\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021149-2ophbjwc/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021149-2ophbjwc/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.43275\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.8901\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 28\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2291\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 71\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621465980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.54553\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.46311\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.53523\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.44893\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.90285\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.22222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.98122\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.58065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.47676\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.48045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.86426\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–‚â–„â–ƒâ–…â–…â–„â–ƒâ–†â–…â–…â–ƒâ–„â–…â–„â–…â–…â–„â–ƒâ–‚â–ƒâ–„â–„â–ˆâ–…â–„â–†â–„â–‡â–…â–…â–…â–„â–†â–…â–„â–â–„â–…â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–â–â–â–ƒâ–‚â–ƒâ–ƒâ–„â–„â–ƒâ–‚â–‚â–„â–„â–‚â–„â–„â–â–â–ƒâ–„â–„â–ˆâ–…â–„â–‚â–„â–†â–…â–„â–…â–„â–†â–…â–„â–â–â–„â–ƒâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–â–‚â–‚â–…â–…â–„â–â–ƒâ–„â–…â–„â–„â–…â–„â–…â–‚â–ƒâ–ƒâ–…â–ƒâ–‚â–‡â–‚â–„â–ƒâ–…â–â–…â–…â–‚â–…â–„â–‡â–…â–ƒâ–‚â–ˆâ–…â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–‚â–â–â–ƒâ–‚â–…â–â–ƒâ–ƒâ–‚â–…â–‚â–ƒâ–ƒâ–„â–â–„â–‚â–…â–„â–„â–…â–‚â–…â–ƒâ–„â–ƒâ–„â–„â–‚â–†â–ƒâ–‡â–„â–„â–â–ˆâ–…â–…â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–‡â–†â–†â–…â–ƒâ–…â–‡â–„â–ƒâ–„â–„â–†â–„â–ˆâ–ƒâ–‡â–†â–‡â–…â–†â–†â–â–ƒâ–„â–…â–‚â–…â–â–„â–…â–…â–‡â–‚â–…â–…â–†â–‚â–‡â–…â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–‚â–„â–…â–…â–…â–†â–†â–†â–…â–‡â–†â–…â–†â–†â–…â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–‚â–â–â–ƒâ–„â–…â–…â–†â–†â–…â–‡â–†â–†â–‡â–‡â–†â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–ƒâ–ƒâ–„â–„â–…â–„â–…â–†â–†â–…â–†â–†â–†â–ˆâ–†â–†â–†â–†â–†â–‡â–†â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–â–â–â–‚â–„â–…â–„â–†â–†â–†â–†â–†â–†â–†â–ˆâ–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–‡â–†â–„â–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–†â–†â–ƒâ–â–ƒâ–†â–ƒâ–†â–†â–†â–†â–†â–†â–ˆâ–†â–†â–ƒâ–†â–ƒâ–ƒâ–†â–â–†â–†â–†â–ˆâ–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–‚â–‚â–‚â–â–‚â–…â–„â–ƒâ–‚â–…â–ƒâ–…â–…â–ˆâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–â–ƒâ–…â–ƒâ–…â–…â–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–ˆâ–ˆâ–†â–â–ˆâ–†â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–‚â–‚â–â–‚â–ˆâ–†â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–…â–ƒâ–…â–ˆâ–‚â–‡â–†â–…â–â–ˆâ–„â–ˆâ–ˆâ–…â–†â–†â–†â–…â–‡â–ˆâ–„â–†â–†â–‡â–„â–ƒâ–„â–…â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_single_lstm__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2ophbjwc\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:13:10.181227: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_single_lstm__multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/5es019y0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021308-5es019y0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 195   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.8 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/81 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 80/81 [00:01<00:00, 43.25it/s, loss=1.11, v_num=19y0, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.089\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:01<00:00, 41.85it/s, loss=1.11, v_num=19y0, BTC_val_a\n",
      "Epoch 1:  99%|â–‰| 80/81 [00:01<00:00, 43.59it/s, loss=1.12, v_num=19y0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:01<00:00, 42.50it/s, loss=1.12, v_num=19y0, BTC_val_a\u001b[A\n",
      "Epoch 2:  99%|â–‰| 80/81 [00:01<00:00, 43.77it/s, loss=1.1, v_num=19y0, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:01<00:00, 42.71it/s, loss=1.1, v_num=19y0, BTC_val_ac\u001b[A\n",
      "Epoch 3:  99%|â–‰| 80/81 [00:01<00:00, 43.27it/s, loss=1.11, v_num=19y0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:01<00:00, 42.25it/s, loss=1.11, v_num=19y0, BTC_val_a\u001b[A\n",
      "Epoch 4:  99%|â–‰| 80/81 [00:01<00:00, 45.53it/s, loss=1.1, v_num=19y0, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:01<00:00, 44.27it/s, loss=1.1, v_num=19y0, BTC_val_ac\u001b[A\n",
      "Epoch 5:  99%|â–‰| 80/81 [00:01<00:00, 46.74it/s, loss=1.11, v_num=19y0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:01<00:00, 45.73it/s, loss=1.11, v_num=19y0, BTC_val_a\u001b[A\n",
      "Epoch 6:  99%|â–‰| 80/81 [00:01<00:00, 44.31it/s, loss=1.1, v_num=19y0, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:01<00:00, 43.48it/s, loss=1.1, v_num=19y0, BTC_val_ac\u001b[A\n",
      "Epoch 7:  99%|â–‰| 80/81 [00:01<00:00, 43.79it/s, loss=1.1, v_num=19y0, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:01<00:00, 42.96it/s, loss=1.1, v_num=19y0, BTC_val_ac\u001b[A\n",
      "Epoch 8:  99%|â–‰| 80/81 [00:01<00:00, 41.53it/s, loss=1.09, v_num=19y0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:01<00:00, 40.58it/s, loss=1.09, v_num=19y0, BTC_val_a\u001b[A\n",
      "Epoch 9:  99%|â–‰| 80/81 [00:01<00:00, 43.60it/s, loss=1.11, v_num=19y0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:01<00:00, 42.82it/s, loss=1.11, v_num=19y0, BTC_val_a\u001b[A\n",
      "Epoch 10:  99%|â–‰| 80/81 [00:01<00:00, 42.42it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:01<00:00, 41.73it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n",
      "Epoch 11:  99%|â–‰| 80/81 [00:02<00:00, 38.42it/s, loss=1.09, v_num=19y0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:02<00:00, 37.90it/s, loss=1.09, v_num=19y0, BTC_val_\u001b[A\n",
      "Epoch 12:  99%|â–‰| 80/81 [00:01<00:00, 44.33it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:01<00:00, 43.51it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n",
      "Epoch 13:  99%|â–‰| 80/81 [00:01<00:00, 44.08it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:01<00:00, 43.29it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n",
      "Epoch 14:  99%|â–‰| 80/81 [00:01<00:00, 43.22it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:01<00:00, 42.46it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n",
      "Epoch 15:  99%|â–‰| 80/81 [00:01<00:00, 44.48it/s, loss=1.11, v_num=19y0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:01<00:00, 43.57it/s, loss=1.11, v_num=19y0, BTC_val_\u001b[A\n",
      "Epoch 16:  99%|â–‰| 80/81 [00:01<00:00, 44.29it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:01<00:00, 43.38it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n",
      "Epoch 17:  99%|â–‰| 80/81 [00:02<00:00, 37.60it/s, loss=1.09, v_num=19y0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:02<00:00, 37.07it/s, loss=1.09, v_num=19y0, BTC_val_\u001b[A\n",
      "Epoch 18:  99%|â–‰| 80/81 [00:01<00:00, 41.54it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:01<00:00, 40.71it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n",
      "Epoch 19:  99%|â–‰| 80/81 [00:02<00:00, 37.46it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:02<00:00, 36.96it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n",
      "Epoch 20:  99%|â–‰| 80/81 [00:01<00:00, 41.65it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.089. Signaling Trainer to stop.\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:01<00:00, 40.98it/s, loss=1.1, v_num=19y0, BTC_val_a\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:01<00:00, 40.91it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 81.81it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.25806450843811035,\n",
      " 'BTC_test_f1': 0.1367289274930954,\n",
      " 'ETH_test_acc': 0.4193548262119293,\n",
      " 'ETH_test_f1': 0.19648092985153198,\n",
      " 'test_loss': 1.1192375421524048}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 76228\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021308-5es019y0/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021308-5es019y0/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.30159\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.1875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.16931\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.11127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1680\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621466038\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.35225\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.24898\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.35619\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.25848\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09563\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.2381\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.10094\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.25806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.13673\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.41935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.11924\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–‚â–…â–„â–ƒâ–…â–„â–„â–‡â–ƒâ–ƒâ–†â–„â–ƒâ–„â–„â–â–…â–ƒâ–ˆâ–„â–ƒâ–„â–ƒâ–‡â–„â–ƒâ–„â–„â–â–â–ƒâ–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–‚â–†â–„â–„â–‡â–„â–…â–ˆâ–„â–…â–†â–†â–‚â–‚â–„â–â–ƒâ–ƒâ–ˆâ–…â–ƒâ–„â–ƒâ–„â–„â–…â–„â–„â–‚â–‚â–ƒâ–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–…â–…â–‚â–…â–…â–ƒâ–„â–ƒâ–‚â–ƒâ–†â–‡â–…â–ƒâ–…â–…â–ˆâ–„â–…â–„â–„â–…â–†â–‡â–„â–ƒâ–…â–„â–‚â–ƒâ–â–‡â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–†â–†â–‚â–…â–…â–ƒâ–…â–„â–ƒâ–„â–†â–ˆâ–„â–‚â–…â–„â–„â–„â–„â–„â–„â–…â–†â–„â–„â–‚â–…â–„â–‚â–„â–â–‡â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–ƒâ–ƒâ–†â–‚â–ƒâ–„â–„â–‚â–ƒâ–…â–â–‚â–ƒâ–„â–‚â–ˆâ–‚â–ƒâ–â–ƒâ–„â–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–„â–„â–…â–„â–ƒâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–ƒâ–‚â–ƒâ–â–…â–„â–‚â–â–…â–ƒâ–†â–…â–‡â–…â–ˆâ–…â–„â–…â–‚â–‡â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–†â–…â–ˆâ–…â–ˆâ–†â–‡â–…â–â–„â–…â–…â–…â–„â–†â–„â–ƒâ–…â–‚â–…â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–„â–‚â–…â–‡â–„â–„â–ƒâ–†â–â–‚â–„â–†â–ƒâ–…â–‡â–†â–ˆâ–ƒâ–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–ƒâ–ƒâ–…â–†â–ˆâ–…â–†â–ƒâ–â–‚â–‚â–ƒâ–„â–‚â–ƒâ–„â–„â–…â–‚â–„â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–†â–„â–ƒâ–‚â–‚â–ƒâ–‚â–â–‚â–‚â–â–â–â–â–â–â–â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–ˆâ–â–â–â–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–‡â–â–â–â–‡â–ˆâ–…â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–â–ˆâ–‡â–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–ˆâ–â–â–â–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–†â–â–â–â–†â–†â–…â–†â–†â–†â–†â–†â–†â–†â–†â–ˆâ–†â–†â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–â–…â–„â–ˆâ–‚â–‚â–‚â–â–‚â–ƒâ–‚â–‚â–„â–…â–„â–…â–‚â–‚â–‚â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_single_lstm__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/5es019y0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:14:16.552940: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm__binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3i7gs46x\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021415-3i7gs46x\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 79/80 [00:03<00:00, 21.36it/s, loss=0.665, v_num=s46x, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.644\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:03<00:00, 21.21it/s, loss=0.665, v_num=s46x, BTC_val_\n",
      "Epoch 1:  99%|â–‰| 79/80 [00:03<00:00, 21.86it/s, loss=0.584, v_num=s46x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.109 >= min_delta = 0.003. New best score: 0.535\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:03<00:00, 21.68it/s, loss=0.584, v_num=s46x, BTC_val_\n",
      "Epoch 2:  99%|â–‰| 79/80 [00:04<00:00, 19.70it/s, loss=0.607, v_num=s46x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.015 >= min_delta = 0.003. New best score: 0.520\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:04<00:00, 19.59it/s, loss=0.607, v_num=s46x, BTC_val_\n",
      "Epoch 3:  99%|â–‰| 79/80 [00:03<00:00, 20.76it/s, loss=0.579, v_num=s46x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:03<00:00, 20.61it/s, loss=0.579, v_num=s46x, BTC_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.035 >= min_delta = 0.003. New best score: 0.484\n",
      "Epoch 4:  99%|â–‰| 79/80 [00:03<00:00, 20.84it/s, loss=0.623, v_num=s46x, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:03<00:00, 20.67it/s, loss=0.623, v_num=s46x, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 79/80 [00:03<00:00, 21.69it/s, loss=0.588, v_num=s46x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆ| 80/80 [00:03<00:00, 21.54it/s, loss=0.588, v_num=s46x, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 79/80 [00:03<00:00, 20.30it/s, loss=0.567, v_num=s46x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:03<00:00, 20.20it/s, loss=0.567, v_num=s46x, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 79/80 [00:04<00:00, 18.77it/s, loss=0.583, v_num=s46x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:04<00:00, 18.68it/s, loss=0.583, v_num=s46x, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 79/80 [00:04<00:00, 19.30it/s, loss=0.585, v_num=s46x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:04<00:00, 19.21it/s, loss=0.585, v_num=s46x, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 79/80 [00:04<00:00, 16.23it/s, loss=0.577, v_num=s46x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:04<00:00, 16.23it/s, loss=0.577, v_num=s46x, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 79/80 [00:03<00:00, 20.56it/s, loss=0.574, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:03<00:00, 20.47it/s, loss=0.574, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 79/80 [00:03<00:00, 21.50it/s, loss=0.564, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:03<00:00, 21.36it/s, loss=0.564, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 79/80 [00:04<00:00, 16.38it/s, loss=0.594, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:04<00:00, 16.36it/s, loss=0.594, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 79/80 [00:05<00:00, 14.56it/s, loss=0.561, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:05<00:00, 14.56it/s, loss=0.561, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 79/80 [00:04<00:00, 18.47it/s, loss=0.554, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:04<00:00, 18.43it/s, loss=0.554, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 79/80 [00:03<00:00, 21.01it/s, loss=0.589, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:03<00:00, 20.92it/s, loss=0.589, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 79/80 [00:03<00:00, 21.27it/s, loss=0.544, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:03<00:00, 21.14it/s, loss=0.544, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 79/80 [00:03<00:00, 20.16it/s, loss=0.588, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:03<00:00, 20.10it/s, loss=0.588, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 79/80 [00:03<00:00, 20.73it/s, loss=0.573, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:03<00:00, 20.63it/s, loss=0.573, v_num=s46x, BTC_val\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.476\n",
      "Epoch 19:  99%|â–‰| 79/80 [00:03<00:00, 20.30it/s, loss=0.563, v_num=s46x, BTC_val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:03<00:00, 20.20it/s, loss=0.563, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 79/80 [00:03<00:00, 20.80it/s, loss=0.56, v_num=s46x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:03<00:00, 20.72it/s, loss=0.56, v_num=s46x, BTC_val_\u001b[A\n",
      "Epoch 21:  99%|â–‰| 79/80 [00:03<00:00, 21.02it/s, loss=0.571, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:03<00:00, 20.87it/s, loss=0.571, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 79/80 [00:03<00:00, 21.26it/s, loss=0.569, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:03<00:00, 21.10it/s, loss=0.569, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 23:  99%|â–‰| 79/80 [00:04<00:00, 17.25it/s, loss=0.547, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:04<00:00, 17.21it/s, loss=0.547, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 24:  99%|â–‰| 79/80 [00:04<00:00, 17.67it/s, loss=0.557, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 80/80 [00:04<00:00, 17.62it/s, loss=0.557, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 25:  99%|â–‰| 79/80 [00:03<00:00, 20.04it/s, loss=0.583, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 80/80 [00:04<00:00, 19.95it/s, loss=0.583, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 26:  99%|â–‰| 79/80 [00:03<00:00, 20.59it/s, loss=0.627, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 80/80 [00:03<00:00, 20.51it/s, loss=0.627, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 27:  99%|â–‰| 79/80 [00:04<00:00, 19.37it/s, loss=0.543, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 80/80 [00:04<00:00, 19.29it/s, loss=0.543, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 28:  99%|â–‰| 79/80 [00:03<00:00, 20.91it/s, loss=0.579, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 80/80 [00:03<00:00, 20.81it/s, loss=0.579, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 29:  99%|â–‰| 79/80 [00:03<00:00, 20.68it/s, loss=0.514, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 80/80 [00:03<00:00, 20.55it/s, loss=0.514, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 30:  99%|â–‰| 79/80 [00:03<00:00, 20.49it/s, loss=0.61, v_num=s46x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 80/80 [00:03<00:00, 20.38it/s, loss=0.61, v_num=s46x, BTC_val_\u001b[A\n",
      "Epoch 31:  99%|â–‰| 79/80 [00:03<00:00, 20.65it/s, loss=0.546, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 80/80 [00:03<00:00, 20.58it/s, loss=0.546, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 32:  99%|â–‰| 79/80 [00:03<00:00, 20.59it/s, loss=0.57, v_num=s46x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 80/80 [00:03<00:00, 20.49it/s, loss=0.57, v_num=s46x, BTC_val_\u001b[A\n",
      "Epoch 33:  99%|â–‰| 79/80 [00:03<00:00, 20.59it/s, loss=0.561, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 80/80 [00:03<00:00, 20.50it/s, loss=0.561, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 34:  99%|â–‰| 79/80 [00:04<00:00, 19.12it/s, loss=0.537, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 80/80 [00:04<00:00, 19.04it/s, loss=0.537, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 35:  99%|â–‰| 79/80 [00:04<00:00, 17.79it/s, loss=0.53, v_num=s46x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 80/80 [00:04<00:00, 17.69it/s, loss=0.53, v_num=s46x, BTC_val_\u001b[A\n",
      "Epoch 36:  99%|â–‰| 79/80 [00:04<00:00, 16.80it/s, loss=0.551, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 80/80 [00:04<00:00, 16.74it/s, loss=0.551, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 37:  99%|â–‰| 79/80 [00:04<00:00, 17.94it/s, loss=0.518, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|â–ˆ| 80/80 [00:04<00:00, 17.88it/s, loss=0.518, v_num=s46x, BTC_val\u001b[A\n",
      "Epoch 38:  99%|â–‰| 79/80 [00:04<00:00, 16.01it/s, loss=0.548, v_num=s46x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|â–ˆ| 80/80 [00:05<00:00, 15.97it/s, loss=0.548, v_num=s46x, BTC_valMonitored metric val_loss did not improve in the last 20 records. Best score: 0.476. Signaling Trainer to stop.\n",
      "\n",
      "Epoch 38: 100%|â–ˆ| 80/80 [00:05<00:00, 15.95it/s, loss=0.548, v_num=s46x, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 40.45it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6774193644523621,\n",
      " 'BTC_test_f1': 0.6566065549850464,\n",
      " 'ETH_test_acc': 0.7096773982048035,\n",
      " 'ETH_test_f1': 0.7084689140319824,\n",
      " 'test_loss': 0.5707361102104187}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 76455\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021415-3i7gs46x/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021415-3i7gs46x/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.6875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.67611\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.6875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.65368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.58472\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 38\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3081\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 170\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621466225\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 139\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.73793\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.72296\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.74584\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.7306\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.52697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.64935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.88889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.88889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.50989\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.67742\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.65661\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.70968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.70847\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.57074\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–â–…â–…â–†â–…â–„â–†â–ˆâ–…â–ƒâ–„â–ƒâ–†â–„â–ƒâ–„â–„â–ƒâ–‚â–ƒâ–…â–„â–†â–„â–ƒâ–ƒâ–†â–ƒâ–‚â–…â–†â–…â–ƒâ–…â–…â–…â–…â–„â–‡â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–â–…â–…â–‡â–…â–…â–†â–ˆâ–…â–„â–…â–ƒâ–†â–…â–ƒâ–…â–…â–„â–ƒâ–„â–…â–…â–†â–…â–„â–„â–†â–„â–ƒâ–…â–‡â–…â–„â–…â–…â–…â–…â–…â–‡â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–â–…â–…â–‡â–…â–…â–†â–‡â–‡â–…â–„â–ƒâ–…â–…â–…â–„â–ˆâ–„â–‡â–„â–…â–†â–†â–…â–†â–…â–ˆâ–…â–†â–…â–†â–‡â–„â–…â–…â–‡â–ƒâ–…â–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–â–…â–…â–‡â–…â–…â–†â–‡â–‡â–„â–„â–ƒâ–…â–…â–…â–„â–ˆâ–„â–‡â–„â–„â–†â–†â–…â–†â–„â–ˆâ–…â–†â–…â–†â–‡â–„â–…â–…â–†â–ƒâ–…â–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–‡â–†â–ƒâ–‚â–…â–…â–‚â–â–‚â–†â–†â–†â–ƒâ–…â–‡â–…â–„â–‡â–„â–†â–„â–„â–„â–†â–…â–ƒâ–â–ƒâ–ˆâ–†â–â–‚â–†â–„â–ƒâ–‚â–…â–ƒâ–‚â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–…â–†â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–â–…â–†â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–â–…â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–…â–ˆâ–…â–ˆâ–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–…â–…â–ˆâ–…â–ˆâ–ˆâ–ˆâ–…â–…â–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–…â–…â–…â–…â–ˆâ–â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–„â–ˆâ–„â–ˆâ–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–…â–…â–ˆâ–…â–ˆâ–ˆâ–ˆâ–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–ˆâ–â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–ƒâ–†â–…â–†â–†â–†â–â–†â–ƒâ–ƒâ–â–â–â–â–â–â–â–â–†â–†â–â–ƒâ–ƒâ–ƒâ–ˆâ–ƒâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–…â–…â–â–â–ƒâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–ƒâ–†â–…â–†â–†â–†â–â–†â–ƒâ–ƒâ–â–â–â–â–â–â–â–â–†â–†â–â–ƒâ–ƒâ–ƒâ–ˆâ–ƒâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–…â–…â–â–â–ƒâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–ƒâ–ƒâ–â–ƒâ–‚â–ƒâ–ƒâ–„â–‚â–‚â–„â–…â–ƒâ–†â–‚â–‚â–ƒâ–â–ƒâ–ƒâ–‚â–ƒâ–â–‚â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‡â–ƒâ–„â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3i7gs46x\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:17:20.369594: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm__binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/354h8i92\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021718-354h8i92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 80/81 [00:03<00:00, 22.19it/s, loss=0.703, v_num=8i92, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.679\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:03<00:00, 22.00it/s, loss=0.703, v_num=8i92, BTC_val_\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:04<00:00, 17.35it/s, loss=0.699, v_num=8i92, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:04<00:00, 17.28it/s, loss=0.699, v_num=8i92, BTC_val_\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:04<00:00, 18.28it/s, loss=0.698, v_num=8i92, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:04<00:00, 18.20it/s, loss=0.698, v_num=8i92, BTC_val_\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:04<00:00, 16.92it/s, loss=0.694, v_num=8i92, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:04<00:00, 16.84it/s, loss=0.694, v_num=8i92, BTC_val_\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:03<00:00, 20.32it/s, loss=0.697, v_num=8i92, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:04<00:00, 20.20it/s, loss=0.697, v_num=8i92, BTC_val_\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:04<00:00, 17.41it/s, loss=0.692, v_num=8i92, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:04<00:00, 17.34it/s, loss=0.692, v_num=8i92, BTC_val_\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:04<00:00, 18.16it/s, loss=0.695, v_num=8i92, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:04<00:00, 18.08it/s, loss=0.695, v_num=8i92, BTC_val_\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:05<00:00, 13.88it/s, loss=0.693, v_num=8i92, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:05<00:00, 13.83it/s, loss=0.693, v_num=8i92, BTC_val_\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆ| 81/81 [00:04<00:00, 17.77it/s, loss=0.696, v_num=8i92, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:04<00:00, 17.68it/s, loss=0.696, v_num=8i92, BTC_val_\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:04<00:00, 16.22it/s, loss=0.694, v_num=8i92, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:05<00:00, 16.16it/s, loss=0.694, v_num=8i92, BTC_val_\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:04<00:00, 16.23it/s, loss=0.691, v_num=8i92, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:05<00:00, 16.15it/s, loss=0.691, v_num=8i92, BTC_val\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:04<00:00, 18.46it/s, loss=0.692, v_num=8i92, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:04<00:00, 18.36it/s, loss=0.692, v_num=8i92, BTC_val\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:04<00:00, 16.64it/s, loss=0.697, v_num=8i92, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:04<00:00, 16.57it/s, loss=0.697, v_num=8i92, BTC_val\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:03<00:00, 20.85it/s, loss=0.695, v_num=8i92, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:03<00:00, 20.74it/s, loss=0.695, v_num=8i92, BTC_val\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:04<00:00, 19.87it/s, loss=0.698, v_num=8i92, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:04<00:00, 19.77it/s, loss=0.698, v_num=8i92, BTC_val\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:04<00:00, 18.57it/s, loss=0.696, v_num=8i92, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:04<00:00, 18.48it/s, loss=0.696, v_num=8i92, BTC_val\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:04<00:00, 18.14it/s, loss=0.69, v_num=8i92, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:04<00:00, 18.05it/s, loss=0.69, v_num=8i92, BTC_val_\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:04<00:00, 17.75it/s, loss=0.693, v_num=8i92, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:04<00:00, 17.65it/s, loss=0.693, v_num=8i92, BTC_val\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:04<00:00, 18.39it/s, loss=0.695, v_num=8i92, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:04<00:00, 18.30it/s, loss=0.695, v_num=8i92, BTC_val\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:04<00:00, 19.20it/s, loss=0.691, v_num=8i92, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:04<00:00, 19.11it/s, loss=0.691, v_num=8i92, BTC_val\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:04<00:00, 16.35it/s, loss=0.682, v_num=8i92, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:04<00:00, 16.27it/s, loss=0.682, v_num=8i92, BTC_val\u001b[A\n",
      "                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.679. Signaling Trainer to stop.\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:04<00:00, 16.21it/s, loss=0.682, v_num=8i92, BTC_val\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 41.77it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.4193548262119293,\n",
      " 'BTC_test_f1': 0.29533159732818604,\n",
      " 'ETH_test_acc': 0.6129032373428345,\n",
      " 'ETH_test_f1': 0.3793548345565796,\n",
      " 'test_loss': 0.6934098601341248}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 76857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021718-354h8i92/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021718-354h8i92/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.46667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.56364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.68429\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1680\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621466343\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.53664\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.45362\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.5264\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.45228\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69115\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.35714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.88889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.47059\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.67809\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.41935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.29533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.37935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.69341\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–ƒâ–†â–‡â–„â–„â–…â–ƒâ–â–ƒâ–…â–„â–…â–ƒâ–„â–‚â–ƒâ–‚â–„â–‚â–†â–…â–†â–…â–ƒâ–…â–„â–„â–„â–„â–‚â–…â–ˆâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–ƒâ–‡â–ˆâ–…â–…â–†â–„â–â–‚â–†â–…â–†â–‚â–„â–‚â–„â–ƒâ–„â–‚â–„â–†â–†â–†â–ƒâ–„â–„â–„â–ƒâ–ƒâ–‚â–…â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–†â–…â–†â–…â–„â–ƒâ–†â–„â–ƒâ–†â–ƒâ–…â–†â–„â–ƒâ–†â–„â–†â–ƒâ–…â–‡â–ƒâ–†â–â–…â–‚â–„â–„â–‡â–„â–ˆâ–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–ˆâ–†â–ˆâ–†â–ƒâ–„â–ˆâ–ƒâ–ƒâ–ˆâ–„â–†â–„â–…â–ƒâ–‡â–…â–ˆâ–„â–„â–ˆâ–„â–‡â–â–„â–‚â–„â–„â–…â–ƒâ–…â–„â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–„â–‚â–‚â–ƒâ–‚â–…â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–ƒâ–â–ƒâ–…â–‚â–ƒâ–‡â–…â–ƒâ–„â–†â–†â–‡â–ƒâ–†â–†â–‡â–ˆâ–„â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–ˆâ–…â–…â–ˆâ–†â–†â–†â–ˆâ–„â–…â–‡â–„â–„â–‡â–„â–„â–‡â–…â–â–…â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–†â–â–‚â–‚â–„â–â–†â–†â–„â–‡â–ƒâ–ˆâ–‡â–„â–„â–‚â–…â–†â–ƒâ–ƒâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–ˆâ–…â–†â–†â–‡â–…â–†â–‡â–…â–…â–…â–…â–„â–†â–ƒâ–ƒâ–…â–ƒâ–â–ƒâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–†â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–ˆâ–â–„â–„â–„â–„â–„â–ˆâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–ˆâ–â–…â–…â–…â–…â–…â–ˆâ–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–ˆâ–‚â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–ˆâ–ƒâ–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–â–„â–ˆâ–ˆâ–†â–‡â–ˆâ–‚â–†â–†â–„â–…â–†â–…â–†â–…â–„â–†â–†â–…â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/354h8i92\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:19:24.416353: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm__multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1o2nrlyp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021922-1o2nrlyp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:03<00:00, 21.85it/s, loss=1.05, v_num=rlyp, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.986\n",
      "Epoch 0: 100%|â–ˆ| 80/80 [00:03<00:00, 21.71it/s, loss=1.05, v_num=rlyp, BTC_val_a\n",
      "Epoch 1:  99%|â–‰| 79/80 [00:03<00:00, 20.65it/s, loss=1.02, v_num=rlyp, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 80/80 [00:03<00:00, 20.51it/s, loss=1.02, v_num=rlyp, BTC_val_a\u001b[A\n",
      "Metric val_loss improved by 0.038 >= min_delta = 0.003. New best score: 0.948\n",
      "Epoch 2:  99%|â–‰| 79/80 [00:03<00:00, 21.51it/s, loss=1.05, v_num=rlyp, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 80/80 [00:03<00:00, 21.34it/s, loss=1.05, v_num=rlyp, BTC_val_a\u001b[A\n",
      "Epoch 3:  99%|â–‰| 79/80 [00:03<00:00, 21.99it/s, loss=0.994, v_num=rlyp, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 80/80 [00:03<00:00, 21.86it/s, loss=0.994, v_num=rlyp, BTC_val_\u001b[A\n",
      "Metric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 0.937\n",
      "Epoch 4:  99%|â–‰| 79/80 [00:04<00:00, 18.25it/s, loss=1, v_num=rlyp, BTC_val_acc=\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 80/80 [00:04<00:00, 18.16it/s, loss=1, v_num=rlyp, BTC_val_acc=\u001b[A\n",
      "Epoch 5:  99%|â–‰| 79/80 [00:03<00:00, 20.47it/s, loss=1.07, v_num=rlyp, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 80/80 [00:03<00:00, 20.34it/s, loss=1.07, v_num=rlyp, BTC_val_a\u001b[A\n",
      "Epoch 6:  99%|â–‰| 79/80 [00:03<00:00, 20.77it/s, loss=1.03, v_num=rlyp, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 80/80 [00:03<00:00, 20.64it/s, loss=1.03, v_num=rlyp, BTC_val_a\u001b[A\n",
      "Epoch 7:  99%|â–‰| 79/80 [00:04<00:00, 17.63it/s, loss=1.03, v_num=rlyp, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 80/80 [00:04<00:00, 17.56it/s, loss=1.03, v_num=rlyp, BTC_val_a\u001b[A\n",
      "Epoch 8:  99%|â–‰| 79/80 [00:03<00:00, 20.25it/s, loss=1.02, v_num=rlyp, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 80/80 [00:03<00:00, 20.08it/s, loss=1.02, v_num=rlyp, BTC_val_a\u001b[A\n",
      "Epoch 9:  99%|â–‰| 79/80 [00:04<00:00, 16.64it/s, loss=1.04, v_num=rlyp, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 80/80 [00:04<00:00, 16.62it/s, loss=1.04, v_num=rlyp, BTC_val_a\u001b[A\n",
      "Epoch 10:  99%|â–‰| 79/80 [00:04<00:00, 19.73it/s, loss=1.02, v_num=rlyp, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 80/80 [00:04<00:00, 19.65it/s, loss=1.02, v_num=rlyp, BTC_val_\u001b[A\n",
      "Epoch 11:  99%|â–‰| 79/80 [00:04<00:00, 16.51it/s, loss=1.05, v_num=rlyp, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 80/80 [00:04<00:00, 16.44it/s, loss=1.05, v_num=rlyp, BTC_val_\u001b[A\n",
      "Epoch 12:  99%|â–‰| 79/80 [00:03<00:00, 19.97it/s, loss=1.04, v_num=rlyp, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 80/80 [00:04<00:00, 19.80it/s, loss=1.04, v_num=rlyp, BTC_val_\u001b[A\n",
      "Epoch 13:  99%|â–‰| 79/80 [00:03<00:00, 19.88it/s, loss=1.03, v_num=rlyp, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 80/80 [00:04<00:00, 19.76it/s, loss=1.03, v_num=rlyp, BTC_val_\u001b[A\n",
      "Epoch 14:  99%|â–‰| 79/80 [00:04<00:00, 16.79it/s, loss=1.06, v_num=rlyp, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 80/80 [00:04<00:00, 16.77it/s, loss=1.06, v_num=rlyp, BTC_val_\u001b[A\n",
      "Epoch 15:  99%|â–‰| 79/80 [00:03<00:00, 19.77it/s, loss=1.03, v_num=rlyp, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 80/80 [00:04<00:00, 19.68it/s, loss=1.03, v_num=rlyp, BTC_val_\u001b[A\n",
      "Epoch 16:  99%|â–‰| 79/80 [00:04<00:00, 19.21it/s, loss=1.03, v_num=rlyp, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 80/80 [00:04<00:00, 19.15it/s, loss=1.03, v_num=rlyp, BTC_val_\u001b[A\n",
      "Epoch 17:  99%|â–‰| 79/80 [00:03<00:00, 20.21it/s, loss=1.02, v_num=rlyp, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 80/80 [00:03<00:00, 20.11it/s, loss=1.02, v_num=rlyp, BTC_val_\u001b[A\n",
      "Epoch 18:  99%|â–‰| 79/80 [00:05<00:00, 15.58it/s, loss=0.989, v_num=rlyp, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 80/80 [00:05<00:00, 15.54it/s, loss=0.989, v_num=rlyp, BTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 79/80 [00:04<00:00, 17.77it/s, loss=0.967, v_num=rlyp, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 80/80 [00:04<00:00, 17.75it/s, loss=0.967, v_num=rlyp, BTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 79/80 [00:04<00:00, 18.51it/s, loss=0.958, v_num=rlyp, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 80/80 [00:04<00:00, 18.46it/s, loss=0.958, v_num=rlyp, BTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 79/80 [00:04<00:00, 17.40it/s, loss=0.97, v_num=rlyp, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 80/80 [00:04<00:00, 17.33it/s, loss=0.97, v_num=rlyp, BTC_val_\u001b[A\n",
      "Epoch 22:  99%|â–‰| 79/80 [00:03<00:00, 20.46it/s, loss=0.963, v_num=rlyp, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 80/80 [00:03<00:00, 20.33it/s, loss=0.963, v_num=rlyp, BTC_val\u001b[A\n",
      "Epoch 23:  99%|â–‰| 79/80 [00:04<00:00, 18.43it/s, loss=0.952, v_num=rlyp, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.937. Signaling Trainer to stop.\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:04<00:00, 18.35it/s, loss=0.952, v_num=rlyp, BTC_val\n",
      "Epoch 23: 100%|â–ˆ| 80/80 [00:04<00:00, 18.33it/s, loss=0.952, v_num=rlyp, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 37.37it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5483871102333069,\n",
      " 'BTC_test_f1': 0.23566308617591858,\n",
      " 'ETH_test_acc': 0.4838709533214569,\n",
      " 'ETH_test_f1': 0.2169238030910492,\n",
      " 'test_loss': 1.00882887840271}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 77173\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021922-1o2nrlyp/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021922-1o2nrlyp/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.30736\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.99903\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 23\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1896\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621466473\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 85\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.4996\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.32438\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.48852\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.32182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.94819\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.20513\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.03617\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.54839\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.23566\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.48387\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.21692\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.00883\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–…â–ˆâ–…â–…â–†â–ˆâ–‚â–ˆâ–ƒâ–†â–†â–…â–‡â–‚â–‡â–…â–†â–…â–…â–…â–†â–„â–†â–†â–…â–…â–†â–†â–â–…â–…â–…â–†â–ˆâ–‡â–‡â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–„â–…â–„â–ƒâ–†â–…â–‚â–…â–‚â–„â–„â–ƒâ–„â–‚â–„â–„â–„â–„â–ƒâ–„â–„â–ƒâ–„â–„â–„â–„â–„â–„â–â–„â–„â–ƒâ–„â–…â–ˆâ–ˆâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–…â–†â–†â–ƒâ–â–†â–ƒâ–†â–ƒâ–‡â–†â–ƒâ–ˆâ–„â–…â–…â–†â–…â–†â–…â–…â–‚â–†â–ƒâ–…â–…â–†â–„â–…â–„â–„â–ƒâ–†â–ƒâ–†â–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–†â–„â–…â–ƒâ–â–„â–‚â–„â–ƒâ–…â–„â–‚â–ˆâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–‚â–„â–ƒâ–„â–„â–„â–ƒâ–„â–ƒâ–ƒâ–‚â–†â–ƒâ–ˆâ–‡â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–…â–ƒâ–„â–†â–†â–„â–‡â–‚â–†â–ƒâ–‡â–‡â–‚â–†â–„â–†â–…â–†â–…â–†â–…â–ˆâ–…â–„â–…â–…â–…â–…â–…â–ƒâ–„â–†â–„â–â–‚â–â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–„â–‡â–†â–‡â–‡â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–…â–‡â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–„â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–ƒâ–„â–„â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–…â–†â–†â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–ƒâ–ƒâ–â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–ƒâ–„â–„â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–‡â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–…â–„â–‚â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–ˆâ–â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–„â–‚â–ƒâ–â–‚â–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–†â–„â–ˆâ–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1o2nrlyp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:21:28.408666: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm__multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3rnafum3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022126-3rnafum3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/81 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:03<00:00, 22.26it/s, loss=1.12, v_num=fum3, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.125\n",
      "Epoch 0: 100%|â–ˆ| 81/81 [00:03<00:00, 22.12it/s, loss=1.12, v_num=fum3, BTC_val_a\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:04<00:00, 17.87it/s, loss=1.11, v_num=fum3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.049 >= min_delta = 0.003. New best score: 1.077\n",
      "Epoch 1: 100%|â–ˆ| 81/81 [00:04<00:00, 17.80it/s, loss=1.11, v_num=fum3, BTC_val_a\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:04<00:00, 17.23it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 81/81 [00:04<00:00, 17.03it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:04<00:00, 19.41it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 81/81 [00:04<00:00, 19.30it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:03<00:00, 21.29it/s, loss=1.11, v_num=fum3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 81/81 [00:03<00:00, 21.17it/s, loss=1.11, v_num=fum3, BTC_val_a\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:04<00:00, 19.26it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 81/81 [00:04<00:00, 19.15it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:04<00:00, 16.36it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 81/81 [00:04<00:00, 16.29it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:03<00:00, 20.29it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 81/81 [00:04<00:00, 20.18it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:03<00:00, 21.15it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 81/81 [00:03<00:00, 21.05it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:03<00:00, 21.86it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 81/81 [00:03<00:00, 21.72it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:04<00:00, 17.14it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 81/81 [00:04<00:00, 17.06it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:04<00:00, 17.30it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 81/81 [00:04<00:00, 17.19it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:04<00:00, 16.60it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 81/81 [00:04<00:00, 16.53it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:04<00:00, 18.55it/s, loss=1.09, v_num=fum3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 81/81 [00:04<00:00, 18.47it/s, loss=1.09, v_num=fum3, BTC_val_\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:04<00:00, 18.86it/s, loss=1.11, v_num=fum3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 81/81 [00:04<00:00, 18.76it/s, loss=1.11, v_num=fum3, BTC_val_\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:04<00:00, 19.31it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 81/81 [00:04<00:00, 19.20it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:05<00:00, 16.01it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 81/81 [00:05<00:00, 15.94it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:05<00:00, 15.99it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 81/81 [00:05<00:00, 15.86it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:04<00:00, 18.90it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 81/81 [00:04<00:00, 18.80it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:04<00:00, 17.94it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 81/81 [00:04<00:00, 17.86it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:04<00:00, 18.84it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 81/81 [00:04<00:00, 18.75it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:04<00:00, 19.53it/s, loss=1.09, v_num=fum3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:04<00:00, 19.43it/s, loss=1.09, v_num=fum3, BTC_val_\u001b[A\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 1.077. Signaling Trainer to stop.\n",
      "Epoch 21: 100%|â–ˆ| 81/81 [00:04<00:00, 19.41it/s, loss=1.09, v_num=fum3, BTC_val_\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 47.54it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.25806450843811035,\n",
      " 'BTC_test_f1': 0.1367289274930954,\n",
      " 'ETH_test_acc': 0.4193548262119293,\n",
      " 'ETH_test_f1': 0.19648092985153198,\n",
      " 'test_loss': 1.105910301208496}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 77464\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022126-3rnafum3/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022126-3rnafum3/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.11429\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.1257\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1760\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 106\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621466592\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 79\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.3751\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.27507\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.35382\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.26463\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09458\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.2381\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.08587\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.25806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.13673\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.41935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.10591\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–…â–„â–†â–„â–ƒâ–†â–…â–…â–ƒâ–„â–…â–…â–…â–„â–…â–ˆâ–ƒâ–…â–„â–„â–„â–…â–…â–â–„â–ƒâ–…â–„â–†â–„â–„â–â–…â–…â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–„â–„â–‡â–„â–„â–‡â–„â–…â–„â–ƒâ–†â–†â–…â–„â–…â–ˆâ–ƒâ–…â–…â–ƒâ–„â–…â–…â–â–ƒâ–„â–…â–„â–„â–…â–„â–â–ƒâ–…â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–„â–„â–…â–„â–…â–„â–„â–ƒâ–…â–„â–ƒâ–†â–ƒâ–„â–„â–ˆâ–„â–„â–ƒâ–‚â–…â–ƒâ–„â–â–†â–†â–„â–„â–…â–„â–„â–…â–ƒâ–†â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–„â–…â–†â–…â–‡â–…â–…â–ƒâ–†â–„â–„â–ˆâ–„â–…â–ƒâ–ˆâ–ƒâ–…â–ƒâ–‚â–†â–„â–„â–â–†â–‡â–ƒâ–…â–„â–„â–„â–„â–ƒâ–‡â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–„â–„â–‚â–ƒâ–…â–„â–ƒâ–…â–ƒâ–…â–ƒâ–‚â–„â–„â–„â–â–„â–ƒâ–…â–„â–„â–„â–ƒâ–ˆâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–…â–„â–ƒâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–‚â–â–„â–„â–„â–‚â–ƒâ–„â–…â–ƒâ–…â–ˆâ–†â–ƒâ–„â–…â–†â–…â–„â–„â–ˆâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–‡â–„â–ˆâ–ˆâ–†â–‡â–…â–…â–ˆâ–‚â–‡â–…â–â–ƒâ–ƒâ–ƒâ–„â–ƒâ–â–â–ƒâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–†â–ˆâ–†â–ƒâ–„â–…â–„â–‚â–…â–„â–…â–ƒâ–†â–…â–†â–†â–‡â–…â–‡â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–…â–…â–ˆâ–†â–…â–‡â–…â–…â–…â–ƒâ–…â–ƒâ–â–„â–ƒâ–„â–ƒâ–„â–‚â–„â–‚â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–†â–ƒâ–„â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–â–…â–…â–„â–„â–ƒâ–…â–„â–…â–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3rnafum3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:23:27.637485: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/vzwwig1b\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022326-vzwwig1b\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 130   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.7 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/73 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 72/73 [00:02<00:00, 29.16it/s, loss=0.633, v_num=ig1b, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.625\n",
      "Epoch 0: 100%|â–ˆ| 73/73 [00:02<00:00, 28.53it/s, loss=0.633, v_num=ig1b, BTC_val_\n",
      "Epoch 1:  99%|â–‰| 72/73 [00:02<00:00, 31.66it/s, loss=0.599, v_num=ig1b, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 73/73 [00:02<00:00, 31.14it/s, loss=0.599, v_num=ig1b, BTC_val_\u001b[A\n",
      "Epoch 2:  99%|â–‰| 72/73 [00:02<00:00, 32.19it/s, loss=0.623, v_num=ig1b, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 73/73 [00:02<00:00, 31.37it/s, loss=0.623, v_num=ig1b, BTC_val_\u001b[A\n",
      "Epoch 3:  99%|â–‰| 72/73 [00:02<00:00, 31.56it/s, loss=0.592, v_num=ig1b, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.036 >= min_delta = 0.003. New best score: 0.589\n",
      "Epoch 3: 100%|â–ˆ| 73/73 [00:02<00:00, 31.05it/s, loss=0.592, v_num=ig1b, BTC_val_\n",
      "Epoch 4:  99%|â–‰| 72/73 [00:02<00:00, 27.68it/s, loss=0.549, v_num=ig1b, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 73/73 [00:03<00:00, 24.12it/s, loss=0.549, v_num=ig1b, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 72/73 [00:02<00:00, 28.12it/s, loss=0.592, v_num=ig1b, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 73/73 [00:02<00:00, 27.58it/s, loss=0.592, v_num=ig1b, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 72/73 [00:02<00:00, 28.00it/s, loss=0.586, v_num=ig1b, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 73/73 [00:02<00:00, 27.43it/s, loss=0.586, v_num=ig1b, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 72/73 [00:02<00:00, 26.55it/s, loss=0.584, v_num=ig1b, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 73/73 [00:02<00:00, 26.15it/s, loss=0.584, v_num=ig1b, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 72/73 [00:02<00:00, 32.17it/s, loss=0.562, v_num=ig1b, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.583\n",
      "Epoch 8: 100%|â–ˆ| 73/73 [00:02<00:00, 31.60it/s, loss=0.562, v_num=ig1b, BTC_val_\n",
      "Epoch 9:  99%|â–‰| 72/73 [00:02<00:00, 28.62it/s, loss=0.576, v_num=ig1b, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 73/73 [00:02<00:00, 28.12it/s, loss=0.576, v_num=ig1b, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 72/73 [00:02<00:00, 26.14it/s, loss=0.616, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 73/73 [00:02<00:00, 25.65it/s, loss=0.616, v_num=ig1b, BTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 72/73 [00:02<00:00, 30.28it/s, loss=0.569, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 73/73 [00:02<00:00, 29.81it/s, loss=0.569, v_num=ig1b, BTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 72/73 [00:02<00:00, 24.09it/s, loss=0.556, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 73/73 [00:03<00:00, 23.70it/s, loss=0.556, v_num=ig1b, BTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 72/73 [00:02<00:00, 26.57it/s, loss=0.582, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 73/73 [00:02<00:00, 26.24it/s, loss=0.582, v_num=ig1b, BTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 72/73 [00:02<00:00, 27.60it/s, loss=0.568, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 73/73 [00:02<00:00, 27.21it/s, loss=0.568, v_num=ig1b, BTC_val\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.577\n",
      "Epoch 15:  99%|â–‰| 72/73 [00:02<00:00, 28.08it/s, loss=0.556, v_num=ig1b, BTC_val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 73/73 [00:02<00:00, 27.48it/s, loss=0.556, v_num=ig1b, BTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 72/73 [00:02<00:00, 27.05it/s, loss=0.555, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 73/73 [00:02<00:00, 26.67it/s, loss=0.555, v_num=ig1b, BTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 72/73 [00:02<00:00, 27.22it/s, loss=0.558, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 73/73 [00:02<00:00, 26.85it/s, loss=0.558, v_num=ig1b, BTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 72/73 [00:02<00:00, 25.07it/s, loss=0.551, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 73/73 [00:02<00:00, 24.69it/s, loss=0.551, v_num=ig1b, BTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 72/73 [00:02<00:00, 30.66it/s, loss=0.569, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 73/73 [00:02<00:00, 30.09it/s, loss=0.569, v_num=ig1b, BTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 72/73 [00:02<00:00, 29.09it/s, loss=0.583, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 73/73 [00:02<00:00, 28.62it/s, loss=0.583, v_num=ig1b, BTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 72/73 [00:03<00:00, 23.11it/s, loss=0.584, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 73/73 [00:03<00:00, 22.88it/s, loss=0.584, v_num=ig1b, BTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 72/73 [00:02<00:00, 28.68it/s, loss=0.597, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 73/73 [00:02<00:00, 28.25it/s, loss=0.597, v_num=ig1b, BTC_val\u001b[A\n",
      "Epoch 23:  99%|â–‰| 72/73 [00:02<00:00, 29.44it/s, loss=0.543, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 73/73 [00:02<00:00, 28.81it/s, loss=0.543, v_num=ig1b, BTC_val\u001b[A\n",
      "Epoch 24:  99%|â–‰| 72/73 [00:02<00:00, 26.76it/s, loss=0.558, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 73/73 [00:02<00:00, 26.30it/s, loss=0.558, v_num=ig1b, BTC_val\u001b[A\n",
      "Epoch 25:  99%|â–‰| 72/73 [00:02<00:00, 31.59it/s, loss=0.562, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 73/73 [00:02<00:00, 30.98it/s, loss=0.562, v_num=ig1b, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26:  99%|â–‰| 72/73 [00:02<00:00, 30.60it/s, loss=0.603, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 73/73 [00:02<00:00, 30.07it/s, loss=0.603, v_num=ig1b, BTC_val\u001b[A\n",
      "Epoch 27:  99%|â–‰| 72/73 [00:02<00:00, 25.56it/s, loss=0.563, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 73/73 [00:02<00:00, 25.17it/s, loss=0.563, v_num=ig1b, BTC_val\u001b[A\n",
      "Epoch 28:  99%|â–‰| 72/73 [00:02<00:00, 29.35it/s, loss=0.56, v_num=ig1b, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 73/73 [00:02<00:00, 28.88it/s, loss=0.56, v_num=ig1b, BTC_val_\u001b[A\n",
      "Epoch 29:  99%|â–‰| 72/73 [00:02<00:00, 24.31it/s, loss=0.546, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 73/73 [00:03<00:00, 23.90it/s, loss=0.546, v_num=ig1b, BTC_val\u001b[A\n",
      "Epoch 30:  99%|â–‰| 72/73 [00:02<00:00, 28.95it/s, loss=0.526, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 73/73 [00:02<00:00, 27.95it/s, loss=0.526, v_num=ig1b, BTC_val\u001b[A\n",
      "Epoch 31:  99%|â–‰| 72/73 [00:02<00:00, 25.15it/s, loss=0.576, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 73/73 [00:02<00:00, 24.82it/s, loss=0.576, v_num=ig1b, BTC_val\u001b[A\n",
      "Epoch 32:  99%|â–‰| 72/73 [00:02<00:00, 28.89it/s, loss=0.55, v_num=ig1b, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 73/73 [00:02<00:00, 28.28it/s, loss=0.55, v_num=ig1b, BTC_val_\u001b[A\n",
      "Epoch 33:  99%|â–‰| 72/73 [00:02<00:00, 27.41it/s, loss=0.554, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 73/73 [00:02<00:00, 27.07it/s, loss=0.554, v_num=ig1b, BTC_val\u001b[A\n",
      "Epoch 34:  99%|â–‰| 72/73 [00:02<00:00, 28.11it/s, loss=0.567, v_num=ig1b, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.577. Signaling Trainer to stop.\n",
      "Epoch 34: 100%|â–ˆ| 73/73 [00:02<00:00, 27.67it/s, loss=0.567, v_num=ig1b, BTC_val\n",
      "Epoch 34: 100%|â–ˆ| 73/73 [00:02<00:00, 27.62it/s, loss=0.567, v_num=ig1b, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 24.81it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.7142857313156128,\n",
      " 'BTC_test_f1': 0.7091330289840698,\n",
      " 'ETH_test_acc': 0.75,\n",
      " 'ETH_test_f1': 0.7492997050285339,\n",
      " 'LTC_test_acc': 0.75,\n",
      " 'LTC_test_f1': 0.7469831109046936,\n",
      " 'test_loss': 0.5304712057113647}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 77802\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022326-vzwwig1b/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022326-vzwwig1b/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.51515\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.46667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.5625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.51515\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.76089\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 34\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2520\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 102\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621466708\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 120\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.72585\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.70716\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.72411\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.71111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.71453\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.69742\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.55891\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.73333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.46667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.61905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.5839\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.71429\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.70913\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.7493\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.74698\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.53047\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–†â–…â–ƒâ–…â–†â–„â–†â–ˆâ–…â–ƒâ–…â–…â–†â–‡â–â–ƒâ–ƒâ–†â–†â–ˆâ–‡â–…â–ƒâ–…â–„â–ƒâ–…â–„â–„â–‡â–…â–†â–ƒâ–„â–…â–†â–…â–†â–‡â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–†â–…â–ƒâ–…â–‡â–„â–†â–ˆâ–…â–ƒâ–„â–…â–†â–‡â–â–ƒâ–ƒâ–†â–†â–ˆâ–‡â–…â–ƒâ–…â–„â–„â–…â–„â–„â–‡â–…â–†â–ƒâ–„â–…â–‡â–…â–‡â–‡â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–…â–‡â–ƒâ–ƒâ–ƒâ–‡â–…â–ˆâ–„â–†â–…â–‡â–…â–‡â–â–ƒâ–†â–‡â–…â–†â–†â–…â–„â–…â–…â–„â–ˆâ–†â–ˆâ–ˆâ–‡â–„â–„â–„â–„â–†â–…â–ˆâ–…â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–„â–‡â–„â–„â–„â–‡â–…â–ˆâ–„â–†â–…â–‡â–…â–‡â–â–„â–†â–‡â–…â–†â–†â–…â–„â–…â–…â–„â–ˆâ–†â–†â–ˆâ–†â–„â–„â–„â–„â–†â–…â–ˆâ–…â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–ˆâ–…â–„â–‚â–…â–…â–…â–…â–…â–„â–„â–…â–‡â–†â–â–…â–‚â–‡â–„â–…â–…â–…â–…â–„â–†â–„â–†â–ƒâ–ˆâ–†â–…â–…â–†â–â–…â–†â–„â–‡â–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–ˆâ–…â–ƒâ–‚â–…â–…â–…â–…â–…â–„â–ƒâ–…â–‡â–†â–â–…â–‚â–‡â–„â–…â–„â–…â–…â–„â–†â–„â–†â–ƒâ–ˆâ–†â–„â–…â–†â–â–…â–†â–ƒâ–‡â–†â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–ƒâ–‚â–‡â–†â–„â–„â–„â–ƒâ–†â–„â–…â–„â–ƒâ–ƒâ–ˆâ–‡â–†â–‚â–„â–‚â–„â–„â–‡â–†â–„â–†â–„â–‡â–…â–‚â–ƒâ–„â–ƒâ–…â–…â–„â–…â–â–‚â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–…â–†â–…â–‡â–†â–‡â–†â–†â–‡â–‡â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–†â–ˆâ–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–â–…â–†â–†â–‡â–†â–‡â–‡â–†â–‡â–ˆâ–†â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–…â–†â–†â–†â–…â–†â–†â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–â–†â–†â–†â–‡â–†â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–â–†â–†â–‡â–‡â–†â–‡â–‡â–†â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–â–†â–…â–‡â–‡â–†â–‡â–‡â–†â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–â–â–…â–â–â–â–â–…â–…â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–â–â–…â–â–â–â–â–…â–…â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–â–ƒâ–ƒâ–ƒâ–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–†â–ƒâ–†â–ˆâ–ˆâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ƒâ–†â–ˆâ–ƒâ–ˆâ–ˆâ–†â–ƒâ–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–â–ƒâ–ƒâ–ƒâ–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–†â–ƒâ–†â–ˆâ–ˆâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ƒâ–†â–ˆâ–ƒâ–ˆâ–ˆâ–†â–ƒâ–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–…â–…â–…â–…â–…â–ˆâ–ˆâ–…â–â–…â–â–…â–…â–ˆâ–â–…â–…â–…â–…â–…â–â–…â–…â–…â–…â–…â–…â–…â–â–…â–…â–…â–…â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–…â–…â–…â–…â–…â–ˆâ–ˆâ–…â–â–…â–â–…â–…â–ˆâ–â–…â–…â–…â–…â–…â–‚â–…â–…â–…â–…â–…â–…â–…â–â–…â–…â–…â–…â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–…â–ˆâ–…â–‚â–…â–„â–„â–ƒâ–‚â–ƒâ–„â–‚â–„â–…â–â–„â–„â–ƒâ–‚â–„â–„â–ƒâ–„â–‚â–‚â–â–„â–„â–â–„â–„â–„â–‚â–„â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/vzwwig1b\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:25:18.408546: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm_loss_weighted_binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/rnqvmitq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022516-rnqvmitq\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 130   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.7 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 73/74 [00:02<00:00, 31.35it/s, loss=0.698, v_num=mitq, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.678\n",
      "Epoch 0: 100%|â–ˆ| 74/74 [00:02<00:00, 30.75it/s, loss=0.698, v_num=mitq, BTC_val_\n",
      "Epoch 1:  99%|â–‰| 73/74 [00:02<00:00, 31.99it/s, loss=0.695, v_num=mitq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 74/74 [00:02<00:00, 31.43it/s, loss=0.695, v_num=mitq, BTC_val_\u001b[A\n",
      "Epoch 2:  99%|â–‰| 73/74 [00:02<00:00, 29.35it/s, loss=0.692, v_num=mitq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 74/74 [00:02<00:00, 28.78it/s, loss=0.692, v_num=mitq, BTC_val_\u001b[A\n",
      "Epoch 3:  99%|â–‰| 73/74 [00:02<00:00, 25.01it/s, loss=0.697, v_num=mitq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 74/74 [00:02<00:00, 24.72it/s, loss=0.697, v_num=mitq, BTC_val_\u001b[A\n",
      "Epoch 4:  99%|â–‰| 73/74 [00:02<00:00, 32.01it/s, loss=0.695, v_num=mitq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 74/74 [00:02<00:00, 31.47it/s, loss=0.695, v_num=mitq, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 73/74 [00:02<00:00, 26.71it/s, loss=0.695, v_num=mitq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 74/74 [00:02<00:00, 26.26it/s, loss=0.695, v_num=mitq, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 73/74 [00:02<00:00, 24.90it/s, loss=0.695, v_num=mitq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 74/74 [00:03<00:00, 24.61it/s, loss=0.695, v_num=mitq, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 73/74 [00:02<00:00, 30.01it/s, loss=0.691, v_num=mitq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 74/74 [00:02<00:00, 29.41it/s, loss=0.691, v_num=mitq, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 73/74 [00:02<00:00, 26.98it/s, loss=0.695, v_num=mitq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 74/74 [00:02<00:00, 26.52it/s, loss=0.695, v_num=mitq, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 73/74 [00:02<00:00, 30.26it/s, loss=0.696, v_num=mitq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 74/74 [00:02<00:00, 29.72it/s, loss=0.696, v_num=mitq, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 73/74 [00:02<00:00, 31.17it/s, loss=0.694, v_num=mitq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 74/74 [00:02<00:00, 30.61it/s, loss=0.694, v_num=mitq, BTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 73/74 [00:02<00:00, 30.48it/s, loss=0.693, v_num=mitq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 74/74 [00:02<00:00, 29.86it/s, loss=0.693, v_num=mitq, BTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 73/74 [00:02<00:00, 28.67it/s, loss=0.694, v_num=mitq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 74/74 [00:02<00:00, 28.09it/s, loss=0.694, v_num=mitq, BTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 73/74 [00:02<00:00, 25.41it/s, loss=0.692, v_num=mitq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 74/74 [00:02<00:00, 25.11it/s, loss=0.692, v_num=mitq, BTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 73/74 [00:02<00:00, 26.92it/s, loss=0.694, v_num=mitq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 74/74 [00:02<00:00, 26.40it/s, loss=0.694, v_num=mitq, BTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 73/74 [00:02<00:00, 29.20it/s, loss=0.693, v_num=mitq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 74/74 [00:02<00:00, 28.76it/s, loss=0.693, v_num=mitq, BTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 73/74 [00:02<00:00, 30.85it/s, loss=0.692, v_num=mitq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 74/74 [00:02<00:00, 30.36it/s, loss=0.692, v_num=mitq, BTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 73/74 [00:02<00:00, 24.79it/s, loss=0.693, v_num=mitq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 74/74 [00:03<00:00, 24.36it/s, loss=0.693, v_num=mitq, BTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 73/74 [00:02<00:00, 30.23it/s, loss=0.698, v_num=mitq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 74/74 [00:02<00:00, 29.71it/s, loss=0.698, v_num=mitq, BTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 73/74 [00:02<00:00, 26.90it/s, loss=0.693, v_num=mitq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 74/74 [00:02<00:00, 26.36it/s, loss=0.693, v_num=mitq, BTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 73/74 [00:02<00:00, 24.64it/s, loss=0.693, v_num=mitq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:03<00:00, 24.29it/s, loss=0.693, v_num=mitq, BTC_val\u001b[A\n",
      "                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.678. Signaling Trainer to stop.\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:03<00:00, 24.26it/s, loss=0.693, v_num=mitq, BTC_val\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 33.81it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.3928571343421936,\n",
      " 'BTC_test_f1': 0.2789115607738495,\n",
      " 'ETH_test_acc': 0.6428571343421936,\n",
      " 'ETH_test_f1': 0.38938775658607483,\n",
      " 'LTC_test_acc': 0.4285714328289032,\n",
      " 'LTC_test_f1': 0.293949156999588,\n",
      " 'test_loss': 0.7067702412605286}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 78074\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022516-rnqvmitq/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022516-rnqvmitq/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.4375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.30435\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.27273\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.73333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.69248\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 65\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621466781\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.47359\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.43582\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.47619\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.44308\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.52554\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.48676\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69339\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.38462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.38462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.38462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.69181\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.39286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.27891\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.64286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.38939\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.42857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.29395\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.70677\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–…â–„â–…â–…â–ˆâ–…â–â–„â–…â–…â–…â–†â–ƒâ–‡â–…â–…â–…â–„â–†â–‚â–„â–‚â–„â–â–…â–†â–…â–‚â–…â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–†â–„â–…â–„â–ˆâ–…â–‚â–„â–…â–…â–…â–†â–ƒâ–‡â–…â–†â–…â–‚â–†â–‚â–ƒâ–‚â–„â–â–…â–‡â–…â–‚â–…â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–…â–‡â–…â–ƒâ–‡â–‡â–…â–…â–‡â–‡â–…â–‡â–†â–‚â–…â–†â–†â–†â–…â–…â–…â–ˆâ–‡â–…â–†â–†â–†â–…â–â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–„â–‡â–„â–‚â–†â–†â–„â–„â–‡â–‡â–…â–†â–…â–‚â–„â–†â–†â–…â–…â–„â–„â–ˆâ–‡â–…â–…â–†â–…â–„â–â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–ƒâ–…â–…â–…â–…â–„â–…â–„â–ƒâ–…â–…â–…â–…â–„â–…â–…â–†â–…â–†â–…â–…â–…â–„â–ƒâ–…â–…â–â–‡â–…â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–‚â–…â–„â–„â–ƒâ–„â–…â–ƒâ–ƒâ–…â–…â–…â–†â–„â–†â–…â–†â–…â–†â–…â–ƒâ–„â–ƒâ–â–‚â–…â–â–‡â–…â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–‡â–‡â–†â–…â–â–ˆâ–‡â–†â–†â–…â–…â–„â–†â–…â–…â–…â–…â–„â–…â–‡â–†â–…â–…â–†â–†â–…â–†â–†â–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–‡â–†â–†â–†â–„â–†â–„â–‡â–ƒâ–‚â–„â–…â–…â–ˆâ–†â–…â–â–‡â–‡â–…â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–‡â–…â–…â–‡â–‚â–†â–„â–ˆâ–…â–„â–…â–†â–„â–ˆâ–…â–…â–â–†â–…â–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–…â–‡â–„â–…â–â–†â–„â–ƒâ–ˆâ–†â–‡â–„â–‚â–„â–†â–‡â–ƒâ–‚â–„â–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–†â–†â–…â–†â–â–†â–…â–…â–ˆâ–…â–†â–…â–‚â–…â–…â–‡â–ƒâ–ƒâ–ƒâ–„â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–†â–†â–‚â–…â–…â–ƒâ–†â–†â–ƒâ–…â–„â–†â–‚â–ƒâ–†â–„â–†â–ƒâ–â–†â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–ˆâ–†â–ƒâ–†â–ƒâ–…â–‡â–ˆâ–†â–…â–„â–‡â–‚â–†â–†â–„â–…â–‚â–â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–†â–‡â–ƒâ–†â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–ƒâ–‚â–‚â–â–â–‚â–ƒâ–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–†â–†â–ƒâ–ƒâ–†â–†â–ƒâ–†â–ƒâ–ƒâ–ƒâ–†â–†â–†â–†â–†â–†â–â–â–ˆâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–„â–„â–‚â–‚â–„â–„â–‚â–„â–‚â–‚â–‚â–„â–„â–„â–„â–„â–„â–â–‚â–ˆâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–â–â–…â–ˆâ–…â–…â–†â–…â–‡â–ˆâ–†â–ƒâ–ƒâ–„â–„â–‚â–â–†â–†â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/rnqvmitq\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:26:32.448375: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/pju93dut\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022630-pju93dut\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 195   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.8 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 72/73 [00:02<00:00, 30.91it/s, loss=1.11, v_num=3dut, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.092\n",
      "Epoch 0: 100%|â–ˆ| 73/73 [00:02<00:00, 30.30it/s, loss=1.11, v_num=3dut, BTC_val_a\n",
      "Epoch 1:  99%|â–‰| 72/73 [00:02<00:00, 31.69it/s, loss=1.1, v_num=3dut, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 73/73 [00:02<00:00, 31.19it/s, loss=1.1, v_num=3dut, BTC_val_ac\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 1.087\n",
      "Epoch 2:  99%|â–‰| 72/73 [00:02<00:00, 29.70it/s, loss=1.05, v_num=3dut, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.064 >= min_delta = 0.003. New best score: 1.023\n",
      "Epoch 2: 100%|â–ˆ| 73/73 [00:02<00:00, 29.08it/s, loss=1.05, v_num=3dut, BTC_val_a\n",
      "Epoch 3:  99%|â–‰| 72/73 [00:02<00:00, 28.91it/s, loss=1.01, v_num=3dut, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 73/73 [00:02<00:00, 28.36it/s, loss=1.01, v_num=3dut, BTC_val_aMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 1.019\n",
      "\n",
      "Epoch 4:  99%|â–‰| 72/73 [00:02<00:00, 25.86it/s, loss=1, v_num=3dut, BTC_val_acc=\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.023 >= min_delta = 0.003. New best score: 0.997\n",
      "Epoch 4: 100%|â–ˆ| 73/73 [00:02<00:00, 25.54it/s, loss=1, v_num=3dut, BTC_val_acc=\n",
      "Epoch 5:  99%|â–‰| 72/73 [00:02<00:00, 29.36it/s, loss=0.993, v_num=3dut, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 73/73 [00:02<00:00, 28.78it/s, loss=0.993, v_num=3dut, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 72/73 [00:03<00:00, 22.57it/s, loss=1, v_num=3dut, BTC_val_acc=\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 73/73 [00:03<00:00, 22.32it/s, loss=1, v_num=3dut, BTC_val_acc=\u001b[A\n",
      "Epoch 7:  99%|â–‰| 72/73 [00:02<00:00, 30.63it/s, loss=0.99, v_num=3dut, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 73/73 [00:02<00:00, 30.00it/s, loss=0.99, v_num=3dut, BTC_val_a\u001b[A\n",
      "Epoch 8:  99%|â–‰| 72/73 [00:03<00:00, 20.92it/s, loss=0.977, v_num=3dut, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 73/73 [00:03<00:00, 20.76it/s, loss=0.977, v_num=3dut, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 72/73 [00:03<00:00, 21.83it/s, loss=0.988, v_num=3dut, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.984\n",
      "Epoch 9: 100%|â–ˆ| 73/73 [00:03<00:00, 21.55it/s, loss=0.988, v_num=3dut, BTC_val_\n",
      "Epoch 10:  99%|â–‰| 72/73 [00:02<00:00, 26.01it/s, loss=0.945, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 73/73 [00:02<00:00, 25.73it/s, loss=0.945, v_num=3dut, BTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 72/73 [00:02<00:00, 28.82it/s, loss=0.953, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 73/73 [00:02<00:00, 27.78it/s, loss=0.953, v_num=3dut, BTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 72/73 [00:03<00:00, 21.92it/s, loss=0.977, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 73/73 [00:03<00:00, 21.76it/s, loss=0.977, v_num=3dut, BTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 72/73 [00:02<00:00, 26.06it/s, loss=0.971, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 73/73 [00:02<00:00, 25.61it/s, loss=0.971, v_num=3dut, BTC_val\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 0.973\n",
      "Epoch 14:  99%|â–‰| 72/73 [00:02<00:00, 24.43it/s, loss=0.938, v_num=3dut, BTC_val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 73/73 [00:03<00:00, 24.19it/s, loss=0.938, v_num=3dut, BTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 72/73 [00:03<00:00, 23.25it/s, loss=0.978, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 73/73 [00:03<00:00, 22.96it/s, loss=0.978, v_num=3dut, BTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 72/73 [00:02<00:00, 28.30it/s, loss=0.933, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 73/73 [00:02<00:00, 27.70it/s, loss=0.933, v_num=3dut, BTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 72/73 [00:02<00:00, 27.02it/s, loss=0.955, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 73/73 [00:02<00:00, 26.56it/s, loss=0.955, v_num=3dut, BTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 72/73 [00:02<00:00, 26.19it/s, loss=0.959, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 73/73 [00:02<00:00, 25.84it/s, loss=0.959, v_num=3dut, BTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 72/73 [00:02<00:00, 27.66it/s, loss=0.956, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 73/73 [00:02<00:00, 27.31it/s, loss=0.956, v_num=3dut, BTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 72/73 [00:02<00:00, 30.01it/s, loss=0.963, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 73/73 [00:02<00:00, 29.31it/s, loss=0.963, v_num=3dut, BTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 72/73 [00:02<00:00, 29.27it/s, loss=0.953, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 73/73 [00:02<00:00, 28.66it/s, loss=0.953, v_num=3dut, BTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 72/73 [00:03<00:00, 23.64it/s, loss=0.967, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 73/73 [00:03<00:00, 23.38it/s, loss=0.967, v_num=3dut, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:  99%|â–‰| 72/73 [00:03<00:00, 21.83it/s, loss=0.929, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 73/73 [00:03<00:00, 21.64it/s, loss=0.929, v_num=3dut, BTC_val\u001b[A\n",
      "Epoch 24:  99%|â–‰| 72/73 [00:02<00:00, 24.93it/s, loss=0.98, v_num=3dut, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 73/73 [00:02<00:00, 24.52it/s, loss=0.98, v_num=3dut, BTC_val_\u001b[A\n",
      "Epoch 25:  99%|â–‰| 72/73 [00:03<00:00, 23.45it/s, loss=0.922, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 73/73 [00:03<00:00, 23.21it/s, loss=0.922, v_num=3dut, BTC_val\u001b[A\n",
      "Epoch 26:  99%|â–‰| 72/73 [00:02<00:00, 29.12it/s, loss=0.943, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 73/73 [00:02<00:00, 28.61it/s, loss=0.943, v_num=3dut, BTC_val\u001b[A\n",
      "Epoch 27:  99%|â–‰| 72/73 [00:02<00:00, 27.58it/s, loss=0.963, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 73/73 [00:02<00:00, 27.09it/s, loss=0.963, v_num=3dut, BTC_val\u001b[A\n",
      "Epoch 28:  99%|â–‰| 72/73 [00:02<00:00, 25.64it/s, loss=0.912, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 73/73 [00:02<00:00, 25.33it/s, loss=0.912, v_num=3dut, BTC_val\u001b[A\n",
      "Epoch 29:  99%|â–‰| 72/73 [00:02<00:00, 29.88it/s, loss=0.954, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 73/73 [00:02<00:00, 29.17it/s, loss=0.954, v_num=3dut, BTC_val\u001b[A\n",
      "Epoch 30:  99%|â–‰| 72/73 [00:02<00:00, 24.92it/s, loss=0.918, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 73/73 [00:02<00:00, 24.61it/s, loss=0.918, v_num=3dut, BTC_val\u001b[A\n",
      "Epoch 31:  99%|â–‰| 72/73 [00:02<00:00, 26.83it/s, loss=0.941, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 73/73 [00:02<00:00, 26.44it/s, loss=0.941, v_num=3dut, BTC_val\u001b[A\n",
      "Epoch 32:  99%|â–‰| 72/73 [00:02<00:00, 24.17it/s, loss=0.907, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 73/73 [00:03<00:00, 23.84it/s, loss=0.907, v_num=3dut, BTC_val\u001b[A\n",
      "Epoch 33:  99%|â–‰| 72/73 [00:02<00:00, 29.01it/s, loss=0.909, v_num=3dut, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.973. Signaling Trainer to stop.\n",
      "Epoch 33: 100%|â–ˆ| 73/73 [00:02<00:00, 28.26it/s, loss=0.909, v_num=3dut, BTC_val\n",
      "Epoch 33: 100%|â–ˆ| 73/73 [00:02<00:00, 28.21it/s, loss=0.909, v_num=3dut, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 28.92it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.4642857015132904,\n",
      " 'BTC_test_f1': 0.41789641976356506,\n",
      " 'ETH_test_acc': 0.5714285969734192,\n",
      " 'ETH_test_f1': 0.5793651938438416,\n",
      " 'LTC_test_acc': 0.4642857015132904,\n",
      " 'LTC_test_f1': 0.4154462218284607,\n",
      " 'test_loss': 0.9363532662391663}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 78278\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022630-pju93dut/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022630-pju93dut/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.4375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.48571\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.63889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.8142\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2448\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 106\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621466896\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 116\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.50653\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.47721\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.51262\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.48927\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.50218\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.4773\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.92487\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.51111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.24444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.13333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.09139\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.46429\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.4179\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.57143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.57937\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.46429\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.41545\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.93635\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–‚â–„â–„â–„â–„â–…â–‚â–†â–ƒâ–â–ˆâ–„â–„â–†â–„â–†â–‚â–…â–…â–†â–„â–…â–†â–„â–„â–ƒâ–ƒâ–ƒâ–…â–…â–ƒâ–…â–…â–„â–„â–†â–…â–…â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–â–ƒâ–ƒâ–„â–„â–…â–‚â–†â–ƒâ–â–ˆâ–ƒâ–„â–…â–„â–†â–â–…â–„â–†â–„â–…â–…â–„â–„â–ƒâ–‚â–„â–„â–…â–ƒâ–…â–…â–ƒâ–„â–†â–„â–…â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–â–‚â–‚â–â–…â–…â–…â–†â–‚â–ƒâ–ƒâ–„â–…â–…â–ˆâ–ƒâ–ƒâ–ƒâ–â–…â–…â–†â–…â–…â–†â–„â–„â–ƒâ–…â–„â–‚â–ƒâ–…â–ƒâ–„â–ˆâ–‚â–„â–ƒâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–â–â–ƒâ–‚â–„â–…â–…â–†â–‚â–ƒâ–ƒâ–„â–…â–…â–ˆâ–ƒâ–‚â–ƒâ–â–†â–…â–†â–…â–„â–†â–„â–„â–ƒâ–„â–‚â–â–ƒâ–†â–ƒâ–„â–ˆâ–‚â–„â–ƒâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–…â–†â–…â–ƒâ–…â–â–…â–…â–„â–…â–…â–‡â–‡â–…â–…â–…â–„â–ˆâ–ƒâ–ˆâ–…â–…â–…â–‡â–‚â–‡â–ƒâ–‚â–†â–…â–„â–†â–†â–…â–ƒâ–‡â–…â–…â–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–ƒâ–†â–„â–ƒâ–…â–â–…â–ƒâ–„â–ƒâ–…â–†â–‡â–…â–ƒâ–„â–ƒâ–‡â–ƒâ–ˆâ–„â–„â–„â–‡â–‚â–‡â–ƒâ–ƒâ–…â–ƒâ–„â–†â–…â–…â–„â–†â–„â–…â–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–ˆâ–ˆâ–‡â–ˆâ–…â–ˆâ–…â–†â–‡â–‡â–†â–„â–„â–ƒâ–„â–…â–…â–ƒâ–„â–„â–„â–„â–†â–ƒâ–…â–ƒâ–†â–ˆâ–„â–…â–‡â–…â–‚â–‡â–‡â–â–„â–„â–…â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–â–„â–ƒâ–…â–„â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–â–â–„â–ƒâ–…â–…â–†â–‡â–‡â–†â–†â–†â–‡â–†â–‡â–‡â–‡â–‡â–†â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–‚â–â–ƒâ–„â–†â–…â–†â–‡â–†â–†â–‡â–†â–ˆâ–†â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–ƒâ–â–„â–„â–†â–†â–†â–‡â–†â–‡â–‡â–†â–ˆâ–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–â–â–ƒâ–…â–…â–…â–†â–†â–†â–†â–‡â–†â–†â–‡â–‡â–‡â–†â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–ˆâ–‡â–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–‚â–â–„â–…â–…â–…â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–ˆâ–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–ˆâ–â–†â–†â–ƒâ–ƒâ–ƒâ–ƒâ–†â–ƒâ–†â–†â–ƒâ–†â–ˆâ–†â–ˆâ–†â–ƒâ–ƒâ–†â–†â–ˆâ–†â–†â–ˆâ–†â–†â–†â–ƒâ–ƒâ–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–†â–â–†â–…â–ƒâ–ƒâ–„â–ƒâ–…â–ƒâ–…â–…â–ƒâ–…â–ˆâ–‡â–ˆâ–‡â–ƒâ–ƒâ–‡â–‡â–ˆâ–…â–‡â–ˆâ–†â–†â–†â–ƒâ–ƒâ–†â–‡â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–ˆâ–ˆâ–â–…â–â–…â–…â–â–…â–…â–…â–…â–â–ˆâ–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–â–…â–…â–…â–…â–â–â–â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–‡â–…â–â–„â–â–„â–„â–â–…â–…â–…â–…â–â–ˆâ–…â–…â–…â–…â–…â–„â–…â–…â–…â–…â–â–„â–…â–…â–…â–â–â–â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–…â–ƒâ–…â–…â–ˆâ–†â–ƒâ–ƒâ–ƒâ–†â–ƒâ–ƒâ–…â–†â–ƒâ–ƒâ–…â–ƒâ–†â–…â–ƒâ–ƒâ–ƒâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–…â–…â–†â–â–ƒâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–ƒâ–‚â–„â–„â–ˆâ–†â–ƒâ–ƒâ–ƒâ–†â–ƒâ–ƒâ–„â–†â–ƒâ–ƒâ–„â–ƒâ–†â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–†â–â–ƒâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–ˆâ–„â–„â–‚â–„â–ƒâ–…â–„â–‚â–„â–…â–„â–â–„â–…â–„â–„â–‚â–ƒâ–†â–„â–ƒâ–‚â–„â–…â–„â–„â–„â–…â–„â–†â–…â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/pju93dut\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:28:26.452061: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm_loss_weighted_multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2mv4skdt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022824-2mv4skdt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 195   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.8 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 73/74 [00:02<00:00, 31.40it/s, loss=1.1, v_num=skdt, BTC_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.075\n",
      "Epoch 0: 100%|â–ˆ| 74/74 [00:02<00:00, 30.62it/s, loss=1.1, v_num=skdt, BTC_val_ac\n",
      "Epoch 1:  99%|â–‰| 73/74 [00:02<00:00, 32.13it/s, loss=1.11, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 74/74 [00:02<00:00, 31.58it/s, loss=1.11, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 2:  99%|â–‰| 73/74 [00:02<00:00, 28.78it/s, loss=1.11, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 74/74 [00:02<00:00, 28.15it/s, loss=1.11, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 3:  99%|â–‰| 73/74 [00:02<00:00, 29.32it/s, loss=1.11, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 74/74 [00:02<00:00, 28.68it/s, loss=1.11, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 4:  99%|â–‰| 73/74 [00:02<00:00, 26.63it/s, loss=1.1, v_num=skdt, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 74/74 [00:02<00:00, 26.25it/s, loss=1.1, v_num=skdt, BTC_val_ac\u001b[A\n",
      "Epoch 5:  99%|â–‰| 73/74 [00:02<00:00, 30.19it/s, loss=1.11, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 74/74 [00:02<00:00, 29.50it/s, loss=1.11, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 6:  99%|â–‰| 73/74 [00:02<00:00, 24.43it/s, loss=1.1, v_num=skdt, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 74/74 [00:03<00:00, 24.19it/s, loss=1.1, v_num=skdt, BTC_val_ac\u001b[A\n",
      "Epoch 7:  99%|â–‰| 73/74 [00:02<00:00, 29.96it/s, loss=1.1, v_num=skdt, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 74/74 [00:02<00:00, 29.37it/s, loss=1.1, v_num=skdt, BTC_val_ac\u001b[A\n",
      "Epoch 8:  99%|â–‰| 73/74 [00:02<00:00, 25.27it/s, loss=1.1, v_num=skdt, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 74/74 [00:02<00:00, 24.86it/s, loss=1.1, v_num=skdt, BTC_val_ac\u001b[A\n",
      "Epoch 9:  99%|â–‰| 73/74 [00:02<00:00, 27.93it/s, loss=1.1, v_num=skdt, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 74/74 [00:02<00:00, 27.54it/s, loss=1.1, v_num=skdt, BTC_val_ac\u001b[A\n",
      "Epoch 10:  99%|â–‰| 73/74 [00:02<00:00, 25.78it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 74/74 [00:02<00:00, 25.34it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 11:  99%|â–‰| 73/74 [00:02<00:00, 30.73it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 74/74 [00:02<00:00, 30.18it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 12:  99%|â–‰| 73/74 [00:02<00:00, 31.00it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 74/74 [00:02<00:00, 30.48it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 13:  99%|â–‰| 73/74 [00:02<00:00, 30.06it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 74/74 [00:02<00:00, 29.44it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 14:  99%|â–‰| 73/74 [00:02<00:00, 26.17it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 74/74 [00:02<00:00, 25.70it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 15:  99%|â–‰| 73/74 [00:02<00:00, 27.92it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 1.068\n",
      "Epoch 15: 100%|â–ˆ| 74/74 [00:02<00:00, 27.55it/s, loss=1.1, v_num=skdt, BTC_val_a\n",
      "Epoch 16:  99%|â–‰| 73/74 [00:02<00:00, 25.41it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 74/74 [00:02<00:00, 25.01it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 17:  99%|â–‰| 73/74 [00:02<00:00, 26.55it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 74/74 [00:02<00:00, 26.19it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 18:  99%|â–‰| 73/74 [00:02<00:00, 27.69it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 74/74 [00:02<00:00, 27.16it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 19:  99%|â–‰| 73/74 [00:02<00:00, 25.00it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 74/74 [00:02<00:00, 24.71it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 20:  99%|â–‰| 73/74 [00:02<00:00, 30.38it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:02<00:00, 29.88it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 21:  99%|â–‰| 73/74 [00:02<00:00, 24.72it/s, loss=1.09, v_num=skdt, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 74/74 [00:03<00:00, 24.35it/s, loss=1.09, v_num=skdt, BTC_val_\u001b[A\n",
      "Epoch 22:  99%|â–‰| 73/74 [00:02<00:00, 24.73it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 74/74 [00:03<00:00, 24.45it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 23:  99%|â–‰| 73/74 [00:02<00:00, 25.17it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 74/74 [00:02<00:00, 24.76it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 24:  99%|â–‰| 73/74 [00:02<00:00, 29.30it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 74/74 [00:02<00:00, 28.85it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 25:  99%|â–‰| 73/74 [00:02<00:00, 27.89it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 74/74 [00:02<00:00, 27.35it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 26:  99%|â–‰| 73/74 [00:03<00:00, 22.59it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 74/74 [00:03<00:00, 22.34it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 27:  99%|â–‰| 73/74 [00:02<00:00, 27.98it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 74/74 [00:02<00:00, 27.60it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 28:  99%|â–‰| 73/74 [00:03<00:00, 24.04it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 74/74 [00:03<00:00, 23.77it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 29:  99%|â–‰| 73/74 [00:02<00:00, 29.45it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 74/74 [00:02<00:00, 29.01it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 30:  99%|â–‰| 73/74 [00:02<00:00, 26.30it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 74/74 [00:02<00:00, 25.81it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 31:  99%|â–‰| 73/74 [00:02<00:00, 25.59it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 74/74 [00:02<00:00, 25.31it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 32:  99%|â–‰| 73/74 [00:02<00:00, 25.19it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 74/74 [00:03<00:00, 23.02it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 33:  99%|â–‰| 73/74 [00:02<00:00, 27.01it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 74/74 [00:02<00:00, 26.63it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 34:  99%|â–‰| 73/74 [00:02<00:00, 26.55it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 74/74 [00:02<00:00, 26.11it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Epoch 35:  99%|â–‰| 73/74 [00:03<00:00, 23.49it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 74/74 [00:03<00:00, 23.22it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n",
      "                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.068. Signaling Trainer to stop.\n",
      "Epoch 35: 100%|â–ˆ| 74/74 [00:03<00:00, 23.19it/s, loss=1.1, v_num=skdt, BTC_val_a\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 53.68it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.25,\n",
      " 'BTC_test_f1': 0.1315789520740509,\n",
      " 'ETH_test_acc': 0.4285714328289032,\n",
      " 'ETH_test_f1': 0.1991342157125473,\n",
      " 'LTC_test_acc': 0.3928571343421936,\n",
      " 'LTC_test_f1': 0.18594105541706085,\n",
      " 'test_loss': 1.1318554878234863}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 78572\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022824-2mv4skdt/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022824-2mv4skdt/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.1875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.15873\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.09524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.21368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.10967\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 35\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2628\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 110\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621467014\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 124\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.3671\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.31675\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.3368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.29041\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.329\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.27977\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.0988\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.18182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.18182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.25641\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.08214\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.13158\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.42857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19913\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.39286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.18594\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.13186\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–„â–†â–„â–ƒâ–…â–„â–ƒâ–…â–„â–ƒâ–„â–…â–ƒâ–†â–â–‚â–…â–…â–‚â–„â–‡â–‚â–†â–…â–…â–†â–…â–„â–„â–…â–‚â–ˆâ–ˆâ–‚â–†â–„â–ƒâ–ƒâ–„â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–ƒâ–‡â–ƒâ–ƒâ–†â–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–„â–ƒâ–…â–â–‚â–†â–…â–â–„â–…â–‚â–‡â–…â–„â–‡â–„â–ƒâ–„â–†â–‚â–ˆâ–ˆâ–â–‡â–„â–ƒâ–ƒâ–„â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–„â–†â–„â–…â–‡â–‡â–…â–…â–†â–…â–…â–…â–‡â–‚â–…â–„â–â–„â–„â–…â–…â–…â–‡â–ˆâ–‡â–†â–…â–ƒâ–ˆâ–…â–…â–ˆâ–…â–„â–„â–…â–„â–ƒâ–‡â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–„â–†â–„â–…â–‡â–‡â–…â–…â–†â–…â–…â–…â–‡â–ƒâ–…â–„â–â–„â–„â–…â–„â–†â–‡â–ˆâ–‡â–†â–„â–ƒâ–‡â–…â–…â–ˆâ–…â–„â–ƒâ–…â–„â–ƒâ–‡â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–ƒâ–‡â–†â–‚â–…â–ˆâ–ƒâ–ƒâ–ƒâ–…â–…â–ƒâ–…â–†â–†â–„â–ˆâ–„â–„â–„â–ƒâ–‚â–‚â–…â–…â–†â–ˆâ–„â–ˆâ–†â–‡â–ˆâ–â–‚â–ƒâ–…â–‚â–ƒâ–‡â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–‚â–‡â–†â–‚â–…â–†â–‚â–ƒâ–‚â–…â–„â–‚â–„â–†â–„â–„â–ˆâ–„â–„â–ƒâ–â–‚â–‚â–…â–„â–…â–†â–„â–†â–†â–‡â–†â–â–‚â–ƒâ–‚â–‚â–‚â–‡â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–ˆâ–ƒâ–†â–†â–ƒâ–‚â–†â–…â–…â–‡â–„â–†â–…â–†â–…â–†â–…â–…â–…â–…â–…â–„â–…â–ƒâ–â–ƒâ–„â–†â–ƒâ–„â–…â–ƒâ–ƒâ–…â–„â–„â–„â–‡â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–„â–„â–…â–…â–…â–â–‚â–…â–…â–…â–„â–„â–„â–ƒâ–†â–‡â–†â–ƒâ–†â–‚â–„â–‡â–†â–ˆâ–†â–…â–…â–†â–ˆâ–ƒâ–‡â–…â–„â–„â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–‚â–ƒâ–†â–…â–…â–â–‚â–…â–†â–…â–…â–…â–„â–‚â–‡â–…â–†â–‚â–†â–ƒâ–ƒâ–…â–…â–†â–†â–„â–„â–‡â–‡â–â–ˆâ–„â–ƒâ–…â–†â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–…â–ƒâ–‚â–…â–„â–â–ˆâ–ƒâ–…â–…â–ˆâ–†â–…â–„â–â–„â–…â–„â–‡â–…â–‡â–ˆâ–…â–…â–ˆâ–…â–‡â–„â–…â–ƒâ–…â–…â–„â–†â–ƒâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–ƒâ–‚â–ƒâ–…â–ƒâ–ƒâ–‡â–„â–†â–„â–ˆâ–‡â–…â–ƒâ–‚â–‚â–†â–ƒâ–ˆâ–…â–„â–…â–ƒâ–â–†â–„â–ƒâ–…â–‚â–â–†â–„â–ƒâ–†â–ƒâ–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–‚â–„â–‡â–‡â–…â–‚â–‚â–„â–ƒâ–„â–‚â–ƒâ–‚â–ƒâ–â–†â–‡â–…â–„â–…â–„â–ƒâ–…â–„â–…â–…â–‡â–ˆâ–…â–†â–…â–†â–…â–ˆâ–‡â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–â–ƒâ–‡â–†â–„â–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–„â–ƒâ–‚â–‚â–„â–†â–…â–…â–„â–‚â–‚â–ƒâ–‚â–„â–ƒâ–…â–ˆâ–ƒâ–„â–…â–„â–ƒâ–‡â–‡â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–„â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–…â–…â–…â–…â–ˆâ–ˆâ–…â–…â–…â–ˆâ–…â–…â–…â–ˆâ–ˆâ–ˆâ–ˆâ–…â–…â–â–…â–…â–â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–…â–…â–…â–…â–ˆâ–ˆâ–…â–…â–…â–ˆâ–…â–…â–…â–ˆâ–ˆâ–ˆâ–ˆâ–…â–…â–â–…â–…â–â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ƒâ–ƒâ–ƒâ–ˆâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ƒâ–ˆâ–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–„â–„â–„â–ˆâ–ˆâ–ˆâ–„â–„â–„â–ˆâ–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–ˆâ–„â–„â–â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ƒâ–ƒâ–‚â–‡â–ƒâ–…â–„â–ƒâ–ƒâ–†â–ƒâ–…â–‚â–„â–…â–â–‚â–…â–…â–†â–†â–‚â–„â–…â–†â–‡â–†â–ˆâ–†â–„â–…â–†â–…â–‡â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2mv4skdt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:30:24.297349: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/385fa5zd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_023022-385fa5zd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 72/73 [00:05<00:00, 13.22it/s, loss=0.663, v_num=a5zd, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.781\n",
      "Epoch 0: 100%|â–ˆ| 73/73 [00:05<00:00, 13.17it/s, loss=0.663, v_num=a5zd, BTC_val_\n",
      "Epoch 1:  99%|â–‰| 72/73 [00:04<00:00, 14.63it/s, loss=0.638, v_num=a5zd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.153 >= min_delta = 0.003. New best score: 0.628\n",
      "Epoch 1: 100%|â–ˆ| 73/73 [00:04<00:00, 14.61it/s, loss=0.638, v_num=a5zd, BTC_val_\n",
      "Epoch 2:  99%|â–‰| 72/73 [00:05<00:00, 13.81it/s, loss=0.587, v_num=a5zd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 73/73 [00:05<00:00, 13.73it/s, loss=0.587, v_num=a5zd, BTC_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.033 >= min_delta = 0.003. New best score: 0.595\n",
      "Epoch 3:  99%|â–‰| 72/73 [00:06<00:00, 11.32it/s, loss=0.588, v_num=a5zd, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 73/73 [00:06<00:00, 11.32it/s, loss=0.588, v_num=a5zd, BTC_val_\u001b[A\n",
      "Epoch 4:  99%|â–‰| 72/73 [00:05<00:00, 12.42it/s, loss=0.572, v_num=a5zd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.589\n",
      "Epoch 4: 100%|â–ˆ| 73/73 [00:05<00:00, 12.42it/s, loss=0.572, v_num=a5zd, BTC_val_\n",
      "Epoch 5:  99%|â–‰| 72/73 [00:05<00:00, 12.49it/s, loss=0.56, v_num=a5zd, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 73/73 [00:05<00:00, 12.47it/s, loss=0.56, v_num=a5zd, BTC_val_a\u001b[A\n",
      "Epoch 6:  99%|â–‰| 72/73 [00:04<00:00, 14.51it/s, loss=0.543, v_num=a5zd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 73/73 [00:05<00:00, 14.44it/s, loss=0.543, v_num=a5zd, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 72/73 [00:05<00:00, 14.19it/s, loss=0.582, v_num=a5zd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 73/73 [00:05<00:00, 14.14it/s, loss=0.582, v_num=a5zd, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 72/73 [00:05<00:00, 14.26it/s, loss=0.599, v_num=a5zd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 73/73 [00:05<00:00, 14.18it/s, loss=0.599, v_num=a5zd, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 72/73 [00:04<00:00, 14.46it/s, loss=0.57, v_num=a5zd, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 73/73 [00:05<00:00, 14.42it/s, loss=0.57, v_num=a5zd, BTC_val_a\u001b[A\n",
      "Epoch 10:  99%|â–‰| 72/73 [00:05<00:00, 12.87it/s, loss=0.564, v_num=a5zd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 73/73 [00:05<00:00, 12.85it/s, loss=0.564, v_num=a5zd, BTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 72/73 [00:05<00:00, 12.50it/s, loss=0.561, v_num=a5zd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 73/73 [00:05<00:00, 12.50it/s, loss=0.561, v_num=a5zd, BTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 72/73 [00:05<00:00, 12.89it/s, loss=0.57, v_num=a5zd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 73/73 [00:05<00:00, 12.85it/s, loss=0.57, v_num=a5zd, BTC_val_\u001b[A\n",
      "Epoch 13:  99%|â–‰| 72/73 [00:05<00:00, 12.91it/s, loss=0.591, v_num=a5zd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 73/73 [00:05<00:00, 12.86it/s, loss=0.591, v_num=a5zd, BTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 72/73 [00:06<00:00, 11.86it/s, loss=0.577, v_num=a5zd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 73/73 [00:06<00:00, 11.85it/s, loss=0.577, v_num=a5zd, BTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 72/73 [00:05<00:00, 12.39it/s, loss=0.582, v_num=a5zd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 73/73 [00:05<00:00, 12.35it/s, loss=0.582, v_num=a5zd, BTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 72/73 [00:05<00:00, 12.21it/s, loss=0.58, v_num=a5zd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 73/73 [00:05<00:00, 12.18it/s, loss=0.58, v_num=a5zd, BTC_val_\u001b[A\n",
      "Epoch 17:  99%|â–‰| 72/73 [00:05<00:00, 13.40it/s, loss=0.533, v_num=a5zd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 73/73 [00:05<00:00, 13.36it/s, loss=0.533, v_num=a5zd, BTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 72/73 [00:05<00:00, 12.58it/s, loss=0.569, v_num=a5zd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 73/73 [00:05<00:00, 12.51it/s, loss=0.569, v_num=a5zd, BTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 72/73 [00:05<00:00, 12.63it/s, loss=0.574, v_num=a5zd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 73/73 [00:05<00:00, 12.61it/s, loss=0.574, v_num=a5zd, BTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 72/73 [00:05<00:00, 13.75it/s, loss=0.601, v_num=a5zd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 73/73 [00:05<00:00, 13.69it/s, loss=0.601, v_num=a5zd, BTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 72/73 [00:05<00:00, 14.15it/s, loss=0.569, v_num=a5zd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 73/73 [00:05<00:00, 14.08it/s, loss=0.569, v_num=a5zd, BTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 72/73 [00:05<00:00, 13.45it/s, loss=0.554, v_num=a5zd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 73/73 [00:05<00:00, 13.41it/s, loss=0.554, v_num=a5zd, BTC_val\u001b[A\n",
      "Epoch 23:  99%|â–‰| 72/73 [00:05<00:00, 13.85it/s, loss=0.549, v_num=a5zd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 73/73 [00:05<00:00, 13.80it/s, loss=0.549, v_num=a5zd, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24:  99%|â–‰| 72/73 [00:05<00:00, 14.11it/s, loss=0.556, v_num=a5zd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 73/73 [00:05<00:00, 14.04it/s, loss=0.556, v_num=a5zd, BTC_val\u001b[A\n",
      "Monitored metric val_loss did not improve in the last 20 records. Best score: 0.589. Signaling Trainer to stop.\n",
      "Epoch 24: 100%|â–ˆ| 73/73 [00:05<00:00, 14.03it/s, loss=0.556, v_num=a5zd, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 27.94it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6785714030265808,\n",
      " 'BTC_test_f1': 0.6746674180030823,\n",
      " 'ETH_test_acc': 0.75,\n",
      " 'ETH_test_f1': 0.7492997050285339,\n",
      " 'LTC_test_acc': 0.7142857313156128,\n",
      " 'LTC_test_f1': 0.7079364657402039,\n",
      " 'test_loss': 0.5483826398849487}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 78859\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_023022-385fa5zd/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_023022-385fa5zd/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.76923\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.76923\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.84615\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.84524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.76923\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.76364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.38035\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1800\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 148\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621467170\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 86\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.72498\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.71365\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.73803\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.72703\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.72498\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.70898\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.55434\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.56364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.61905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.61905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.6302\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.67857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.67467\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.7493\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.71429\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.70794\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.54838\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–‚â–ƒâ–…â–†â–†â–†â–…â–†â–†â–†â–‡â–ƒâ–†â–‡â–„â–…â–†â–†â–ƒâ–„â–†â–ƒâ–‡â–ˆâ–…â–†â–†â–…â–‚â–â–„â–„â–…â–ƒâ–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–â–ƒâ–…â–†â–†â–†â–„â–†â–†â–†â–‡â–ƒâ–†â–‡â–„â–…â–†â–†â–ƒâ–„â–†â–ƒâ–‡â–ˆâ–…â–†â–‡â–…â–‚â–â–„â–„â–…â–â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–â–â–ƒâ–„â–ˆâ–†â–†â–†â–†â–†â–†â–…â–†â–ˆâ–…â–…â–…â–†â–†â–ƒâ–…â–„â–…â–„â–„â–†â–†â–‡â–…â–…â–„â–‡â–ƒâ–…â–†â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–â–â–ƒâ–„â–ˆâ–…â–‡â–‡â–‡â–†â–…â–…â–†â–ˆâ–…â–…â–…â–‡â–†â–ƒâ–…â–„â–…â–„â–„â–‡â–‡â–‡â–…â–…â–„â–‡â–ƒâ–„â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–â–„â–…â–„â–†â–…â–…â–†â–…â–…â–…â–…â–„â–‡â–‚â–‡â–…â–†â–…â–‚â–„â–…â–…â–†â–†â–‡â–ˆâ–‡â–…â–…â–…â–…â–‡â–†â–‡â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–â–„â–…â–„â–†â–„â–†â–†â–†â–…â–„â–…â–„â–‡â–‚â–‡â–…â–†â–†â–‚â–„â–…â–†â–†â–†â–‡â–ˆâ–‡â–…â–…â–…â–…â–‡â–†â–‡â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–†â–…â–„â–…â–ƒâ–„â–„â–ƒâ–„â–„â–ƒâ–„â–„â–ƒâ–…â–„â–ƒâ–ƒâ–„â–ˆâ–„â–…â–‚â–‚â–ƒâ–‚â–â–‚â–…â–…â–…â–ƒâ–„â–ƒâ–ƒâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–…â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–â–…â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–â–…â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–â–†â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–ƒâ–†â–†â–ˆâ–ƒâ–ƒâ–ƒâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ƒâ–ƒâ–†â–ƒâ–†â–ƒâ–ƒâ–ˆâ–ƒâ–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–„â–†â–†â–ˆâ–„â–„â–„â–ˆâ–„â–„â–„â–„â–ˆâ–„â–„â–†â–„â–†â–„â–„â–ˆâ–„â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–â–†â–ƒâ–ƒâ–ƒâ–ˆâ–†â–ˆâ–ƒâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ƒâ–†â–ƒâ–†â–†â–ƒâ–†â–ƒâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–â–†â–„â–„â–„â–ˆâ–†â–ˆâ–„â–ˆâ–„â–„â–„â–„â–ˆâ–ˆâ–„â–†â–„â–†â–†â–„â–†â–„â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–â–…â–…â–…â–â–…â–…â–…â–â–…â–…â–â–…â–â–ˆâ–…â–…â–…â–…â–ˆâ–…â–…â–…â–â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–â–…â–…â–…â–â–…â–…â–…â–â–…â–…â–â–…â–â–ˆâ–…â–…â–…â–…â–ˆâ–…â–ƒâ–…â–â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–‚â–â–‚â–â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–„â–ƒâ–‚â–‚â–â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/385fa5zd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:33:06.610252: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_loss_weighted_binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2ypw6dcq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_023305-2ypw6dcq\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                           | 0/74 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|â–ˆ| 74/74 [00:04<00:00, 15.34it/s, loss=0.706, v_num=6dcq, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.708\n",
      "Epoch 0: 100%|â–ˆ| 74/74 [00:04<00:00, 15.24it/s, loss=0.706, v_num=6dcq, BTC_val_\n",
      "Epoch 1: 100%|â–ˆ| 74/74 [00:05<00:00, 14.30it/s, loss=0.696, v_num=6dcq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 74/74 [00:05<00:00, 14.13it/s, loss=0.696, v_num=6dcq, BTC_val_\u001b[A\n",
      "Epoch 2:  99%|â–‰| 73/74 [00:06<00:00, 11.99it/s, loss=0.7, v_num=6dcq, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 74/74 [00:06<00:00, 12.00it/s, loss=0.7, v_num=6dcq, BTC_val_ac\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 74/74 [00:06<00:00, 11.64it/s, loss=0.695, v_num=6dcq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 74/74 [00:06<00:00, 11.60it/s, loss=0.695, v_num=6dcq, BTC_val_\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 74/74 [00:05<00:00, 14.08it/s, loss=0.696, v_num=6dcq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 74/74 [00:05<00:00, 14.02it/s, loss=0.696, v_num=6dcq, BTC_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.704\n",
      "Epoch 5: 100%|â–ˆ| 74/74 [00:05<00:00, 14.25it/s, loss=0.694, v_num=6dcq, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 74/74 [00:05<00:00, 14.17it/s, loss=0.694, v_num=6dcq, BTC_val_\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 74/74 [00:06<00:00, 10.75it/s, loss=0.697, v_num=6dcq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 74/74 [00:06<00:00, 10.71it/s, loss=0.697, v_num=6dcq, BTC_val_\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 74/74 [00:07<00:00, 10.29it/s, loss=0.694, v_num=6dcq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 74/74 [00:07<00:00, 10.24it/s, loss=0.694, v_num=6dcq, BTC_val_\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 74/74 [00:07<00:00,  9.77it/s, loss=0.693, v_num=6dcq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 74/74 [00:07<00:00,  9.49it/s, loss=0.693, v_num=6dcq, BTC_val_\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 74/74 [00:06<00:00, 11.96it/s, loss=0.691, v_num=6dcq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 74/74 [00:06<00:00, 11.91it/s, loss=0.691, v_num=6dcq, BTC_val_\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 74/74 [00:05<00:00, 12.74it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 74/74 [00:05<00:00, 12.68it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 74/74 [00:05<00:00, 12.75it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 74/74 [00:05<00:00, 12.68it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 74/74 [00:06<00:00, 12.15it/s, loss=0.696, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 74/74 [00:06<00:00, 12.09it/s, loss=0.696, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 74/74 [00:06<00:00, 11.95it/s, loss=0.696, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 74/74 [00:06<00:00, 11.89it/s, loss=0.696, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 74/74 [00:05<00:00, 12.46it/s, loss=0.696, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 74/74 [00:05<00:00, 12.40it/s, loss=0.696, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 74/74 [00:05<00:00, 14.20it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 74/74 [00:05<00:00, 14.13it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 74/74 [00:05<00:00, 13.98it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 74/74 [00:05<00:00, 13.91it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 0.701\n",
      "Epoch 17: 100%|â–ˆ| 74/74 [00:05<00:00, 12.36it/s, loss=0.695, v_num=6dcq, BTC_val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 74/74 [00:06<00:00, 12.31it/s, loss=0.695, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 74/74 [00:05<00:00, 12.60it/s, loss=0.695, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 74/74 [00:05<00:00, 12.54it/s, loss=0.695, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 74/74 [00:06<00:00, 11.84it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 74/74 [00:06<00:00, 11.80it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:06<00:00, 11.90it/s, loss=0.692, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:06<00:00, 11.85it/s, loss=0.692, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 74/74 [00:06<00:00, 11.38it/s, loss=0.692, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 74/74 [00:06<00:00, 11.22it/s, loss=0.692, v_num=6dcq, BTC_val\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 0.697\n",
      "Epoch 22: 100%|â–ˆ| 74/74 [00:06<00:00, 10.86it/s, loss=0.694, v_num=6dcq, BTC_val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 74/74 [00:06<00:00, 10.82it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 74/74 [00:05<00:00, 12.35it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 74/74 [00:06<00:00, 12.30it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 74/74 [00:06<00:00, 11.21it/s, loss=0.692, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 74/74 [00:06<00:00, 11.17it/s, loss=0.692, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 74/74 [00:06<00:00, 11.69it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 74/74 [00:06<00:00, 11.63it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 74/74 [00:05<00:00, 13.11it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 74/74 [00:05<00:00, 13.04it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 74/74 [00:06<00:00, 11.29it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 74/74 [00:06<00:00, 11.24it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 74/74 [00:06<00:00, 10.79it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 74/74 [00:06<00:00, 10.75it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 74/74 [00:06<00:00, 11.91it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 74/74 [00:06<00:00, 11.86it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 74/74 [00:06<00:00, 11.99it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 74/74 [00:06<00:00, 11.93it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 74/74 [00:06<00:00, 11.54it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 74/74 [00:06<00:00, 11.49it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 74/74 [00:06<00:00, 12.08it/s, loss=0.695, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 74/74 [00:06<00:00, 12.01it/s, loss=0.695, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 74/74 [00:05<00:00, 13.94it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 74/74 [00:05<00:00, 13.87it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 74/74 [00:06<00:00, 11.23it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 74/74 [00:06<00:00, 11.18it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|â–ˆ| 74/74 [00:06<00:00, 11.93it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 74/74 [00:06<00:00, 11.87it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 74/74 [00:06<00:00, 10.71it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.693\n",
      "Epoch 36: 100%|â–ˆ| 74/74 [00:06<00:00, 10.67it/s, loss=0.693, v_num=6dcq, BTC_val\n",
      "Epoch 37: 100%|â–ˆ| 74/74 [00:06<00:00, 11.56it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.689\n",
      "Epoch 37: 100%|â–ˆ| 74/74 [00:06<00:00, 11.51it/s, loss=0.694, v_num=6dcq, BTC_val\n",
      "Epoch 38: 100%|â–ˆ| 74/74 [00:05<00:00, 12.55it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|â–ˆ| 74/74 [00:05<00:00, 12.50it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 39: 100%|â–ˆ| 74/74 [00:06<00:00, 11.33it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|â–ˆ| 74/74 [00:06<00:00, 11.28it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 40: 100%|â–ˆ| 74/74 [00:06<00:00, 10.94it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|â–ˆ| 74/74 [00:06<00:00, 10.91it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 41: 100%|â–ˆ| 74/74 [00:06<00:00, 11.98it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|â–ˆ| 74/74 [00:06<00:00, 11.94it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 42: 100%|â–ˆ| 74/74 [00:05<00:00, 13.49it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|â–ˆ| 74/74 [00:05<00:00, 13.41it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 43: 100%|â–ˆ| 74/74 [00:06<00:00, 11.85it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|â–ˆ| 74/74 [00:06<00:00, 11.80it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 44: 100%|â–ˆ| 74/74 [00:05<00:00, 12.74it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|â–ˆ| 74/74 [00:05<00:00, 12.68it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 45: 100%|â–ˆ| 74/74 [00:06<00:00, 12.28it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|â–ˆ| 74/74 [00:06<00:00, 12.23it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 46: 100%|â–ˆ| 74/74 [00:05<00:00, 13.93it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|â–ˆ| 74/74 [00:05<00:00, 13.86it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 47: 100%|â–ˆ| 74/74 [00:06<00:00, 10.74it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|â–ˆ| 74/74 [00:06<00:00, 10.70it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 48: 100%|â–ˆ| 74/74 [00:05<00:00, 13.96it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|â–ˆ| 74/74 [00:05<00:00, 13.88it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 49: 100%|â–ˆ| 74/74 [00:06<00:00, 11.34it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|â–ˆ| 74/74 [00:06<00:00, 11.29it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 50: 100%|â–ˆ| 74/74 [00:06<00:00, 10.90it/s, loss=0.695, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|â–ˆ| 74/74 [00:06<00:00, 10.84it/s, loss=0.695, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 51: 100%|â–ˆ| 74/74 [00:06<00:00, 10.99it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|â–ˆ| 74/74 [00:06<00:00, 10.94it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 52: 100%|â–ˆ| 74/74 [00:06<00:00, 10.79it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|â–ˆ| 74/74 [00:06<00:00, 10.75it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 53: 100%|â–ˆ| 74/74 [00:05<00:00, 13.86it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|â–ˆ| 74/74 [00:05<00:00, 13.79it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 54: 100%|â–ˆ| 74/74 [00:05<00:00, 14.15it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|â–ˆ| 74/74 [00:05<00:00, 14.08it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 55: 100%|â–ˆ| 74/74 [00:06<00:00, 11.90it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|â–ˆ| 74/74 [00:06<00:00, 11.85it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 56: 100%|â–ˆ| 74/74 [00:06<00:00, 11.41it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|â–ˆ| 74/74 [00:06<00:00, 11.36it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n",
      "Epoch 57: 100%|â–ˆ| 74/74 [00:05<00:00, 13.91it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|â–ˆ| 74/74 [00:05<00:00, 13.83it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n",
      "                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.689. Signaling Trainer to stop.\n",
      "Epoch 57: 100%|â–ˆ| 74/74 [00:05<00:00, 13.82it/s, loss=0.693, v_num=6dcq, BTC_val\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 27.34it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.3928571343421936,\n",
      " 'BTC_test_f1': 0.2789115607738495,\n",
      " 'ETH_test_acc': 0.6428571343421936,\n",
      " 'ETH_test_f1': 0.38938775658607483,\n",
      " 'LTC_test_acc': 0.4285714328289032,\n",
      " 'LTC_test_f1': 0.293949156999588,\n",
      " 'test_loss': 0.6936858296394348}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 79215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_023305-2ypw6dcq/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_023305-2ypw6dcq/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.54656\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.56078\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.69079\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 57\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4234\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 366\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621467551\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.47619\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.42176\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.48745\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.42444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.4961\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.42666\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.6934\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.27273\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.27273\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.27273\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.69462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.39286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.27891\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.64286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.38939\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.42857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.29395\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.69369\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–‡â–…â–…â–„â–…â–…â–†â–…â–…â–†â–†â–ƒâ–‡â–â–„â–„â–„â–…â–†â–…â–†â–„â–†â–ƒâ–…â–ˆâ–‡â–…â–†â–„â–ƒâ–„â–†â–‡â–ˆâ–…â–„â–„â–ƒâ–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–‡â–„â–…â–„â–„â–„â–†â–…â–†â–†â–†â–ƒâ–‡â–â–ƒâ–„â–„â–…â–†â–„â–†â–ƒâ–†â–ƒâ–…â–ˆâ–‡â–„â–†â–„â–ƒâ–ƒâ–…â–‡â–ˆâ–ƒâ–‚â–„â–‚â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–‡â–„â–„â–ƒâ–ƒâ–ˆâ–„â–‡â–‡â–„â–…â–…â–ƒâ–†â–„â–…â–ˆâ–â–…â–…â–ƒâ–†â–†â–„â–ˆâ–…â–…â–‚â–…â–„â–…â–…â–…â–…â–‡â–…â–ƒâ–„â–‚â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–‡â–ƒâ–‚â–ƒâ–ƒâ–‡â–ƒâ–‡â–‡â–ƒâ–„â–„â–ƒâ–†â–„â–…â–ˆâ–â–„â–…â–ƒâ–†â–…â–„â–ˆâ–„â–…â–‚â–…â–„â–…â–„â–„â–…â–ƒâ–…â–ƒâ–ƒâ–â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–…â–†â–…â–…â–…â–†â–…â–‡â–†â–„â–†â–†â–†â–†â–†â–‡â–†â–…â–†â–…â–‡â–…â–†â–†â–†â–†â–‡â–‡â–…â–‡â–ˆâ–…â–†â–†â–â–†â–…â–†â–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–…â–…â–„â–…â–…â–…â–…â–‡â–†â–„â–†â–†â–…â–†â–…â–‡â–†â–„â–…â–„â–‡â–…â–…â–…â–†â–†â–‡â–‡â–„â–‡â–ˆâ–„â–…â–†â–â–…â–„â–†â–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–‚â–ˆâ–‡â–…â–…â–„â–„â–‚â–â–…â–‚â–„â–‚â–ƒâ–„â–ƒâ–‚â–…â–ƒâ–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–„â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–…â–‡â–ˆâ–„â–†â–‚â–†â–ƒâ–†â–ˆâ–‡â–„â–ˆâ–„â–ƒâ–„â–‚â–‚â–ƒâ–†â–‚â–‚â–‚â–‚â–‡â–„â–‡â–…â–…â–…â–…â–…â–ƒâ–‡â–„â–†â–…â–…â–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–„â–…â–†â–„â–‡â–…â–‡â–…â–†â–‡â–†â–†â–ˆâ–†â–ƒâ–†â–…â–„â–…â–‡â–…â–â–ƒâ–â–ˆâ–ƒâ–ˆâ–…â–…â–‡â–…â–†â–…â–ˆâ–†â–†â–…â–†â–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–„â–…â–ˆâ–†â–â–…â–ƒâ–„â–…â–†â–ƒâ–ƒâ–ƒâ–‚â–‚â–‡â–…â–„â–ƒâ–ƒâ–„â–‚â–„â–ƒâ–‚â–ƒâ–‚â–ƒâ–â–‚â–‡â–…â–ƒâ–ƒâ–ƒâ–„â–â–‚â–â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–„â–„â–‡â–…â–…â–‡â–†â–†â–†â–‡â–„â–…â–…â–…â–‚â–ˆâ–‡â–…â–…â–…â–†â–‚â–ƒâ–â–…â–â–†â–„â–ƒâ–…â–‡â–†â–†â–†â–…â–…â–ƒâ–…â–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–ƒâ–†â–ƒâ–â–„â–…â–ˆâ–ƒâ–„â–„â–‚â–…â–‚â–…â–…â–†â–…â–†â–ƒâ–â–…â–„â–ƒâ–†â–„â–…â–…â–…â–„â–„â–…â–„â–‚â–ƒâ–‚â–„â–…â–†â–ƒâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–ƒâ–…â–„â–‚â–…â–†â–ˆâ–…â–„â–…â–ƒâ–†â–„â–†â–„â–‡â–†â–†â–…â–ƒâ–†â–‚â–â–ƒâ–…â–ƒâ–‡â–†â–„â–†â–†â–…â–…â–†â–…â–…â–…â–‡â–ƒâ–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–‚â–ƒâ–ƒâ–‚â–â–â–â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–â–â–â–â–â–â–ˆâ–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–„â–„â–„â–„â–ˆâ–„â–â–â–â–â–â–â–„â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–â–â–â–â–â–â–ˆâ–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–…â–…â–…â–ˆâ–…â–â–â–â–â–â–â–…â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–…â–ˆâ–ˆâ–â–ˆâ–â–â–â–â–â–â–ˆâ–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–ˆâ–…â–…â–â–…â–â–â–â–â–â–â–…â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–„â–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2ypw6dcq\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:39:27.052627: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3sm4rqy9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_023925-3sm4rqy9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/73 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 72/73 [00:04<00:00, 14.58it/s, loss=1.11, v_num=rqy9, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.098\n",
      "Epoch 0: 100%|â–ˆ| 73/73 [00:05<00:00, 14.51it/s, loss=1.11, v_num=rqy9, BTC_val_a\n",
      "Epoch 1:  99%|â–‰| 72/73 [00:05<00:00, 13.87it/s, loss=1.11, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 73/73 [00:05<00:00, 13.85it/s, loss=1.11, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Epoch 2:  99%|â–‰| 72/73 [00:05<00:00, 13.82it/s, loss=1.11, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 73/73 [00:05<00:00, 13.77it/s, loss=1.11, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Epoch 3:  99%|â–‰| 72/73 [00:06<00:00, 11.67it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 73/73 [00:06<00:00, 11.67it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n",
      "Epoch 4:  99%|â–‰| 72/73 [00:06<00:00, 11.99it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 73/73 [00:06<00:00, 11.99it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n",
      "Epoch 5:  99%|â–‰| 72/73 [00:05<00:00, 12.00it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 73/73 [00:06<00:00, 12.00it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n",
      "Epoch 6:  99%|â–‰| 72/73 [00:05<00:00, 12.82it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 73/73 [00:05<00:00, 12.80it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n",
      "Epoch 7:  99%|â–‰| 72/73 [00:06<00:00, 11.72it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 73/73 [00:06<00:00, 11.71it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n",
      "Epoch 8:  99%|â–‰| 72/73 [00:05<00:00, 12.12it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 73/73 [00:06<00:00, 12.11it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n",
      "Epoch 9:  99%|â–‰| 72/73 [00:06<00:00, 11.70it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 73/73 [00:06<00:00, 11.69it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n",
      "Epoch 10:  99%|â–‰| 72/73 [00:05<00:00, 12.83it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 73/73 [00:05<00:00, 12.82it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Epoch 11:  99%|â–‰| 72/73 [00:05<00:00, 13.69it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 73/73 [00:05<00:00, 13.63it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Epoch 12:  99%|â–‰| 72/73 [00:05<00:00, 14.13it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 73/73 [00:05<00:00, 14.05it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Epoch 13:  99%|â–‰| 72/73 [00:06<00:00, 11.31it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 73/73 [00:06<00:00, 11.30it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Epoch 14:  99%|â–‰| 72/73 [00:05<00:00, 14.10it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 73/73 [00:05<00:00, 14.07it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Epoch 15:  99%|â–‰| 72/73 [00:05<00:00, 12.06it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 73/73 [00:06<00:00, 12.04it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Epoch 16:  99%|â–‰| 72/73 [00:05<00:00, 12.06it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 73/73 [00:06<00:00, 12.07it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Epoch 17:  99%|â–‰| 72/73 [00:06<00:00, 10.94it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 73/73 [00:06<00:00, 10.93it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Epoch 18:  99%|â–‰| 72/73 [00:06<00:00, 11.24it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 73/73 [00:06<00:00, 11.26it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Epoch 19:  99%|â–‰| 72/73 [00:06<00:00, 11.22it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 73/73 [00:06<00:00, 11.23it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Epoch 20:  99%|â–‰| 72/73 [00:05<00:00, 12.65it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.098. Signaling Trainer to stop.\n",
      "Epoch 20: 100%|â–ˆ| 73/73 [00:05<00:00, 12.63it/s, loss=1.1, v_num=rqy9, BTC_val_a\n",
      "Epoch 20: 100%|â–ˆ| 73/73 [00:05<00:00, 12.62it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 34.62it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5357142686843872,\n",
      " 'BTC_test_f1': 0.2323809713125229,\n",
      " 'ETH_test_acc': 0.5,\n",
      " 'ETH_test_f1': 0.2212051898241043,\n",
      " 'LTC_test_acc': 0.4285714328289032,\n",
      " 'LTC_test_f1': 0.1999756544828415,\n",
      " 'test_loss': 1.0990955829620361}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 80025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_023925-3sm4rqy9/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_023925-3sm4rqy9/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.18182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.4329\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.09175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 133\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621467698\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.46388\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.27602\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.44386\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.26423\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.46475\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.27176\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09899\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.18182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.22222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.25641\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.09812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.53571\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.23238\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.22121\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.42857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.19998\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.0991\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–„â–…â–‚â–„â–…â–ƒâ–‚â–â–…â–„â–ƒâ–…â–…â–ƒâ–…â–ƒâ–‚â–‡â–…â–…â–ƒâ–„â–ƒâ–†â–†â–‚â–ˆâ–„â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–„â–ˆâ–ƒâ–…â–„â–„â–ƒâ–â–‡â–„â–ƒâ–†â–„â–ƒâ–„â–ƒâ–„â–„â–…â–…â–ƒâ–ƒâ–‚â–…â–ƒâ–ƒâ–ˆâ–ƒâ–„â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–„â–ƒâ–…â–‚â–‡â–„â–‚â–‚â–…â–…â–„â–‚â–ƒâ–ƒâ–ƒâ–‚â–‡â–‡â–…â–…â–ˆâ–‡â–â–â–…â–‚â–…â–†â–†â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–‡â–…â–†â–ƒâ–„â–…â–„â–â–‡â–ˆâ–ƒâ–…â–‚â–„â–„â–â–‡â–†â–…â–…â–ˆâ–‡â–â–â–ƒâ–‚â–ƒâ–„â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–…â–‚â–ƒâ–†â–ƒâ–â–ƒâ–…â–ƒâ–…â–„â–‚â–†â–‡â–†â–ƒâ–†â–…â–ƒâ–†â–„â–„â–ƒâ–ƒâ–†â–„â–†â–ˆâ–‚â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–†â–‚â–„â–„â–‚â–â–‚â–…â–ƒâ–„â–„â–â–ƒâ–ˆâ–‚â–â–„â–‚â–‚â–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–…â–…â–â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–ƒâ–‡â–„â–ˆâ–‚â–‡â–ƒâ–…â–ƒâ–‚â–‚â–†â–‚â–‚â–„â–…â–â–‚â–â–ƒâ–ƒâ–‚â–‡â–ƒâ–ƒâ–†â–ƒâ–‚â–„â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–„â–„â–„â–ƒâ–â–†â–„â–„â–…â–…â–†â–„â–‡â–‡â–…â–ˆâ–†â–ˆâ–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–ˆâ–…â–ˆâ–…â–…â–…â–„â–ƒâ–†â–ˆâ–…â–ƒâ–â–‡â–ƒâ–„â–…â–ƒâ–„â–…â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–ƒâ–‚â–„â–…â–â–‡â–„â–†â–„â–„â–‡â–…â–ˆâ–†â–‡â–ˆâ–†â–‡â–ˆâ–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–…â–ƒâ–ˆâ–ˆâ–†â–‡â–…â–‡â–…â–†â–‡â–†â–„â–†â–†â–…â–„â–â–…â–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–„â–„â–„â–„â–â–…â–…â–…â–„â–„â–…â–†â–†â–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–†â–…â–ˆâ–†â–ƒâ–ƒâ–‡â–ƒâ–ƒâ–…â–â–ˆâ–â–‚â–„â–ƒâ–…â–ƒâ–„â–…â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–†â–„â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–ˆâ–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–ˆâ–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–ˆâ–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–ˆâ–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ƒâ–ˆâ–…â–„â–„â–ƒâ–‚â–ƒâ–ƒâ–â–â–â–…â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3sm4rqy9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:41:52.986769: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_loss_weighted_multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2o16u86o\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024151-2o16u86o\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|â–ˆ| 74/74 [00:05<00:00, 12.92it/s, loss=1.13, v_num=u86o, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.084\n",
      "Epoch 0: 100%|â–ˆ| 74/74 [00:05<00:00, 12.86it/s, loss=1.13, v_num=u86o, BTC_val_a\n",
      "Epoch 1: 100%|â–ˆ| 74/74 [00:05<00:00, 12.66it/s, loss=1.11, v_num=u86o, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 74/74 [00:05<00:00, 12.61it/s, loss=1.11, v_num=u86o, BTC_val_a\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 74/74 [00:05<00:00, 13.45it/s, loss=1.11, v_num=u86o, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 74/74 [00:05<00:00, 13.38it/s, loss=1.11, v_num=u86o, BTC_val_a\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆ| 74/74 [00:06<00:00, 10.93it/s, loss=1.1, v_num=u86o, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 74/74 [00:06<00:00, 10.84it/s, loss=1.1, v_num=u86o, BTC_val_ac\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 74/74 [00:07<00:00,  9.56it/s, loss=1.1, v_num=u86o, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 74/74 [00:07<00:00,  9.53it/s, loss=1.1, v_num=u86o, BTC_val_ac\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 74/74 [00:05<00:00, 12.64it/s, loss=1.1, v_num=u86o, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 74/74 [00:05<00:00, 12.59it/s, loss=1.1, v_num=u86o, BTC_val_ac\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 74/74 [00:06<00:00, 12.30it/s, loss=1.09, v_num=u86o, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 74/74 [00:06<00:00, 12.25it/s, loss=1.09, v_num=u86o, BTC_val_a\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 74/74 [00:05<00:00, 12.96it/s, loss=1.1, v_num=u86o, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 74/74 [00:05<00:00, 12.90it/s, loss=1.1, v_num=u86o, BTC_val_ac\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 74/74 [00:05<00:00, 13.21it/s, loss=1.1, v_num=u86o, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 74/74 [00:05<00:00, 13.13it/s, loss=1.1, v_num=u86o, BTC_val_ac\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 74/74 [00:06<00:00, 12.27it/s, loss=1.09, v_num=u86o, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 74/74 [00:06<00:00, 12.20it/s, loss=1.09, v_num=u86o, BTC_val_a\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 74/74 [00:06<00:00, 12.31it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 74/74 [00:06<00:00, 12.24it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 74/74 [00:05<00:00, 12.93it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 74/74 [00:05<00:00, 12.73it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 74/74 [00:05<00:00, 12.51it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 74/74 [00:05<00:00, 12.45it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 74/74 [00:06<00:00, 11.86it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 74/74 [00:06<00:00, 11.80it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 74/74 [00:05<00:00, 12.34it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 74/74 [00:06<00:00, 12.27it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 74/74 [00:06<00:00, 12.24it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 74/74 [00:06<00:00, 12.18it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 74/74 [00:05<00:00, 12.76it/s, loss=1.09, v_num=u86o, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 74/74 [00:05<00:00, 12.70it/s, loss=1.09, v_num=u86o, BTC_val_\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 74/74 [00:06<00:00, 11.39it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 74/74 [00:06<00:00, 11.34it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 74/74 [00:05<00:00, 12.72it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 74/74 [00:05<00:00, 12.59it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 74/74 [00:06<00:00, 12.00it/s, loss=1.11, v_num=u86o, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 74/74 [00:06<00:00, 11.95it/s, loss=1.11, v_num=u86o, BTC_val_\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:06<00:00, 12.09it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.084. Signaling Trainer to stop.\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:06<00:00, 12.04it/s, loss=1.1, v_num=u86o, BTC_val_a\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:06<00:00, 12.03it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 32.69it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.2142857164144516,\n",
      " 'BTC_test_f1': 0.11375661939382553,\n",
      " 'ETH_test_acc': 0.3571428656578064,\n",
      " 'ETH_test_f1': 0.2409733235836029,\n",
      " 'LTC_test_acc': 0.25,\n",
      " 'LTC_test_f1': 0.17094017565250397,\n",
      " 'test_loss': 1.101166844367981}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 80369\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024151-2o16u86o/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024151-2o16u86o/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.24444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.25098\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.4375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.32955\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.10041\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 137\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621467848\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.36623\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.31748\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.34545\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.29993\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.33766\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.29871\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.18182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.18182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.25641\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.09315\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.21429\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.11376\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.35714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.24097\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.17094\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.10117\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–‚â–ˆâ–†â–„â–ƒâ–„â–†â–â–„â–…â–…â–ƒâ–…â–‡â–…â–„â–…â–„â–„â–…â–‚â–…â–ˆâ–„â–…â–†â–…â–‡â–…â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–‚â–‡â–…â–„â–‚â–ƒâ–†â–â–„â–„â–…â–ƒâ–„â–ˆâ–†â–ƒâ–…â–‚â–„â–„â–‚â–„â–ˆâ–ƒâ–†â–…â–…â–‡â–…â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–‚â–‚â–…â–‚â–„â–…â–…â–‚â–‡â–â–„â–‚â–â–…â–‚â–â–„â–„â–â–„â–‚â–â–ˆâ–ˆâ–‚â–‡â–…â–â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–ƒâ–‚â–…â–ƒâ–ƒâ–†â–†â–ƒâ–‡â–â–…â–ƒâ–‚â–„â–ƒâ–â–…â–ƒâ–‚â–„â–ƒâ–‚â–ˆâ–ˆâ–ƒâ–‡â–†â–â–ƒâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–„â–†â–…â–â–ƒâ–„â–…â–…â–„â–ƒâ–ƒâ–‚â–†â–„â–ƒâ–ˆâ–„â–„â–ƒâ–…â–…â–…â–…â–…â–ƒâ–„â–„â–…â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–„â–†â–„â–â–‚â–ƒâ–…â–…â–„â–ƒâ–ƒâ–‚â–„â–„â–ƒâ–ˆâ–„â–ƒâ–ƒâ–…â–„â–…â–ƒâ–…â–ƒâ–ƒâ–ƒâ–„â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–ˆâ–‚â–‚â–†â–†â–†â–ƒâ–ˆâ–„â–…â–„â–†â–ƒâ–„â–„â–ƒâ–„â–ƒâ–…â–ƒâ–„â–„â–‚â–â–…â–ƒâ–‚â–‚â–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–ƒâ–ˆâ–â–†â–‚â–…â–‡â–…â–„â–…â–…â–‚â–…â–†â–ƒâ–†â–‡â–…â–ƒâ–‡â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–…â–…â–â–„â–„â–†â–…â–ƒâ–†â–„â–†â–‚â–„â–‡â–…â–ˆâ–‡â–ƒâ–…â–ˆâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–ƒâ–…â–â–„â–‚â–…â–…â–…â–…â–…â–‡â–…â–‚â–‚â–„â–ˆâ–ˆâ–…â–‡â–‚â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–„â–‚â–â–ƒâ–…â–†â–†â–„â–†â–„â–ˆâ–…â–‚â–ƒâ–…â–†â–‡â–ƒâ–†â–‚â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–„â–‡â–‚â–„â–†â–…â–â–‡â–ƒâ–ˆâ–‡â–†â–…â–…â–ˆâ–ˆâ–„â–ˆâ–„â–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–†â–ƒâ–â–â–‡â–†â–â–…â–…â–„â–ˆâ–†â–‚â–…â–‡â–†â–„â–…â–…â–…â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–„â–„â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–â–â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–†â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–â–ˆâ–†â–„â–ƒâ–„â–‚â–…â–„â–‚â–ƒâ–ƒâ–ƒâ–„â–‚â–‚â–â–ƒâ–ƒâ–…â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2o16u86o\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:44:24.174296: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm__binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/187zic8f\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024422-187zic8f\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 130   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.7 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|â–ˆ| 73/73 [00:02<00:00, 29.04it/s, loss=0.583, v_num=ic8f, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 73/73 [00:02<00:00, 28.84it/s, loss=0.583, v_num=ic8f, BTC_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved. New best score: 0.562\n",
      "Epoch 1:  99%|â–‰| 72/73 [00:02<00:00, 31.61it/s, loss=0.572, v_num=ic8f, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 73/73 [00:02<00:00, 31.09it/s, loss=0.572, v_num=ic8f, BTC_val_\u001b[A\n",
      "Epoch 2:  99%|â–‰| 72/73 [00:02<00:00, 31.52it/s, loss=0.563, v_num=ic8f, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.554\n",
      "Epoch 2: 100%|â–ˆ| 73/73 [00:02<00:00, 30.97it/s, loss=0.563, v_num=ic8f, BTC_val_\n",
      "Epoch 3:  99%|â–‰| 72/73 [00:02<00:00, 28.93it/s, loss=0.557, v_num=ic8f, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 73/73 [00:02<00:00, 28.36it/s, loss=0.557, v_num=ic8f, BTC_val_\u001b[A\n",
      "Epoch 4:  99%|â–‰| 72/73 [00:02<00:00, 28.87it/s, loss=0.567, v_num=ic8f, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 73/73 [00:02<00:00, 28.36it/s, loss=0.567, v_num=ic8f, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 72/73 [00:02<00:00, 26.80it/s, loss=0.594, v_num=ic8f, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 73/73 [00:02<00:00, 26.45it/s, loss=0.594, v_num=ic8f, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 72/73 [00:03<00:00, 23.06it/s, loss=0.575, v_num=ic8f, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 73/73 [00:03<00:00, 22.78it/s, loss=0.575, v_num=ic8f, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 72/73 [00:02<00:00, 27.62it/s, loss=0.591, v_num=ic8f, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 73/73 [00:02<00:00, 27.18it/s, loss=0.591, v_num=ic8f, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 72/73 [00:02<00:00, 30.03it/s, loss=0.546, v_num=ic8f, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 73/73 [00:02<00:00, 29.39it/s, loss=0.546, v_num=ic8f, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 72/73 [00:03<00:00, 23.03it/s, loss=0.539, v_num=ic8f, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 73/73 [00:03<00:00, 22.80it/s, loss=0.539, v_num=ic8f, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 72/73 [00:02<00:00, 26.41it/s, loss=0.584, v_num=ic8f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 73/73 [00:02<00:00, 25.91it/s, loss=0.584, v_num=ic8f, BTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 72/73 [00:02<00:00, 26.54it/s, loss=0.58, v_num=ic8f, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 73/73 [00:02<00:00, 26.15it/s, loss=0.58, v_num=ic8f, BTC_val_\u001b[A\n",
      "Epoch 12:  99%|â–‰| 72/73 [00:02<00:00, 30.12it/s, loss=0.55, v_num=ic8f, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 73/73 [00:02<00:00, 29.59it/s, loss=0.55, v_num=ic8f, BTC_val_\u001b[A\n",
      "Epoch 13:  99%|â–‰| 72/73 [00:02<00:00, 26.24it/s, loss=0.56, v_num=ic8f, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 73/73 [00:02<00:00, 25.77it/s, loss=0.56, v_num=ic8f, BTC_val_\u001b[A\n",
      "Epoch 14:  99%|â–‰| 72/73 [00:02<00:00, 24.22it/s, loss=0.584, v_num=ic8f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 73/73 [00:03<00:00, 23.99it/s, loss=0.584, v_num=ic8f, BTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 72/73 [00:02<00:00, 25.44it/s, loss=0.551, v_num=ic8f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 73/73 [00:02<00:00, 24.97it/s, loss=0.551, v_num=ic8f, BTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 72/73 [00:02<00:00, 25.12it/s, loss=0.548, v_num=ic8f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 73/73 [00:02<00:00, 24.85it/s, loss=0.548, v_num=ic8f, BTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 72/73 [00:02<00:00, 24.82it/s, loss=0.544, v_num=ic8f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 73/73 [00:02<00:00, 24.41it/s, loss=0.544, v_num=ic8f, BTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 72/73 [00:02<00:00, 29.36it/s, loss=0.555, v_num=ic8f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 73/73 [00:02<00:00, 28.72it/s, loss=0.555, v_num=ic8f, BTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 72/73 [00:02<00:00, 29.35it/s, loss=0.562, v_num=ic8f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 73/73 [00:02<00:00, 28.91it/s, loss=0.562, v_num=ic8f, BTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 72/73 [00:02<00:00, 28.28it/s, loss=0.58, v_num=ic8f, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 73/73 [00:02<00:00, 27.89it/s, loss=0.58, v_num=ic8f, BTC_val_\u001b[A\n",
      "Epoch 21:  99%|â–‰| 72/73 [00:03<00:00, 22.55it/s, loss=0.554, v_num=ic8f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 73/73 [00:03<00:00, 22.32it/s, loss=0.554, v_num=ic8f, BTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 72/73 [00:02<00:00, 28.93it/s, loss=0.567, v_num=ic8f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.554. Signaling Trainer to stop.\n",
      "Epoch 22: 100%|â–ˆ| 73/73 [00:02<00:00, 28.51it/s, loss=0.567, v_num=ic8f, BTC_val\n",
      "Epoch 22: 100%|â–ˆ| 73/73 [00:02<00:00, 28.46it/s, loss=0.567, v_num=ic8f, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 56.96it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6785714030265808,\n",
      " 'BTC_test_f1': 0.6750550866127014,\n",
      " 'ETH_test_acc': 0.6785714030265808,\n",
      " 'ETH_test_f1': 0.6694621443748474,\n",
      " 'LTC_test_acc': 0.75,\n",
      " 'LTC_test_f1': 0.7454982399940491,\n",
      " 'test_loss': 0.5896449685096741}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 80732\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024422-187zic8f/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024422-187zic8f/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.6875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.65368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.56364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.60141\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 22\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1656\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621467934\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 79\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.71453\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.69841\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.71976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.70568\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.70409\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.68924\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.56223\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.56364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.61905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.61905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.58625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.67857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.67506\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.67857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.66946\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.7455\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.58964\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–„â–…â–ƒâ–ˆâ–…â–„â–†â–…â–â–‡â–‚â–‚â–…â–†â–…â–‚â–ƒâ–‚â–…â–†â–…â–…â–…â–…â–„â–„â–…â–…â–‡â–…â–†â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–ƒâ–ƒâ–ƒâ–ˆâ–„â–ƒâ–†â–…â–â–‡â–‚â–‚â–…â–†â–…â–‚â–ƒâ–‚â–ƒâ–†â–…â–„â–…â–…â–ƒâ–ƒâ–…â–„â–‡â–„â–†â–…â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–…â–†â–„â–†â–„â–„â–…â–ƒâ–‚â–ˆâ–„â–‚â–„â–…â–…â–…â–…â–„â–†â–‡â–…â–†â–â–…â–ƒâ–…â–…â–„â–‡â–„â–†â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–†â–†â–ƒâ–†â–ƒâ–„â–†â–ƒâ–ƒâ–ˆâ–„â–ƒâ–„â–†â–…â–…â–†â–„â–†â–‡â–†â–†â–â–…â–‚â–…â–†â–„â–‡â–„â–†â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–†â–…â–ƒâ–‡â–‡â–‡â–ˆâ–â–…â–‡â–…â–…â–ƒâ–†â–‡â–‡â–…â–ƒâ–…â–‡â–ƒâ–‡â–†â–‡â–†â–†â–ˆâ–…â–‡â–ƒâ–ˆâ–‡â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–…â–ƒâ–‚â–‡â–‡â–‡â–ˆâ–â–…â–‡â–„â–„â–ƒâ–„â–‡â–‡â–„â–ƒâ–„â–†â–ƒâ–‡â–„â–‡â–†â–…â–ˆâ–„â–‡â–‚â–ˆâ–‡â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–…â–†â–‡â–‚â–ƒâ–…â–‚â–…â–†â–â–…â–†â–…â–„â–„â–…â–…â–ˆâ–„â–ƒâ–ƒâ–ƒâ–…â–ƒâ–†â–…â–ƒâ–…â–â–…â–‚â–‚â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–†â–†â–‡â–†â–‡â–†â–‡â–†â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–†â–†â–ˆâ–ˆâ–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–â–†â–†â–‡â–‡â–‡â–‡â–‡â–†â–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–†â–†â–†â–†â–‡â–†â–†â–†â–‡â–‡â–†â–‡â–†â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–â–†â–‡â–‡â–‡â–‡â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–â–…â–‡â–†â–†â–‡â–†â–ˆâ–ˆâ–‡â–‡â–†â–‡â–†â–‡â–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–â–…â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–…â–…â–ˆâ–…â–…â–â–â–â–…â–…â–â–â–ˆâ–â–â–ˆâ–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–…â–…â–ˆâ–…â–…â–â–â–â–…â–…â–â–â–ˆâ–â–â–ˆâ–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–†â–â–ƒâ–â–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–†â–†â–ƒâ–†â–†â–ƒâ–ˆâ–ƒâ–†â–†â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–…â–â–ƒâ–â–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–†â–†â–ƒâ–†â–†â–ƒâ–ˆâ–ƒâ–†â–†â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–ˆâ–…â–…â–ˆâ–…â–…â–…â–ˆâ–…â–…â–…â–…â–â–ˆâ–…â–â–…â–…â–…â–…â–…â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–†â–…â–ƒâ–ˆâ–…â–…â–…â–ˆâ–…â–…â–…â–…â–â–ˆâ–…â–â–…â–…â–…â–…â–…â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–‚â–†â–â–†â–…â–‡â–…â–†â–…â–„â–†â–…â–ƒâ–‡â–‡â–„â–†â–„â–„â–ˆâ–…â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/187zic8f\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:45:49.412436: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm__binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3bkiy2jg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024547-3bkiy2jg\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 130   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.7 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 73/74 [00:02<00:00, 30.78it/s, loss=0.709, v_num=y2jg, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.684\n",
      "Epoch 0: 100%|â–ˆ| 74/74 [00:02<00:00, 30.11it/s, loss=0.709, v_num=y2jg, BTC_val_\n",
      "Epoch 1:  99%|â–‰| 73/74 [00:02<00:00, 31.10it/s, loss=0.704, v_num=y2jg, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 74/74 [00:02<00:00, 30.41it/s, loss=0.704, v_num=y2jg, BTC_val_\u001b[A\n",
      "Epoch 2:  99%|â–‰| 73/74 [00:02<00:00, 31.59it/s, loss=0.693, v_num=y2jg, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 74/74 [00:02<00:00, 31.04it/s, loss=0.693, v_num=y2jg, BTC_val_\u001b[A\n",
      "Epoch 3:  99%|â–‰| 73/74 [00:02<00:00, 32.03it/s, loss=0.698, v_num=y2jg, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 74/74 [00:02<00:00, 31.42it/s, loss=0.698, v_num=y2jg, BTC_val_\u001b[A\n",
      "Epoch 4:  99%|â–‰| 73/74 [00:02<00:00, 31.99it/s, loss=0.692, v_num=y2jg, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.677\n",
      "Epoch 4: 100%|â–ˆ| 74/74 [00:02<00:00, 31.34it/s, loss=0.692, v_num=y2jg, BTC_val_\n",
      "Epoch 5:  99%|â–‰| 73/74 [00:02<00:00, 29.03it/s, loss=0.696, v_num=y2jg, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 74/74 [00:02<00:00, 28.31it/s, loss=0.696, v_num=y2jg, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 73/74 [00:02<00:00, 29.85it/s, loss=0.693, v_num=y2jg, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 74/74 [00:02<00:00, 29.28it/s, loss=0.693, v_num=y2jg, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 73/74 [00:02<00:00, 30.97it/s, loss=0.695, v_num=y2jg, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 74/74 [00:02<00:00, 30.37it/s, loss=0.695, v_num=y2jg, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 73/74 [00:02<00:00, 32.23it/s, loss=0.694, v_num=y2jg, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 74/74 [00:02<00:00, 31.69it/s, loss=0.694, v_num=y2jg, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 73/74 [00:02<00:00, 30.35it/s, loss=0.695, v_num=y2jg, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 74/74 [00:02<00:00, 29.56it/s, loss=0.695, v_num=y2jg, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 73/74 [00:02<00:00, 24.95it/s, loss=0.694, v_num=y2jg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 74/74 [00:03<00:00, 24.64it/s, loss=0.694, v_num=y2jg, BTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 73/74 [00:02<00:00, 25.60it/s, loss=0.692, v_num=y2jg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 74/74 [00:02<00:00, 25.19it/s, loss=0.692, v_num=y2jg, BTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 73/74 [00:02<00:00, 29.24it/s, loss=0.692, v_num=y2jg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 74/74 [00:02<00:00, 28.51it/s, loss=0.692, v_num=y2jg, BTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 73/74 [00:02<00:00, 27.74it/s, loss=0.698, v_num=y2jg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 74/74 [00:02<00:00, 27.27it/s, loss=0.698, v_num=y2jg, BTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 73/74 [00:02<00:00, 26.95it/s, loss=0.692, v_num=y2jg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 74/74 [00:02<00:00, 26.46it/s, loss=0.692, v_num=y2jg, BTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 73/74 [00:02<00:00, 26.85it/s, loss=0.696, v_num=y2jg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 74/74 [00:02<00:00, 26.52it/s, loss=0.696, v_num=y2jg, BTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 73/74 [00:02<00:00, 26.79it/s, loss=0.693, v_num=y2jg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 74/74 [00:02<00:00, 26.39it/s, loss=0.693, v_num=y2jg, BTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 73/74 [00:02<00:00, 26.14it/s, loss=0.694, v_num=y2jg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 74/74 [00:03<00:00, 24.52it/s, loss=0.694, v_num=y2jg, BTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 73/74 [00:02<00:00, 28.16it/s, loss=0.692, v_num=y2jg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 74/74 [00:02<00:00, 27.71it/s, loss=0.692, v_num=y2jg, BTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 73/74 [00:03<00:00, 23.13it/s, loss=0.694, v_num=y2jg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 74/74 [00:03<00:00, 22.78it/s, loss=0.694, v_num=y2jg, BTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 73/74 [00:02<00:00, 30.08it/s, loss=0.695, v_num=y2jg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:02<00:00, 29.51it/s, loss=0.695, v_num=y2jg, BTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 73/74 [00:02<00:00, 27.96it/s, loss=0.694, v_num=y2jg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 74/74 [00:02<00:00, 27.40it/s, loss=0.694, v_num=y2jg, BTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 73/74 [00:02<00:00, 25.37it/s, loss=0.691, v_num=y2jg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 74/74 [00:02<00:00, 25.07it/s, loss=0.691, v_num=y2jg, BTC_val\u001b[A\n",
      "Epoch 23:  99%|â–‰| 73/74 [00:02<00:00, 29.85it/s, loss=0.691, v_num=y2jg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 74/74 [00:02<00:00, 29.41it/s, loss=0.691, v_num=y2jg, BTC_val\u001b[A\n",
      "Epoch 24:  99%|â–‰| 73/74 [00:02<00:00, 25.29it/s, loss=0.695, v_num=y2jg, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 74/74 [00:02<00:00, 24.89it/s, loss=0.695, v_num=y2jg, BTC_valMonitored metric val_loss did not improve in the last 20 records. Best score: 0.677. Signaling Trainer to stop.\n",
      "\n",
      "Epoch 24: 100%|â–ˆ| 74/74 [00:02<00:00, 24.85it/s, loss=0.695, v_num=y2jg, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 44.20it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.3928571343421936,\n",
      " 'BTC_test_f1': 0.2789115607738495,\n",
      " 'ETH_test_acc': 0.6428571343421936,\n",
      " 'ETH_test_f1': 0.38938775658607483,\n",
      " 'LTC_test_acc': 0.4285714328289032,\n",
      " 'LTC_test_f1': 0.293949156999588,\n",
      " 'test_loss': 0.6996669173240662}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 80952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024547-3bkiy2jg/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024547-3bkiy2jg/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.4375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.30435\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.3125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.2381\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.27273\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.69268\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 76\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621468023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 86\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.54026\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.36885\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.50216\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.34727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.49177\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.34737\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69361\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.38462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.38462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.38462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.68654\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.39286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.27891\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.64286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.38939\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.42857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.29395\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.69967\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–…â–„â–…â–‚â–â–ˆâ–ƒâ–†â–„â–‡â–‡â–„â–„â–…â–†â–†â–†â–†â–†â–ˆâ–ƒâ–…â–ƒâ–‚â–…â–…â–†â–ƒâ–†â–…â–‡â–„â–…â–†â–ƒâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–ƒâ–„â–„â–‚â–â–‡â–ƒâ–‡â–…â–ˆâ–ˆâ–‚â–…â–„â–‡â–†â–…â–ƒâ–…â–‡â–ƒâ–„â–‚â–â–„â–„â–‡â–ƒâ–†â–„â–ˆâ–‚â–†â–…â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–„â–„â–ƒâ–‚â–…â–…â–…â–†â–„â–‡â–‚â–…â–…â–ˆâ–†â–…â–‡â–‡â–‡â–†â–†â–â–„â–„â–„â–ƒâ–ƒâ–…â–„â–„â–ƒâ–„â–†â–…â–†â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–…â–…â–„â–‚â–†â–„â–†â–‡â–…â–†â–ƒâ–ƒâ–†â–ˆâ–‡â–ƒâ–„â–„â–‡â–ƒâ–‡â–‚â–„â–…â–‚â–ƒâ–„â–…â–‚â–‚â–„â–‚â–…â–…â–…â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–‡â–…â–„â–„â–â–‡â–†â–…â–‡â–†â–†â–„â–‡â–†â–†â–†â–ˆâ–‡â–„â–‡â–†â–‡â–…â–ˆâ–ˆâ–…â–„â–ƒâ–„â–†â–â–…â–…â–‡â–†â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–‡â–„â–ƒâ–„â–â–‡â–†â–…â–‡â–…â–†â–‚â–‡â–†â–†â–…â–†â–„â–‚â–„â–†â–†â–ƒâ–ˆâ–†â–„â–„â–ƒâ–‚â–…â–â–ƒâ–ƒâ–„â–†â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–â–†â–†â–†â–ˆâ–ƒâ–„â–‚â–ƒâ–ƒâ–„â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–…â–ƒâ–„â–„â–…â–„â–„â–„â–…â–…â–…â–„â–…â–†â–„â–„â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–…â–ƒâ–„â–‚â–†â–‚â–…â–â–ƒâ–â–ƒâ–†â–‡â–‡â–‡â–†â–ƒâ–‡â–…â–…â–†â–ƒâ–„â–„â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–…â–‡â–‡â–„â–‡â–†â–‡â–†â–ƒâ–ƒâ–‡â–‚â–‚â–‚â–ˆâ–‚â–…â–‚â–‡â–†â–„â–„â–„â–ƒâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–„â–ƒâ–†â–…â–ˆâ–†â–‚â–ƒâ–â–ˆâ–â–„â–„â–†â–â–„â–ƒâ–„â–„â–†â–ƒâ–†â–‡â–…â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–…â–‡â–‡â–…â–ˆâ–ˆâ–†â–‡â–ƒâ–†â–†â–â–‚â–‚â–†â–‚â–‡â–‚â–ˆâ–†â–ƒâ–…â–…â–„â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–ˆâ–…â–â–„â–…â–†â–…â–„â–„â–†â–†â–…â–ƒâ–ƒâ–„â–‚â–†â–…â–ƒâ–†â–„â–ƒâ–‚â–†â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–‡â–ˆâ–†â–…â–ˆâ–ˆâ–ˆâ–‡â–…â–…â–ˆâ–ƒâ–ƒâ–â–ˆâ–‚â–ˆâ–‚â–‡â–†â–…â–…â–ƒâ–…â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–ˆâ–â–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–ˆâ–â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ƒâ–ˆâ–ƒâ–…â–â–„â–„â–„â–†â–â–…â–…â–…â–…â–…â–…â–…â–…â–„â–‚â–„â–…â–…â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3bkiy2jg\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:47:23.634543: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm__multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1yic4mss\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024722-1yic4mss\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 195   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.8 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|â–ˆ| 73/73 [00:02<00:00, 28.73it/s, loss=1.03, v_num=4mss, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.039\n",
      "Epoch 0: 100%|â–ˆ| 73/73 [00:02<00:00, 28.46it/s, loss=1.03, v_num=4mss, BTC_val_a\n",
      "Epoch 1:  99%|â–‰| 72/73 [00:02<00:00, 32.13it/s, loss=1.04, v_num=4mss, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 73/73 [00:02<00:00, 31.38it/s, loss=1.04, v_num=4mss, BTC_val_a\u001b[A\n",
      "Epoch 2:  99%|â–‰| 72/73 [00:02<00:00, 30.20it/s, loss=1.03, v_num=4mss, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.026 >= min_delta = 0.003. New best score: 1.013\n",
      "Epoch 2: 100%|â–ˆ| 73/73 [00:02<00:00, 29.60it/s, loss=1.03, v_num=4mss, BTC_val_a\n",
      "Epoch 3:  99%|â–‰| 72/73 [00:02<00:00, 30.80it/s, loss=0.997, v_num=4mss, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 0.992\n",
      "Epoch 3: 100%|â–ˆ| 73/73 [00:02<00:00, 30.15it/s, loss=0.997, v_num=4mss, BTC_val_\n",
      "Epoch 4:  99%|â–‰| 72/73 [00:02<00:00, 28.18it/s, loss=0.955, v_num=4mss, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.019 >= min_delta = 0.003. New best score: 0.973\n",
      "Epoch 4: 100%|â–ˆ| 73/73 [00:02<00:00, 27.67it/s, loss=0.955, v_num=4mss, BTC_val_\n",
      "Epoch 5:  99%|â–‰| 72/73 [00:02<00:00, 27.62it/s, loss=0.957, v_num=4mss, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 73/73 [00:02<00:00, 27.20it/s, loss=0.957, v_num=4mss, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|â–‰| 72/73 [00:02<00:00, 31.66it/s, loss=0.933, v_num=4mss, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 73/73 [00:02<00:00, 31.15it/s, loss=0.933, v_num=4mss, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|â–‰| 72/73 [00:02<00:00, 32.28it/s, loss=0.917, v_num=4mss, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 73/73 [00:02<00:00, 31.72it/s, loss=0.917, v_num=4mss, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 72/73 [00:02<00:00, 32.09it/s, loss=0.902, v_num=4mss, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 73/73 [00:02<00:00, 31.41it/s, loss=0.902, v_num=4mss, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 72/73 [00:02<00:00, 30.96it/s, loss=0.896, v_num=4mss, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.024 >= min_delta = 0.003. New best score: 0.950\n",
      "Epoch 9: 100%|â–ˆ| 73/73 [00:02<00:00, 30.26it/s, loss=0.896, v_num=4mss, BTC_val_\n",
      "Epoch 10:  99%|â–‰| 72/73 [00:02<00:00, 29.62it/s, loss=0.915, v_num=4mss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 73/73 [00:02<00:00, 28.92it/s, loss=0.915, v_num=4mss, BTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 72/73 [00:02<00:00, 27.08it/s, loss=0.938, v_num=4mss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 73/73 [00:02<00:00, 26.71it/s, loss=0.938, v_num=4mss, BTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 72/73 [00:02<00:00, 27.06it/s, loss=0.908, v_num=4mss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 73/73 [00:02<00:00, 26.49it/s, loss=0.908, v_num=4mss, BTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 72/73 [00:02<00:00, 31.19it/s, loss=0.924, v_num=4mss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 73/73 [00:02<00:00, 30.46it/s, loss=0.924, v_num=4mss, BTC_val\u001b[A\n",
      "Epoch 14:  99%|â–‰| 72/73 [00:02<00:00, 29.61it/s, loss=0.921, v_num=4mss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 73/73 [00:02<00:00, 29.19it/s, loss=0.921, v_num=4mss, BTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 72/73 [00:02<00:00, 26.62it/s, loss=0.918, v_num=4mss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 73/73 [00:02<00:00, 26.19it/s, loss=0.918, v_num=4mss, BTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 72/73 [00:02<00:00, 28.99it/s, loss=0.927, v_num=4mss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 73/73 [00:02<00:00, 28.40it/s, loss=0.927, v_num=4mss, BTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 72/73 [00:02<00:00, 24.63it/s, loss=0.935, v_num=4mss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 73/73 [00:02<00:00, 24.38it/s, loss=0.935, v_num=4mss, BTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 72/73 [00:02<00:00, 26.69it/s, loss=0.948, v_num=4mss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 73/73 [00:02<00:00, 26.19it/s, loss=0.948, v_num=4mss, BTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 72/73 [00:02<00:00, 24.86it/s, loss=0.909, v_num=4mss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 73/73 [00:02<00:00, 24.59it/s, loss=0.909, v_num=4mss, BTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 72/73 [00:02<00:00, 26.73it/s, loss=0.907, v_num=4mss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 73/73 [00:02<00:00, 26.29it/s, loss=0.907, v_num=4mss, BTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 72/73 [00:02<00:00, 30.55it/s, loss=0.909, v_num=4mss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 73/73 [00:02<00:00, 29.93it/s, loss=0.909, v_num=4mss, BTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 72/73 [00:02<00:00, 25.40it/s, loss=0.957, v_num=4mss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 73/73 [00:02<00:00, 25.13it/s, loss=0.957, v_num=4mss, BTC_val\u001b[A\n",
      "Epoch 23:  99%|â–‰| 72/73 [00:03<00:00, 23.94it/s, loss=0.901, v_num=4mss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 73/73 [00:03<00:00, 21.87it/s, loss=0.901, v_num=4mss, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24:  99%|â–‰| 72/73 [00:02<00:00, 29.14it/s, loss=0.918, v_num=4mss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 73/73 [00:02<00:00, 28.72it/s, loss=0.918, v_num=4mss, BTC_val\u001b[A\n",
      "Epoch 25:  99%|â–‰| 72/73 [00:02<00:00, 28.98it/s, loss=0.914, v_num=4mss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 73/73 [00:02<00:00, 28.44it/s, loss=0.914, v_num=4mss, BTC_val\u001b[A\n",
      "Epoch 26:  99%|â–‰| 72/73 [00:02<00:00, 24.11it/s, loss=0.875, v_num=4mss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 73/73 [00:03<00:00, 23.88it/s, loss=0.875, v_num=4mss, BTC_val\u001b[A\n",
      "Epoch 27:  99%|â–‰| 72/73 [00:02<00:00, 29.91it/s, loss=0.861, v_num=4mss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 73/73 [00:02<00:00, 29.40it/s, loss=0.861, v_num=4mss, BTC_val\u001b[A\n",
      "Epoch 28:  99%|â–‰| 72/73 [00:02<00:00, 27.89it/s, loss=0.881, v_num=4mss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 73/73 [00:02<00:00, 27.38it/s, loss=0.881, v_num=4mss, BTC_val\u001b[A\n",
      "Epoch 29:  99%|â–‰| 72/73 [00:02<00:00, 24.71it/s, loss=0.908, v_num=4mss, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.950. Signaling Trainer to stop.\n",
      "Epoch 29: 100%|â–ˆ| 73/73 [00:02<00:00, 24.42it/s, loss=0.908, v_num=4mss, BTC_val\n",
      "Epoch 29: 100%|â–ˆ| 73/73 [00:02<00:00, 24.38it/s, loss=0.908, v_num=4mss, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 51.85it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5357142686843872,\n",
      " 'BTC_test_f1': 0.3350889980792999,\n",
      " 'ETH_test_acc': 0.6428571343421936,\n",
      " 'ETH_test_f1': 0.5395269393920898,\n",
      " 'LTC_test_acc': 0.5357142686843872,\n",
      " 'LTC_test_f1': 0.40317463874816895,\n",
      " 'test_loss': 0.8673278093338013}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 81190\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024722-1yic4mss/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024722-1yic4mss/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.22944\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.50952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.50909\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.99466\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 88\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621468130\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.53003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.39505\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.52567\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.41504\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.51436\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.38816\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.91315\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.37037\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.01231\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.53571\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.33509\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.64286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.53953\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.53571\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.40317\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.86733\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–„â–†â–ƒâ–„â–ƒâ–ƒâ–„â–ƒâ–ƒâ–…â–…â–…â–ˆâ–„â–ƒâ–‚â–ƒâ–…â–†â–ƒâ–„â–…â–…â–ƒâ–„â–†â–„â–‚â–„â–ƒâ–†â–…â–…â–ƒâ–‡â–‚â–‚â–ƒâ–…â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–‚â–ƒâ–‚â–‚â–‚â–‚â–„â–ƒâ–‚â–„â–„â–…â–†â–ƒâ–ƒâ–â–‚â–ƒâ–†â–…â–„â–„â–‡â–„â–…â–‡â–…â–‚â–ƒâ–‚â–‡â–„â–„â–‚â–ˆâ–‚â–‚â–ƒâ–„â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–„â–…â–„â–…â–â–„â–ƒâ–„â–„â–„â–„â–„â–†â–†â–„â–„â–‚â–…â–„â–„â–„â–…â–†â–‚â–†â–…â–„â–ƒâ–„â–†â–†â–…â–‡â–ˆâ–‡â–‚â–„â–‡â–ˆâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–‚â–„â–‚â–‚â–â–ƒâ–‚â–ƒâ–ƒâ–„â–„â–ƒâ–„â–„â–‚â–ƒâ–â–…â–ƒâ–ƒâ–ƒâ–…â–†â–‚â–†â–„â–„â–ƒâ–ƒâ–†â–†â–…â–…â–ˆâ–†â–‚â–‚â–†â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–‚â–ƒâ–ˆâ–ƒâ–…â–„â–†â–â–†â–ƒâ–„â–ƒâ–‚â–ƒâ–„â–„â–ƒâ–†â–„â–†â–ƒâ–ƒâ–…â–„â–„â–ƒâ–‡â–…â–„â–„â–ƒâ–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–â–â–â–‚â–‚â–‚â–ƒâ–â–‚â–ˆâ–ƒâ–„â–‚â–‡â–‚â–„â–ƒâ–„â–‚â–‚â–‚â–„â–†â–„â–†â–ƒâ–†â–…â–‚â–…â–…â–„â–„â–‡â–…â–ƒâ–ƒâ–ƒâ–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–†â–…â–„â–„â–„â–„â–„â–…â–‚â–â–„â–„â–ƒâ–„â–ˆâ–ƒâ–ƒâ–„â–„â–„â–„â–ƒâ–â–‡â–â–â–„â–…â–„â–‚â–‚â–„â–ƒâ–‚â–â–ƒâ–ƒâ–ƒâ–â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–ƒâ–ƒâ–ƒâ–„â–…â–…â–„â–†â–†â–†â–†â–„â–ˆâ–„â–…â–†â–…â–†â–…â–…â–†â–†â–…â–†â–†â–†â–…â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–‚â–â–â–â–ƒâ–„â–…â–…â–†â–†â–‡â–†â–†â–ˆâ–†â–‡â–‡â–†â–‡â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–ƒâ–ƒâ–„â–ƒâ–‡â–†â–…â–‡â–‡â–†â–†â–‡â–†â–†â–†â–†â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–†â–ˆâ–†â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–ƒâ–â–â–‚â–ƒâ–…â–†â–†â–‡â–‡â–‡â–†â–ˆâ–‡â–‡â–†â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–â–ƒâ–ƒâ–„â–…â–„â–…â–†â–†â–…â–†â–…â–…â–†â–…â–‡â–„â–†â–†â–…â–†â–†â–†â–‡â–…â–ˆâ–†â–†â–‡â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–‚â–â–â–â–ƒâ–„â–…â–†â–†â–…â–‡â–†â–‡â–‡â–‡â–‡â–†â–†â–‡â–†â–‡â–‡â–‡â–ˆâ–†â–ˆâ–†â–‡â–‡â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–‡â–†â–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–†â–†â–ƒâ–â–†â–ƒâ–†â–†â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–†â–†â–†â–â–ƒâ–ƒâ–ˆâ–ƒâ–â–ƒâ–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–‚â–‚â–‚â–‚â–„â–‡â–†â–„â–â–…â–„â–‡â–†â–„â–„â–„â–„â–„â–†â–‡â–†â–â–„â–„â–ˆâ–„â–â–„â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–…â–ˆâ–ˆâ–ˆâ–…â–…â–…â–…â–…â–…â–ˆâ–…â–â–â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–â–â–â–â–ˆâ–ƒâ–ƒâ–ˆâ–†â–†â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ƒâ–‚â–‚â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–ˆâ–ˆâ–ˆâ–ˆâ–†â–â–†â–†â–†â–†â–†â–â–ƒâ–ƒâ–†â–†â–ƒâ–†â–†â–â–ƒâ–†â–ƒâ–†â–â–†â–†â–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–â–â–â–â–ˆâ–â–†â–ˆâ–†â–ˆâ–†â–â–„â–ƒâ–†â–ˆâ–ƒâ–†â–ˆâ–â–ƒâ–†â–ƒâ–†â–â–†â–ˆâ–†â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–ˆâ–†â–„â–ƒâ–ƒâ–„â–„â–…â–â–…â–‚â–…â–…â–†â–„â–‡â–…â–‚â–ƒâ–„â–†â–‡â–ƒâ–…â–†â–‡â–„â–„â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1yic4mss\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:49:05.012405: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm__multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1rn6aec6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024903-1rn6aec6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type        | Params\n",
      "---------------------------------------------------\n",
      "0 | lstm_1             | LSTM        | 67.1 K\n",
      "1 | batch_norm1        | BatchNorm2d | 256   \n",
      "2 | dropout            | Dropout     | 0     \n",
      "3 | linear1            | ModuleList  | 8.3 K \n",
      "4 | activation         | ReLU        | 0     \n",
      "5 | output_layers      | ModuleList  | 195   \n",
      "6 | cross_entropy_loss | ModuleList  | 0     \n",
      "7 | f1_score           | F1          | 0     \n",
      "8 | accuracy_score     | Accuracy    | 0     \n",
      "---------------------------------------------------\n",
      "75.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.8 K    Total params\n",
      "0.303     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 73/74 [00:02<00:00, 31.31it/s, loss=1.12, v_num=aec6, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.092\n",
      "Epoch 0: 100%|â–ˆ| 74/74 [00:02<00:00, 30.77it/s, loss=1.12, v_num=aec6, BTC_val_a\n",
      "Epoch 1:  99%|â–‰| 73/74 [00:02<00:00, 32.72it/s, loss=1.11, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 74/74 [00:02<00:00, 31.83it/s, loss=1.11, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 2:  99%|â–‰| 73/74 [00:02<00:00, 31.21it/s, loss=1.11, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 1.083\n",
      "Epoch 2: 100%|â–ˆ| 74/74 [00:02<00:00, 30.46it/s, loss=1.11, v_num=aec6, BTC_val_a\n",
      "Epoch 3:  99%|â–‰| 73/74 [00:02<00:00, 29.84it/s, loss=1.1, v_num=aec6, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 74/74 [00:02<00:00, 29.19it/s, loss=1.1, v_num=aec6, BTC_val_ac\u001b[A\n",
      "Epoch 4:  99%|â–‰| 73/74 [00:02<00:00, 29.42it/s, loss=1.09, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 74/74 [00:02<00:00, 28.99it/s, loss=1.09, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 5:  99%|â–‰| 73/74 [00:02<00:00, 32.12it/s, loss=1.1, v_num=aec6, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 74/74 [00:02<00:00, 31.45it/s, loss=1.1, v_num=aec6, BTC_val_ac\u001b[A\n",
      "Epoch 6:  99%|â–‰| 73/74 [00:02<00:00, 28.63it/s, loss=1.11, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 1.073\n",
      "Epoch 6: 100%|â–ˆ| 74/74 [00:02<00:00, 28.04it/s, loss=1.11, v_num=aec6, BTC_val_a\n",
      "Epoch 7:  99%|â–‰| 73/74 [00:02<00:00, 28.92it/s, loss=1.11, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 74/74 [00:02<00:00, 28.49it/s, loss=1.11, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 8:  99%|â–‰| 73/74 [00:02<00:00, 32.33it/s, loss=1.1, v_num=aec6, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 74/74 [00:02<00:00, 31.75it/s, loss=1.1, v_num=aec6, BTC_val_ac\u001b[A\n",
      "Epoch 9:  99%|â–‰| 73/74 [00:02<00:00, 31.32it/s, loss=1.1, v_num=aec6, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 74/74 [00:02<00:00, 30.71it/s, loss=1.1, v_num=aec6, BTC_val_ac\u001b[A\n",
      "Epoch 10:  99%|â–‰| 73/74 [00:02<00:00, 30.87it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 74/74 [00:02<00:00, 30.16it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 11:  99%|â–‰| 73/74 [00:02<00:00, 30.25it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 74/74 [00:02<00:00, 29.48it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 12:  99%|â–‰| 73/74 [00:02<00:00, 26.41it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 74/74 [00:02<00:00, 26.00it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 13:  99%|â–‰| 73/74 [00:02<00:00, 27.22it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 74/74 [00:02<00:00, 26.73it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 14:  99%|â–‰| 73/74 [00:02<00:00, 25.04it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 74/74 [00:02<00:00, 24.78it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 15:  99%|â–‰| 73/74 [00:02<00:00, 31.34it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 74/74 [00:02<00:00, 30.67it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 16:  99%|â–‰| 73/74 [00:02<00:00, 25.58it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 74/74 [00:02<00:00, 25.17it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 17:  99%|â–‰| 73/74 [00:02<00:00, 27.71it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 74/74 [00:02<00:00, 27.30it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 18:  99%|â–‰| 73/74 [00:02<00:00, 26.68it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 74/74 [00:02<00:00, 26.18it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 19:  99%|â–‰| 73/74 [00:02<00:00, 26.93it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 74/74 [00:02<00:00, 26.60it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 20:  99%|â–‰| 73/74 [00:02<00:00, 27.11it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:02<00:00, 26.50it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 21:  99%|â–‰| 73/74 [00:02<00:00, 27.53it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 74/74 [00:02<00:00, 27.02it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 22:  99%|â–‰| 73/74 [00:03<00:00, 24.30it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 74/74 [00:03<00:00, 24.00it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 23:  99%|â–‰| 73/74 [00:02<00:00, 24.91it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 1.067\n",
      "Epoch 23: 100%|â–ˆ| 74/74 [00:03<00:00, 24.53it/s, loss=1.1, v_num=aec6, BTC_val_a\n",
      "Epoch 24:  99%|â–‰| 73/74 [00:02<00:00, 27.79it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 74/74 [00:02<00:00, 27.33it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 25:  99%|â–‰| 73/74 [00:03<00:00, 23.53it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 74/74 [00:03<00:00, 22.79it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 26:  99%|â–‰| 73/74 [00:02<00:00, 28.32it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 74/74 [00:02<00:00, 27.86it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 27:  99%|â–‰| 73/74 [00:02<00:00, 28.60it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 74/74 [00:02<00:00, 27.76it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 28:  99%|â–‰| 73/74 [00:02<00:00, 25.26it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 74/74 [00:02<00:00, 24.93it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 29:  99%|â–‰| 73/74 [00:02<00:00, 24.82it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 74/74 [00:03<00:00, 24.43it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 30:  99%|â–‰| 73/74 [00:03<00:00, 24.31it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 74/74 [00:03<00:00, 24.04it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 31:  99%|â–‰| 73/74 [00:02<00:00, 24.52it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 74/74 [00:03<00:00, 24.12it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 32:  99%|â–‰| 73/74 [00:02<00:00, 24.37it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 74/74 [00:03<00:00, 24.10it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 33:  99%|â–‰| 73/74 [00:03<00:00, 22.63it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 1.063\n",
      "Epoch 33: 100%|â–ˆ| 74/74 [00:03<00:00, 22.35it/s, loss=1.1, v_num=aec6, BTC_val_a\n",
      "Epoch 34:  99%|â–‰| 73/74 [00:02<00:00, 24.71it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 74/74 [00:03<00:00, 24.39it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 1.056\n",
      "Epoch 35:  99%|â–‰| 73/74 [00:03<00:00, 22.92it/s, loss=1.09, v_num=aec6, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 74/74 [00:03<00:00, 22.67it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n",
      "Epoch 36:  99%|â–‰| 73/74 [00:02<00:00, 29.21it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 74/74 [00:02<00:00, 28.65it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n",
      "Epoch 37:  99%|â–‰| 73/74 [00:03<00:00, 22.19it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|â–ˆ| 74/74 [00:03<00:00, 21.96it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 38:  99%|â–‰| 73/74 [00:02<00:00, 25.95it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|â–ˆ| 74/74 [00:02<00:00, 25.58it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 39:  99%|â–‰| 73/74 [00:02<00:00, 27.05it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|â–ˆ| 74/74 [00:02<00:00, 26.41it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 40:  99%|â–‰| 73/74 [00:02<00:00, 28.64it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|â–ˆ| 74/74 [00:02<00:00, 28.16it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n",
      "Epoch 41:  99%|â–‰| 73/74 [00:03<00:00, 21.60it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|â–ˆ| 74/74 [00:03<00:00, 21.34it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 42:  99%|â–‰| 73/74 [00:02<00:00, 25.57it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|â–ˆ| 74/74 [00:02<00:00, 25.16it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 43:  99%|â–‰| 73/74 [00:02<00:00, 27.04it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 1.047\n",
      "Epoch 43: 100%|â–ˆ| 74/74 [00:02<00:00, 25.87it/s, loss=1.1, v_num=aec6, BTC_val_a\n",
      "Epoch 44:  99%|â–‰| 73/74 [00:03<00:00, 23.59it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|â–ˆ| 74/74 [00:03<00:00, 23.28it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 45:  99%|â–‰| 73/74 [00:03<00:00, 22.76it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|â–ˆ| 74/74 [00:03<00:00, 22.52it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 46:  99%|â–‰| 73/74 [00:02<00:00, 26.17it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|â–ˆ| 74/74 [00:03<00:00, 24.16it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 47:  99%|â–‰| 73/74 [00:03<00:00, 23.24it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|â–ˆ| 74/74 [00:03<00:00, 23.01it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 48:  99%|â–‰| 73/74 [00:02<00:00, 25.24it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|â–ˆ| 74/74 [00:02<00:00, 24.82it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n",
      "Epoch 49:  99%|â–‰| 73/74 [00:02<00:00, 25.61it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|â–ˆ| 74/74 [00:02<00:00, 25.29it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 50:  99%|â–‰| 73/74 [00:02<00:00, 29.40it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|â–ˆ| 74/74 [00:02<00:00, 28.82it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 51:  99%|â–‰| 73/74 [00:03<00:00, 21.35it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|â–ˆ| 74/74 [00:03<00:00, 21.17it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 52:  99%|â–‰| 73/74 [00:03<00:00, 22.42it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|â–ˆ| 74/74 [00:03<00:00, 22.14it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 53:  99%|â–‰| 73/74 [00:02<00:00, 24.90it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|â–ˆ| 74/74 [00:03<00:00, 24.58it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n",
      "Epoch 54:  99%|â–‰| 73/74 [00:03<00:00, 22.03it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|â–ˆ| 74/74 [00:03<00:00, 21.82it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 55:  99%|â–‰| 73/74 [00:02<00:00, 26.82it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|â–ˆ| 74/74 [00:02<00:00, 26.42it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 56:  99%|â–‰| 73/74 [00:02<00:00, 25.83it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|â–ˆ| 74/74 [00:02<00:00, 25.29it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 57:  99%|â–‰| 73/74 [00:02<00:00, 27.63it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|â–ˆ| 74/74 [00:02<00:00, 27.24it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 58:  99%|â–‰| 73/74 [00:02<00:00, 25.96it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|â–ˆ| 74/74 [00:02<00:00, 25.52it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n",
      "Epoch 59:  99%|â–‰| 73/74 [00:02<00:00, 26.15it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|â–ˆ| 74/74 [00:02<00:00, 25.65it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 60:  99%|â–‰| 73/74 [00:03<00:00, 22.98it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 60: 100%|â–ˆ| 74/74 [00:03<00:00, 22.65it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 61:  99%|â–‰| 73/74 [00:02<00:00, 27.86it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|â–ˆ| 74/74 [00:02<00:00, 27.45it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Epoch 62:  99%|â–‰| 73/74 [00:03<00:00, 21.69it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 62: 100%|â–ˆ| 74/74 [00:03<00:00, 21.40it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n",
      "Epoch 63:  99%|â–‰| 73/74 [00:02<00:00, 27.50it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.047. Signaling Trainer to stop.\n",
      "Epoch 63: 100%|â–ˆ| 74/74 [00:02<00:00, 27.11it/s, loss=1.1, v_num=aec6, BTC_val_a\n",
      "Epoch 63: 100%|â–ˆ| 74/74 [00:02<00:00, 27.07it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 49.83it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.25,\n",
      " 'BTC_test_f1': 0.1315789520740509,\n",
      " 'ETH_test_acc': 0.4285714328289032,\n",
      " 'ETH_test_f1': 0.1991342157125473,\n",
      " 'LTC_test_acc': 0.3928571343421936,\n",
      " 'LTC_test_f1': 0.18594105541706085,\n",
      " 'test_loss': 1.135124683380127}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 81471\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024903-1rn6aec6/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024903-1rn6aec6/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.4375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.34799\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.4375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.33868\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.3125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.25641\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.08357\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 63\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4672\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 193\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621468336\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 221\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.37489\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.27562\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.35671\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.27088\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.34892\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.2646\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09686\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.18182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.18182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.25641\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.0764\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.13158\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.42857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19913\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.39286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.18594\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.13512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–ƒâ–ƒâ–„â–â–†â–ƒâ–ˆâ–‚â–ƒâ–ƒâ–ƒâ–â–„â–‚â–†â–†â–…â–…â–†â–†â–ƒâ–…â–…â–…â–†â–„â–…â–„â–‡â–†â–‡â–…â–†â–†â–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–ƒâ–ƒâ–„â–â–ˆâ–ƒâ–ˆâ–‚â–„â–ƒâ–‚â–‚â–„â–‚â–…â–„â–…â–…â–…â–…â–ƒâ–„â–…â–ƒâ–…â–„â–…â–ƒâ–…â–…â–†â–„â–…â–†â–‡â–ƒâ–„â–ƒâ–ƒâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–…â–…â–†â–ƒâ–†â–ˆâ–†â–…â–†â–…â–…â–‚â–‡â–â–…â–…â–ˆâ–ˆâ–…â–…â–…â–„â–†â–…â–ˆâ–ƒâ–…â–ƒâ–„â–‚â–…â–‡â–†â–†â–†â–…â–…â–…â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–…â–„â–„â–ƒâ–†â–‡â–†â–…â–…â–…â–ƒâ–‚â–†â–â–ƒâ–„â–‡â–ˆâ–…â–ƒâ–„â–ƒâ–†â–ƒâ–ˆâ–‚â–„â–‚â–ƒâ–â–…â–…â–…â–„â–„â–„â–„â–„â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–ƒâ–„â–†â–ƒâ–ƒâ–…â–„â–‡â–‡â–†â–†â–…â–‡â–…â–„â–‡â–†â–†â–„â–ˆâ–„â–„â–„â–ˆâ–‡â–„â–‚â–…â–†â–†â–†â–†â–…â–†â–†â–â–ˆâ–ˆâ–ƒâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–‚â–„â–„â–ƒâ–ƒâ–…â–ƒâ–‡â–‡â–…â–…â–…â–…â–„â–ƒâ–‡â–…â–†â–ƒâ–ˆâ–ƒâ–ƒâ–ƒâ–†â–ˆâ–ƒâ–‚â–ƒâ–…â–…â–…â–…â–„â–„â–…â–â–ˆâ–†â–‚â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–†â–„â–‚â–ˆâ–ƒâ–‚â–â–‚â–‚â–ƒâ–ƒâ–„â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–„â–ƒâ–â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–„â–‚â–‚â–ƒâ–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–ƒâ–†â–ƒâ–ƒâ–ƒâ–…â–ƒâ–…â–…â–‡â–†â–„â–„â–ˆâ–…â–„â–„â–„â–„â–‡â–†â–‡â–…â–†â–†â–‡â–†â–…â–‡â–…â–†â–…â–„â–…â–…â–ƒâ–ˆâ–‡â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–„â–…â–‡â–„â–…â–…â–‡â–„â–ˆâ–„â–†â–…â–‚â–…â–…â–…â–†â–„â–ƒâ–‚â–ƒâ–„â–…â–â–ƒâ–‚â–‚â–„â–‚â–‚â–‚â–‚â–„â–‚â–ƒâ–â–‚â–„â–„â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–â–ƒâ–„â–†â–‡â–†â–‡â–…â–†â–…â–„â–„â–ƒâ–†â–„â–†â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–ˆâ–…â–†â–†â–†â–†â–ˆâ–…â–‡â–‡â–…â–†â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–ƒâ–‚â–ƒâ–„â–ˆâ–‡â–‡â–‡â–†â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–…â–‡â–„â–…â–‚â–â–ƒâ–‚â–â–‚â–‚â–„â–â–â–â–ƒâ–â–‡â–ƒâ–„â–ƒâ–ƒâ–‚â–ƒâ–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–ƒâ–„â–â–ƒâ–…â–„â–‚â–ƒâ–„â–ˆâ–‚â–…â–…â–†â–â–†â–‡â–ƒâ–ƒâ–„â–„â–‡â–„â–…â–…â–†â–…â–…â–â–ƒâ–„â–…â–„â–†â–„â–…â–ˆâ–…â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–‡â–ˆâ–„â–…â–ˆâ–†â–†â–…â–ˆâ–ˆâ–…â–†â–…â–‡â–â–‡â–‡â–…â–…â–ƒâ–ƒâ–…â–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–â–â–ƒâ–ƒâ–„â–…â–ƒâ–ƒâ–‡â–ƒâ–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–ƒâ–ƒâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–‚â–‚â–…â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–‚â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–ˆâ–â–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–â–ˆâ–ˆâ–â–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–ƒâ–â–ˆâ–â–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–â–ƒâ–ˆâ–‚â–ƒâ–â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–ˆâ–â–…â–â–ˆâ–ˆâ–â–ˆâ–â–ˆâ–â–ˆâ–â–â–…â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–„â–â–†â–â–„â–„â–â–ˆâ–â–„â–â–„â–â–â–‚â–â–â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–…â–ˆâ–„â–…â–„â–„â–„â–ƒâ–„â–„â–„â–„â–„â–…â–„â–„â–…â–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–ƒâ–„â–‚â–„â–â–„â–‚â–‚â–„â–ƒâ–…â–‚â–ƒâ–‚â–â–ƒâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1rn6aec6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:52:33.592465: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm__binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3fmo25vq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_025232-3fmo25vq\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|â–‰| 72/73 [00:04<00:00, 14.57it/s, loss=0.683, v_num=25vq, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.691\n",
      "Epoch 0: 100%|â–ˆ| 73/73 [00:05<00:00, 14.54it/s, loss=0.683, v_num=25vq, BTC_val_\n",
      "Epoch 1:  99%|â–‰| 72/73 [00:05<00:00, 13.96it/s, loss=0.617, v_num=25vq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.068 >= min_delta = 0.003. New best score: 0.623\n",
      "Epoch 1: 100%|â–ˆ| 73/73 [00:05<00:00, 13.90it/s, loss=0.617, v_num=25vq, BTC_val_\n",
      "Epoch 2:  99%|â–‰| 72/73 [00:05<00:00, 14.28it/s, loss=0.563, v_num=25vq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.016 >= min_delta = 0.003. New best score: 0.608\n",
      "Epoch 2: 100%|â–ˆ| 73/73 [00:05<00:00, 14.23it/s, loss=0.563, v_num=25vq, BTC_val_\n",
      "Epoch 3:  99%|â–‰| 72/73 [00:06<00:00, 11.62it/s, loss=0.602, v_num=25vq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 73/73 [00:06<00:00, 11.62it/s, loss=0.602, v_num=25vq, BTC_val_\u001b[A\n",
      "Epoch 4:  99%|â–‰| 72/73 [00:07<00:00,  9.66it/s, loss=0.607, v_num=25vq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 73/73 [00:07<00:00,  9.69it/s, loss=0.607, v_num=25vq, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|â–‰| 72/73 [00:05<00:00, 12.35it/s, loss=0.601, v_num=25vq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 73/73 [00:05<00:00, 12.34it/s, loss=0.601, v_num=25vq, BTC_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.015 >= min_delta = 0.003. New best score: 0.593\n",
      "Epoch 6:  99%|â–‰| 72/73 [00:05<00:00, 13.86it/s, loss=0.57, v_num=25vq, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 73/73 [00:05<00:00, 13.80it/s, loss=0.57, v_num=25vq, BTC_val_a\u001b[A\n",
      "Epoch 7:  99%|â–‰| 72/73 [00:06<00:00, 10.82it/s, loss=0.591, v_num=25vq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 73/73 [00:06<00:00, 10.83it/s, loss=0.591, v_num=25vq, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|â–‰| 72/73 [00:07<00:00,  9.71it/s, loss=0.577, v_num=25vq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 73/73 [00:07<00:00,  9.72it/s, loss=0.577, v_num=25vq, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|â–‰| 72/73 [00:05<00:00, 13.56it/s, loss=0.547, v_num=25vq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 73/73 [00:05<00:00, 13.50it/s, loss=0.547, v_num=25vq, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|â–‰| 72/73 [00:05<00:00, 12.57it/s, loss=0.578, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 73/73 [00:05<00:00, 12.54it/s, loss=0.578, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 11:  99%|â–‰| 72/73 [00:06<00:00, 11.47it/s, loss=0.564, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 73/73 [00:06<00:00, 11.46it/s, loss=0.564, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 12:  99%|â–‰| 72/73 [00:05<00:00, 12.20it/s, loss=0.563, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 73/73 [00:05<00:00, 12.18it/s, loss=0.563, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 13:  99%|â–‰| 72/73 [00:06<00:00, 11.88it/s, loss=0.586, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.586\n",
      "Epoch 13: 100%|â–ˆ| 73/73 [00:06<00:00, 11.86it/s, loss=0.586, v_num=25vq, BTC_val\n",
      "Epoch 14:  99%|â–‰| 72/73 [00:06<00:00, 11.60it/s, loss=0.601, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 73/73 [00:06<00:00, 11.58it/s, loss=0.601, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 15:  99%|â–‰| 72/73 [00:06<00:00, 11.92it/s, loss=0.589, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 73/73 [00:06<00:00, 11.93it/s, loss=0.589, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 16:  99%|â–‰| 72/73 [00:05<00:00, 12.54it/s, loss=0.585, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 73/73 [00:05<00:00, 12.51it/s, loss=0.585, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 17:  99%|â–‰| 72/73 [00:06<00:00, 11.94it/s, loss=0.551, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 73/73 [00:06<00:00, 11.94it/s, loss=0.551, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 72/73 [00:06<00:00, 11.82it/s, loss=0.554, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 73/73 [00:06<00:00, 11.81it/s, loss=0.554, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 72/73 [00:05<00:00, 12.21it/s, loss=0.558, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 73/73 [00:05<00:00, 12.20it/s, loss=0.558, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 20:  99%|â–‰| 72/73 [00:05<00:00, 12.90it/s, loss=0.533, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 73/73 [00:05<00:00, 12.86it/s, loss=0.533, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 21:  99%|â–‰| 72/73 [00:05<00:00, 12.05it/s, loss=0.553, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 73/73 [00:06<00:00, 12.03it/s, loss=0.553, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 72/73 [00:05<00:00, 13.21it/s, loss=0.556, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 73/73 [00:05<00:00, 13.17it/s, loss=0.556, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 23:  99%|â–‰| 72/73 [00:05<00:00, 13.18it/s, loss=0.59, v_num=25vq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 73/73 [00:05<00:00, 13.13it/s, loss=0.59, v_num=25vq, BTC_val_\u001b[A\n",
      "Epoch 24:  99%|â–‰| 72/73 [00:06<00:00, 11.15it/s, loss=0.549, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 73/73 [00:06<00:00, 11.14it/s, loss=0.549, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 25:  99%|â–‰| 72/73 [00:06<00:00, 11.02it/s, loss=0.528, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 73/73 [00:06<00:00, 11.03it/s, loss=0.528, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 26:  99%|â–‰| 72/73 [00:05<00:00, 14.30it/s, loss=0.524, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 73/73 [00:05<00:00, 14.23it/s, loss=0.524, v_num=25vq, BTC_val\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.032 >= min_delta = 0.003. New best score: 0.554\n",
      "Epoch 27:  99%|â–‰| 72/73 [00:05<00:00, 13.36it/s, loss=0.562, v_num=25vq, BTC_val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 73/73 [00:05<00:00, 13.33it/s, loss=0.562, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 28:  99%|â–‰| 72/73 [00:05<00:00, 12.18it/s, loss=0.537, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 73/73 [00:06<00:00, 12.11it/s, loss=0.537, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 29:  99%|â–‰| 72/73 [00:06<00:00, 11.43it/s, loss=0.538, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 73/73 [00:06<00:00, 11.41it/s, loss=0.538, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 30:  99%|â–‰| 72/73 [00:05<00:00, 12.34it/s, loss=0.511, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 73/73 [00:05<00:00, 12.31it/s, loss=0.511, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 31:  99%|â–‰| 72/73 [00:05<00:00, 13.70it/s, loss=0.552, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 73/73 [00:05<00:00, 13.68it/s, loss=0.552, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 32:  99%|â–‰| 72/73 [00:05<00:00, 13.80it/s, loss=0.524, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 73/73 [00:05<00:00, 13.74it/s, loss=0.524, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 33:  99%|â–‰| 72/73 [00:05<00:00, 14.16it/s, loss=0.502, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 73/73 [00:05<00:00, 14.08it/s, loss=0.502, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 34:  99%|â–‰| 72/73 [00:05<00:00, 13.52it/s, loss=0.538, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 73/73 [00:05<00:00, 13.49it/s, loss=0.538, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 35:  99%|â–‰| 72/73 [00:06<00:00, 11.90it/s, loss=0.557, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 73/73 [00:06<00:00, 11.90it/s, loss=0.557, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 36:  99%|â–‰| 72/73 [00:06<00:00, 11.61it/s, loss=0.554, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|â–ˆ| 73/73 [00:06<00:00, 11.60it/s, loss=0.554, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 37:  99%|â–‰| 72/73 [00:06<00:00, 11.24it/s, loss=0.545, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|â–ˆ| 73/73 [00:06<00:00, 11.24it/s, loss=0.545, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 38:  99%|â–‰| 72/73 [00:05<00:00, 12.33it/s, loss=0.529, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|â–ˆ| 73/73 [00:05<00:00, 12.31it/s, loss=0.529, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 39:  99%|â–‰| 72/73 [00:06<00:00, 10.45it/s, loss=0.553, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|â–ˆ| 73/73 [00:06<00:00, 10.47it/s, loss=0.553, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 40:  99%|â–‰| 72/73 [00:06<00:00, 11.99it/s, loss=0.515, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|â–ˆ| 73/73 [00:06<00:00, 11.99it/s, loss=0.515, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 41:  99%|â–‰| 72/73 [00:05<00:00, 13.87it/s, loss=0.499, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|â–ˆ| 73/73 [00:05<00:00, 13.79it/s, loss=0.499, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 42:  99%|â–‰| 72/73 [00:05<00:00, 13.94it/s, loss=0.505, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|â–ˆ| 73/73 [00:05<00:00, 13.88it/s, loss=0.505, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 43:  99%|â–‰| 72/73 [00:05<00:00, 13.20it/s, loss=0.478, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|â–ˆ| 73/73 [00:05<00:00, 13.18it/s, loss=0.478, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 44:  99%|â–‰| 72/73 [00:06<00:00, 11.25it/s, loss=0.491, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|â–ˆ| 73/73 [00:06<00:00, 11.24it/s, loss=0.491, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 45:  99%|â–‰| 72/73 [00:06<00:00, 11.76it/s, loss=0.532, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|â–ˆ| 73/73 [00:06<00:00, 11.73it/s, loss=0.532, v_num=25vq, BTC_val\u001b[A\n",
      "Epoch 46:  99%|â–‰| 72/73 [00:07<00:00, 10.14it/s, loss=0.514, v_num=25vq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.554. Signaling Trainer to stop.\n",
      "Epoch 46: 100%|â–ˆ| 73/73 [00:07<00:00, 10.16it/s, loss=0.514, v_num=25vq, BTC_val\n",
      "Epoch 46: 100%|â–ˆ| 73/73 [00:07<00:00, 10.15it/s, loss=0.514, v_num=25vq, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 33.13it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6785714030265808,\n",
      " 'BTC_test_f1': 0.6746674180030823,\n",
      " 'ETH_test_acc': 0.6785714030265808,\n",
      " 'ETH_test_f1': 0.6744208335876465,\n",
      " 'LTC_test_acc': 0.7142857313156128,\n",
      " 'LTC_test_f1': 0.7079364657402039,\n",
      " 'test_loss': 0.5609579086303711}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 81967\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_025232-3fmo25vq/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_025232-3fmo25vq/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.56364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.45894\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.49206\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.70908\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 46\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 290\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621468642\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 161\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.75283\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.74235\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.74674\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.73339\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.74064\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.7291\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.51025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.56364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.46667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.61905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.61646\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.67857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.67467\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.67857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.67442\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.71429\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.70794\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.56096\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–ƒâ–…â–…â–ˆâ–…â–‡â–†â–…â–…â–ˆâ–…â–†â–‡â–…â–‡â–ˆâ–†â–†â–ˆâ–„â–†â–†â–ˆâ–ƒâ–„â–†â–…â–‡â–†â–ˆâ–†â–†â–…â–†â–â–…â–„â–†â–…â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–‚â–…â–…â–ˆâ–…â–‡â–†â–…â–…â–ˆâ–…â–†â–‡â–…â–‡â–ˆâ–†â–†â–†â–„â–†â–†â–ˆâ–ƒâ–„â–†â–…â–‡â–†â–ˆâ–†â–†â–…â–…â–â–…â–„â–…â–…â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–‚â–†â–‚â–…â–â–†â–‡â–ƒâ–†â–†â–…â–ƒâ–†â–…â–ƒâ–‡â–‚â–…â–†â–‚â–‚â–†â–…â–‚â–‚â–ƒâ–‚â–…â–…â–‡â–ˆâ–†â–ƒâ–…â–â–ˆâ–ƒâ–…â–‡â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–ƒâ–†â–ƒâ–…â–‚â–†â–‡â–„â–†â–†â–…â–„â–†â–…â–„â–‡â–ƒâ–…â–„â–ƒâ–ƒâ–‡â–…â–ƒâ–ƒâ–„â–ƒâ–…â–…â–‡â–ˆâ–†â–„â–…â–‚â–ˆâ–„â–…â–‡â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–„â–„â–†â–†â–„â–†â–‡â–…â–…â–ˆâ–…â–„â–ˆâ–„â–…â–†â–†â–…â–„â–…â–„â–†â–†â–ƒâ–„â–†â–„â–†â–†â–†â–†â–†â–†â–â–„â–†â–…â–…â–…â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–„â–„â–‡â–‡â–„â–†â–‡â–…â–…â–ˆâ–…â–„â–ˆâ–„â–…â–†â–†â–…â–„â–…â–„â–†â–‡â–„â–„â–…â–„â–‡â–†â–†â–†â–‡â–†â–â–„â–†â–…â–…â–…â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–‡â–…â–†â–ƒâ–‡â–ƒâ–‚â–…â–…â–‚â–…â–„â–‚â–†â–…â–ƒâ–„â–…â–ƒâ–„â–‡â–ƒâ–‚â–ˆâ–‡â–„â–†â–„â–…â–â–â–ƒâ–„â–†â–ˆâ–ƒâ–„â–ƒâ–„â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–…â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–â–…â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–ƒâ–†â–ƒâ–ƒâ–ƒâ–†â–ƒâ–ƒâ–ˆâ–ƒâ–ˆâ–ƒâ–ˆâ–†â–ƒâ–†â–†â–ƒâ–†â–†â–†â–ƒâ–†â–ƒâ–†â–†â–†â–ˆâ–ˆâ–ˆâ–†â–†â–ƒâ–†â–†â–ƒâ–ƒâ–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–„â–†â–„â–„â–„â–†â–„â–„â–ˆâ–„â–ˆâ–„â–ˆâ–†â–„â–†â–†â–„â–†â–†â–†â–„â–†â–„â–†â–†â–†â–ˆâ–ˆâ–ˆâ–†â–†â–„â–†â–†â–„â–„â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–â–†â–ƒâ–†â–†â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–†â–ƒâ–†â–ƒâ–ƒâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–†â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–†â–ƒâ–ƒâ–†â–†â–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–â–†â–„â–†â–†â–„â–„â–„â–„â–„â–„â–„â–„â–…â–„â–†â–„â–„â–ˆâ–„â–„â–„â–„â–„â–„â–„â–„â–†â–„â–„â–„â–„â–„â–†â–„â–„â–†â–†â–†â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–…â–…â–…â–…â–…â–â–â–…â–ˆâ–…â–…â–â–…â–ˆâ–…â–…â–…â–…â–…â–…â–â–…â–…â–…â–…â–…â–…â–…â–…â–…â–â–…â–…â–…â–â–…â–…â–…â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–…â–…â–…â–…â–…â–â–â–…â–ˆâ–ƒâ–…â–â–…â–†â–…â–…â–…â–…â–…â–…â–â–…â–…â–…â–…â–…â–…â–…â–…â–ƒâ–â–…â–…â–…â–â–…â–…â–…â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–…â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–„â–‚â–„â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–…â–â–â–ƒâ–‚â–‚â–ƒâ–ƒâ–„â–‡â–ƒâ–‚â–†â–ˆâ–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3fmo25vq\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 02:57:40.574165: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm__binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1o3x1f2w\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_025739-1o3x1f2w\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/74 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|â–ˆ| 74/74 [00:05<00:00, 13.39it/s, loss=0.706, v_num=1f2w, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.703\n",
      "Epoch 0: 100%|â–ˆ| 74/74 [00:05<00:00, 13.31it/s, loss=0.706, v_num=1f2w, BTC_val_\n",
      "Epoch 1: 100%|â–ˆ| 74/74 [00:05<00:00, 13.58it/s, loss=0.692, v_num=1f2w, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.034 >= min_delta = 0.003. New best score: 0.669\n",
      "Epoch 1: 100%|â–ˆ| 74/74 [00:05<00:00, 13.51it/s, loss=0.692, v_num=1f2w, BTC_val_\n",
      "Epoch 2: 100%|â–ˆ| 74/74 [00:06<00:00, 12.29it/s, loss=0.691, v_num=1f2w, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 74/74 [00:06<00:00, 12.23it/s, loss=0.691, v_num=1f2w, BTC_val_\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 74/74 [00:05<00:00, 13.43it/s, loss=0.693, v_num=1f2w, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 74/74 [00:05<00:00, 13.37it/s, loss=0.693, v_num=1f2w, BTC_val_\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 74/74 [00:05<00:00, 12.63it/s, loss=0.7, v_num=1f2w, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 74/74 [00:05<00:00, 12.57it/s, loss=0.7, v_num=1f2w, BTC_val_ac\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 74/74 [00:05<00:00, 14.29it/s, loss=0.694, v_num=1f2w, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 74/74 [00:05<00:00, 14.20it/s, loss=0.694, v_num=1f2w, BTC_val_\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 74/74 [00:05<00:00, 14.62it/s, loss=0.694, v_num=1f2w, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 74/74 [00:05<00:00, 14.50it/s, loss=0.694, v_num=1f2w, BTC_val_\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 74/74 [00:05<00:00, 12.62it/s, loss=0.693, v_num=1f2w, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 74/74 [00:05<00:00, 12.55it/s, loss=0.693, v_num=1f2w, BTC_val_\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 74/74 [00:06<00:00, 12.28it/s, loss=0.693, v_num=1f2w, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 74/74 [00:06<00:00, 12.22it/s, loss=0.693, v_num=1f2w, BTC_val_\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 74/74 [00:06<00:00, 11.39it/s, loss=0.699, v_num=1f2w, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 74/74 [00:06<00:00, 11.34it/s, loss=0.699, v_num=1f2w, BTC_val_\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 74/74 [00:06<00:00, 11.85it/s, loss=0.694, v_num=1f2w, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 74/74 [00:06<00:00, 11.79it/s, loss=0.694, v_num=1f2w, BTC_val\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 74/74 [00:05<00:00, 13.77it/s, loss=0.692, v_num=1f2w, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 74/74 [00:05<00:00, 13.69it/s, loss=0.692, v_num=1f2w, BTC_val\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 74/74 [00:06<00:00, 11.41it/s, loss=0.695, v_num=1f2w, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 74/74 [00:06<00:00, 11.36it/s, loss=0.695, v_num=1f2w, BTC_val\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 74/74 [00:06<00:00, 11.55it/s, loss=0.694, v_num=1f2w, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 74/74 [00:06<00:00, 11.51it/s, loss=0.694, v_num=1f2w, BTC_val\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 74/74 [00:06<00:00, 12.30it/s, loss=0.693, v_num=1f2w, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 74/74 [00:06<00:00, 12.25it/s, loss=0.693, v_num=1f2w, BTC_val\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 74/74 [00:06<00:00, 11.69it/s, loss=0.693, v_num=1f2w, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 74/74 [00:06<00:00, 11.64it/s, loss=0.693, v_num=1f2w, BTC_val\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 74/74 [00:06<00:00, 12.31it/s, loss=0.694, v_num=1f2w, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 74/74 [00:06<00:00, 12.26it/s, loss=0.694, v_num=1f2w, BTC_val\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 74/74 [00:06<00:00, 11.96it/s, loss=0.696, v_num=1f2w, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 74/74 [00:06<00:00, 11.91it/s, loss=0.696, v_num=1f2w, BTC_val\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 74/74 [00:05<00:00, 13.88it/s, loss=0.693, v_num=1f2w, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 74/74 [00:05<00:00, 13.81it/s, loss=0.693, v_num=1f2w, BTC_val\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 74/74 [00:06<00:00, 12.29it/s, loss=0.69, v_num=1f2w, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 74/74 [00:06<00:00, 12.24it/s, loss=0.69, v_num=1f2w, BTC_val_\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:06<00:00, 12.31it/s, loss=0.692, v_num=1f2w, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:06<00:00, 12.26it/s, loss=0.692, v_num=1f2w, BTC_val\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 74/74 [00:05<00:00, 13.05it/s, loss=0.695, v_num=1f2w, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 74/74 [00:05<00:00, 12.98it/s, loss=0.695, v_num=1f2w, BTC_val\u001b[A\n",
      "                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.669. Signaling Trainer to stop.\n",
      "Epoch 21: 100%|â–ˆ| 74/74 [00:05<00:00, 12.97it/s, loss=0.695, v_num=1f2w, BTC_val\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 24.68it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.3928571343421936,\n",
      " 'BTC_test_f1': 0.2789115607738495,\n",
      " 'ETH_test_acc': 0.6428571343421936,\n",
      " 'ETH_test_f1': 0.38938775658607483,\n",
      " 'LTC_test_acc': 0.4285714328289032,\n",
      " 'LTC_test_f1': 0.293949156999588,\n",
      " 'test_loss': 0.7038599848747253}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 82654\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_025739-1o3x1f2w/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_025739-1o3x1f2w/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.38462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.45894\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.5625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.36\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.68735\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 138\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621468797\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 76\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.53074\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.38029\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.50736\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.35524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.50043\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.36355\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69296\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.38462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.38462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.38462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.6806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.39286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.27891\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.64286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.38939\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.42857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.29395\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.70386\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–â–„â–ˆâ–…â–…â–…â–â–„â–ˆâ–â–‚â–…â–„â–†â–…â–…â–…â–‡â–…â–†â–…â–…â–…â–†â–‡â–…â–…â–„â–…â–…â–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–â–„â–ˆâ–…â–…â–…â–â–‚â–ˆâ–â–‚â–…â–‚â–ƒâ–…â–…â–„â–…â–ƒâ–…â–…â–ƒâ–„â–…â–ƒâ–„â–„â–‚â–…â–ƒâ–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–ƒâ–„â–„â–„â–…â–„â–„â–…â–…â–…â–„â–…â–…â–…â–ƒâ–†â–ˆâ–ˆâ–†â–„â–†â–‡â–„â–…â–‡â–â–…â–…â–†â–†â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–ƒâ–„â–„â–ƒâ–†â–ƒâ–„â–„â–…â–…â–„â–…â–…â–ƒâ–ƒâ–†â–ˆâ–†â–†â–‚â–‡â–„â–‚â–„â–†â–â–ƒâ–…â–†â–„â–ƒâ–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–†â–‚â–ƒâ–…â–…â–„â–â–‚â–â–â–ƒâ–†â–„â–…â–‚â–†â–…â–„â–†â–ˆâ–â–„â–‚â–…â–…â–ƒâ–†â–ƒâ–â–†â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–†â–‚â–ƒâ–…â–…â–„â–â–‚â–â–â–‚â–†â–„â–ƒâ–‚â–†â–ƒâ–‚â–ƒâ–ˆâ–â–‚â–‚â–„â–ƒâ–‚â–…â–ƒâ–â–ƒâ–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–ˆâ–†â–â–‚â–ƒâ–ƒâ–„â–ƒâ–‚â–„â–„â–‚â–ƒâ–‚â–„â–‚â–â–â–â–‚â–‚â–‚â–„â–‚â–â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–ƒâ–ˆâ–…â–„â–‚â–ƒâ–…â–…â–ˆâ–†â–„â–†â–‡â–†â–ˆâ–†â–ˆâ–„â–…â–†â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–…â–†â–ˆâ–‡â–…â–†â–„â–„â–†â–ƒâ–†â–„â–ƒâ–ƒâ–†â–…â–„â–ƒâ–…â–„â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–ƒâ–†â–â–†â–„â–ƒâ–„â–ƒâ–„â–ˆâ–ˆâ–†â–†â–‡â–ƒâ–…â–„â–†â–‡â–ˆâ–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–‡â–ˆâ–†â–ˆâ–†â–‡â–…â–†â–‡â–ƒâ–ˆâ–…â–„â–„â–†â–…â–„â–ƒâ–‡â–†â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–ˆâ–ƒâ–„â–ˆâ–â–â–‚â–…â–ˆâ–…â–†â–„â–‚â–†â–„â–‚â–ƒâ–ƒâ–â–ƒâ–†â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–ˆâ–†â–‡â–ˆâ–…â–†â–„â–†â–ˆâ–‚â–‡â–„â–ƒâ–ƒâ–…â–ƒâ–ƒâ–‚â–…â–„â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–…â–ƒâ–‚â–ƒâ–ƒâ–‚â–â–â–â–‚â–â–â–â–â–â–‚â–â–‚â–â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–ˆâ–â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1o3x1f2w\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 03:00:17.004036: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm__multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/14vn2tag\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_030015-14vn2tag\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/73 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 72/73 [00:04<00:00, 14.72it/s, loss=1.05, v_num=2tag, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.047\n",
      "Epoch 0: 100%|â–ˆ| 73/73 [00:04<00:00, 14.64it/s, loss=1.05, v_num=2tag, BTC_val_a\n",
      "Epoch 1:  99%|â–‰| 72/73 [00:05<00:00, 13.68it/s, loss=1.04, v_num=2tag, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 73/73 [00:05<00:00, 13.67it/s, loss=1.04, v_num=2tag, BTC_val_a\u001b[A\n",
      "Epoch 2:  99%|â–‰| 72/73 [00:05<00:00, 13.73it/s, loss=1.03, v_num=2tag, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 1.036\n",
      "Epoch 2: 100%|â–ˆ| 73/73 [00:05<00:00, 13.70it/s, loss=1.03, v_num=2tag, BTC_val_a\n",
      "Epoch 3:  99%|â–‰| 72/73 [00:06<00:00, 11.77it/s, loss=1.04, v_num=2tag, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 73/73 [00:06<00:00, 11.78it/s, loss=1.04, v_num=2tag, BTC_val_a\u001b[A\n",
      "Epoch 4:  99%|â–‰| 72/73 [00:05<00:00, 12.68it/s, loss=1.02, v_num=2tag, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 73/73 [00:05<00:00, 12.66it/s, loss=1.02, v_num=2tag, BTC_val_a\u001b[A\n",
      "Epoch 5:  99%|â–‰| 72/73 [00:04<00:00, 14.52it/s, loss=1.04, v_num=2tag, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 73/73 [00:05<00:00, 14.43it/s, loss=1.04, v_num=2tag, BTC_val_a\u001b[A\n",
      "Epoch 6:  99%|â–‰| 72/73 [00:05<00:00, 14.23it/s, loss=1.03, v_num=2tag, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 73/73 [00:05<00:00, 14.20it/s, loss=1.03, v_num=2tag, BTC_val_a\u001b[A\n",
      "Epoch 7:  99%|â–‰| 72/73 [00:04<00:00, 14.48it/s, loss=1.01, v_num=2tag, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 73/73 [00:05<00:00, 14.39it/s, loss=1.01, v_num=2tag, BTC_val_a\u001b[A\n",
      "Epoch 8:  99%|â–‰| 72/73 [00:05<00:00, 14.20it/s, loss=1.01, v_num=2tag, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 73/73 [00:05<00:00, 14.17it/s, loss=1.01, v_num=2tag, BTC_val_a\u001b[A\n",
      "Epoch 9:  99%|â–‰| 72/73 [00:05<00:00, 12.66it/s, loss=1.06, v_num=2tag, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 73/73 [00:05<00:00, 12.66it/s, loss=1.06, v_num=2tag, BTC_val_a\u001b[A\n",
      "Epoch 10:  99%|â–‰| 72/73 [00:05<00:00, 13.70it/s, loss=1.04, v_num=2tag, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 73/73 [00:05<00:00, 13.64it/s, loss=1.04, v_num=2tag, BTC_val_\u001b[A\n",
      "Epoch 11:  99%|â–‰| 72/73 [00:05<00:00, 14.38it/s, loss=1.02, v_num=2tag, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 73/73 [00:05<00:00, 14.34it/s, loss=1.02, v_num=2tag, BTC_val_\u001b[A\n",
      "Epoch 12:  99%|â–‰| 72/73 [00:05<00:00, 14.02it/s, loss=1.01, v_num=2tag, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 73/73 [00:05<00:00, 13.96it/s, loss=1.01, v_num=2tag, BTC_val_\u001b[A\n",
      "Epoch 13:  99%|â–‰| 72/73 [00:05<00:00, 14.16it/s, loss=1.03, v_num=2tag, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 1.031\n",
      "Epoch 13: 100%|â–ˆ| 73/73 [00:05<00:00, 14.08it/s, loss=1.03, v_num=2tag, BTC_val_\n",
      "Epoch 14:  99%|â–‰| 72/73 [00:05<00:00, 12.79it/s, loss=0.978, v_num=2tag, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 1.018\n",
      "Epoch 14: 100%|â–ˆ| 73/73 [00:05<00:00, 12.77it/s, loss=0.978, v_num=2tag, BTC_val\n",
      "Epoch 15:  99%|â–‰| 72/73 [00:05<00:00, 12.40it/s, loss=0.972, v_num=2tag, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 73/73 [00:05<00:00, 12.39it/s, loss=0.972, v_num=2tag, BTC_val\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 1.015\n",
      "Epoch 16:  99%|â–‰| 72/73 [00:05<00:00, 13.12it/s, loss=0.974, v_num=2tag, BTC_val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.042 >= min_delta = 0.003. New best score: 0.972\n",
      "Epoch 16: 100%|â–ˆ| 73/73 [00:05<00:00, 13.09it/s, loss=0.974, v_num=2tag, BTC_val\n",
      "Epoch 17:  99%|â–‰| 72/73 [00:06<00:00, 11.99it/s, loss=0.956, v_num=2tag, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 73/73 [00:06<00:00, 11.99it/s, loss=0.956, v_num=2tag, BTC_val\u001b[A\n",
      "Epoch 18:  99%|â–‰| 72/73 [00:05<00:00, 12.50it/s, loss=0.941, v_num=2tag, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 73/73 [00:05<00:00, 12.49it/s, loss=0.941, v_num=2tag, BTC_val\u001b[A\n",
      "Epoch 19:  99%|â–‰| 72/73 [00:06<00:00, 11.66it/s, loss=0.96, v_num=2tag, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 73/73 [00:06<00:00, 11.66it/s, loss=0.96, v_num=2tag, BTC_val_\u001b[A\n",
      "Epoch 20:  99%|â–‰| 72/73 [00:06<00:00, 11.70it/s, loss=0.94, v_num=2tag, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 73/73 [00:06<00:00, 11.68it/s, loss=0.94, v_num=2tag, BTC_val_\u001b[A\n",
      "Epoch 21:  99%|â–‰| 72/73 [00:06<00:00, 11.98it/s, loss=0.955, v_num=2tag, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|â–ˆ| 73/73 [00:06<00:00, 11.96it/s, loss=0.955, v_num=2tag, BTC_val\u001b[A\n",
      "Epoch 22:  99%|â–‰| 72/73 [00:05<00:00, 12.18it/s, loss=0.928, v_num=2tag, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|â–ˆ| 73/73 [00:06<00:00, 12.16it/s, loss=0.928, v_num=2tag, BTC_val\u001b[A\n",
      "Epoch 23:  99%|â–‰| 72/73 [00:06<00:00, 10.88it/s, loss=0.911, v_num=2tag, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|â–ˆ| 73/73 [00:06<00:00, 10.87it/s, loss=0.911, v_num=2tag, BTC_val\u001b[A\n",
      "Epoch 24:  99%|â–‰| 72/73 [00:06<00:00, 10.82it/s, loss=0.956, v_num=2tag, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|â–ˆ| 73/73 [00:06<00:00, 10.82it/s, loss=0.956, v_num=2tag, BTC_val\u001b[A\n",
      "Epoch 25:  99%|â–‰| 72/73 [00:06<00:00, 11.91it/s, loss=0.89, v_num=2tag, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|â–ˆ| 73/73 [00:06<00:00, 11.90it/s, loss=0.89, v_num=2tag, BTC_val_\u001b[A\n",
      "Epoch 26:  99%|â–‰| 72/73 [00:05<00:00, 13.11it/s, loss=0.905, v_num=2tag, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|â–ˆ| 73/73 [00:05<00:00, 13.10it/s, loss=0.905, v_num=2tag, BTC_val\u001b[A\n",
      "Epoch 27:  99%|â–‰| 72/73 [00:05<00:00, 14.05it/s, loss=0.929, v_num=2tag, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|â–ˆ| 73/73 [00:05<00:00, 13.99it/s, loss=0.929, v_num=2tag, BTC_val\u001b[A\n",
      "Epoch 28:  99%|â–‰| 72/73 [00:05<00:00, 13.99it/s, loss=0.915, v_num=2tag, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|â–ˆ| 73/73 [00:05<00:00, 13.95it/s, loss=0.915, v_num=2tag, BTC_val\u001b[A\n",
      "Epoch 29:  99%|â–‰| 72/73 [00:05<00:00, 13.98it/s, loss=0.945, v_num=2tag, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|â–ˆ| 73/73 [00:05<00:00, 13.90it/s, loss=0.945, v_num=2tag, BTC_val\u001b[A\n",
      "Epoch 30:  99%|â–‰| 72/73 [00:06<00:00, 11.56it/s, loss=0.911, v_num=2tag, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|â–ˆ| 73/73 [00:06<00:00, 11.53it/s, loss=0.911, v_num=2tag, BTC_val\u001b[A\n",
      "Epoch 31:  99%|â–‰| 72/73 [00:06<00:00, 11.67it/s, loss=0.916, v_num=2tag, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|â–ˆ| 73/73 [00:06<00:00, 11.68it/s, loss=0.916, v_num=2tag, BTC_val\u001b[A\n",
      "Epoch 32:  99%|â–‰| 72/73 [00:06<00:00, 11.51it/s, loss=0.932, v_num=2tag, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|â–ˆ| 73/73 [00:06<00:00, 11.52it/s, loss=0.932, v_num=2tag, BTC_val\u001b[A\n",
      "Epoch 33:  99%|â–‰| 72/73 [00:05<00:00, 12.03it/s, loss=0.906, v_num=2tag, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|â–ˆ| 73/73 [00:06<00:00, 11.98it/s, loss=0.906, v_num=2tag, BTC_val\u001b[A\n",
      "Epoch 34:  99%|â–‰| 72/73 [00:06<00:00, 11.63it/s, loss=0.909, v_num=2tag, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|â–ˆ| 73/73 [00:06<00:00, 11.62it/s, loss=0.909, v_num=2tag, BTC_val\u001b[A\n",
      "Epoch 35:  99%|â–‰| 72/73 [00:05<00:00, 12.35it/s, loss=0.928, v_num=2tag, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|â–ˆ| 73/73 [00:05<00:00, 12.32it/s, loss=0.928, v_num=2tag, BTC_val\u001b[A\n",
      "Epoch 36:  99%|â–‰| 72/73 [00:06<00:00, 11.32it/s, loss=0.903, v_num=2tag, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.972. Signaling Trainer to stop.\n",
      "Epoch 36: 100%|â–ˆ| 73/73 [00:06<00:00, 11.31it/s, loss=0.903, v_num=2tag, BTC_val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|â–ˆ| 73/73 [00:06<00:00, 11.31it/s, loss=0.903, v_num=2tag, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 28.05it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5357142686843872,\n",
      " 'BTC_test_f1': 0.2323809713125229,\n",
      " 'ETH_test_acc': 0.5,\n",
      " 'ETH_test_f1': 0.2212051898241043,\n",
      " 'LTC_test_acc': 0.4285714328289032,\n",
      " 'LTC_test_f1': 0.1999756544828415,\n",
      " 'test_loss': 0.9154524207115173}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 83017\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_030015-14vn2tag/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_030015-14vn2tag/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.54456\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.1875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.14701\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.3125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.21442\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.88663\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 36\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2664\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621469037\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.54308\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.45122\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.5309\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.43051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.54569\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.43189\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.90727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.37037\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.38889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.04834\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.53571\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.23238\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.22121\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.42857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.19998\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.91545\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–…â–‡â–‡â–…â–„â–‡â–‡â–…â–‡â–…â–ˆâ–„â–‡â–„â–†â–„â–ƒâ–…â–…â–†â–â–†â–†â–„â–ƒâ–„â–„â–†â–…â–†â–‡â–…â–…â–…â–‡â–‡â–†â–…â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–ƒâ–„â–„â–ƒâ–„â–„â–„â–ƒâ–†â–ƒâ–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–„â–ƒâ–…â–â–†â–…â–ƒâ–‚â–ƒâ–„â–†â–†â–…â–‡â–‡â–„â–†â–†â–ˆâ–‡â–…â–„â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–…â–‡â–…â–„â–ƒâ–†â–„â–„â–…â–ˆâ–‡â–ƒâ–‡â–ƒâ–ƒâ–ƒâ–‚â–„â–…â–…â–„â–†â–ƒâ–†â–…â–ƒâ–†â–„â–ƒâ–†â–†â–ƒâ–ƒâ–†â–„â–†â–„â–„â–ƒâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–ƒâ–„â–ƒâ–ƒâ–‚â–„â–ƒâ–ƒâ–ƒâ–…â–„â–‚â–„â–‚â–‚â–‚â–â–„â–ƒâ–…â–ƒâ–„â–‚â–‡â–…â–‚â–‡â–…â–„â–„â–‡â–ƒâ–„â–ˆâ–†â–ˆâ–†â–†â–„â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–†â–‡â–„â–…â–„â–ƒâ–ˆâ–†â–ƒâ–†â–…â–‚â–†â–ƒâ–„â–‚â–â–‡â–ƒâ–ƒâ–â–…â–‚â–…â–‚â–†â–†â–…â–…â–†â–†â–‚â–‚â–…â–‚â–ˆâ–‚â–…â–ˆâ–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–ƒâ–‚â–‚â–â–ƒâ–â–„â–â–ƒâ–„â–‚â–…â–ˆâ–ƒâ–‚â–‚â–„â–‚â–‡â–‚â–…â–ˆâ–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–†â–†â–†â–†â–‡â–‡â–…â–‡â–…â–…â–„â–ˆâ–„â–‡â–†â–ˆâ–‡â–„â–…â–†â–†â–…â–†â–…â–…â–…â–…â–ƒâ–…â–ƒâ–„â–‡â–‡â–„â–…â–â–‡â–†â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–â–‚â–‚â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–â–‚â–‚â–â–ƒâ–ƒâ–…â–†â–„â–†â–†â–„â–„â–‡â–†â–‡â–ˆâ–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–„â–„â–…â–†â–…â–†â–‡â–†â–‡â–†â–‡â–ˆâ–ˆâ–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–„â–ƒâ–‚â–†â–„â–„â–†â–‡â–…â–‡â–†â–ˆâ–ˆâ–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–„â–„â–„â–…â–…â–…â–†â–‡â–‡â–‡â–†â–‡â–ˆâ–ˆâ–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–â–ƒâ–„â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–„â–ƒâ–„â–…â–…â–†â–‡â–ƒâ–‡â–…â–†â–†â–†â–‡â–†â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–‚â–„â–„â–„â–…â–…â–†â–‡â–†â–‡â–†â–†â–‡â–ˆâ–ˆâ–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–â–â–â–ˆâ–â–ˆâ–â–â–â–â–â–â–â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–…â–‚â–â–…â–‚â–…â–‚â–ˆâ–…â–ˆâ–…â–…â–…â–…â–…â–…â–…â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–…â–…â–…â–…â–…â–…â–…â–…â–â–â–â–â–ˆâ–…â–…â–â–â–â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–„â–‚â–…â–„â–ƒâ–ƒâ–â–ˆâ–…â–„â–ƒâ–â–ƒâ–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–…â–†â–ˆâ–†â–†â–…â–ƒâ–…â–ƒâ–ƒâ–…â–â–ƒâ–ƒâ–ƒâ–†â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–…â–‚â–‚â–…â–â–…â–ˆâ–…â–‡â–ƒâ–ƒâ–„â–ƒâ–ƒâ–„â–â–ƒâ–ƒâ–ƒâ–…â–„â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–…â–…â–„â–…â–„â–„â–…â–„â–„â–…â–…â–„â–„â–„â–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–„â–‚â–ƒâ–„â–…â–ƒâ–…â–„â–†â–†â–ˆâ–‡â–†â–‡â–‡â–†â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/14vn2tag\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 03:04:18.191291: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm__multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/37qvnx2r\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_030416-37qvnx2r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|â–‰| 73/74 [00:04<00:00, 15.04it/s, loss=1.12, v_num=nx2r, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆ| 74/74 [00:04<00:00, 15.00it/s, loss=1.12, v_num=nx2r, BTC_val_a\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved. New best score: 1.066\n",
      "Epoch 1: 100%|â–ˆ| 74/74 [00:05<00:00, 14.65it/s, loss=1.1, v_num=nx2r, BTC_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆ| 74/74 [00:05<00:00, 14.57it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 74/74 [00:05<00:00, 14.74it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆ| 74/74 [00:05<00:00, 14.66it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 74/74 [00:05<00:00, 14.23it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆ| 74/74 [00:05<00:00, 14.14it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n",
      "Epoch 4:  99%|â–‰| 73/74 [00:06<00:00, 11.18it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆ| 74/74 [00:06<00:00, 11.19it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 74/74 [00:05<00:00, 13.13it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|â–ˆ| 74/74 [00:05<00:00, 13.07it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 74/74 [00:06<00:00, 12.19it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|â–ˆ| 74/74 [00:06<00:00, 12.13it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 74/74 [00:06<00:00, 12.27it/s, loss=1.11, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|â–ˆ| 74/74 [00:06<00:00, 12.22it/s, loss=1.11, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 74/74 [00:05<00:00, 12.34it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|â–ˆ| 74/74 [00:06<00:00, 12.29it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 74/74 [00:05<00:00, 13.51it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|â–ˆ| 74/74 [00:05<00:00, 13.44it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 74/74 [00:05<00:00, 14.13it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|â–ˆ| 74/74 [00:05<00:00, 14.06it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Epoch 11:  99%|â–‰| 73/74 [00:05<00:00, 14.14it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|â–ˆ| 74/74 [00:05<00:00, 14.11it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 74/74 [00:05<00:00, 12.49it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|â–ˆ| 74/74 [00:05<00:00, 12.44it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 74/74 [00:05<00:00, 13.22it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|â–ˆ| 74/74 [00:05<00:00, 13.16it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 74/74 [00:05<00:00, 13.33it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|â–ˆ| 74/74 [00:05<00:00, 13.26it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 74/74 [00:06<00:00, 12.23it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|â–ˆ| 74/74 [00:06<00:00, 12.17it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 74/74 [00:05<00:00, 12.89it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|â–ˆ| 74/74 [00:05<00:00, 12.82it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 74/74 [00:06<00:00, 11.86it/s, loss=1.09, v_num=nx2r, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|â–ˆ| 74/74 [00:06<00:00, 11.80it/s, loss=1.09, v_num=nx2r, BTC_val_\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 74/74 [00:06<00:00, 11.54it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|â–ˆ| 74/74 [00:06<00:00, 11.49it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 74/74 [00:06<00:00, 11.41it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|â–ˆ| 74/74 [00:06<00:00, 11.37it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:05<00:00, 12.36it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.066. Signaling Trainer to stop.\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:06<00:00, 12.30it/s, loss=1.1, v_num=nx2r, BTC_val_a\n",
      "Epoch 20: 100%|â–ˆ| 74/74 [00:06<00:00, 12.29it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 27.34it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.25,\n",
      " 'BTC_test_f1': 0.1315789520740509,\n",
      " 'ETH_test_acc': 0.4285714328289032,\n",
      " 'ETH_test_f1': 0.1991342157125473,\n",
      " 'LTC_test_acc': 0.3928571343421936,\n",
      " 'LTC_test_f1': 0.18594105541706085,\n",
      " 'test_loss': 1.1109517812728882}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 83545\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_030416-37qvnx2r/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_030416-37qvnx2r/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.25877\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.30501\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.3125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.18519\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.09871\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 130\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621469186\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.3619\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.2683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.34199\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.26888\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.32727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.2613\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09938\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.18182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.18182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.25641\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.08925\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.13158\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.42857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19913\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.39286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.18594\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.11095\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step â–â–†â–ˆâ–ƒâ–ƒâ–‚â–†â–‚â–…â–…â–†â–†â–ˆâ–†â–ƒâ–ƒâ–…â–‚â–‡â–â–…â–‚â–†â–†â–ƒâ–…â–…â–†â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step â–‚â–‚â–‡â–„â–‚â–‚â–‚â–‚â–ƒâ–„â–„â–…â–ˆâ–†â–ƒâ–ƒâ–ƒâ–ƒâ–…â–â–‚â–‚â–„â–…â–‚â–„â–„â–„â–ƒâ–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step â–…â–†â–ƒâ–‚â–…â–‚â–…â–†â–…â–…â–â–…â–…â–†â–ƒâ–…â–â–…â–‚â–â–‡â–…â–‡â–‡â–…â–ƒâ–†â–ˆâ–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step â–†â–…â–ƒâ–ƒâ–†â–‚â–„â–†â–„â–…â–â–ƒâ–…â–†â–„â–…â–â–„â–‚â–‚â–‡â–„â–†â–ˆâ–„â–ƒâ–…â–‡â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step â–„â–…â–â–‡â–…â–â–…â–‚â–„â–â–…â–…â–â–…â–‡â–‚â–„â–†â–„â–…â–„â–â–ˆâ–…â–ƒâ–ƒâ–‡â–†â–‡â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step â–„â–„â–â–‡â–†â–â–‚â–‚â–„â–â–…â–…â–â–…â–ˆâ–‚â–ƒâ–‡â–„â–…â–‚â–â–†â–…â–‚â–ƒâ–†â–…â–…â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step â–†â–ƒâ–…â–ƒâ–…â–ˆâ–‚â–†â–ƒâ–†â–…â–…â–†â–‚â–ƒâ–…â–…â–ƒâ–…â–„â–ƒâ–…â–â–â–„â–„â–„â–‚â–‚â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch â–ƒâ–â–†â–‡â–†â–…â–†â–…â–‡â–…â–‡â–…â–…â–†â–‡â–†â–†â–‡â–‡â–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch â–ƒâ–ƒâ–ˆâ–„â–ƒâ–ƒâ–„â–‚â–†â–â–„â–ƒâ–‚â–‚â–â–‚â–„â–â–…â–„â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch â–ƒâ–â–â–‚â–â–„â–‚â–â–…â–ˆâ–†â–ƒâ–„â–„â–…â–ƒâ–‡â–„â–†â–…â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch â–‡â–‡â–‡â–‚â–‚â–†â–…â–„â–ˆâ–‡â–‡â–†â–†â–„â–…â–ƒâ–ˆâ–â–…â–„â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch â–„â–…â–…â–â–‚â–â–„â–…â–‡â–ˆâ–„â–†â–‡â–‡â–…â–„â–‡â–„â–…â–†â–ƒ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch â–‡â–ˆâ–ˆâ–‚â–‚â–‚â–„â–„â–‡â–…â–„â–…â–†â–†â–‚â–‚â–†â–â–„â–„â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch â–ˆâ–„â–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–…â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss â–â–ƒâ–…â–†â–†â–ˆâ–‡â–‡â–‡â–…â–ƒâ–„â–…â–…â–„â–†â–…â–…â–…â–…â–†\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/37qvnx2r\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for c in (ParameterGrid(param_grid)):\n",
    "    config = CONFIG.copy()\n",
    "    config.update(c)\n",
    "    script = \"--currency-list \" + \" \".join([i for i in c[\"currency_list\"]])\n",
    "    script += \" --lstm-list \" + \" \".join([str(i) for i in c[\"lstm_hidden_sizes\"]])\n",
    "    script += \" -trend \" + str(1 if c[\"remove_trend\"] else 0)\n",
    "    script += \" -classes \" + str(c[\"n_classes\"] )\n",
    "    script += \" -weight \" + str(1 if c[\"loss_weight_calculate\"] else 0) \n",
    "\n",
    "    experiment(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "04984682def58a97e4300fcfdea82226e95c772fd8b0b63e42875ad1781ae0ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
