{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"metadata":{"interpreter":{"hash":"04984682def58a97e4300fcfdea82226e95c772fd8b0b63e42875ad1781ae0ab"}},"colab":{"name":"09_ak_experimenting_lstm-2.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"6I-Ad0m4WEM8"},"source":["#aysenur\n","#experiment 1 de jupyter içerisinde weight update edilmediğini farkettim\n","#detayını gitmedim ama gpu da accelerator kullanmakla alakalı o yüzden her şeyi .py scripte aldım\n","#scripte aldıklarım aşağıda sırasıyla verildi, bir kere denemesi yapıldı\n","#sonrasında belirlenen parametrelerle script çalıştırıldı\n","#CosineWarmupScheduler eklendi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sOIssxJtXxll"},"source":["#https://medium.com/analytics-vidhya/how-to-use-google-colab-with-github-via-google-drive-68efb23a42d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n3OUk8RuauMF","executionInfo":{"status":"ok","timestamp":1625569182105,"user_tz":-180,"elapsed":442,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}},"outputId":"2372255e-43d4-46bf-fad2-0c96b659c3c9"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Tue Jul  6 10:59:31 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XFrQ1E7EWjtf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625569208533,"user_tz":-180,"elapsed":23794,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}},"outputId":"4cc9d3c8-25d4-4349-a178-7a32c35893eb"},"source":["!pip install pytorch_lightning\n","!pip install wandb\n","!pip install ta"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting pytorch_lightning\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/5e/19c817ad2670c1d822642ed7bfc4d9d4c30c2f8eaefebcd575a3188d7319/pytorch_lightning-1.3.8-py3-none-any.whl (813kB)\n","\u001b[K     |████████████████████████████████| 819kB 6.6MB/s \n","\u001b[?25hCollecting PyYAML<=5.4.1,>=5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n","\u001b[K     |████████████████████████████████| 645kB 46.8MB/s \n","\u001b[?25hCollecting torchmetrics>=0.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/8b/de8df9044ca2ac5dfc6b13b9ad3b3ebe6b3a45807311102b569d680e811f/torchmetrics-0.4.1-py3-none-any.whl (234kB)\n","\u001b[K     |████████████████████████████████| 235kB 52.1MB/s \n","\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/3a/666e63625a19883ae8e1674099e631f9737bd5478c4790e5ad49c5ac5261/fsspec-2021.6.1-py3-none-any.whl (115kB)\n","\u001b[K     |████████████████████████████████| 122kB 54.3MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.41.1)\n","Collecting pyDeprecate==0.3.0\n","  Downloading https://files.pythonhosted.org/packages/14/52/aa227a0884df71ed1957649085adf2b8bc2a1816d037c2f18b3078854516/pyDeprecate-0.3.0-py3-none-any.whl\n","Collecting tensorboard!=2.5.0,>=2.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/21/eebd23060763fedeefb78bc2b286e00fa1d8abda6f70efa2ee08c28af0d4/tensorboard-2.4.1-py3-none-any.whl (10.6MB)\n","\u001b[K     |████████████████████████████████| 10.6MB 36.7MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.9.0+cu102)\n","Requirement already satisfied: pillow!=8.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (7.1.2)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n","Collecting future>=0.17.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n","\u001b[K     |████████████████████████████████| 829kB 46.6MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (20.9)\n","Collecting aiohttp; extra == \"http\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 46.2MB/s \n","\u001b[?25hRequirement already satisfied: requests; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.12.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.0.1)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.36.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.15.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.3.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.8.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.31.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.12.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (57.0.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.4.4)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.34.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch_lightning) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (2.4.7)\n","Requirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.2.0)\n","Collecting yarl<2.0,>=1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n","\u001b[K     |████████████████████████████████| 296kB 43.4MB/s \n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n","\u001b[K     |████████████████████████████████| 143kB 44.7MB/s \n","\u001b[?25hCollecting async-timeout<4.0,>=3.0\n","  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.5.30)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (4.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (4.2.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.3.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.4.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.1.1)\n","Building wheels for collected packages: future\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491070 sha256=6a0a69acdda092f19375802034846a02196691ee7615af855191c4457c18b94b\n","  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n","Successfully built future\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement tensorboard~=2.5, but you'll have tensorboard 2.4.1 which is incompatible.\u001b[0m\n","Installing collected packages: PyYAML, torchmetrics, multidict, yarl, async-timeout, aiohttp, fsspec, pyDeprecate, tensorboard, future, pytorch-lightning\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Found existing installation: tensorboard 2.5.0\n","    Uninstalling tensorboard-2.5.0:\n","      Successfully uninstalled tensorboard-2.5.0\n","  Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","Successfully installed PyYAML-5.4.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-2021.6.1 future-0.18.2 multidict-5.1.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.8 tensorboard-2.4.1 torchmetrics-0.4.1 yarl-1.6.3\n","Collecting wandb\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/f6/91c07f54c2162854f5028aaa13f576ca17a3bc0cf6da02c2ad5baddae128/wandb-0.10.33-py2.py3-none-any.whl (1.8MB)\n","\u001b[K     |████████████████████████████████| 1.8MB 4.9MB/s \n","\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n","Collecting shortuuid>=0.5.0\n","  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n","Collecting sentry-sdk>=0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\n","\u001b[K     |████████████████████████████████| 133kB 24.1MB/s \n","\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Collecting subprocess32>=3.5.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n","\u001b[K     |████████████████████████████████| 102kB 11.8MB/s \n","\u001b[?25hCollecting configparser>=3.8.1\n","  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.1)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n","Collecting GitPython>=1.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/91/b38c4fabb6e5092ab23492ded4f318ab7299b19263272b703478038c0fbc/GitPython-3.1.18-py3-none-any.whl (170kB)\n","\u001b[K     |████████████████████████████████| 174kB 24.0MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Collecting pathtools\n","  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (57.0.0)\n","Collecting gitdb<5,>=4.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n","\u001b[K     |████████████████████████████████| 71kB 9.8MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n","Collecting smmap<5,>=3.0.1\n","  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n","Building wheels for collected packages: subprocess32, pathtools\n","  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6502 sha256=a0e1810876191ab69792d708302a3078a7726daa4094013f0e5eb1f22a636583\n","  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8807 sha256=bba5a155db23997b661628ebc8ee63838db58ce8a35b9f4d0693fd4fd6a8593a\n","  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n","Successfully built subprocess32 pathtools\n","Installing collected packages: shortuuid, sentry-sdk, subprocess32, configparser, docker-pycreds, smmap, gitdb, GitPython, pathtools, wandb\n","Successfully installed GitPython-3.1.18 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.1.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.33\n","Collecting ta\n","  Downloading https://files.pythonhosted.org/packages/a9/22/a355ecf2d67da8150332d22ef65c3a1f79109528279bf5d40735b6f2bd72/ta-0.7.0.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ta) (1.19.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ta) (1.1.5)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->ta) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ta) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->ta) (1.15.0)\n","Building wheels for collected packages: ta\n","  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ta: filename=ta-0.7.0-cp37-none-any.whl size=28716 sha256=a1a60531a66bb0058d5b7ee7bcb26f2c8bf96c23efc493203b0813b4ad03bd5b\n","  Stored in directory: /root/.cache/pip/wheels/dd/88/30/de9553fb54a474eb7480b937cdbb140bdda613d29cf4da7994\n","Successfully built ta\n","Installing collected packages: ta\n","Successfully installed ta-0.7.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lLnyGjnyWOmm","executionInfo":{"status":"ok","timestamp":1625569242631,"user_tz":-180,"elapsed":34109,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}},"outputId":"fd30d54c-caaa-4fc2-b909-5cce377c7e88"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","import os \n","os.chdir('/content/drive/MyDrive/aysenurk/multi_task_price_change_prediction/notebooks')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uCFtqBAfem1Z","executionInfo":{"status":"ok","timestamp":1625569467307,"user_tz":-180,"elapsed":423,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}},"outputId":"c7005b55-edab-41b7-be22-01cd3a9c8520"},"source":["!git status"],"execution_count":12,"outputs":[{"output_type":"stream","text":["On branch master\n","Your branch is up to date with 'origin/master'.\n","\n","Changes to be committed:\n","  (use \"git reset HEAD <file>...\" to unstage)\n","\n","\t\u001b[32mmodified:   09_ak_experimenting_lstm-2.ipynb\u001b[m\n","\t\u001b[32mmodified:   10_ak_experimenting_multi_head_attention.ipynb\u001b[m\n","\n","Changes not staged for commit:\n","  (use \"git add/rm <file>...\" to update what will be committed)\n","  (use \"git checkout -- <file>...\" to discard changes in working directory)\n","\n","\t\u001b[31mdeleted:    ../pipelines/multi_task_price_change_prediction/DataPreparation.py\u001b[m\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CKPqYSiOfHdC","executionInfo":{"status":"ok","timestamp":1625569466129,"user_tz":-180,"elapsed":1494,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}}},"source":["!git commi"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"5b86SSBfWENB","executionInfo":{"status":"ok","timestamp":1625569247858,"user_tz":-180,"elapsed":5255,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}}},"source":["import pandas as pd\n","import numpy as np\n","\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import DataLoader, Dataset\n","\n","import pytorch_lightning as pl\n","\n","from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","\n","from pytorch_lightning.loggers import WandbLogger\n","from pytorch_lightning.loggers import TensorBoardLogger\n","import wandb\n","\n","from DataPreparation import get_data"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"YtUScNj3WEND","executionInfo":{"status":"ok","timestamp":1625569247867,"user_tz":-180,"elapsed":146,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}}},"source":["class TimeSeriesDataset(Dataset):\n","    def __init__(self, \n","                 currency_list,\n","                 x: np.ndarray, \n","                 y: np.ndarray,\n","                 data_use_type,\n","                 train_percentage,\n","                 val_percentage,\n","                 test_percentage,\n","                 seq_len, \n","                 ):\n","        self.currencies = currency_list\n","        self.n_currencies = len(self.currencies)\n","        self.x = torch.tensor(x[:self.n_currencies].astype(float))#.float()\n","        self.y = torch.tensor(y[:self.n_currencies].astype(long))#.long()\n","        self.seq_len = seq_len\n","        self.data_use_type = data_use_type\n","        \n","        \n","        #self.train_size = int(len(self.x[0]) * train_percentage)\n","        self.val_size = int(len(self.x[0]) * val_percentage)\n","        self.test_size = int(len(self.x[0]) * test_percentage)\n","        self.train_size = len(self.x[0]) - self.val_size - self.test_size \n","        print(self.test_size, self.val_size, self.train_size) \n","        \n","        self.train_mean = [self.x[i][:self.train_size].mean() for i in range(self.n_currencies)]\n","        self.train_std = [self.x[i][:self.train_size].std() for i in range(self.n_currencies)]\n","        \n","#         self.train_min = [self.x[i][:self.train_size].min() for i in range(n_currencies)]\n","#         self.train_max = [self.x[i][:self.train_size].max() for i in range(n_currencies)]\n","        \n","    def __len__(self):\n","        \n","        if self.data_use_type == \"train\":\n","            return self.train_size - ( self.seq_len)\n","\n","        elif self.data_use_type == \"val\":\n","            return self.val_size\n","  \n","        else:\n","            return self.test_size\n","        \n","    \n","    def __getitem__(self, index):\n","        \n","        item = dict()\n","        \n","        if self.data_use_type ==\"val\":\n","            index = self.train_size + index - self.seq_len\n","            \n","        elif self.data_use_type ==\"test\":\n","            index = self.train_size + self.val_size + index - self.seq_len\n","        \n","        for i in range(self.n_currencies):\n","            window = self.x[i][index:index+self.seq_len]\n","            window = (window -self.train_mean[i]) / self.train_std[i]\n","            \n","            item[self.currencies[i] + \"_window\"] = window\n","            item[self.currencies[i] + \"_label\"]  = self.y[i][index+self.seq_len]\n","\n","        return item"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":234},"id":"ek_jiKYkOMb7","executionInfo":{"status":"error","timestamp":1625569247892,"user_tz":-180,"elapsed":163,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}},"outputId":"8b4ce23f-1ce9-4a85-a378-01683cfa61f7"},"source":["X, y, features, dfs = get_data(CURRENCY_LST,\n","                            N_CLASSES,\n","                             FREQUENCY, \n","                             WINDOW_SIZE,\n","                             neutral_quantile = NEUTRAL_QUANTILE,\n","                             log_price=True,\n","                             remove_trend=REMOVE_TREND,\n","                             include_indicators = True,\n","                             include_imfs = False\n","                            )"],"execution_count":6,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-1c04da62c10d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m X, y, features, dfs = get_data(CURRENCY_LST,\n\u001b[0m\u001b[1;32m      2\u001b[0m                             \u001b[0mN_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                              \u001b[0mFREQUENCY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                              \u001b[0mWINDOW_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                              \u001b[0mneutral_quantile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNEUTRAL_QUANTILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'CURRENCY_LST' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":345},"id":"R9_m-yO2PFif","executionInfo":{"status":"error","timestamp":1625565594757,"user_tz":-180,"elapsed":635,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}},"outputId":"5fb9ecaf-1aef-4d8b-8704-2fa487316d6b"},"source":["train_dataset, val_dataset, test_dataset = [TimeSeriesDataset(CURRENCY_LST, \n","                                                          X, \n","                                                          y, \n","                                                          dtype, \n","                                                          TRAIN_PERCENTAGE, \n","                                                          VAL_PERCENTAGE, \n","                                                          TEST_PERCENTAGE, \n","                                                          WINDOW_SIZE) for dtype in ['train', 'val', 'test']]"],"execution_count":62,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-62-c872876de484>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                           \u001b[0mVAL_PERCENTAGE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                           \u001b[0mTEST_PERCENTAGE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                                           WINDOW_SIZE) for dtype in ['train', 'val', 'test']]\n\u001b[0m","\u001b[0;32m<ipython-input-62-c872876de484>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                           \u001b[0mVAL_PERCENTAGE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                           \u001b[0mTEST_PERCENTAGE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                                           WINDOW_SIZE) for dtype in ['train', 'val', 'test']]\n\u001b[0m","\u001b[0;32m<ipython-input-60-bc4338f4ac92>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, currency_list, x, y, data_use_type, train_percentage, val_percentage, test_percentage, seq_len)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrency_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_currencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_currencies\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.float()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_currencies\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.long()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2017-08-20'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"85gkfh7dOQyD","executionInfo":{"status":"ok","timestamp":1625565054086,"user_tz":-180,"elapsed":435,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}},"outputId":"31404859-3919-4948-e9cf-0fcf3fbb3709"},"source":["X[0]"],"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[4120.98, 4211.08, 4032.62, ..., -1.2968661684355842,\n","        -1.3053488973032046, -4.639119923081947],\n","       [4069.13, 4119.62, 3911.79, ..., -1.7201422317065118,\n","        -1.7351085545220357, -6.279462693812022],\n","       [4016.0, 4104.82, 3400.0, ..., 0.5976095617529875,\n","        0.5958309583631305, -5.719379801543956],\n","       ...,\n","       [57797.35, 57911.02, 56035.25, ..., -2.1144501324126486,\n","        -2.1371248289668188, 1220.353645672893],\n","       [56578.21, 58981.44, 56435.0, ..., 1.0448898966580966,\n","        1.039468653491582, 1234.1498875166858],\n","       [57169.39, 57200.0, 53046.69, ..., -6.943191102791191,\n","        -7.196003096966841, 1141.5173112287287]], dtype=object)"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"gAeS6TcBWENF","executionInfo":{"status":"ok","timestamp":1625563436041,"user_tz":-180,"elapsed":6,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}}},"source":["class CosineWarmupScheduler(torch.optim.lr_scheduler._LRScheduler):\n","    \n","    def __init__(self, optimizer, warmup, max_iters):\n","        self.warmup = warmup\n","        self.max_num_iters = max_iters\n","        super().__init__(optimizer)\n","        \n","    def get_lr(self):\n","        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n","        return [base_lr * lr_factor for base_lr in self.base_lrs]\n","    \n","    def get_lr_factor(self, epoch):\n","        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n","        if epoch <= self.warmup:\n","            lr_factor *= epoch * 1.0 / self.warmup\n","        return lr_factor"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"PJggzp5TWENG","executionInfo":{"status":"ok","timestamp":1625563436676,"user_tz":-180,"elapsed":320,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}}},"source":["class LSTM_based_classification_model(pl.LightningModule):\n","    def __init__(self,\n","                 train_dataset,\n","                 val_dataset,\n","                 test_dataset,\n","                 calculate_loss_weights,\n","                 currencies,\n","                 num_classes,\n","                 window_size,\n","                 input_size,\n","                 batch_size,\n","                 lstm_hidden_sizes,\n","                 bidirectional,\n","                 learning_rate = 1e-3,\n","                 scheduler_step = 10,\n","                 scheduler_gamma = 0.1,\n","                 ):\n","        \n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.currencies = currencies\n","        self.num_tasks = len(currencies)\n","        self.window_size = window_size\n","        self.input_size = input_size\n","        self.batch_size = batch_size\n","        \n","        self.lstm_hidden_sizes = lstm_hidden_sizes\n","        self.bidirectional = bidirectional \n","        \n","        if calculate_loss_weights:\n","            loss_weights = []\n","            for i in range(self.num_tasks):\n","                train_labels = [int(train_dataset[n][self.currencies[i] +\"_label\"] )for n in range(train_dataset.__len__())]\n","                samples_size = pd.DataFrame({\"label\": train_labels}).groupby(\"label\").size().to_numpy()\n","                loss_weights.append((1 / samples_size) * sum(samples_size)/2)\n","            self.weights = loss_weights\n","        else:\n","            self.weights = None\n","        \n","        self.lstm_1 = nn.LSTM(input_size = self.input_size, \n","                              num_layers=1, \n","                              batch_first=True, \n","                              hidden_size = self.lstm_hidden_sizes[0], \n","                              bidirectional = bidirectional)\n","        self.batch_norm1 = nn.BatchNorm2d(num_features=self.lstm_hidden_sizes[0])\n","        \n","        if len(self.lstm_hidden_sizes) > 1:\n","            self.lstm_2 = nn.LSTM(input_size = self.lstm_hidden_sizes[0], \n","                                  num_layers=1, \n","                                  batch_first=True, \n","                                  hidden_size = self.lstm_hidden_sizes[1], \n","                                  bidirectional = bidirectional)\n","            self.batch_norm2 = nn.BatchNorm2d(num_features=self.lstm_hidden_sizes[1])\n","\n","            self.lstm_3 = nn.LSTM(input_size = self.lstm_hidden_sizes[1], \n","                                  num_layers=1, \n","                                  batch_first=True, \n","                                  hidden_size = self.lstm_hidden_sizes[2], \n","                                  bidirectional = bidirectional)\n","            self.batch_norm3 = nn.BatchNorm2d(num_features=self.lstm_hidden_sizes[2])\n","        \n","        self.dropout = nn.Dropout(0.5)\n","        \n","        self.linear1 =[nn.Linear(self.lstm_hidden_sizes[-1], int(self.lstm_hidden_sizes[-1]/2))] * self.num_tasks\n","        self.linear1 = torch.nn.ModuleList(self.linear1)\n","        self.activation = nn.ReLU()\n","        \n","        self.output_layers = [nn.Linear(int(self.lstm_hidden_sizes[-1]/2), self.num_classes)] * self.num_tasks\n","        self.output_layers = torch.nn.ModuleList(self.output_layers)\n","        \n","        if self.weights != None:\n","            self.cross_entropy_loss = [nn.CrossEntropyLoss(weight= torch.tensor(weights).float()) for weights in self.weights]\n","        else:\n","            self.cross_entropy_loss = [nn.CrossEntropyLoss() for _ in range(self.num_tasks)]\n","        \n","        self.cross_entropy_loss = torch.nn.ModuleList(self.cross_entropy_loss)\n","        \n","        self.f1_score = pl.metrics.F1(num_classes=self.num_classes, average=\"macro\")\n","        self.accuracy_score = pl.metrics.Accuracy()\n","        \n","        self.train_dl = DataLoader(train_dataset, batch_size=self.batch_size, shuffle = True)\n","        self.val_dl = DataLoader(val_dataset, batch_size=self.batch_size)\n","        self.test_dl = DataLoader(test_dataset, batch_size=self.batch_size)\n","        \n","        self.learning_rate = learning_rate\n","        self.scheduler_step = scheduler_step\n","        self.scheduler_gamma = scheduler_gamma\n","        \n","    def forward(self, x, i):\n","\n","        batch_size = x.size()[0]\n","        \n","        x = x.view(batch_size, self.window_size, self.input_size) #(batch, window_len, feature_size)\n","        x, _  = self.lstm_1(x)\n","        \n","        x = self.dropout(x)\n","\n","        x = x.reshape(x.size()[-1], batch_size, self.window_size) #(feature_size, batch, window_len)\n","        x = self.batch_norm1(x.unsqueeze(0))\n","        \n","        if len(self.lstm_hidden_sizes) > 1:\n","            \n","            x = x.view(batch_size, self.window_size, x.size()[1])\n","            x, _  = self.lstm_2(x)\n","\n","            x = self.dropout(x)\n","\n","            x = x.reshape(x.size()[-1], batch_size, self.window_size) #(feature_size, batch, window_len)\n","            x = self.batch_norm2(x.unsqueeze(0))\n","\n","            x = x.view(batch_size, self.window_size, x.size()[1])\n","            x, _  = self.lstm_3(x)\n","\n","            x = self.dropout(x)\n","\n","            x = x.reshape(x.size()[-1], batch_size, self.window_size) #(feature_size, batch, window_len)\n","            x = self.batch_norm3(x.unsqueeze(0))\n","        \n","        x = x.view(batch_size, self.window_size, x.size()[1])\n","        x = x[:, -1, :] # equivalent to return sequence = False on keras :)\n","        \n","        x = self.dropout(x)\n","        \n","        x = self.linear1[i](x)\n","        x = self.activation(x)\n","                 \n","        output = self.output_layers[i](x)\n","    \n","        return output\n","    \n","    \n","    def training_step(self, batch, batch_nb):\n","        \n","        loss = (torch.tensor(0.0, device=\"cuda:0\", requires_grad=True) + \\\n","                torch.tensor(0.0, device=\"cuda:0\", requires_grad=True)) \n","        # araştırılabilir\n","        for i in range(self.num_tasks):\n","            x, y = batch[self.currencies[i] + \"_window\"], batch[self.currencies[i] + \"_label\"]\n","\n","            output = self.forward(x, i)\n","            #loss = F.nll_loss(output, y)\n","            loss += self.cross_entropy_loss[i](output, y)\n","            \n","            acc = self.accuracy_score(torch.max(output, dim=1)[1], y)\n","            self.log(self.currencies[i] +'_train_acc', acc, on_epoch=True, prog_bar=True)\n","\n","            f1 = self.f1_score(torch.max(output, dim=1)[1], y)\n","            self.log(self.currencies[i] +'_train_f1', f1, on_epoch=True, prog_bar=True)\n","        \n","        loss = loss / torch.tensor(self.num_tasks)\n","        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n","        \n","        return loss \n","    \n","    def validation_step(self, batch, batch_nb):\n","        loss = torch.tensor(0.0, device=\"cuda:0\") + torch.tensor(0.0, device=\"cuda:0\")\n","        \n","        for i in range(self.num_tasks):\n","            x, y = batch[self.currencies[i] + \"_window\"], batch[self.currencies[i] + \"_label\"]\n","\n","            output = self(x, i)\n","            #loss = F.nll_loss(output, y)\n","            loss += self.cross_entropy_loss[i](output, y)\n"," \n","            acc = self.accuracy_score(torch.max(output, dim=1)[1], y)\n","            self.log(self.currencies[i] +'_val_acc', acc, on_epoch=True, prog_bar=True, reduce_fx=torch.mean)\n","\n","            f1 = self.f1_score(torch.max(output, dim=1)[1], y)\n","            self.log(self.currencies[i] +'_val_f1', f1, on_epoch=True, prog_bar=True, reduce_fx=torch.mean)\n","        \n","        loss = loss / torch.tensor(self.num_tasks)\n","        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n","    \n","    def test_step(self, batch, batch_nb):\n","        loss = torch.tensor(0.0, device=\"cuda:0\") + torch.tensor(0.0, device=\"cuda:0\")\n","        \n","        for i in range(self.num_tasks):\n","            x, y = batch[ self.currencies[i] + \"_window\"], batch[self.currencies[i] + \"_label\"]\n","\n","            output = self(x, i)\n","#             print(y, torch.max(output, dim=1)[1])\n","#             print(F.softmax(output)) # mantıken fark etmiyor\n","            loss += self.cross_entropy_loss[i](output, y)\n","            \n","            acc = self.accuracy_score(torch.max(output, dim=1)[1], y)\n","            self.log(self.currencies[i] +'_test_acc', acc, on_epoch=True, reduce_fx=torch.mean)\n","\n","            f1 = self.f1_score(torch.max(output, dim=1)[1], y)\n","            self.log(self.currencies[i] +'_test_f1', f1, on_epoch=True, reduce_fx=torch.mean)\n","        \n","        loss = loss / torch.tensor(self.num_tasks)\n","        self.log('test_loss', loss, on_epoch=True, reduce_fx=torch.mean)\n","\n","        \n","    def configure_optimizers(self):\n","        \n","        optimizer = torch.optim.AdamW(model.parameters(), lr= self.learning_rate)#AdamW does weight decay\n","#         scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n","#                                                     step_size=self.scheduler_step, \n","#                                                     gamma=self.scheduler_gamma)\n","        \n","        self.lr_scheduler = CosineWarmupScheduler(optimizer, \n","                                                  warmup=50, \n","                                                  max_iters=150* self.train_dl.__len__())\n","        return [optimizer]#, [{\"scheduler\": scheduler}]\n","    \n","    def optimizer_step(self, *args, **kwargs):\n","        super().optimizer_step(*args, **kwargs)\n","        self.lr_scheduler.step() # Step per iteration\n","    \n","    def train_dataloader(self):\n","        return self.train_dl\n","\n","    def val_dataloader(self):\n","        return self.val_dl\n","\n","    def test_dataloader(self):\n","        return self.test_dl"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"MDSC6zPkWENQ","executionInfo":{"status":"ok","timestamp":1625563437483,"user_tz":-180,"elapsed":438,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}}},"source":["def name_model(config):\n","    task = \"multi_task_\" + \"_\".join(config[\"currency_list\"]) if len(config[\"currency_list\"]) > 1 else \"single_task_\" + config[\"currency_list\"][0]\n","    classification = \"multi_classification\" if config[\"n_classes\"] > 2 else \"binary_classification\"\n","    lstm = \"stack_lstm\" if len(config[\"lstm_hidden_sizes\"]) > 1 else \"single_lstm\"\n","    trend_removed = \"trend_removed\" if config[\"remove_trend\"] else \"\"\n","    loss_weighted = \"loss_weighted\" if config[\"loss_weight_calculate\"] else \"\"\n","    indicators  = \"indicators\" if config[\"indicators\"] else \"\"\n","    imfs = \"imfs\" if config[\"imfs\"] else \"\"\n","    return \"_\".join([task, lstm, loss_weighted, classification, trend_removed])"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"jU7lWxsYWENR","executionInfo":{"status":"ok","timestamp":1625563437857,"user_tz":-180,"elapsed":7,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}}},"source":["#deneme 1"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2wxKjKMrdXH","executionInfo":{"status":"ok","timestamp":1625563438719,"user_tz":-180,"elapsed":3,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}}},"source":[""],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JE51wOa3F1Si","executionInfo":{"status":"ok","timestamp":1625563439042,"user_tz":-180,"elapsed":14,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}},"outputId":"693dba4a-f606-4dfa-f913-38c052b671f5"},"source":["X.shape"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 1354, 1)"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"jXok5ufCWENS","executionInfo":{"status":"ok","timestamp":1625563439832,"user_tz":-180,"elapsed":7,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}}},"source":["CONFIG = {#fix for this project\n","          \"window_size\": 50, \n","          \"dataset_percentages\": [0.97, 0.007, 0.023],\n","          \"frenquency\": \"D\", \n","          \"neutral_quantile\": 0.33,\n","          \"batch_size\": 16,\n","          \"bidirectional\": False}"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"vvTEHVEcWENT","executionInfo":{"status":"ok","timestamp":1625563440236,"user_tz":-180,"elapsed":9,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}}},"source":["config = CONFIG.copy()\n","config.update({\"n_classes\": 2,\n","          \"currency_list\": ['BTC'],\n","          \"remove_trend\": True,\n","          \"lstm_hidden_sizes\": [128, 128, 128],\n","          \"loss_weight_calculate\": False, \n","          \"indicators\": True, \n","          \"imfs\": False})"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":466},"id":"CcE6NLzyWENT","executionInfo":{"status":"error","timestamp":1625563441760,"user_tz":-180,"elapsed":846,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}},"outputId":"03ca7bd5-f7d0-49e1-84ac-9d5eee73bb35"},"source":["MODEL_NAME = name_model(config)\n","\n","CURRENCY_LST = config[\"currency_list\"]\n","N_CLASSES = config[\"n_classes\"]\n","LSTM_HIDDEN_SIZES = config[\"lstm_hidden_sizes\"]\n","BIDIRECTIONAL = config[\"bidirectional\"]\n","REMOVE_TREND =config[\"remove_trend\"]\n","LOSS_WEIGHT_CALCULATE = config[\"loss_weight_calculate\"]\n","\n","TRAIN_PERCENTAGE, VAL_PERCENTAGE, TEST_PERCENTAGE = config[\"dataset_percentages\"] \n","WINDOW_SIZE = config[\"window_size\"]\n","FREQUENCY = config[\"frenquency\"]\n","NEUTRAL_QUANTILE = config[\"neutral_quantile\"] if N_CLASSES > 2 else 0 \n","BATCH_SIZE= config[\"batch_size\"]\n","INDICATORS = config[\"indicators\"]\n","IMFS = config[\"imfs\"]\n","#####\n","\n","X, y, features, dfs = get_data(CURRENCY_LST,\n","                            N_CLASSES,\n","                             FREQUENCY, \n","                             WINDOW_SIZE,\n","                             neutral_quantile = NEUTRAL_QUANTILE,\n","                             log_price=True,\n","                             remove_trend=REMOVE_TREND,\n","                             include_indicators = INDICATORS,\n","                             include_imfs = IMFS\n","                            )\n","INPUT_FEATURE_SIZE = X.shape[-1]\n","\n","train_dataset, val_dataset, test_dataset = [TimeSeriesDataset(CURRENCY_LST, \n","                                                          X, \n","                                                          y, \n","                                                          dtype, \n","                                                          TRAIN_PERCENTAGE, \n","                                                          VAL_PERCENTAGE, \n","                                                          TEST_PERCENTAGE, \n","                                                          WINDOW_SIZE) for dtype in ['train', 'val', 'test']]\n","\n","config[\"dataset_sizes\"] = [len(train_dataset), len(val_dataset), len(test_dataset)]\n","####\n","wandb.init(project=\"deneme\",\n","           config=config,\n","           name = MODEL_NAME)\n","logger = WandbLogger()\n","#     #logger = TensorBoardLogger(\"../output/models/lstm_model_logs\", name=\"lstm_multi_task\")\n","\n","model = LSTM_based_classification_model(\n","    train_dataset = train_dataset,\n","     val_dataset = val_dataset,\n","     test_dataset = test_dataset,\n","     calculate_loss_weights = LOSS_WEIGHT_CALCULATE,\n","     currencies = CURRENCY_LST,\n","     num_classes = N_CLASSES,\n","     window_size = WINDOW_SIZE,\n","     input_size = INPUT_FEATURE_SIZE,\n","     batch_size=BATCH_SIZE,\n","     lstm_hidden_sizes = LSTM_HIDDEN_SIZES,\n","     bidirectional = BIDIRECTIONAL)\n","\n","early_stop_callback = EarlyStopping(\n","   monitor='val_loss',\n","   min_delta=0.003,\n","   patience=20,\n","   verbose=True,\n","   mode='min'\n",")\n","\n","trainer = pl.Trainer(gpus=-1, \n","                     max_epochs= 150,\n","                     logger = logger, \n","                     callbacks=[early_stop_callback])\n","trainer.fit(model)\n","\n","trainer.test()\n","wandb.finish()"],"execution_count":34,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n","  dip[i] = 100 * (self._dip[i] / self._trs[i])\n","/usr/local/lib/python3.7/dist-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n","  din[i] = 100 * (self._din[i] / self._trs[i])\n"],"name":"stderr"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-6b05fe6783c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m                                                           \u001b[0mVAL_PERCENTAGE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                                                           \u001b[0mTEST_PERCENTAGE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                                                           WINDOW_SIZE) for dtype in ['train', 'val', 'test']]\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dataset_sizes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-34-6b05fe6783c6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     36\u001b[0m                                                           \u001b[0mVAL_PERCENTAGE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                                                           \u001b[0mTEST_PERCENTAGE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                                                           WINDOW_SIZE) for dtype in ['train', 'val', 'test']]\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dataset_sizes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-0307795998bf>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, currency_list, x, y, data_use_type, train_percentage, val_percentage, test_percentage, seq_len)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrency_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_currencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_currencies\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_currencies\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."]}]},{"cell_type":"code","metadata":{"id":"0qUKMfNEWENW"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WwIKmFnNWENW"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9BiNc_hcWENW"},"source":["#denemeleri tamamlandı experiment_lstm.py dosyasına yazıldı işlemler\n","def experiment(script):\n","    !python ../pipelines/multi_task_price_change_prediction/experiment_lstm.py $script"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TSJlHMoWWENX"},"source":["CONFIG = {#fix for this project\n","          \"window_size\": 50, \n","          \"dataset_percentages\": [0.97, 0.007, 0.023],\n","          \"frenquency\": \"D\", \n","          \"neutral_quantile\": 0.33,\n","          \"batch_size\": 16,\n","          \"bidirectional\": False}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xJoL5IXsWENX"},"source":["from sklearn.model_selection import ParameterGrid\n","param_grid = {\"n_classes\": [2,3],\n","          \"currency_list\": [['BTC'], ['ETH'], ['LTC'], ['BTC', 'ETH'],  ['BTC', 'ETH', 'LTC']],\n","          \"remove_trend\": [True, False],\n","          \"lstm_hidden_sizes\": [[128], [128, 128, 128]],\n","          \"loss_weight_calculate\": [True, False],\n","          \"indicators\": [True],\n","          \"imfs\": [False]}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"SrJ_AVsyWENX","outputId":"b17f073e-3d19-4f13-e13b-06cea3d980e4"},"source":["for c in (ParameterGrid(param_grid)):\n","    config = CONFIG.copy()\n","    config.update(c)\n","    script = \"--currency-list \" + \" \".join([i for i in c[\"currency_list\"]])\n","    script += \" --lstm-list \" + \" \".join([str(i) for i in c[\"lstm_hidden_sizes\"]])\n","    script += \" -trend \" + str(1 if c[\"remove_trend\"] else 0)\n","    script += \" -indicators \" + str(1 if c[\"indicators\"] else 0)\n","    script += \" -imfs \" + str(1 if c[\"imfs\"] else 0)\n","    script += \" -classes \" + str(c[\"n_classes\"] )\n","    script += \" -weight \" + str(1 if c[\"loss_weight_calculate\"] else 0) \n","\n","    experiment(script)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 00:55:10.435653: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_single_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1eprgmz4\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005508-1eprgmz4\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 130   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.7 K    Trainable params\n","0         Non-trainable params\n","75.7 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 79/80 [00:01<00:00, 64.35it/s, loss=0.583, v_num=gmz4, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.522\n","Epoch 0: 100%|█| 80/80 [00:01<00:00, 58.94it/s, loss=0.583, v_num=gmz4, BTC_val_\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 49.57it/s, loss=0.574, v_num=gmz4, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.518\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 49.31it/s, loss=0.574, v_num=gmz4, BTC_val_\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 66.29it/s, loss=0.597, v_num=gmz4, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 65.69it/s, loss=0.597, v_num=gmz4, BTC_val_\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:01<00:00, 57.03it/s, loss=0.605, v_num=gmz4, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:01<00:00, 56.59it/s, loss=0.605, v_num=gmz4, BTC_val_\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 65.17it/s, loss=0.568, v_num=gmz4, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 64.54it/s, loss=0.568, v_num=gmz4, BTC_val_\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:01<00:00, 65.26it/s, loss=0.589, v_num=gmz4, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.513\n","Epoch 5: 100%|█| 80/80 [00:01<00:00, 64.73it/s, loss=0.589, v_num=gmz4, BTC_val_\n","Epoch 6: 100%|█| 80/80 [00:01<00:00, 59.09it/s, loss=0.602, v_num=gmz4, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:01<00:00, 58.63it/s, loss=0.602, v_num=gmz4, BTC_val_\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:01<00:00, 61.36it/s, loss=0.577, v_num=gmz4, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:01<00:00, 60.86it/s, loss=0.577, v_num=gmz4, BTC_val_\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:01<00:00, 64.61it/s, loss=0.563, v_num=gmz4, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:01<00:00, 64.11it/s, loss=0.563, v_num=gmz4, BTC_val_\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:01<00:00, 60.67it/s, loss=0.6, v_num=gmz4, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:01<00:00, 60.18it/s, loss=0.6, v_num=gmz4, BTC_val_ac\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:01<00:00, 62.58it/s, loss=0.532, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.500\n","Epoch 10: 100%|█| 80/80 [00:01<00:00, 62.18it/s, loss=0.532, v_num=gmz4, BTC_val\n","Epoch 11: 100%|█| 80/80 [00:01<00:00, 57.30it/s, loss=0.525, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:01<00:00, 56.74it/s, loss=0.525, v_num=gmz4, BTC_val\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:01<00:00, 56.69it/s, loss=0.579, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:01<00:00, 56.33it/s, loss=0.579, v_num=gmz4, BTC_val\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:01<00:00, 64.55it/s, loss=0.539, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:01<00:00, 64.01it/s, loss=0.539, v_num=gmz4, BTC_val\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:01<00:00, 63.39it/s, loss=0.587, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:01<00:00, 62.93it/s, loss=0.587, v_num=gmz4, BTC_val\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:01<00:00, 63.79it/s, loss=0.586, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:01<00:00, 63.09it/s, loss=0.586, v_num=gmz4, BTC_val\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:01<00:00, 61.62it/s, loss=0.562, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:01<00:00, 61.07it/s, loss=0.562, v_num=gmz4, BTC_val\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:01<00:00, 64.26it/s, loss=0.577, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:01<00:00, 63.81it/s, loss=0.577, v_num=gmz4, BTC_val\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:01<00:00, 63.12it/s, loss=0.564, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:01<00:00, 62.59it/s, loss=0.564, v_num=gmz4, BTC_val\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:01<00:00, 64.54it/s, loss=0.601, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:01<00:00, 64.00it/s, loss=0.601, v_num=gmz4, BTC_val\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:01<00:00, 65.16it/s, loss=0.574, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:01<00:00, 64.73it/s, loss=0.574, v_num=gmz4, BTC_val\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:01<00:00, 63.60it/s, loss=0.585, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:01<00:00, 63.17it/s, loss=0.585, v_num=gmz4, BTC_val\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:01<00:00, 65.31it/s, loss=0.581, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:01<00:00, 64.62it/s, loss=0.581, v_num=gmz4, BTC_val\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:01<00:00, 64.89it/s, loss=0.581, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:01<00:00, 64.28it/s, loss=0.581, v_num=gmz4, BTC_val\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:01<00:00, 64.74it/s, loss=0.605, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:01<00:00, 64.08it/s, loss=0.605, v_num=gmz4, BTC_val\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:01<00:00, 64.34it/s, loss=0.603, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:01<00:00, 63.76it/s, loss=0.603, v_num=gmz4, BTC_val\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:01<00:00, 68.81it/s, loss=0.54, v_num=gmz4, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:01<00:00, 68.37it/s, loss=0.54, v_num=gmz4, BTC_val_\u001b[A\n","Epoch 27: 100%|█| 80/80 [00:01<00:00, 72.99it/s, loss=0.579, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 80/80 [00:01<00:00, 71.80it/s, loss=0.579, v_num=gmz4, BTC_val\u001b[A\n","Epoch 28: 100%|█| 80/80 [00:01<00:00, 62.39it/s, loss=0.545, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 80/80 [00:01<00:00, 62.00it/s, loss=0.545, v_num=gmz4, BTC_val\u001b[A\n","Epoch 29: 100%|█| 80/80 [00:01<00:00, 58.77it/s, loss=0.565, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 80/80 [00:01<00:00, 58.22it/s, loss=0.565, v_num=gmz4, BTC_val\u001b[A\n","Epoch 30: 100%|█| 80/80 [00:01<00:00, 64.64it/s, loss=0.574, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.496\n","Epoch 30: 100%|█| 80/80 [00:01<00:00, 64.20it/s, loss=0.574, v_num=gmz4, BTC_val\n","Epoch 31: 100%|█| 80/80 [00:01<00:00, 63.25it/s, loss=0.57, v_num=gmz4, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 80/80 [00:01<00:00, 62.62it/s, loss=0.57, v_num=gmz4, BTC_val_\u001b[A\n","Epoch 32: 100%|█| 80/80 [00:01<00:00, 59.01it/s, loss=0.561, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 80/80 [00:01<00:00, 58.59it/s, loss=0.561, v_num=gmz4, BTC_val\u001b[A\n","Epoch 33: 100%|█| 80/80 [00:01<00:00, 64.66it/s, loss=0.581, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 33: 100%|█| 80/80 [00:01<00:00, 64.15it/s, loss=0.581, v_num=gmz4, BTC_val\u001b[A\n","Epoch 34: 100%|█| 80/80 [00:01<00:00, 71.50it/s, loss=0.602, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 80/80 [00:01<00:00, 70.95it/s, loss=0.602, v_num=gmz4, BTC_val\u001b[A\n","Epoch 35: 100%|█| 80/80 [00:01<00:00, 64.59it/s, loss=0.572, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 35: 100%|█| 80/80 [00:01<00:00, 64.03it/s, loss=0.572, v_num=gmz4, BTC_val\u001b[A\n","Epoch 36: 100%|█| 80/80 [00:01<00:00, 65.71it/s, loss=0.521, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 36: 100%|█| 80/80 [00:01<00:00, 65.27it/s, loss=0.521, v_num=gmz4, BTC_val\u001b[A\n","Epoch 37: 100%|█| 80/80 [00:01<00:00, 63.20it/s, loss=0.536, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 37: 100%|█| 80/80 [00:01<00:00, 62.69it/s, loss=0.536, v_num=gmz4, BTC_val\u001b[A\n","Epoch 38: 100%|█| 80/80 [00:01<00:00, 70.88it/s, loss=0.548, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 38: 100%|█| 80/80 [00:01<00:00, 70.37it/s, loss=0.548, v_num=gmz4, BTC_val\u001b[A\n","Epoch 39: 100%|█| 80/80 [00:01<00:00, 64.81it/s, loss=0.567, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 39: 100%|█| 80/80 [00:01<00:00, 64.23it/s, loss=0.567, v_num=gmz4, BTC_val\u001b[A\n","Epoch 40: 100%|█| 80/80 [00:01<00:00, 60.07it/s, loss=0.604, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 40: 100%|█| 80/80 [00:01<00:00, 59.62it/s, loss=0.604, v_num=gmz4, BTC_val\u001b[A\n","Epoch 41: 100%|█| 80/80 [00:01<00:00, 62.65it/s, loss=0.573, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 41: 100%|█| 80/80 [00:01<00:00, 62.08it/s, loss=0.573, v_num=gmz4, BTC_val\u001b[A\n","Epoch 42: 100%|█| 80/80 [00:01<00:00, 63.66it/s, loss=0.531, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 42: 100%|█| 80/80 [00:01<00:00, 63.21it/s, loss=0.531, v_num=gmz4, BTC_val\u001b[A\n","Epoch 43: 100%|█| 80/80 [00:01<00:00, 64.76it/s, loss=0.569, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 43: 100%|█| 80/80 [00:01<00:00, 64.23it/s, loss=0.569, v_num=gmz4, BTC_val\u001b[A\n","Epoch 44: 100%|█| 80/80 [00:01<00:00, 62.39it/s, loss=0.549, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 44: 100%|█| 80/80 [00:01<00:00, 61.78it/s, loss=0.549, v_num=gmz4, BTC_val\u001b[A\n","Epoch 45: 100%|█| 80/80 [00:01<00:00, 71.66it/s, loss=0.589, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 45: 100%|█| 80/80 [00:01<00:00, 71.01it/s, loss=0.589, v_num=gmz4, BTC_val\u001b[A\n","Epoch 46: 100%|█| 80/80 [00:01<00:00, 71.16it/s, loss=0.517, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 46: 100%|█| 80/80 [00:01<00:00, 70.62it/s, loss=0.517, v_num=gmz4, BTC_val\u001b[A\n","Epoch 47: 100%|█| 80/80 [00:01<00:00, 70.49it/s, loss=0.569, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 47: 100%|█| 80/80 [00:01<00:00, 69.78it/s, loss=0.569, v_num=gmz4, BTC_val\u001b[A\n","Epoch 48: 100%|█| 80/80 [00:01<00:00, 70.31it/s, loss=0.568, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 48: 100%|█| 80/80 [00:01<00:00, 69.72it/s, loss=0.568, v_num=gmz4, BTC_val\u001b[A\n","Epoch 49: 100%|█| 80/80 [00:01<00:00, 68.86it/s, loss=0.586, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 49: 100%|█| 80/80 [00:01<00:00, 68.01it/s, loss=0.586, v_num=gmz4, BTC_val\u001b[A\n","Epoch 50: 100%|█| 80/80 [00:01<00:00, 64.09it/s, loss=0.575, v_num=gmz4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.496. Signaling Trainer to stop.\n","Epoch 50: 100%|█| 80/80 [00:01<00:00, 63.63it/s, loss=0.575, v_num=gmz4, BTC_val\n","Epoch 50: 100%|█| 80/80 [00:01<00:00, 63.47it/s, loss=0.575, v_num=gmz4, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 88.43it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.7096773982048035,\n"," 'BTC_test_f1': 0.7047099471092224,\n"," 'test_loss': 0.6141742467880249}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 62830\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005508-1eprgmz4/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005508-1eprgmz4/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.74603\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.42823\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 50\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4029\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 73\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621461382\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 182\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.72922\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.7152\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.55561\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.77778\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.775\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.5299\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.70968\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.70471\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.61417\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▅▄▅▇▇▄▅▄▅▅▆▅▅▆▆▆▄▇▄▅▆▇▇▅▁▅▅▆▄▄▆▄▅▅▅▅▆█▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▅▄▅▇▇▄▅▄▅▅▆▅▅▆▆▆▄▇▄▅▆▇▇▅▁▅▅▆▄▄▆▄▄▄▅▄▆█▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▄▆▃▂▂▆▄▆▄▄▂▃▅▃▃▃▄▃▅▄▃▃▃▃█▃▄▄▅▅▃▅▄▃▄▄▂▁▃▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▆▆▆▆▆▇▇▇▆▇▇▇▇▇▇▇█▇▇▇▇▇▇▇███▇█▇██▇██████\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▆▆▅▆▆▆▇▇▆▇▇▇▆▇▇▇█▇▇▇▇▇▇▇██▇▇█▇██▇██████\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▃▂▂▂▃▃▂▂▂▂▂▂▂▂▁▂▂▂▂▁▂▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁███████████████████████▁███████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁███████████████████████▁███████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄▃▆▇▃█▃▅▁▃▄▃▃▃▄▅▄▃▂▂█▄▁▆▁▁▂▅▂▄▅▄▅▂▅▃▄▅▃▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_single_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1eprgmz4\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 00:56:32.154796: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_single_lstm_loss_weighted_binary_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/34r9vs22\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005630-34r9vs22\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 130   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.7 K    Trainable params\n","0         Non-trainable params\n","75.7 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 80/81 [00:01<00:00, 60.08it/s, loss=0.694, v_num=vs22, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.695\n","Epoch 0: 100%|█| 81/81 [00:01<00:00, 58.93it/s, loss=0.694, v_num=vs22, BTC_val_\n","Epoch 1:  99%|▉| 80/81 [00:01<00:00, 62.76it/s, loss=0.704, v_num=vs22, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 81/81 [00:01<00:00, 61.48it/s, loss=0.704, v_num=vs22, BTC_val_\u001b[A\n","Epoch 2:  99%|▉| 80/81 [00:01<00:00, 52.67it/s, loss=0.695, v_num=vs22, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 81/81 [00:01<00:00, 51.93it/s, loss=0.695, v_num=vs22, BTC_val_\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:01<00:00, 49.11it/s, loss=0.711, v_num=vs22, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:01<00:00, 48.69it/s, loss=0.711, v_num=vs22, BTC_val_\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:01<00:00, 47.58it/s, loss=0.692, v_num=vs22, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:01<00:00, 47.02it/s, loss=0.692, v_num=vs22, BTC_val_\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:02<00:00, 39.14it/s, loss=0.692, v_num=vs22, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:02<00:00, 38.99it/s, loss=0.692, v_num=vs22, BTC_val_\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.691\n","Epoch 6:  99%|▉| 80/81 [00:01<00:00, 53.35it/s, loss=0.693, v_num=vs22, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:01<00:00, 52.42it/s, loss=0.693, v_num=vs22, BTC_val_\u001b[A\n","Epoch 7:  99%|▉| 80/81 [00:01<00:00, 67.74it/s, loss=0.691, v_num=vs22, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:01<00:00, 66.59it/s, loss=0.691, v_num=vs22, BTC_val_\u001b[A\n","Epoch 8:  99%|▉| 80/81 [00:01<00:00, 73.49it/s, loss=0.699, v_num=vs22, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:01<00:00, 71.75it/s, loss=0.699, v_num=vs22, BTC_val_\u001b[A\n","Epoch 9:  99%|▉| 80/81 [00:01<00:00, 57.54it/s, loss=0.696, v_num=vs22, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:01<00:00, 56.77it/s, loss=0.696, v_num=vs22, BTC_val_\u001b[A\n","Epoch 10:  99%|▉| 80/81 [00:01<00:00, 70.71it/s, loss=0.693, v_num=vs22, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:01<00:00, 69.31it/s, loss=0.693, v_num=vs22, BTC_val\u001b[A\n","Epoch 11:  99%|▉| 80/81 [00:00<00:00, 85.69it/s, loss=0.692, v_num=vs22, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:00<00:00, 83.33it/s, loss=0.692, v_num=vs22, BTC_val\u001b[A\n","Epoch 12:  99%|▉| 80/81 [00:00<00:00, 89.13it/s, loss=0.697, v_num=vs22, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:00<00:00, 86.81it/s, loss=0.697, v_num=vs22, BTC_val\u001b[A\n","Epoch 13:  99%|▉| 80/81 [00:00<00:00, 80.54it/s, loss=0.695, v_num=vs22, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:01<00:00, 77.84it/s, loss=0.695, v_num=vs22, BTC_val\u001b[A\n","Epoch 14:  99%|▉| 80/81 [00:00<00:00, 89.85it/s, loss=0.695, v_num=vs22, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:00<00:00, 86.24it/s, loss=0.695, v_num=vs22, BTC_val\u001b[A\n","Epoch 15:  99%|▉| 80/81 [00:01<00:00, 56.41it/s, loss=0.694, v_num=vs22, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:01<00:00, 53.26it/s, loss=0.694, v_num=vs22, BTC_val\u001b[A\n","Epoch 16:  99%|▉| 80/81 [00:01<00:00, 77.37it/s, loss=0.691, v_num=vs22, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:01<00:00, 75.83it/s, loss=0.691, v_num=vs22, BTC_val\u001b[A\n","Epoch 17:  99%|▉| 80/81 [00:00<00:00, 84.71it/s, loss=0.698, v_num=vs22, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:00<00:00, 82.69it/s, loss=0.698, v_num=vs22, BTC_val\u001b[A\n","Epoch 18:  99%|▉| 80/81 [00:00<00:00, 85.99it/s, loss=0.7, v_num=vs22, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:00<00:00, 83.07it/s, loss=0.7, v_num=vs22, BTC_val_a\u001b[A\n","Epoch 19:  99%|▉| 80/81 [00:01<00:00, 76.43it/s, loss=0.697, v_num=vs22, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:01<00:00, 74.96it/s, loss=0.697, v_num=vs22, BTC_val\u001b[A\n","Epoch 20:  99%|▉| 80/81 [00:01<00:00, 78.43it/s, loss=0.694, v_num=vs22, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:01<00:00, 76.52it/s, loss=0.694, v_num=vs22, BTC_val\u001b[A\n","Epoch 21:  99%|▉| 80/81 [00:01<00:00, 79.96it/s, loss=0.695, v_num=vs22, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 81/81 [00:01<00:00, 77.81it/s, loss=0.695, v_num=vs22, BTC_val\u001b[A\n","Epoch 22:  99%|▉| 80/81 [00:01<00:00, 76.90it/s, loss=0.694, v_num=vs22, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 81/81 [00:01<00:00, 75.19it/s, loss=0.694, v_num=vs22, BTC_val\u001b[A\n","Epoch 23:  99%|▉| 80/81 [00:00<00:00, 84.77it/s, loss=0.694, v_num=vs22, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 81/81 [00:00<00:00, 82.72it/s, loss=0.694, v_num=vs22, BTC_val\u001b[A\n","Epoch 24:  99%|▉| 80/81 [00:00<00:00, 85.56it/s, loss=0.696, v_num=vs22, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 81/81 [00:00<00:00, 83.26it/s, loss=0.696, v_num=vs22, BTC_val\u001b[A\n","Epoch 25:  99%|▉| 80/81 [00:01<00:00, 79.30it/s, loss=0.692, v_num=vs22, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.691. Signaling Trainer to stop.\n","Epoch 25: 100%|█| 81/81 [00:01<00:00, 77.14it/s, loss=0.692, v_num=vs22, BTC_val\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 25: 100%|█| 81/81 [00:01<00:00, 76.89it/s, loss=0.692, v_num=vs22, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 98.63it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.5806451439857483,\n"," 'BTC_test_f1': 0.36725807189941406,\n"," 'test_loss': 0.6884135603904724}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 63051\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005630-34r9vs22/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005630-34r9vs22/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.69882\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 25\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2080\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 41\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621461431\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 93\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.50827\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.4836\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69266\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.66667\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.58462\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.69276\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.58065\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.36726\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.68841\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▆▄▅▆▃▇▅█▁▃▃▅▃▅▅▃▆▂▄▅▄▃▃▅▅▄▄▃▇▆▆▅▄▃▆▄▄▃▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▆▅▆▇▄▇▅▄▁▄▄▆▃▆▆▂▃▂▅▆▄▃▂▆▅▅▄▃█▆▇▃▂▄▇▅▅▂▆▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▃▅▅▄▇▂▅▁█▇▆▅▇▆▆▆▅█▆▆▆▆▆▇▆▇▆▆▅▅▅▄▆▆▆▇▆▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▄▅▃▄▃▁▄▄▄▁▄▁▃▅▂▁█▅▂▂▅▂▅▄▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▅▃▄▃▃▁▅▅▃▃▃▂▄▃▃▁█▅▃▂▄▃▅▅▃▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ▇█▅▄▄▄▂▂▁▃▂▂▂▂▂▂▁▂▂▂▂▂▁▁▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▅▁▁▁▅▁▁▁▅▅▅▅▅▅▅▅▅▁█▁▁▅█▅▁█\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▂▁▁▁▂▁▁▁▂▂▆▆▂▂▂▂▂▁█▁▁▂█▆▁█\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄█▄▃█▁▂▂▃▃▃▂▅▃▃▃▂▂▂▂▂▃▂▂▂▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_single_lstm_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/34r9vs22\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 00:57:21.529125: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_single_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2ae5c60y\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005720-2ae5c60y\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 195   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.8 K    Trainable params\n","0         Non-trainable params\n","75.8 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 79/80 [00:00<00:00, 84.83it/s, loss=1.11, v_num=c60y, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.077\n","Epoch 0: 100%|█| 80/80 [00:01<00:00, 76.98it/s, loss=1.11, v_num=c60y, BTC_val_a\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 73.57it/s, loss=1.1, v_num=c60y, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 72.73it/s, loss=1.1, v_num=c60y, BTC_val_ac\u001b[A\n","Epoch 2: 100%|█| 80/80 [00:00<00:00, 84.88it/s, loss=1.11, v_num=c60y, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 1.073\n","Epoch 2: 100%|█| 80/80 [00:00<00:00, 84.12it/s, loss=1.11, v_num=c60y, BTC_val_a\n","Epoch 3: 100%|█| 80/80 [00:00<00:00, 82.21it/s, loss=1.07, v_num=c60y, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:00<00:00, 81.11it/s, loss=1.07, v_num=c60y, BTC_val_a\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 65.46it/s, loss=1.02, v_num=c60y, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 1.062\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 64.98it/s, loss=1.02, v_num=c60y, BTC_val_a\n","Epoch 5: 100%|█| 80/80 [00:01<00:00, 78.39it/s, loss=1.02, v_num=c60y, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:01<00:00, 77.46it/s, loss=1.02, v_num=c60y, BTC_val_a\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:01<00:00, 78.44it/s, loss=1.02, v_num=c60y, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:01<00:00, 77.54it/s, loss=1.02, v_num=c60y, BTC_val_a\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:00<00:00, 86.44it/s, loss=1.02, v_num=c60y, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:00<00:00, 85.69it/s, loss=1.02, v_num=c60y, BTC_val_a\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:00<00:00, 89.43it/s, loss=0.998, v_num=c60y, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:00<00:00, 88.60it/s, loss=0.998, v_num=c60y, BTC_val_\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:00<00:00, 85.94it/s, loss=1.02, v_num=c60y, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:00<00:00, 85.20it/s, loss=1.02, v_num=c60y, BTC_val_a\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:00<00:00, 85.53it/s, loss=0.988, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:00<00:00, 84.37it/s, loss=0.988, v_num=c60y, BTC_val\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:01<00:00, 76.37it/s, loss=1.01, v_num=c60y, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:01<00:00, 75.71it/s, loss=1.01, v_num=c60y, BTC_val_\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:01<00:00, 75.98it/s, loss=0.997, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:01<00:00, 75.34it/s, loss=0.997, v_num=c60y, BTC_val\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:00<00:00, 83.49it/s, loss=0.999, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:00<00:00, 82.76it/s, loss=0.999, v_num=c60y, BTC_val\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:00<00:00, 87.50it/s, loss=0.966, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:00<00:00, 86.81it/s, loss=0.966, v_num=c60y, BTC_val\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:01<00:00, 75.64it/s, loss=0.957, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:01<00:00, 74.97it/s, loss=0.957, v_num=c60y, BTC_val\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:01<00:00, 78.53it/s, loss=0.976, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:01<00:00, 77.90it/s, loss=0.976, v_num=c60y, BTC_val\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:00<00:00, 83.19it/s, loss=0.999, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:00<00:00, 82.43it/s, loss=0.999, v_num=c60y, BTC_val\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:00<00:00, 87.44it/s, loss=0.957, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:00<00:00, 86.64it/s, loss=0.957, v_num=c60y, BTC_val\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:00<00:00, 90.99it/s, loss=0.956, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:00<00:00, 90.09it/s, loss=0.956, v_num=c60y, BTC_val\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:00<00:00, 89.70it/s, loss=0.961, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:00<00:00, 88.84it/s, loss=0.961, v_num=c60y, BTC_val\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:00<00:00, 88.79it/s, loss=0.969, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:00<00:00, 88.03it/s, loss=0.969, v_num=c60y, BTC_val\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:00<00:00, 86.23it/s, loss=0.974, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:00<00:00, 85.33it/s, loss=0.974, v_num=c60y, BTC_val\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.037 >= min_delta = 0.003. New best score: 1.025\n","Epoch 23: 100%|█| 80/80 [00:00<00:00, 87.11it/s, loss=0.941, v_num=c60y, BTC_val\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:00<00:00, 86.12it/s, loss=0.941, v_num=c60y, BTC_val\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:01<00:00, 77.40it/s, loss=0.944, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:01<00:00, 76.62it/s, loss=0.944, v_num=c60y, BTC_val\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:00<00:00, 84.82it/s, loss=0.977, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:00<00:00, 84.09it/s, loss=0.977, v_num=c60y, BTC_val\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:00<00:00, 87.49it/s, loss=0.955, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:00<00:00, 86.62it/s, loss=0.955, v_num=c60y, BTC_val\u001b[A\n","Epoch 27: 100%|█| 80/80 [00:01<00:00, 77.31it/s, loss=0.964, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 80/80 [00:01<00:00, 76.31it/s, loss=0.964, v_num=c60y, BTC_val\u001b[A\n","Epoch 28: 100%|█| 80/80 [00:01<00:00, 76.52it/s, loss=0.922, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 80/80 [00:01<00:00, 75.82it/s, loss=0.922, v_num=c60y, BTC_val\u001b[A\n","Epoch 29: 100%|█| 80/80 [00:01<00:00, 78.04it/s, loss=0.91, v_num=c60y, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 80/80 [00:01<00:00, 77.30it/s, loss=0.91, v_num=c60y, BTC_val_\u001b[A\n","Epoch 30: 100%|█| 80/80 [00:00<00:00, 86.47it/s, loss=0.936, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 80/80 [00:00<00:00, 85.67it/s, loss=0.936, v_num=c60y, BTC_val\u001b[A\n","Epoch 31: 100%|█| 80/80 [00:00<00:00, 88.89it/s, loss=0.893, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 80/80 [00:00<00:00, 88.11it/s, loss=0.893, v_num=c60y, BTC_val\u001b[A\n","Epoch 32: 100%|█| 80/80 [00:00<00:00, 87.91it/s, loss=0.976, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 80/80 [00:00<00:00, 87.08it/s, loss=0.976, v_num=c60y, BTC_val\u001b[A\n","Epoch 33: 100%|█| 80/80 [00:00<00:00, 90.09it/s, loss=0.937, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 33: 100%|█| 80/80 [00:00<00:00, 89.03it/s, loss=0.937, v_num=c60y, BTC_val\u001b[A\n","Epoch 34: 100%|█| 80/80 [00:00<00:00, 87.40it/s, loss=0.882, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 80/80 [00:00<00:00, 86.62it/s, loss=0.882, v_num=c60y, BTC_val\u001b[A\n","Epoch 35: 100%|█| 80/80 [00:00<00:00, 89.72it/s, loss=0.906, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 35: 100%|█| 80/80 [00:00<00:00, 88.83it/s, loss=0.906, v_num=c60y, BTC_val\u001b[A\n","Epoch 36: 100%|█| 80/80 [00:00<00:00, 89.64it/s, loss=0.982, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 36: 100%|█| 80/80 [00:00<00:00, 88.76it/s, loss=0.982, v_num=c60y, BTC_val\u001b[A\n","Epoch 37: 100%|█| 80/80 [00:00<00:00, 88.28it/s, loss=0.97, v_num=c60y, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 37: 100%|█| 80/80 [00:00<00:00, 87.48it/s, loss=0.97, v_num=c60y, BTC_val_\u001b[A\n","Epoch 38: 100%|█| 80/80 [00:00<00:00, 88.19it/s, loss=0.943, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 38: 100%|█| 80/80 [00:00<00:00, 87.35it/s, loss=0.943, v_num=c60y, BTC_val\u001b[A\n","Epoch 39: 100%|█| 80/80 [00:01<00:00, 79.93it/s, loss=0.928, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 39: 100%|█| 80/80 [00:01<00:00, 79.22it/s, loss=0.928, v_num=c60y, BTC_val\u001b[A\n","Epoch 40: 100%|█| 80/80 [00:01<00:00, 73.34it/s, loss=0.913, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 40: 100%|█| 80/80 [00:01<00:00, 72.77it/s, loss=0.913, v_num=c60y, BTC_val\u001b[A\n","Epoch 41: 100%|█| 80/80 [00:00<00:00, 82.23it/s, loss=0.975, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 41: 100%|█| 80/80 [00:00<00:00, 81.45it/s, loss=0.975, v_num=c60y, BTC_val\u001b[A\n","Epoch 42: 100%|█| 80/80 [00:00<00:00, 87.90it/s, loss=0.917, v_num=c60y, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.025. Signaling Trainer to stop.\n","Epoch 42: 100%|█| 80/80 [00:00<00:00, 86.96it/s, loss=0.917, v_num=c60y, BTC_val\n","Epoch 42: 100%|█| 80/80 [00:00<00:00, 86.58it/s, loss=0.917, v_num=c60y, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n"],"name":"stdout"},{"output_type":"stream","text":["Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 118.86it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.4516128897666931,\n"," 'BTC_test_f1': 0.40499287843704224,\n"," 'test_loss': 0.9798224568367004}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 63252\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005720-2ae5c60y/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005720-2ae5c60y/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.61355\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.70869\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 42\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3397\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 50\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621461490\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 153\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.49406\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.47716\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.93209\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.33333\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.33968\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.2079\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.45161\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.40499\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.97982\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▂▃▂▂▁▄▄▃▃█▃▄▃▄▃▃▂▃▅▄▄▅▄▃▃▅▃▃▆▃▂▅▄▅▃▄▃▃▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▂▂▂▁▄▃▃▂█▄▄▃▄▃▃▂▃▆▄▄▅▃▃▃▅▄▃▆▃▂▅▅▆▃▄▃▄▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▅▆▆█▄▄▃▅▁▅▄▅▂▅▄▄▃▂▃▂▂▃▃▄▂▅█▅▃▇▃▃▁▇▂▃▃▄▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▁▃▃▃▅▅▅▆▆▇▆▆▆▇▇▆▆▇████▇▇▆▇▇▇█▇██▇███▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▁▂▃▄▅▅▅▆▆▇▆▆▇▆▇▇▆▇██▇█▆▇▆▇▇▇▇▇▇█▇▇█▇▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ██▇▇▅▅▄▄▃▄▃▄▄▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc █▁▆▃▆▆▃▁▃▃▆▃▃▃█▃▃▃▆█▆█▃▃▆▆▃▆█▆▃▃▃▆▃▁▆▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▅▁▃▄▆▆▃▂▃▄▆▃▃▄█▄▄▃▆█▆█▄▄▆▆▄▆█▆▄▄▄▅▄▂▆▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▃▅▃▃▂▃▄▅▄▄▃▆▇▆▅▅▆▆▄▄▄▁▄▄▃▅▆▂▃▅█▇▄▅▇▇▅▅▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_single_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2ae5c60y\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 00:58:21.463723: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_single_lstm_loss_weighted_multi_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3u51l2tj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005820-3u51l2tj\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 195   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.8 K    Trainable params\n","0         Non-trainable params\n","75.8 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 80/81 [00:00<00:00, 91.74it/s, loss=1.12, v_num=l2tj, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.172\n","Epoch 0: 100%|█| 81/81 [00:00<00:00, 88.76it/s, loss=1.12, v_num=l2tj, BTC_val_a\n","Epoch 1:  99%|▉| 80/81 [00:00<00:00, 80.17it/s, loss=1.1, v_num=l2tj, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 81/81 [00:01<00:00, 77.90it/s, loss=1.1, v_num=l2tj, BTC_val_ac\u001b[A\n","Epoch 2:  99%|▉| 80/81 [00:01<00:00, 68.42it/s, loss=1.11, v_num=l2tj, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.023 >= min_delta = 0.003. New best score: 1.148\n","Epoch 2: 100%|█| 81/81 [00:01<00:00, 66.22it/s, loss=1.11, v_num=l2tj, BTC_val_a\n","Epoch 3:  99%|▉| 80/81 [00:01<00:00, 77.62it/s, loss=1.1, v_num=l2tj, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:01<00:00, 75.65it/s, loss=1.1, v_num=l2tj, BTC_val_ac\u001b[A\n","Epoch 4:  99%|▉| 80/81 [00:01<00:00, 74.42it/s, loss=1.15, v_num=l2tj, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:01<00:00, 72.34it/s, loss=1.15, v_num=l2tj, BTC_val_a\u001b[A\n","Epoch 5:  99%|▉| 80/81 [00:00<00:00, 91.82it/s, loss=1.1, v_num=l2tj, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:00<00:00, 89.43it/s, loss=1.1, v_num=l2tj, BTC_val_ac\u001b[A\n","Epoch 6:  99%|▉| 80/81 [00:00<00:00, 91.29it/s, loss=1.11, v_num=l2tj, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:00<00:00, 88.56it/s, loss=1.11, v_num=l2tj, BTC_val_a\u001b[A\n","Epoch 7:  99%|▉| 80/81 [00:00<00:00, 92.71it/s, loss=1.09, v_num=l2tj, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:00<00:00, 90.01it/s, loss=1.09, v_num=l2tj, BTC_val_a\u001b[A\n","Epoch 8:  99%|▉| 80/81 [00:00<00:00, 91.53it/s, loss=1.09, v_num=l2tj, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 1.136\n","Epoch 8: 100%|█| 81/81 [00:00<00:00, 89.13it/s, loss=1.09, v_num=l2tj, BTC_val_a\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 9:  99%|▉| 80/81 [00:00<00:00, 87.09it/s, loss=1.1, v_num=l2tj, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:00<00:00, 84.18it/s, loss=1.1, v_num=l2tj, BTC_val_ac\u001b[A\n","Epoch 10:  99%|▉| 80/81 [00:01<00:00, 79.60it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:01<00:00, 77.27it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Epoch 11:  99%|▉| 80/81 [00:00<00:00, 87.03it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:00<00:00, 84.85it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n","Epoch 12:  99%|▉| 80/81 [00:00<00:00, 92.35it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:00<00:00, 89.88it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.018 >= min_delta = 0.003. New best score: 1.119\n","Epoch 13:  99%|▉| 80/81 [00:00<00:00, 84.05it/s, loss=1.09, v_num=l2tj, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:00<00:00, 81.68it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Epoch 14:  99%|▉| 80/81 [00:01<00:00, 78.14it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:01<00:00, 76.04it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Epoch 15:  99%|▉| 80/81 [00:00<00:00, 86.18it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:01<00:00, 79.19it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n","Epoch 16:  99%|▉| 80/81 [00:00<00:00, 89.59it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:00<00:00, 87.30it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Epoch 17:  99%|▉| 80/81 [00:00<00:00, 88.52it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:00<00:00, 85.76it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Epoch 18:  99%|▉| 80/81 [00:00<00:00, 90.70it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:00<00:00, 88.30it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Epoch 19:  99%|▉| 80/81 [00:00<00:00, 88.19it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:00<00:00, 85.78it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n","Epoch 20:  99%|▉| 80/81 [00:00<00:00, 90.62it/s, loss=1.08, v_num=l2tj, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:00<00:00, 88.19it/s, loss=1.08, v_num=l2tj, BTC_val_\u001b[A\n","Epoch 21:  99%|▉| 80/81 [00:00<00:00, 90.38it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 81/81 [00:00<00:00, 88.11it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n","Epoch 22:  99%|▉| 80/81 [00:00<00:00, 91.36it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 81/81 [00:00<00:00, 89.04it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n","Epoch 23:  99%|▉| 80/81 [00:00<00:00, 92.50it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 81/81 [00:00<00:00, 90.06it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Epoch 24:  99%|▉| 80/81 [00:00<00:00, 88.94it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 81/81 [00:00<00:00, 86.24it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Epoch 25:  99%|▉| 80/81 [00:00<00:00, 89.93it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 81/81 [00:00<00:00, 87.67it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Epoch 26:  99%|▉| 80/81 [00:00<00:00, 92.18it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 81/81 [00:00<00:00, 89.72it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n","Epoch 27:  99%|▉| 80/81 [00:00<00:00, 93.06it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 81/81 [00:00<00:00, 90.66it/s, loss=1.1, v_num=l2tj, BTC_val_a\u001b[A\n","Epoch 28:  99%|▉| 80/81 [00:00<00:00, 88.75it/s, loss=1.08, v_num=l2tj, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 81/81 [00:00<00:00, 86.52it/s, loss=1.08, v_num=l2tj, BTC_val_\u001b[A\n","Epoch 29:  99%|▉| 80/81 [00:00<00:00, 90.61it/s, loss=1.11, v_num=l2tj, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 81/81 [00:00<00:00, 88.08it/s, loss=1.11, v_num=l2tj, BTC_val_\u001b[A\n","Epoch 30:  99%|▉| 80/81 [00:00<00:00, 81.74it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 81/81 [00:01<00:00, 79.39it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Epoch 31:  99%|▉| 80/81 [00:01<00:00, 75.22it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 81/81 [00:01<00:00, 73.42it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Epoch 32:  99%|▉| 80/81 [00:00<00:00, 83.52it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.119. Signaling Trainer to stop.\n","Epoch 32: 100%|█| 81/81 [00:00<00:00, 81.35it/s, loss=1.09, v_num=l2tj, BTC_val_\n","Epoch 32: 100%|█| 81/81 [00:00<00:00, 81.09it/s, loss=1.09, v_num=l2tj, BTC_val_\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 100.74it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.29032257199287415,\n"," 'BTC_test_f1': 0.21669228374958038,\n"," 'test_loss': 1.1167445182800293}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 63464\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005820-3u51l2tj/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005820-3u51l2tj/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.4375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.28571\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.07213\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 32\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2640\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 40\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621461540\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 118\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.35461\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.30432\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09402\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.37635\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.29032\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.21669\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.11674\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▄▄▄▄▇▅▁▄▄█▅▅▃▆▄▇▅▄▃▃▇▆▄▅▇▄▄▄▄▄▇▅▄▄▄▂▃▆▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▄▃▄▄▆▄▁▄▃█▅▅▃▆▃▆▄▄▃▃▆▅▃▅▇▃▄▃▄▄▅▅▄▄▃▂▂▆▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▄▄▃▃▁▁█▂▄▂▂▂▃▂▃▁▂▂▃▃▂▁▃▃▂▄▂▂▃▂▂▂▃▂▃▃▂▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▄▂▃▂▃█▃▃█▃▅▅▃▂▇▄▅▃▂▃▃▇▆▅█▇▅▅▅▅▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▅▃▅▃▄▇▄▅█▄▆▅▄▃▆▅▅▃▅▄▅█▇▆▇█▆▆▅▆▅▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▆▃▅▃▂▄▃▂▂▃▂▃▃▂▂▃▁▂▂▂▂▂▂▁▂▂▁▃▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▆▆▆▁▆▁▁▁▆▆▆▆▁▆▆▆▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▃▃▃▁▃▁▁▁▃▃▃▃▁▃▃▃▃█▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▂▂▁▄▂▂▂▂▁▂▂▂▁▁▁▂▂▃▄▃▃▄▁▂▂▃▄▃█▄▆▂▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_single_lstm_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3u51l2tj\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 00:59:11.607819: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/28qzo7z2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005909-28qzo7z2\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 130   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 79/80 [00:01<00:00, 44.84it/s, loss=0.705, v_num=o7z2, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.656\n","Epoch 0: 100%|█| 80/80 [00:01<00:00, 44.14it/s, loss=0.705, v_num=o7z2, BTC_val_\n","Epoch 1:  99%|▉| 79/80 [00:01<00:00, 46.54it/s, loss=0.623, v_num=o7z2, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.102 >= min_delta = 0.003. New best score: 0.553\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 46.12it/s, loss=0.623, v_num=o7z2, BTC_val_\n","Epoch 2:  99%|▉| 79/80 [00:01<00:00, 44.42it/s, loss=0.642, v_num=o7z2, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.029 >= min_delta = 0.003. New best score: 0.524\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 43.67it/s, loss=0.642, v_num=o7z2, BTC_val_\n","Epoch 3:  99%|▉| 79/80 [00:01<00:00, 44.18it/s, loss=0.576, v_num=o7z2, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.516\n","Epoch 3: 100%|█| 80/80 [00:01<00:00, 43.30it/s, loss=0.576, v_num=o7z2, BTC_val_\n","Epoch 4:  99%|▉| 79/80 [00:01<00:00, 44.81it/s, loss=0.617, v_num=o7z2, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 44.27it/s, loss=0.617, v_num=o7z2, BTC_val_\u001b[A\n","Epoch 5:  99%|▉| 79/80 [00:01<00:00, 44.51it/s, loss=0.598, v_num=o7z2, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.499\n","Epoch 5: 100%|█| 80/80 [00:01<00:00, 44.14it/s, loss=0.598, v_num=o7z2, BTC_val_\n","Epoch 6:  99%|▉| 79/80 [00:01<00:00, 42.60it/s, loss=0.595, v_num=o7z2, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:01<00:00, 41.79it/s, loss=0.595, v_num=o7z2, BTC_val_\u001b[A\n","Epoch 7:  99%|▉| 79/80 [00:01<00:00, 43.45it/s, loss=0.59, v_num=o7z2, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:01<00:00, 43.05it/s, loss=0.59, v_num=o7z2, BTC_val_a\u001b[A\n","Epoch 8:  99%|▉| 79/80 [00:01<00:00, 45.45it/s, loss=0.556, v_num=o7z2, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:01<00:00, 44.96it/s, loss=0.556, v_num=o7z2, BTC_val_\u001b[A\n","Epoch 9:  99%|▉| 79/80 [00:02<00:00, 38.42it/s, loss=0.589, v_num=o7z2, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:02<00:00, 38.14it/s, loss=0.589, v_num=o7z2, BTC_val_\u001b[A\n","Epoch 10:  99%|▉| 79/80 [00:01<00:00, 39.98it/s, loss=0.591, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:02<00:00, 39.60it/s, loss=0.591, v_num=o7z2, BTC_val\u001b[A\n","Epoch 11:  99%|▉| 79/80 [00:01<00:00, 46.67it/s, loss=0.556, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:01<00:00, 46.25it/s, loss=0.556, v_num=o7z2, BTC_val\u001b[A\n","Epoch 12:  99%|▉| 79/80 [00:01<00:00, 39.60it/s, loss=0.56, v_num=o7z2, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.482\n","Epoch 12: 100%|█| 80/80 [00:02<00:00, 39.33it/s, loss=0.56, v_num=o7z2, BTC_val_\n","Epoch 13:  99%|▉| 79/80 [00:02<00:00, 37.15it/s, loss=0.605, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:02<00:00, 36.86it/s, loss=0.605, v_num=o7z2, BTC_val\u001b[A\n","Epoch 14:  99%|▉| 79/80 [00:02<00:00, 33.95it/s, loss=0.576, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:02<00:00, 33.79it/s, loss=0.576, v_num=o7z2, BTC_val\u001b[A\n","Epoch 15:  99%|▉| 79/80 [00:02<00:00, 39.17it/s, loss=0.566, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:02<00:00, 38.97it/s, loss=0.566, v_num=o7z2, BTC_val\u001b[A\n","Epoch 16:  99%|▉| 79/80 [00:02<00:00, 37.73it/s, loss=0.568, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:02<00:00, 37.55it/s, loss=0.568, v_num=o7z2, BTC_val\u001b[A\n","Epoch 17:  99%|▉| 79/80 [00:02<00:00, 38.88it/s, loss=0.57, v_num=o7z2, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:02<00:00, 38.65it/s, loss=0.57, v_num=o7z2, BTC_val_\u001b[A\n","Epoch 18:  99%|▉| 79/80 [00:02<00:00, 39.12it/s, loss=0.593, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:02<00:00, 38.84it/s, loss=0.593, v_num=o7z2, BTC_val\u001b[A\n","Epoch 19:  99%|▉| 79/80 [00:02<00:00, 39.39it/s, loss=0.517, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:02<00:00, 39.15it/s, loss=0.517, v_num=o7z2, BTC_val\u001b[A\n","Epoch 20:  99%|▉| 79/80 [00:01<00:00, 45.12it/s, loss=0.628, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:01<00:00, 44.32it/s, loss=0.628, v_num=o7z2, BTC_val\u001b[A\n","Epoch 21:  99%|▉| 79/80 [00:02<00:00, 38.63it/s, loss=0.642, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:02<00:00, 38.34it/s, loss=0.642, v_num=o7z2, BTC_val\u001b[A\n","Epoch 22:  99%|▉| 79/80 [00:02<00:00, 34.84it/s, loss=0.54, v_num=o7z2, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:02<00:00, 34.70it/s, loss=0.54, v_num=o7z2, BTC_val_\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 23:  99%|▉| 79/80 [00:02<00:00, 38.15it/s, loss=0.574, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 0.468\n","Epoch 23: 100%|█| 80/80 [00:02<00:00, 37.50it/s, loss=0.574, v_num=o7z2, BTC_val\n","Epoch 24:  99%|▉| 79/80 [00:02<00:00, 30.02it/s, loss=0.591, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:02<00:00, 29.87it/s, loss=0.591, v_num=o7z2, BTC_val\u001b[A\n","Epoch 25:  99%|▉| 79/80 [00:02<00:00, 34.04it/s, loss=0.562, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:02<00:00, 33.89it/s, loss=0.562, v_num=o7z2, BTC_val\u001b[A\n","Epoch 26:  99%|▉| 79/80 [00:02<00:00, 35.86it/s, loss=0.557, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.015 >= min_delta = 0.003. New best score: 0.454\n","Epoch 26: 100%|█| 80/80 [00:02<00:00, 35.67it/s, loss=0.557, v_num=o7z2, BTC_val\n","Epoch 27:  99%|▉| 79/80 [00:02<00:00, 30.79it/s, loss=0.591, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 80/80 [00:02<00:00, 30.45it/s, loss=0.591, v_num=o7z2, BTC_val\u001b[A\n","Epoch 28:  99%|▉| 79/80 [00:02<00:00, 33.67it/s, loss=0.579, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 80/80 [00:02<00:00, 33.51it/s, loss=0.579, v_num=o7z2, BTC_val\u001b[A\n","Epoch 29:  99%|▉| 79/80 [00:02<00:00, 34.03it/s, loss=0.599, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 80/80 [00:02<00:00, 33.45it/s, loss=0.599, v_num=o7z2, BTC_val\u001b[A\n","Epoch 30:  99%|▉| 79/80 [00:02<00:00, 30.53it/s, loss=0.522, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 80/80 [00:02<00:00, 30.43it/s, loss=0.522, v_num=o7z2, BTC_val\u001b[A\n","Epoch 31:  99%|▉| 79/80 [00:02<00:00, 33.20it/s, loss=0.594, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 80/80 [00:02<00:00, 33.10it/s, loss=0.594, v_num=o7z2, BTC_val\u001b[A\n","Epoch 32:  99%|▉| 79/80 [00:02<00:00, 31.71it/s, loss=0.565, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 80/80 [00:02<00:00, 31.63it/s, loss=0.565, v_num=o7z2, BTC_val\u001b[A\n","Epoch 33:  99%|▉| 79/80 [00:02<00:00, 34.49it/s, loss=0.572, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 33: 100%|█| 80/80 [00:02<00:00, 34.35it/s, loss=0.572, v_num=o7z2, BTC_val\u001b[A\n","Epoch 34:  99%|▉| 79/80 [00:02<00:00, 27.96it/s, loss=0.702, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 80/80 [00:02<00:00, 27.89it/s, loss=0.702, v_num=o7z2, BTC_val\u001b[A\n","Epoch 35:  99%|▉| 79/80 [00:01<00:00, 44.00it/s, loss=0.698, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 35: 100%|█| 80/80 [00:01<00:00, 43.58it/s, loss=0.698, v_num=o7z2, BTC_val\u001b[A\n","Epoch 36:  99%|▉| 79/80 [00:01<00:00, 42.63it/s, loss=0.699, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 36: 100%|█| 80/80 [00:01<00:00, 42.25it/s, loss=0.699, v_num=o7z2, BTC_val\u001b[A\n","Epoch 37:  99%|▉| 79/80 [00:02<00:00, 33.52it/s, loss=0.694, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 37: 100%|█| 80/80 [00:02<00:00, 33.12it/s, loss=0.694, v_num=o7z2, BTC_val\u001b[A\n","Epoch 38:  99%|▉| 79/80 [00:02<00:00, 31.89it/s, loss=0.666, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 38: 100%|█| 80/80 [00:02<00:00, 31.74it/s, loss=0.666, v_num=o7z2, BTC_val\u001b[A\n","Epoch 39:  99%|▉| 79/80 [00:02<00:00, 29.79it/s, loss=0.657, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 39: 100%|█| 80/80 [00:02<00:00, 29.44it/s, loss=0.657, v_num=o7z2, BTC_val\u001b[A\n","Epoch 40:  99%|▉| 79/80 [00:02<00:00, 36.60it/s, loss=0.651, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 40: 100%|█| 80/80 [00:02<00:00, 36.17it/s, loss=0.651, v_num=o7z2, BTC_val\u001b[A\n","Epoch 41:  99%|▉| 79/80 [00:02<00:00, 30.10it/s, loss=0.593, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 41: 100%|█| 80/80 [00:02<00:00, 30.05it/s, loss=0.593, v_num=o7z2, BTC_val\u001b[A\n","Epoch 42:  99%|▉| 79/80 [00:01<00:00, 44.42it/s, loss=0.671, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 42: 100%|█| 80/80 [00:01<00:00, 44.05it/s, loss=0.671, v_num=o7z2, BTC_val\u001b[A\n","Epoch 43:  99%|▉| 79/80 [00:01<00:00, 44.26it/s, loss=0.598, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 43: 100%|█| 80/80 [00:01<00:00, 43.87it/s, loss=0.598, v_num=o7z2, BTC_val\u001b[A\n","Epoch 44:  99%|▉| 79/80 [00:02<00:00, 29.72it/s, loss=0.588, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 44: 100%|█| 80/80 [00:02<00:00, 29.45it/s, loss=0.588, v_num=o7z2, BTC_val\u001b[A\n","Epoch 45:  99%|▉| 79/80 [00:01<00:00, 44.02it/s, loss=0.633, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 45: 100%|█| 80/80 [00:01<00:00, 43.65it/s, loss=0.633, v_num=o7z2, BTC_val\u001b[A\n","Epoch 46:  99%|▉| 79/80 [00:01<00:00, 40.12it/s, loss=0.596, v_num=o7z2, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.454. Signaling Trainer to stop.\n","Epoch 46: 100%|█| 80/80 [00:02<00:00, 39.86it/s, loss=0.596, v_num=o7z2, BTC_val\n","Epoch 46: 100%|█| 80/80 [00:02<00:00, 39.80it/s, loss=0.596, v_num=o7z2, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 33.33it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.6774193644523621,\n"," 'BTC_test_f1': 0.6765552759170532,\n"," 'test_loss': 0.5754806995391846}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 63622\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005909-28qzo7z2/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_005909-28qzo7z2/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.875\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.86667\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.53766\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 46\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3713\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 111\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621461660\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 168\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.68567\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.67186\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.58554\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.66667\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.64935\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.58857\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.67742\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.67656\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.57548\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▃▅▆▇▃▄▄▃▅▆▆▆▆▆▄▄▇▅▆▆▆▆▆▄▇▃▄█▄▄▁▄▂▆▅▆▄▅▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▃▅▆▇▃▄▄▃▅▆▅▆▆▆▄▄▇▅▆▆▆▆▆▄▇▃▄█▄▃▁▄▂▆▄▆▃▃▆▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▅▅▂▆▆▇▅▆▄▃▇▅▄▅▇▃▃▅▃▄▃▃█▃█▅▂█▆▆▆▆▅▄▃▅▃▁▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▂▄▆▇▇▇▇▇▇▇█▇▇█▇█████▇▇█▇█████▃▁▂▂▃▅▆▆▇▆▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▂▄▆▇▇▇▇▇▇▇█▇▇▇▇███▇█▇▇█▇█████▃▁▂▃▄▆▆▆▇▆▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▄▄▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▆██▇▇▄▃▄▃▃▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▃▅▆█▅▆▆▆▆▅▆▆▆▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆▆▅▁▃▃▆▆▆▆▆▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▂▅▇█▅▇▇▇▇▅▇▇▇▇▇▅▇▇▇▇▇▇▇▇▇▇▇▇▇▄▁▂▃▇▇▇▇▇▇▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▇▄▃▃▅▂▃▃▂▃▃▂▃▃▂▃▂▃▃▂▁▂▁▁▁▂▂▃▂████▆▅▃▄▃▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/28qzo7z2\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:01:15.522870: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_loss_weighted_binary_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2ve67el4\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010114-2ve67el4\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 130   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0: 100%|█| 81/81 [00:01<00:00, 46.46it/s, loss=0.698, v_num=7el4, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.706\n","Epoch 0: 100%|█| 81/81 [00:01<00:00, 46.13it/s, loss=0.698, v_num=7el4, BTC_val_\n","Epoch 1:  99%|▉| 80/81 [00:01<00:00, 46.00it/s, loss=0.717, v_num=7el4, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 0.694\n","Epoch 1: 100%|█| 81/81 [00:01<00:00, 45.55it/s, loss=0.717, v_num=7el4, BTC_val_\n","Epoch 2:  99%|▉| 80/81 [00:01<00:00, 44.63it/s, loss=0.695, v_num=7el4, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 81/81 [00:01<00:00, 43.99it/s, loss=0.695, v_num=7el4, BTC_val_\u001b[A\n","Epoch 3:  99%|▉| 80/81 [00:02<00:00, 39.70it/s, loss=0.701, v_num=7el4, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:02<00:00, 39.38it/s, loss=0.701, v_num=7el4, BTC_val_\u001b[A\n","Epoch 4:  99%|▉| 80/81 [00:01<00:00, 44.75it/s, loss=0.694, v_num=7el4, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:01<00:00, 44.28it/s, loss=0.694, v_num=7el4, BTC_val_\u001b[A\n","Epoch 5:  99%|▉| 80/81 [00:01<00:00, 45.19it/s, loss=0.693, v_num=7el4, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:01<00:00, 44.78it/s, loss=0.693, v_num=7el4, BTC_val_\u001b[A\n","Epoch 6:  99%|▉| 80/81 [00:02<00:00, 38.24it/s, loss=0.697, v_num=7el4, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:02<00:00, 38.01it/s, loss=0.697, v_num=7el4, BTC_val_\u001b[A\n","Epoch 7:  99%|▉| 80/81 [00:01<00:00, 44.90it/s, loss=0.694, v_num=7el4, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:01<00:00, 44.48it/s, loss=0.694, v_num=7el4, BTC_val_\u001b[A\n","Epoch 8:  99%|▉| 80/81 [00:01<00:00, 44.86it/s, loss=0.697, v_num=7el4, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:01<00:00, 44.46it/s, loss=0.697, v_num=7el4, BTC_val_\u001b[A\n","Epoch 9:  99%|▉| 80/81 [00:01<00:00, 44.94it/s, loss=0.697, v_num=7el4, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:01<00:00, 44.37it/s, loss=0.697, v_num=7el4, BTC_val_\u001b[A\n","Epoch 10:  99%|▉| 80/81 [00:02<00:00, 34.44it/s, loss=0.694, v_num=7el4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:02<00:00, 34.29it/s, loss=0.694, v_num=7el4, BTC_val\u001b[A\n","Epoch 11:  99%|▉| 80/81 [00:02<00:00, 33.66it/s, loss=0.69, v_num=7el4, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:02<00:00, 33.48it/s, loss=0.69, v_num=7el4, BTC_val_\u001b[A\n","Epoch 12:  99%|▉| 80/81 [00:01<00:00, 44.04it/s, loss=0.697, v_num=7el4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:01<00:00, 43.61it/s, loss=0.697, v_num=7el4, BTC_val\u001b[A\n","Epoch 13:  99%|▉| 80/81 [00:01<00:00, 42.64it/s, loss=0.694, v_num=7el4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:01<00:00, 42.23it/s, loss=0.694, v_num=7el4, BTC_val\u001b[A\n","Epoch 14:  99%|▉| 80/81 [00:01<00:00, 44.79it/s, loss=0.695, v_num=7el4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:01<00:00, 44.22it/s, loss=0.695, v_num=7el4, BTC_val\u001b[A\n","Epoch 15:  99%|▉| 80/81 [00:01<00:00, 42.49it/s, loss=0.696, v_num=7el4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:01<00:00, 40.76it/s, loss=0.696, v_num=7el4, BTC_val\u001b[A\n","Epoch 16:  99%|▉| 80/81 [00:02<00:00, 38.18it/s, loss=0.695, v_num=7el4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:02<00:00, 37.89it/s, loss=0.695, v_num=7el4, BTC_val\u001b[A\n","Epoch 17:  99%|▉| 80/81 [00:02<00:00, 34.43it/s, loss=0.696, v_num=7el4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:02<00:00, 34.26it/s, loss=0.696, v_num=7el4, BTC_val\u001b[A\n","Epoch 18:  99%|▉| 80/81 [00:02<00:00, 32.51it/s, loss=0.696, v_num=7el4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:02<00:00, 32.39it/s, loss=0.696, v_num=7el4, BTC_val\u001b[A\n","Epoch 19:  99%|▉| 80/81 [00:01<00:00, 44.62it/s, loss=0.695, v_num=7el4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:01<00:00, 44.09it/s, loss=0.695, v_num=7el4, BTC_val\u001b[A\n","Epoch 20:  99%|▉| 80/81 [00:01<00:00, 44.38it/s, loss=0.693, v_num=7el4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:01<00:00, 43.90it/s, loss=0.693, v_num=7el4, BTC_val\u001b[A\n","Epoch 21:  99%|▉| 80/81 [00:02<00:00, 37.87it/s, loss=0.692, v_num=7el4, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 81/81 [00:02<00:00, 37.53it/s, loss=0.692, v_num=7el4, BTC_val\u001b[A\n","                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.694. Signaling Trainer to stop.\n","Epoch 21: 100%|█| 81/81 [00:02<00:00, 37.48it/s, loss=0.692, v_num=7el4, BTC_val\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 95.21it/s]\n"],"name":"stdout"},{"output_type":"stream","text":["--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.5806451439857483,\n"," 'BTC_test_f1': 0.36725807189941406,\n"," 'test_loss': 0.6858224272727966}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 63928\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010114-2ve67el4/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010114-2ve67el4/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.68493\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 21\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1760\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 52\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621461726\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 79\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.48857\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.46133\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69365\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.30769\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.69426\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.58065\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.36726\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.68582\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▅▇▄▅▄▅▆▇▆▇▅█▆▆▆▁▄▇▅▅▅▆▄▅▆▆▅▆▇▇▃▇▆▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▄▇▄▅▄▅▆▇▆▇▅█▅▆▆▁▃▇▅▅▅▅▄▄▆▄▅▆▇▇▃▅▆▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▂█▄▅▄▃▁▄▃▅▃▃▄▄▄▄▃▃▄▄▄▄▄▄▃▃▃▄▄▄▃▄▄▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▇▂▃▃▄▅▁▄▆▃▃▄▃▁▄▃▃▁▃▆█▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch █▃▃▃▅▆▄▅▄▅▁▆▅▁▆▄▄▃▅▇█▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ▄█▄▃▃▂▃▂▂▂▁▁▂▂▁▁▁▂▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▁▄▂▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2ve67el4\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:02:21.455998: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1i8md9yy\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010220-1i8md9yy\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 195   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Training: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0: 100%|█| 80/80 [00:01<00:00, 41.46it/s, loss=1.13, v_num=d9yy, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.081\n","Epoch 0: 100%|█| 80/80 [00:01<00:00, 41.17it/s, loss=1.13, v_num=d9yy, BTC_val_a\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 44.67it/s, loss=1.11, v_num=d9yy, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 44.39it/s, loss=1.11, v_num=d9yy, BTC_val_a\u001b[A\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 45.77it/s, loss=1.11, v_num=d9yy, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 45.47it/s, loss=1.11, v_num=d9yy, BTC_val_a\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:01<00:00, 45.92it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:01<00:00, 45.62it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 46.28it/s, loss=1.09, v_num=d9yy, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 46.00it/s, loss=1.09, v_num=d9yy, BTC_val_a\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:02<00:00, 33.79it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:02<00:00, 33.61it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:02<00:00, 38.02it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:02<00:00, 37.82it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:02<00:00, 36.93it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:02<00:00, 36.30it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:01<00:00, 43.09it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:01<00:00, 42.83it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:01<00:00, 44.67it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:01<00:00, 44.38it/s, loss=1.1, v_num=d9yy, BTC_val_ac\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:01<00:00, 44.10it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:01<00:00, 43.78it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:02<00:00, 38.89it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:02<00:00, 38.67it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:02<00:00, 38.67it/s, loss=1.11, v_num=d9yy, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:02<00:00, 38.44it/s, loss=1.11, v_num=d9yy, BTC_val_\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:02<00:00, 34.10it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:02<00:00, 33.93it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:02<00:00, 34.64it/s, loss=1.09, v_num=d9yy, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:02<00:00, 34.47it/s, loss=1.09, v_num=d9yy, BTC_val_\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:02<00:00, 34.10it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:02<00:00, 33.94it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:02<00:00, 39.78it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:02<00:00, 39.55it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:02<00:00, 35.69it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:02<00:00, 35.48it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:02<00:00, 34.52it/s, loss=1.11, v_num=d9yy, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:02<00:00, 34.35it/s, loss=1.11, v_num=d9yy, BTC_val_\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:02<00:00, 38.64it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:02<00:00, 38.41it/s, loss=1.1, v_num=d9yy, BTC_val_a\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:01<00:00, 43.81it/s, loss=1.09, v_num=d9yy, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.081. Signaling Trainer to stop.\n","Epoch 20: 100%|█| 80/80 [00:01<00:00, 43.55it/s, loss=1.09, v_num=d9yy, BTC_val_\n","Epoch 20: 100%|█| 80/80 [00:01<00:00, 43.47it/s, loss=1.09, v_num=d9yy, BTC_val_\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 93.99it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.35483869910240173,\n"," 'BTC_test_f1': 0.17448680102825165,\n"," 'test_loss': 1.0824896097183228}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 64134\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010220-1i8md9yy/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010220-1i8md9yy/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.39057\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.07944\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1659\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 51\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621461791\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.43785\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.30441\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09759\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.09321\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.35484\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.17449\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.08249\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▅▃▃▄▂▆▃▅█▃▃▆▂▅▆█▃▄▃▂▃▁▆▇▃▅▂▄█▄▇▅█\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▆▄▃▃▂▇▃▄▆▃▃▅▃▅▇█▄▃▃▂▃▁▅▆▃▅▃▅▅▂▆▄▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅▇▅▁█▃█▁▂▄▃▅▄▅▃▅▅▅▅▆▄▆▃▃▇▆▇▅▃▅▄▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▃▂▄▂▂▄▄▅▁▃▃▆▃▁▅▄▇▅▄▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▆▇█▆▄▇██▅▅▆▇▆▅▅▃▃▇▁▅▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▄▅▃▂▂▂▂▂▃▂▂▂▁▂▁▁▃▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▅▇▄▅▄▆▄▄▇▆▅█▆▆▆▆▆▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1i8md9yy\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:03:27.328989: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_loss_weighted_multi_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/ubu3zph0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010325-ubu3zph0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 195   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 0:  99%|▉| 80/81 [00:01<00:00, 41.94it/s, loss=1.13, v_num=zph0, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.155\n","Epoch 0: 100%|█| 81/81 [00:01<00:00, 41.50it/s, loss=1.13, v_num=zph0, BTC_val_a\n","Epoch 1:  99%|▉| 80/81 [00:01<00:00, 45.16it/s, loss=1.11, v_num=zph0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.034 >= min_delta = 0.003. New best score: 1.121\n","Epoch 1: 100%|█| 81/81 [00:01<00:00, 44.75it/s, loss=1.11, v_num=zph0, BTC_val_a\n","Epoch 2:  99%|▉| 80/81 [00:01<00:00, 44.53it/s, loss=1.08, v_num=zph0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.020 >= min_delta = 0.003. New best score: 1.101\n","Epoch 2: 100%|█| 81/81 [00:01<00:00, 43.97it/s, loss=1.08, v_num=zph0, BTC_val_a\n","Epoch 3:  99%|▉| 80/81 [00:01<00:00, 43.05it/s, loss=1.11, v_num=zph0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:01<00:00, 42.39it/s, loss=1.11, v_num=zph0, BTC_val_a\u001b[A\n","Epoch 4:  99%|▉| 80/81 [00:02<00:00, 32.60it/s, loss=1.1, v_num=zph0, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:02<00:00, 32.41it/s, loss=1.1, v_num=zph0, BTC_val_ac\u001b[A\n","Epoch 5:  99%|▉| 80/81 [00:02<00:00, 39.01it/s, loss=1.09, v_num=zph0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:02<00:00, 38.73it/s, loss=1.09, v_num=zph0, BTC_val_a\u001b[A\n","Epoch 6:  99%|▉| 80/81 [00:02<00:00, 37.63it/s, loss=1.09, v_num=zph0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:02<00:00, 37.39it/s, loss=1.09, v_num=zph0, BTC_val_a\u001b[A\n","Epoch 7:  99%|▉| 80/81 [00:02<00:00, 33.96it/s, loss=1.11, v_num=zph0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:02<00:00, 33.80it/s, loss=1.11, v_num=zph0, BTC_val_a\u001b[A\n","Epoch 8:  99%|▉| 80/81 [00:01<00:00, 41.59it/s, loss=1.11, v_num=zph0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:01<00:00, 41.28it/s, loss=1.11, v_num=zph0, BTC_val_a\u001b[A\n","Epoch 9:  99%|▉| 80/81 [00:02<00:00, 38.13it/s, loss=1.11, v_num=zph0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:02<00:00, 37.81it/s, loss=1.11, v_num=zph0, BTC_val_a\u001b[A\n","Epoch 10:  99%|▉| 80/81 [00:02<00:00, 28.36it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:02<00:00, 28.31it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n","Epoch 11:  99%|▉| 80/81 [00:01<00:00, 45.42it/s, loss=1.11, v_num=zph0, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:01<00:00, 44.82it/s, loss=1.11, v_num=zph0, BTC_val_\u001b[A\n","Epoch 12:  99%|▉| 80/81 [00:01<00:00, 43.91it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:01<00:00, 43.49it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n","Epoch 13:  99%|▉| 80/81 [00:01<00:00, 44.07it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:01<00:00, 43.66it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n","Epoch 14:  99%|▉| 80/81 [00:02<00:00, 33.13it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:02<00:00, 32.58it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n","Epoch 15:  99%|▉| 80/81 [00:02<00:00, 32.04it/s, loss=1.09, v_num=zph0, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:02<00:00, 31.24it/s, loss=1.09, v_num=zph0, BTC_val_\u001b[A\n","Epoch 16:  99%|▉| 80/81 [00:02<00:00, 39.35it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:02<00:00, 39.03it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n","Epoch 17:  99%|▉| 80/81 [00:01<00:00, 45.41it/s, loss=1.12, v_num=zph0, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:01<00:00, 44.96it/s, loss=1.12, v_num=zph0, BTC_val_\u001b[A\n","Epoch 18:  99%|▉| 80/81 [00:01<00:00, 45.16it/s, loss=1.08, v_num=zph0, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:01<00:00, 44.69it/s, loss=1.08, v_num=zph0, BTC_val_\u001b[A\n","Epoch 19:  99%|▉| 80/81 [00:01<00:00, 45.80it/s, loss=1.08, v_num=zph0, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:01<00:00, 45.21it/s, loss=1.08, v_num=zph0, BTC_val_\u001b[A\n","Epoch 20:  99%|▉| 80/81 [00:02<00:00, 31.81it/s, loss=1.09, v_num=zph0, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:02<00:00, 31.71it/s, loss=1.09, v_num=zph0, BTC_val_\u001b[A\n","Epoch 21:  99%|▉| 80/81 [00:02<00:00, 34.03it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 81/81 [00:02<00:00, 33.88it/s, loss=1.1, v_num=zph0, BTC_val_a\u001b[A\n","Epoch 22:  99%|▉| 80/81 [00:02<00:00, 33.77it/s, loss=1.11, v_num=zph0, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.101. Signaling Trainer to stop.\n","Epoch 22: 100%|█| 81/81 [00:02<00:00, 33.64it/s, loss=1.11, v_num=zph0, BTC_val_\n","Epoch 22: 100%|█| 81/81 [00:02<00:00, 33.60it/s, loss=1.11, v_num=zph0, BTC_val_\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 88.19it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.25806450843811035,\n"," 'BTC_test_f1': 0.1367289274930954,\n"," 'test_loss': 1.135680913925171}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 64340\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010325-ubu3zph0/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010325-ubu3zph0/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.43275\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.0438\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 22\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1840\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 58\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621461863\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 82\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.35225\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.32388\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.10167\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.22222\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.12121\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.11175\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.25806\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.13673\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.13568\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▄▅▄▅▄▂▄▅▄▄▃▃▅▅▄▁▄▅▂▄▅▄▅▃▃▅▅▃▃▅▄█▃▄▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▄▄▃▄▄▂▄▃▄▃▃▂▅▄▄▁▄▅▂▃▅▃▄▂▃▄▅▃▂▄▃█▂▄▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▅▆▅█▇▅▆▆▆▅▄▅▅▅▇▅▃▇▅▄▅▄▃▅▄▄▅▅▄▄▁▆▆▂▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▃▃▇▅▇▆▆▇▆▄▆▇▆▆▇▆▇▁▇█▅█▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▃▂▄▃▆▅▂▆▄▄▅▄▅▅▄▆▅▃▆█▁▇▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▃▅▄▃▃▃▃▃▃▃▃▃▃▃▁▃▁▂▃▁▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁███▁███████████████▁█▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁███▁███████████████▁█▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▄▁▁▃▁▂▃▂▂▃▃▁▃▄▄▄▅▃▄▆▆▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/ubu3zph0\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:04:39.216279: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_single_lstm__binary_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3edrij0v\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010437-3edrij0v\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 130   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.7 K    Trainable params\n","0         Non-trainable params\n","75.7 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 79/80 [00:01<00:00, 77.47it/s, loss=0.527, v_num=ij0v, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.505\n","Epoch 0: 100%|█| 80/80 [00:01<00:00, 70.47it/s, loss=0.527, v_num=ij0v, BTC_val_\n","Epoch 1:  99%|▉| 79/80 [00:01<00:00, 78.11it/s, loss=0.6, v_num=ij0v, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 75.79it/s, loss=0.6, v_num=ij0v, BTC_val_ac\u001b[A\n","Epoch 2:  99%|▉| 79/80 [00:00<00:00, 83.99it/s, loss=0.573, v_num=ij0v, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 80/80 [00:00<00:00, 82.07it/s, loss=0.573, v_num=ij0v, BTC_val_\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:00<00:00, 87.57it/s, loss=0.6, v_num=ij0v, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:00<00:00, 86.81it/s, loss=0.6, v_num=ij0v, BTC_val_ac\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:00<00:00, 90.43it/s, loss=0.588, v_num=ij0v, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:00<00:00, 89.56it/s, loss=0.588, v_num=ij0v, BTC_val_\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:00<00:00, 88.50it/s, loss=0.622, v_num=ij0v, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:00<00:00, 87.74it/s, loss=0.622, v_num=ij0v, BTC_val_\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:00<00:00, 89.26it/s, loss=0.629, v_num=ij0v, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:00<00:00, 88.49it/s, loss=0.629, v_num=ij0v, BTC_val_\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:00<00:00, 89.64it/s, loss=0.582, v_num=ij0v, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:00<00:00, 88.81it/s, loss=0.582, v_num=ij0v, BTC_val_\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:00<00:00, 87.96it/s, loss=0.572, v_num=ij0v, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:00<00:00, 87.15it/s, loss=0.572, v_num=ij0v, BTC_val_\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:01<00:00, 78.80it/s, loss=0.592, v_num=ij0v, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:01<00:00, 78.07it/s, loss=0.592, v_num=ij0v, BTC_val_\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:01<00:00, 70.60it/s, loss=0.525, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:01<00:00, 69.75it/s, loss=0.525, v_num=ij0v, BTC_val\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:01<00:00, 78.54it/s, loss=0.582, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:01<00:00, 77.74it/s, loss=0.582, v_num=ij0v, BTC_val\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:01<00:00, 75.02it/s, loss=0.615, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:01<00:00, 74.34it/s, loss=0.615, v_num=ij0v, BTC_val\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:00<00:00, 83.84it/s, loss=0.578, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:00<00:00, 83.08it/s, loss=0.578, v_num=ij0v, BTC_val\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:00<00:00, 90.43it/s, loss=0.626, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:00<00:00, 89.64it/s, loss=0.626, v_num=ij0v, BTC_val\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:00<00:00, 88.69it/s, loss=0.546, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:00<00:00, 87.85it/s, loss=0.546, v_num=ij0v, BTC_val\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:00<00:00, 91.32it/s, loss=0.564, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:00<00:00, 90.46it/s, loss=0.564, v_num=ij0v, BTC_val\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:00<00:00, 88.97it/s, loss=0.566, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:00<00:00, 88.18it/s, loss=0.566, v_num=ij0v, BTC_val\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:00<00:00, 88.99it/s, loss=0.586, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.501\n","Epoch 18: 100%|█| 80/80 [00:00<00:00, 88.18it/s, loss=0.586, v_num=ij0v, BTC_val\n","Epoch 19: 100%|█| 80/80 [00:00<00:00, 83.94it/s, loss=0.579, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:00<00:00, 83.06it/s, loss=0.579, v_num=ij0v, BTC_val\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:01<00:00, 75.09it/s, loss=0.543, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:01<00:00, 74.48it/s, loss=0.543, v_num=ij0v, BTC_val\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:01<00:00, 74.36it/s, loss=0.579, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:01<00:00, 73.73it/s, loss=0.579, v_num=ij0v, BTC_val\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:01<00:00, 66.50it/s, loss=0.585, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:01<00:00, 65.89it/s, loss=0.585, v_num=ij0v, BTC_val\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:00<00:00, 81.85it/s, loss=0.574, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:00<00:00, 81.13it/s, loss=0.574, v_num=ij0v, BTC_val\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:01<00:00, 78.09it/s, loss=0.584, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:01<00:00, 77.44it/s, loss=0.584, v_num=ij0v, BTC_val\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:01<00:00, 79.33it/s, loss=0.608, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:01<00:00, 78.57it/s, loss=0.608, v_num=ij0v, BTC_val\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:00<00:00, 89.78it/s, loss=0.601, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:00<00:00, 88.97it/s, loss=0.601, v_num=ij0v, BTC_val\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 27: 100%|█| 80/80 [00:00<00:00, 88.61it/s, loss=0.574, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 80/80 [00:00<00:00, 87.77it/s, loss=0.574, v_num=ij0v, BTC_val\u001b[A\n","Epoch 28: 100%|█| 80/80 [00:00<00:00, 88.22it/s, loss=0.561, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 80/80 [00:00<00:00, 87.34it/s, loss=0.561, v_num=ij0v, BTC_val\u001b[A\n","Epoch 29: 100%|█| 80/80 [00:00<00:00, 86.90it/s, loss=0.588, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 80/80 [00:00<00:00, 86.05it/s, loss=0.588, v_num=ij0v, BTC_val\u001b[A\n","Epoch 30: 100%|█| 80/80 [00:00<00:00, 80.19it/s, loss=0.568, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 80/80 [00:01<00:00, 79.28it/s, loss=0.568, v_num=ij0v, BTC_val\u001b[A\n","Epoch 31: 100%|█| 80/80 [00:00<00:00, 87.90it/s, loss=0.587, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 80/80 [00:00<00:00, 87.09it/s, loss=0.587, v_num=ij0v, BTC_val\u001b[A\n","Epoch 32: 100%|█| 80/80 [00:00<00:00, 88.21it/s, loss=0.577, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 80/80 [00:00<00:00, 87.44it/s, loss=0.577, v_num=ij0v, BTC_val\u001b[A\n","Epoch 33: 100%|█| 80/80 [00:00<00:00, 88.24it/s, loss=0.576, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 33: 100%|█| 80/80 [00:00<00:00, 86.98it/s, loss=0.576, v_num=ij0v, BTC_val\u001b[A\n","Epoch 34: 100%|█| 80/80 [00:00<00:00, 88.06it/s, loss=0.534, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 80/80 [00:00<00:00, 87.18it/s, loss=0.534, v_num=ij0v, BTC_val\u001b[A\n","Epoch 35: 100%|█| 80/80 [00:00<00:00, 87.19it/s, loss=0.587, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 35: 100%|█| 80/80 [00:00<00:00, 86.46it/s, loss=0.587, v_num=ij0v, BTC_val\u001b[A\n","Epoch 36: 100%|█| 80/80 [00:00<00:00, 88.16it/s, loss=0.572, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 36: 100%|█| 80/80 [00:00<00:00, 87.29it/s, loss=0.572, v_num=ij0v, BTC_val\u001b[A\n","Epoch 37: 100%|█| 80/80 [00:00<00:00, 87.13it/s, loss=0.546, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 37: 100%|█| 80/80 [00:00<00:00, 86.32it/s, loss=0.546, v_num=ij0v, BTC_val\u001b[A\n","Epoch 38: 100%|█| 80/80 [00:01<00:00, 70.08it/s, loss=0.548, v_num=ij0v, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.501. Signaling Trainer to stop.\n","Epoch 38: 100%|█| 80/80 [00:01<00:00, 69.31it/s, loss=0.548, v_num=ij0v, BTC_val\n","Epoch 38: 100%|█| 80/80 [00:01<00:00, 69.12it/s, loss=0.548, v_num=ij0v, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 106.29it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.7096773982048035,\n"," 'BTC_test_f1': 0.7012333869934082,\n"," 'test_loss': 0.6024167537689209}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 64543\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010437-3edrij0v/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010437-3edrij0v/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.56078\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.78639\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 38\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3081\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 47\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621461924\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 139\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.71496\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.69673\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.56223\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.77778\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.775\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.51207\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.70968\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.70123\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.60242\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▄▇▇▅▅▆▁▆█▅▄▅▆▃▇▇▅█▂▅▇██▅▆█▆▃█▇▇▅▄▃▅▆▅▆▇▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▄▇▇▆▅▆▁▇█▄▄▅▇▃▇▇▆█▂▅▇██▅▇█▇▃▇▇▇▅▄▄▆▆▅▆▇▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▄▂▃▅▃▃█▂▂▄▆▃▃▆▄▂▄▃▆▅▂▁▂▅▂▁▄▆▁▁▂▃▄▆▄▃▄▄▃▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▅▅▅▅▅▆▆▅▆▆▇▆▇▆▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▆▆▅▆▅▆▆▆▇▆▇▆▇▆▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▅▄▄▄▄▃▅▃▄▃▃▃▄▄▃▃▃▃▂▃▂▃▃▂▃▃▂▃▃▂▃▂▂▂▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ██▁█▁████████████████████▁▁███▁████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ██▂█▁████████████████████▂▂███▂████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▃▄▇█▄▆▆▆▄▁▂▅▃▅▅▄▃▁▅█▅▂▆▅▄▃▅▃▅▃▃▃▃▆▁▁▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_single_lstm__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3edrij0v\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:05:33.523984: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_single_lstm__binary_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/10fa4e86\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010532-10fa4e86\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 130   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.7 K    Trainable params\n","0         Non-trainable params\n","75.7 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n"],"name":"stdout"},{"output_type":"stream","text":["/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 80/81 [00:00<00:00, 85.95it/s, loss=0.696, v_num=4e86, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.704\n","Epoch 0: 100%|█| 81/81 [00:00<00:00, 83.41it/s, loss=0.696, v_num=4e86, BTC_val_\n","Epoch 1:  99%|▉| 80/81 [00:01<00:00, 79.12it/s, loss=0.695, v_num=4e86, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 81/81 [00:01<00:00, 76.31it/s, loss=0.695, v_num=4e86, BTC_val_\u001b[A\n","Epoch 2:  99%|▉| 80/81 [00:00<00:00, 88.42it/s, loss=0.693, v_num=4e86, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.695\n","Epoch 2: 100%|█| 81/81 [00:00<00:00, 86.32it/s, loss=0.693, v_num=4e86, BTC_val_\n","Epoch 3:  99%|▉| 80/81 [00:00<00:00, 87.65it/s, loss=0.699, v_num=4e86, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.692\n","Epoch 3: 100%|█| 81/81 [00:00<00:00, 84.93it/s, loss=0.699, v_num=4e86, BTC_val_\n","Epoch 4:  99%|▉| 80/81 [00:00<00:00, 80.24it/s, loss=0.693, v_num=4e86, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:01<00:00, 78.05it/s, loss=0.693, v_num=4e86, BTC_val_\u001b[A\n","Epoch 5:  99%|▉| 80/81 [00:01<00:00, 79.83it/s, loss=0.696, v_num=4e86, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:01<00:00, 77.52it/s, loss=0.696, v_num=4e86, BTC_val_\u001b[A\n","Epoch 6:  99%|▉| 80/81 [00:00<00:00, 83.77it/s, loss=0.691, v_num=4e86, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:00<00:00, 81.46it/s, loss=0.691, v_num=4e86, BTC_val_\u001b[A\n","Epoch 7:  99%|▉| 80/81 [00:01<00:00, 71.08it/s, loss=0.7, v_num=4e86, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:01<00:00, 69.46it/s, loss=0.7, v_num=4e86, BTC_val_ac\u001b[A\n","Epoch 8:  99%|▉| 80/81 [00:00<00:00, 80.15it/s, loss=0.697, v_num=4e86, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:01<00:00, 78.22it/s, loss=0.697, v_num=4e86, BTC_val_\u001b[A\n","Epoch 9:  99%|▉| 80/81 [00:00<00:00, 86.91it/s, loss=0.689, v_num=4e86, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:00<00:00, 84.75it/s, loss=0.689, v_num=4e86, BTC_val_\u001b[A\n","Epoch 10:  99%|▉| 80/81 [00:00<00:00, 92.72it/s, loss=0.683, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:00<00:00, 90.04it/s, loss=0.683, v_num=4e86, BTC_val\u001b[A\n","Epoch 11:  99%|▉| 80/81 [00:00<00:00, 92.41it/s, loss=0.687, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:00<00:00, 89.87it/s, loss=0.687, v_num=4e86, BTC_val\u001b[A\n","Epoch 12:  99%|▉| 80/81 [00:00<00:00, 92.38it/s, loss=0.696, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:00<00:00, 89.79it/s, loss=0.696, v_num=4e86, BTC_val\u001b[A\n","Epoch 13:  99%|▉| 80/81 [00:00<00:00, 91.42it/s, loss=0.686, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:00<00:00, 89.14it/s, loss=0.686, v_num=4e86, BTC_val\u001b[A\n","Epoch 14:  99%|▉| 80/81 [00:00<00:00, 92.24it/s, loss=0.695, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:00<00:00, 89.52it/s, loss=0.695, v_num=4e86, BTC_val\u001b[A\n","Epoch 15:  99%|▉| 80/81 [00:00<00:00, 91.70it/s, loss=0.693, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:00<00:00, 88.73it/s, loss=0.693, v_num=4e86, BTC_val\u001b[A\n","Epoch 16:  99%|▉| 80/81 [00:00<00:00, 82.38it/s, loss=0.695, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:01<00:00, 79.83it/s, loss=0.695, v_num=4e86, BTC_val\u001b[A\n","Epoch 17:  99%|▉| 80/81 [00:01<00:00, 76.28it/s, loss=0.693, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:01<00:00, 74.35it/s, loss=0.693, v_num=4e86, BTC_val\u001b[A\n","Epoch 18:  99%|▉| 80/81 [00:01<00:00, 73.00it/s, loss=0.69, v_num=4e86, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:01<00:00, 70.65it/s, loss=0.69, v_num=4e86, BTC_val_\u001b[A\n","Epoch 19:  99%|▉| 80/81 [00:01<00:00, 69.18it/s, loss=0.688, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:01<00:00, 67.60it/s, loss=0.688, v_num=4e86, BTC_val\u001b[A\n","Epoch 20:  99%|▉| 80/81 [00:01<00:00, 76.31it/s, loss=0.69, v_num=4e86, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:01<00:00, 74.46it/s, loss=0.69, v_num=4e86, BTC_val_\u001b[A\n","Epoch 21:  99%|▉| 80/81 [00:00<00:00, 85.92it/s, loss=0.688, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 81/81 [00:00<00:00, 83.35it/s, loss=0.688, v_num=4e86, BTC_val\u001b[A\n","Epoch 22:  99%|▉| 80/81 [00:01<00:00, 78.76it/s, loss=0.691, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.688\n","Epoch 22: 100%|█| 81/81 [00:01<00:00, 76.71it/s, loss=0.691, v_num=4e86, BTC_val\n","Epoch 23:  99%|▉| 80/81 [00:01<00:00, 77.47it/s, loss=0.689, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 81/81 [00:01<00:00, 74.87it/s, loss=0.689, v_num=4e86, BTC_val\u001b[A\n","Epoch 24:  99%|▉| 80/81 [00:01<00:00, 78.58it/s, loss=0.691, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 81/81 [00:01<00:00, 76.13it/s, loss=0.691, v_num=4e86, BTC_val\u001b[A\n","Epoch 25:  99%|▉| 80/81 [00:00<00:00, 81.11it/s, loss=0.693, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 81/81 [00:01<00:00, 79.31it/s, loss=0.693, v_num=4e86, BTC_val\u001b[A\n","Epoch 26:  99%|▉| 80/81 [00:00<00:00, 90.84it/s, loss=0.687, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 81/81 [00:00<00:00, 88.24it/s, loss=0.687, v_num=4e86, BTC_val\u001b[A\n","Epoch 27:  99%|▉| 80/81 [00:00<00:00, 90.02it/s, loss=0.692, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 81/81 [00:00<00:00, 87.74it/s, loss=0.692, v_num=4e86, BTC_val\u001b[A\n","Epoch 28:  99%|▉| 80/81 [00:00<00:00, 90.80it/s, loss=0.689, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 81/81 [00:00<00:00, 88.17it/s, loss=0.689, v_num=4e86, BTC_val\u001b[A\n","Epoch 29:  99%|▉| 80/81 [00:00<00:00, 83.80it/s, loss=0.693, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 81/81 [00:00<00:00, 81.07it/s, loss=0.693, v_num=4e86, BTC_val\u001b[A\n","Epoch 30:  99%|▉| 80/81 [00:01<00:00, 58.45it/s, loss=0.695, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 81/81 [00:01<00:00, 57.67it/s, loss=0.695, v_num=4e86, BTC_val\u001b[A\n","Epoch 31:  99%|▉| 80/81 [00:01<00:00, 74.02it/s, loss=0.686, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 81/81 [00:01<00:00, 72.33it/s, loss=0.686, v_num=4e86, BTC_val\u001b[A\n","Epoch 32:  99%|▉| 80/81 [00:00<00:00, 84.73it/s, loss=0.687, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 81/81 [00:00<00:00, 82.70it/s, loss=0.687, v_num=4e86, BTC_val\u001b[A\n","Epoch 33:  99%|▉| 80/81 [00:00<00:00, 82.46it/s, loss=0.693, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 33: 100%|█| 81/81 [00:01<00:00, 80.61it/s, loss=0.693, v_num=4e86, BTC_val\u001b[A\n","Epoch 34:  99%|▉| 80/81 [00:00<00:00, 91.49it/s, loss=0.689, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 81/81 [00:00<00:00, 89.07it/s, loss=0.689, v_num=4e86, BTC_val\u001b[A\n","Epoch 35:  99%|▉| 80/81 [00:00<00:00, 89.36it/s, loss=0.692, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 35: 100%|█| 81/81 [00:00<00:00, 86.92it/s, loss=0.692, v_num=4e86, BTC_val\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 36:  99%|▉| 80/81 [00:00<00:00, 90.62it/s, loss=0.696, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 36: 100%|█| 81/81 [00:00<00:00, 88.01it/s, loss=0.696, v_num=4e86, BTC_val\u001b[A\n","Epoch 37:  99%|▉| 80/81 [00:00<00:00, 88.52it/s, loss=0.687, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 37: 100%|█| 81/81 [00:00<00:00, 86.11it/s, loss=0.687, v_num=4e86, BTC_val\u001b[A\n","Epoch 38:  99%|▉| 80/81 [00:00<00:00, 91.19it/s, loss=0.694, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 38: 100%|█| 81/81 [00:00<00:00, 88.75it/s, loss=0.694, v_num=4e86, BTC_val\u001b[A\n","Epoch 39:  99%|▉| 80/81 [00:00<00:00, 90.08it/s, loss=0.686, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 39: 100%|█| 81/81 [00:00<00:00, 87.51it/s, loss=0.686, v_num=4e86, BTC_val\u001b[A\n","Epoch 40:  99%|▉| 80/81 [00:00<00:00, 83.32it/s, loss=0.691, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 40: 100%|█| 81/81 [00:01<00:00, 80.83it/s, loss=0.691, v_num=4e86, BTC_val\u001b[A\n","Epoch 41:  99%|▉| 80/81 [00:01<00:00, 72.15it/s, loss=0.696, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 41: 100%|█| 81/81 [00:01<00:00, 70.40it/s, loss=0.696, v_num=4e86, BTC_val\u001b[A\n","Epoch 42:  99%|▉| 80/81 [00:01<00:00, 77.08it/s, loss=0.688, v_num=4e86, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.688. Signaling Trainer to stop.\n","Epoch 42: 100%|█| 81/81 [00:01<00:00, 75.27it/s, loss=0.688, v_num=4e86, BTC_val\n","Epoch 42: 100%|█| 81/81 [00:01<00:00, 75.03it/s, loss=0.688, v_num=4e86, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 136.33it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.4193548262119293,\n"," 'BTC_test_f1': 0.29533159732818604,\n"," 'test_loss': 0.6970884799957275}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 64852\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010532-10fa4e86/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010532-10fa4e86/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.4375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.30435\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.71306\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 42\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3440\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 52\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621461984\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 154\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.53822\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.34643\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.68955\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.55556\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.35714\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.69086\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.41935\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.29533\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.69709\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▃▅▃▂▃▂▄▄▅█▁█▇▃▃▆▅▆▃▂█▃▅▅▆█▄▃▆▅▃▆█▄▅▃▇▅▄▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▄▆▄▃▄▃▄▃▄▅▁█▅▂▂▄▅▇▃▂▅▃▄▄▄▅▃▃▄▄▂▄▅▃▄▃▅▄▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▆▅██▆▅▅▄▁▆▃▃▆▆▄▄▅▅▆▃▅▅▅▄▃▅▅▄▅▆▄▃▅▅▅▃▃▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▄▄▃▃█▁▅▅▇▆▇▇█▇▇▇▆▇▇█▇▆▆▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ██▆█▅▅▄▃▃▆▄▆▅▁▁▂▄▃▂▃▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ██▄▅▄▄▃▄▂▂▂▂▂▂▂▃▂▂▁▂▁▂▂▂▂▂▂▂▂▁▁▂▁▂▁▂▁▁▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▇█▄▂▃▅▅▃▄▃▅▆▄▃▂▂▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_single_lstm__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/10fa4e86\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:06:34.207615: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_single_lstm__multi_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2uzkcaq3\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010632-2uzkcaq3\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 195   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.8 K    Trainable params\n","0         Non-trainable params\n","75.8 K    Total params\n","0.303     Total estimated model params size (MB)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 79/80 [00:00<00:00, 85.90it/s, loss=1.03, v_num=caq3, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.004\n","Epoch 0: 100%|█| 80/80 [00:01<00:00, 78.24it/s, loss=1.03, v_num=caq3, BTC_val_a\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 1: 100%|█| 80/80 [00:01<00:00, 79.51it/s, loss=1.01, v_num=caq3, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 78.81it/s, loss=1.01, v_num=caq3, BTC_val_a\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.991\n","Epoch 2: 100%|█| 80/80 [00:00<00:00, 89.34it/s, loss=1.01, v_num=caq3, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 80/80 [00:00<00:00, 88.45it/s, loss=1.01, v_num=caq3, BTC_val_a\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:01<00:00, 65.37it/s, loss=1.02, v_num=caq3, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:01<00:00, 64.85it/s, loss=1.02, v_num=caq3, BTC_val_a\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 74.51it/s, loss=0.983, v_num=caq3, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.033 >= min_delta = 0.003. New best score: 0.958\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 73.94it/s, loss=0.983, v_num=caq3, BTC_val_\n","Epoch 5: 100%|█| 80/80 [00:00<00:00, 80.84it/s, loss=0.983, v_num=caq3, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:00<00:00, 80.13it/s, loss=0.983, v_num=caq3, BTC_val_\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:01<00:00, 75.02it/s, loss=0.957, v_num=caq3, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:01<00:00, 74.40it/s, loss=0.957, v_num=caq3, BTC_val_\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:01<00:00, 76.80it/s, loss=0.977, v_num=caq3, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:01<00:00, 76.22it/s, loss=0.977, v_num=caq3, BTC_val_\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:01<00:00, 73.65it/s, loss=0.979, v_num=caq3, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:01<00:00, 73.08it/s, loss=0.979, v_num=caq3, BTC_val_\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:01<00:00, 74.30it/s, loss=0.928, v_num=caq3, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:01<00:00, 73.73it/s, loss=0.928, v_num=caq3, BTC_val_\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:01<00:00, 74.87it/s, loss=0.916, v_num=caq3, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:01<00:00, 74.33it/s, loss=0.916, v_num=caq3, BTC_val\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:00<00:00, 88.30it/s, loss=0.968, v_num=caq3, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:00<00:00, 87.48it/s, loss=0.968, v_num=caq3, BTC_val\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:00<00:00, 89.85it/s, loss=0.924, v_num=caq3, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:00<00:00, 89.02it/s, loss=0.924, v_num=caq3, BTC_val\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:00<00:00, 84.82it/s, loss=0.974, v_num=caq3, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:00<00:00, 83.86it/s, loss=0.974, v_num=caq3, BTC_val\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:00<00:00, 82.61it/s, loss=0.984, v_num=caq3, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:00<00:00, 81.55it/s, loss=0.984, v_num=caq3, BTC_val\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:01<00:00, 71.28it/s, loss=0.944, v_num=caq3, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:01<00:00, 70.76it/s, loss=0.944, v_num=caq3, BTC_val\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:01<00:00, 74.36it/s, loss=0.91, v_num=caq3, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:01<00:00, 73.47it/s, loss=0.91, v_num=caq3, BTC_val_\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:00<00:00, 83.81it/s, loss=0.947, v_num=caq3, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:00<00:00, 83.11it/s, loss=0.947, v_num=caq3, BTC_val\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:00<00:00, 89.35it/s, loss=0.959, v_num=caq3, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:00<00:00, 88.55it/s, loss=0.959, v_num=caq3, BTC_val\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:00<00:00, 87.22it/s, loss=0.886, v_num=caq3, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:00<00:00, 86.46it/s, loss=0.886, v_num=caq3, BTC_val\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:00<00:00, 90.41it/s, loss=0.895, v_num=caq3, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:00<00:00, 89.57it/s, loss=0.895, v_num=caq3, BTC_val\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:00<00:00, 89.13it/s, loss=0.892, v_num=caq3, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:00<00:00, 88.31it/s, loss=0.892, v_num=caq3, BTC_val\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:00<00:00, 90.13it/s, loss=0.944, v_num=caq3, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:00<00:00, 89.35it/s, loss=0.944, v_num=caq3, BTC_val\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:00<00:00, 88.78it/s, loss=0.909, v_num=caq3, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:00<00:00, 87.99it/s, loss=0.909, v_num=caq3, BTC_val\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:00<00:00, 88.99it/s, loss=0.952, v_num=caq3, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:00<00:00, 88.15it/s, loss=0.952, v_num=caq3, BTC_val\u001b[A\n","                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.958. Signaling Trainer to stop.\n","Epoch 24: 100%|█| 80/80 [00:00<00:00, 87.86it/s, loss=0.952, v_num=caq3, BTC_val\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 141.36it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.5483871102333069,\n"," 'BTC_test_f1': 0.23566308617591858,\n"," 'test_loss': 0.9320668578147888}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 65034\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010632-2uzkcaq3/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010632-2uzkcaq3/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.31746\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.01229\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 24\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1975\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 34\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462026\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 89\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.55424\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.40886\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.90553\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.04314\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.54839\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.23566\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.93207\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▃▃▆▄▄▅▃▃▄▆▂▄▅▃▅▅▅▅▄▄▄▃▅▆▃▁▃▃▄▇▃▃█▃▃▅▃▇▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▂▂▃▃▂▂▂▂▂▂▂▂▄▃▂▄▃▄▃▂▅▂▄▄▂▁▂▃▄▅▄▃█▄▃▄▄▅▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▇▄▅▆▆▆▆▆▅▅▄▄▆▅▅▂▄▃▄▄█▂▂▅▅▅▆▄▁▇▆▁▅█▅▇▁▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▃▃▄▄▄▄▅▇▆▆▆▆▇▆▆▆▇▆█▅█▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▃▂▂▁▂▂▃▄▅▆▆▇▇▇▇▇▆▇█▇▇▇█▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▇▆▆▅▄▃▃▂▃▂▃▂▂▃▂▁▂▁▂▂▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▄▄▄▄▄▄▄▄▁▄▄█▄▄█▄█▄▄▄▁█▄█▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▂▂▂▂▂▂▂▂▁▃▃█▃▃█▃█▂▇▆▂█▆█▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▃▂▃▃▁▃▂▄▄▆▇▄▇▅▃▇▅▇▄▅█▄▄▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_single_lstm__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2uzkcaq3\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:07:15.662317: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_single_lstm__multi_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2md3k6a8\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010714-2md3k6a8\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 195   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.8 K    Trainable params\n","0         Non-trainable params\n","75.8 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 80/81 [00:00<00:00, 93.81it/s, loss=1.12, v_num=k6a8, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 0: 100%|█| 81/81 [00:00<00:00, 90.68it/s, loss=1.12, v_num=k6a8, BTC_val_a\u001b[A\n","                                                                                \u001b[AMetric val_loss improved. New best score: 1.301\n","Epoch 1:  99%|▉| 80/81 [00:01<00:00, 78.28it/s, loss=1.1, v_num=k6a8, BTC_val_ac\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.103 >= min_delta = 0.003. New best score: 1.199\n","Epoch 1: 100%|█| 81/81 [00:01<00:00, 75.97it/s, loss=1.1, v_num=k6a8, BTC_val_ac\n","Epoch 2:  99%|▉| 80/81 [00:00<00:00, 81.81it/s, loss=1.12, v_num=k6a8, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 1.189\n","Epoch 2: 100%|█| 81/81 [00:01<00:00, 79.44it/s, loss=1.12, v_num=k6a8, BTC_val_a\n","Epoch 3:  99%|▉| 80/81 [00:01<00:00, 79.28it/s, loss=1.1, v_num=k6a8, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:01<00:00, 77.12it/s, loss=1.1, v_num=k6a8, BTC_val_ac\u001b[A\n","Epoch 4:  99%|▉| 80/81 [00:01<00:00, 76.05it/s, loss=1.11, v_num=k6a8, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:01<00:00, 73.91it/s, loss=1.11, v_num=k6a8, BTC_val_a\u001b[A\n","Epoch 5:  99%|▉| 80/81 [00:00<00:00, 84.25it/s, loss=1.09, v_num=k6a8, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:00<00:00, 82.30it/s, loss=1.09, v_num=k6a8, BTC_val_a\u001b[A\n","Epoch 6:  99%|▉| 80/81 [00:00<00:00, 90.15it/s, loss=1.09, v_num=k6a8, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:00<00:00, 87.82it/s, loss=1.09, v_num=k6a8, BTC_val_a\u001b[A\n","Epoch 7:  99%|▉| 80/81 [00:01<00:00, 75.81it/s, loss=1.09, v_num=k6a8, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:01<00:00, 73.68it/s, loss=1.09, v_num=k6a8, BTC_val_a\u001b[A\n","Epoch 8:  99%|▉| 80/81 [00:01<00:00, 68.70it/s, loss=1.08, v_num=k6a8, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:01<00:00, 67.31it/s, loss=1.08, v_num=k6a8, BTC_val_a\u001b[A\n","Epoch 9:  99%|▉| 80/81 [00:01<00:00, 77.13it/s, loss=1.09, v_num=k6a8, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.033 >= min_delta = 0.003. New best score: 1.155\n","Epoch 9: 100%|█| 81/81 [00:01<00:00, 75.38it/s, loss=1.09, v_num=k6a8, BTC_val_a\n","Epoch 10:  99%|▉| 80/81 [00:00<00:00, 89.14it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:00<00:00, 86.50it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n","Epoch 11:  99%|▉| 80/81 [00:01<00:00, 79.37it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:01<00:00, 77.43it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n","Epoch 12:  99%|▉| 80/81 [00:00<00:00, 86.04it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:00<00:00, 84.02it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n","Epoch 13:  99%|▉| 80/81 [00:00<00:00, 91.42it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:00<00:00, 89.09it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n","Epoch 14:  99%|▉| 80/81 [00:00<00:00, 81.07it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:01<00:00, 79.09it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n","Epoch 15:  99%|▉| 80/81 [00:00<00:00, 81.64it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 1.134\n","Epoch 15: 100%|█| 81/81 [00:01<00:00, 78.87it/s, loss=1.1, v_num=k6a8, BTC_val_a\n","Epoch 16:  99%|▉| 80/81 [00:01<00:00, 73.13it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:01<00:00, 70.64it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n","Epoch 17:  99%|▉| 80/81 [00:01<00:00, 63.96it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:01<00:00, 62.67it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n","Epoch 18:  99%|▉| 80/81 [00:01<00:00, 72.37it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:01<00:00, 70.68it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n","Epoch 19:  99%|▉| 80/81 [00:00<00:00, 82.06it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:01<00:00, 80.04it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n","Epoch 20:  99%|▉| 80/81 [00:00<00:00, 82.28it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:01<00:00, 80.02it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n","Epoch 21:  99%|▉| 80/81 [00:00<00:00, 90.93it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 81/81 [00:00<00:00, 88.35it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n","Epoch 22:  99%|▉| 80/81 [00:00<00:00, 90.37it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 81/81 [00:00<00:00, 87.79it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n","Epoch 23:  99%|▉| 80/81 [00:00<00:00, 89.54it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 81/81 [00:00<00:00, 86.95it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n","Epoch 24:  99%|▉| 80/81 [00:00<00:00, 89.79it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 81/81 [00:00<00:00, 87.41it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n","Epoch 25:  99%|▉| 80/81 [00:00<00:00, 91.33it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 81/81 [00:00<00:00, 88.99it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n","Epoch 26:  99%|▉| 80/81 [00:00<00:00, 90.44it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 81/81 [00:00<00:00, 88.00it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n","Epoch 27:  99%|▉| 80/81 [00:00<00:00, 90.64it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 81/81 [00:00<00:00, 88.19it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n","Epoch 28:  99%|▉| 80/81 [00:00<00:00, 86.51it/s, loss=1.11, v_num=k6a8, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 81/81 [00:00<00:00, 83.86it/s, loss=1.11, v_num=k6a8, BTC_val_\u001b[A\n","Epoch 29:  99%|▉| 80/81 [00:01<00:00, 58.95it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 81/81 [00:01<00:00, 57.96it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n","Epoch 30:  99%|▉| 80/81 [00:00<00:00, 83.17it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 81/81 [00:00<00:00, 81.07it/s, loss=1.1, v_num=k6a8, BTC_val_a\u001b[A\n","Epoch 31:  99%|▉| 80/81 [00:00<00:00, 88.10it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 81/81 [00:00<00:00, 85.89it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n","Epoch 32:  99%|▉| 80/81 [00:00<00:00, 92.25it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 81/81 [00:00<00:00, 89.41it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n","Epoch 33:  99%|▉| 80/81 [00:00<00:00, 90.20it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 33: 100%|█| 81/81 [00:00<00:00, 87.62it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n","Epoch 34:  99%|▉| 80/81 [00:00<00:00, 89.39it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 81/81 [00:00<00:00, 86.96it/s, loss=1.09, v_num=k6a8, BTC_val_\u001b[A\n","Epoch 35:  99%|▉| 80/81 [00:00<00:00, 89.97it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.134. Signaling Trainer to stop.\n","Epoch 35: 100%|█| 81/81 [00:00<00:00, 87.19it/s, loss=1.08, v_num=k6a8, BTC_val_\n","Epoch 35: 100%|█| 81/81 [00:00<00:00, 86.81it/s, loss=1.08, v_num=k6a8, BTC_val_\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 145.81it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.25806450843811035,\n"," 'BTC_test_f1': 0.1367289274930954,\n"," 'test_loss': 1.1771870851516724}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 65178\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010714-2md3k6a8/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010714-2md3k6a8/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.3125\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.16667\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.06331\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 35\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2880\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 45\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462079\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 129\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.37273\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.24619\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.08933\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.44169\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.25806\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.13673\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.17719\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▂▃▄▄▄▁▃▄▆▃▃█▅▄▄▅▄▄▃▄▃▅▆▆▆▄▄▃▄▃▄▄▆▅▅▄▅▄▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▃▃▄▄▁▃▄▄▃▃█▄▄▄▄▄▃▃▄▂▄▅▅▆▃▃▂▃▂▃▄▅▃▅▃▄▃▄▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅▄▄▃▃█▃▄▂▄▄▁▂▃▄▁▂▂▄▄▃▂▂▁▂▃▃▃▃▃▂▃▁▂▂▂▃▂▃▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▃▁▃▄▄▅▇▂▅▅▅█▅▅▄▃▇▅▄▆▅▅▅▇█▅▆▇█▅▇▇▇▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▅█▅▇█▇▆▄▇▆▅▄▇▇▅▆▄▅▂▄▅▂▃▃▂▄▂▄▄▅▃▅▅▅▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▅▃▃▃▂▂▂▃▂▁▂▁▂▂▁▂▂▂▂▂▂▂▁▂▂▁▂▂▂▁▁▁▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁████▁████████████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁████▁████████████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▂▂▄▃▃▃▂▃▁▃▂▅▂▁▁▃▂▃▃▄▁▄▂▂▃▄▅▄▄▂▄█▅██\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_single_lstm__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2md3k6a8\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:08:09.843103: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm__binary_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/32175azg\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010808-32175azg\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 130   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 0: 100%|█| 80/80 [00:01<00:00, 43.06it/s, loss=0.701, v_num=5azg, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.657\n","Epoch 0: 100%|█| 80/80 [00:01<00:00, 42.71it/s, loss=0.701, v_num=5azg, BTC_val_\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 45.55it/s, loss=0.657, v_num=5azg, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 45.25it/s, loss=0.657, v_num=5azg, BTC_val_\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.093 >= min_delta = 0.003. New best score: 0.564\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 44.26it/s, loss=0.618, v_num=5azg, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.032 >= min_delta = 0.003. New best score: 0.532\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 43.93it/s, loss=0.618, v_num=5azg, BTC_val_\n","Epoch 3: 100%|█| 80/80 [00:02<00:00, 33.46it/s, loss=0.58, v_num=5azg, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.063 >= min_delta = 0.003. New best score: 0.469\n","Epoch 3: 100%|█| 80/80 [00:02<00:00, 33.29it/s, loss=0.58, v_num=5azg, BTC_val_a\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 43.25it/s, loss=0.593, v_num=5azg, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 42.89it/s, loss=0.593, v_num=5azg, BTC_val_\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:01<00:00, 44.46it/s, loss=0.577, v_num=5azg, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:01<00:00, 44.18it/s, loss=0.577, v_num=5azg, BTC_val_\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:01<00:00, 43.28it/s, loss=0.641, v_num=5azg, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:01<00:00, 43.02it/s, loss=0.641, v_num=5azg, BTC_val_\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:02<00:00, 39.16it/s, loss=0.605, v_num=5azg, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:02<00:00, 38.93it/s, loss=0.605, v_num=5azg, BTC_val_\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:01<00:00, 44.14it/s, loss=0.602, v_num=5azg, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:01<00:00, 43.87it/s, loss=0.602, v_num=5azg, BTC_val_\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:01<00:00, 43.86it/s, loss=0.594, v_num=5azg, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:01<00:00, 43.57it/s, loss=0.594, v_num=5azg, BTC_val_\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:02<00:00, 39.90it/s, loss=0.569, v_num=5azg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:02<00:00, 39.67it/s, loss=0.569, v_num=5azg, BTC_val\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:01<00:00, 43.14it/s, loss=0.551, v_num=5azg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:01<00:00, 42.86it/s, loss=0.551, v_num=5azg, BTC_val\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:01<00:00, 46.14it/s, loss=0.612, v_num=5azg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:01<00:00, 45.84it/s, loss=0.612, v_num=5azg, BTC_val\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:01<00:00, 45.27it/s, loss=0.614, v_num=5azg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:01<00:00, 44.99it/s, loss=0.614, v_num=5azg, BTC_val\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:01<00:00, 44.96it/s, loss=0.576, v_num=5azg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:01<00:00, 44.66it/s, loss=0.576, v_num=5azg, BTC_val\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:02<00:00, 31.70it/s, loss=0.581, v_num=5azg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:02<00:00, 31.53it/s, loss=0.581, v_num=5azg, BTC_val\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:02<00:00, 36.85it/s, loss=0.588, v_num=5azg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:02<00:00, 36.66it/s, loss=0.588, v_num=5azg, BTC_val\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:02<00:00, 33.90it/s, loss=0.583, v_num=5azg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:02<00:00, 33.73it/s, loss=0.583, v_num=5azg, BTC_val\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:02<00:00, 32.11it/s, loss=0.603, v_num=5azg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:02<00:00, 31.74it/s, loss=0.603, v_num=5azg, BTC_val\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:02<00:00, 37.47it/s, loss=0.543, v_num=5azg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:02<00:00, 37.27it/s, loss=0.543, v_num=5azg, BTC_val\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:01<00:00, 42.53it/s, loss=0.58, v_num=5azg, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:01<00:00, 42.27it/s, loss=0.58, v_num=5azg, BTC_val_\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:02<00:00, 29.83it/s, loss=0.589, v_num=5azg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:02<00:00, 29.67it/s, loss=0.589, v_num=5azg, BTC_val\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:02<00:00, 29.88it/s, loss=0.584, v_num=5azg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:02<00:00, 29.75it/s, loss=0.584, v_num=5azg, BTC_val\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:02<00:00, 33.95it/s, loss=0.521, v_num=5azg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.469. Signaling Trainer to stop.\n","Epoch 23: 100%|█| 80/80 [00:02<00:00, 33.79it/s, loss=0.521, v_num=5azg, BTC_val\n","Epoch 23: 100%|█| 80/80 [00:02<00:00, 33.57it/s, loss=0.521, v_num=5azg, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 88.87it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.7096773982048035,\n"," 'BTC_test_f1': 0.7047099471092224,\n"," 'test_loss': 0.602120041847229}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 65375\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010808-32175azg/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010808-32175azg/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.74603\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.59597\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 23\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1896\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 59\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462147\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 85\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.72447\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.71129\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.56374\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.77778\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.775\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.466\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.70968\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.70471\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.60212\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▂▆▃▆█▃▇▇▇▂▁▆▃▁▇█▅▆▃▆▆▇▃▇▇▆▁▁▃▆▇▁▃▃▃█▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▃▆▃▆█▄▆▆▇▃▂▆▂▂▆█▅▆▄▆▆▇▄▇▆▆▂▂▂▅▇▁▄▄▄█▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅▅▅▃▄▅▄▃▃▆▇▂▄▇▂▁▄▆▅▃▂▅▆▃▁▄▅█▆▃▂▇▆▅▄▂▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▄▆▆▇▇▇▇▇▇█▇▇▇███▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▄▆▇▇▇▇▇█▇█▇▇▇▇██▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▃▃▃▃▂▂▂▂▁▂▂▂▂▂▂▂▂▁▁▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▃▆█▆▆▆▆▆▆▆▆▆▆▃▆▆▆▆▆▆▃▃▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▅▇█▇▇▇▇▇▇▇▇▇▇▅▇▇▇▇▇▇▅▅▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▅▃▁▃▃▄▂▃▃▁▁▃▂▃▂▄▃▃▃▁▄▄▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/32175azg\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:09:23.869660: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm__binary_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3cc6sl3t\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010922-3cc6sl3t\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 130   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 80/81 [00:01<00:00, 46.50it/s, loss=0.713, v_num=sl3t, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.691\n","Epoch 0: 100%|█| 81/81 [00:01<00:00, 45.91it/s, loss=0.713, v_num=sl3t, BTC_val_\n","Epoch 1:  99%|▉| 80/81 [00:01<00:00, 45.46it/s, loss=0.694, v_num=sl3t, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 81/81 [00:01<00:00, 45.08it/s, loss=0.694, v_num=sl3t, BTC_val_\u001b[A\n","Epoch 2:  99%|▉| 80/81 [00:01<00:00, 47.32it/s, loss=0.716, v_num=sl3t, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 81/81 [00:01<00:00, 46.83it/s, loss=0.716, v_num=sl3t, BTC_val_\u001b[A\n","Epoch 3:  99%|▉| 80/81 [00:01<00:00, 44.63it/s, loss=0.685, v_num=sl3t, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:01<00:00, 44.14it/s, loss=0.685, v_num=sl3t, BTC_val_\u001b[A\n","Epoch 4:  99%|▉| 80/81 [00:01<00:00, 41.40it/s, loss=0.683, v_num=sl3t, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:01<00:00, 40.91it/s, loss=0.683, v_num=sl3t, BTC_val_\u001b[A\n","Epoch 5:  99%|▉| 80/81 [00:01<00:00, 45.59it/s, loss=0.706, v_num=sl3t, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:01<00:00, 45.12it/s, loss=0.706, v_num=sl3t, BTC_val_\u001b[A\n","Epoch 6:  99%|▉| 80/81 [00:02<00:00, 39.34it/s, loss=0.694, v_num=sl3t, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:02<00:00, 39.07it/s, loss=0.694, v_num=sl3t, BTC_val_\u001b[A\n","Epoch 7:  99%|▉| 80/81 [00:01<00:00, 45.74it/s, loss=0.691, v_num=sl3t, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:01<00:00, 45.30it/s, loss=0.691, v_num=sl3t, BTC_val_\u001b[A\n","Epoch 8:  99%|▉| 80/81 [00:01<00:00, 46.02it/s, loss=0.684, v_num=sl3t, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:01<00:00, 45.55it/s, loss=0.684, v_num=sl3t, BTC_val_\u001b[A\n","Epoch 9:  99%|▉| 80/81 [00:01<00:00, 45.57it/s, loss=0.692, v_num=sl3t, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:01<00:00, 45.10it/s, loss=0.692, v_num=sl3t, BTC_val_\u001b[A\n","Epoch 10:  99%|▉| 80/81 [00:02<00:00, 33.04it/s, loss=0.692, v_num=sl3t, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:02<00:00, 32.79it/s, loss=0.692, v_num=sl3t, BTC_val\u001b[A\n","Epoch 11:  99%|▉| 80/81 [00:01<00:00, 41.05it/s, loss=0.692, v_num=sl3t, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:01<00:00, 40.63it/s, loss=0.692, v_num=sl3t, BTC_val\u001b[A\n","Epoch 12:  99%|▉| 80/81 [00:01<00:00, 44.33it/s, loss=0.686, v_num=sl3t, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:01<00:00, 43.91it/s, loss=0.686, v_num=sl3t, BTC_val\u001b[A\n","Epoch 13:  99%|▉| 80/81 [00:02<00:00, 39.36it/s, loss=0.687, v_num=sl3t, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:02<00:00, 39.09it/s, loss=0.687, v_num=sl3t, BTC_val\u001b[A\n","Epoch 14:  99%|▉| 80/81 [00:02<00:00, 33.54it/s, loss=0.694, v_num=sl3t, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:02<00:00, 33.36it/s, loss=0.694, v_num=sl3t, BTC_val\u001b[A\n","Epoch 15:  99%|▉| 80/81 [00:01<00:00, 45.83it/s, loss=0.686, v_num=sl3t, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:01<00:00, 43.95it/s, loss=0.686, v_num=sl3t, BTC_val\u001b[A\n","Epoch 16:  99%|▉| 80/81 [00:02<00:00, 34.48it/s, loss=0.69, v_num=sl3t, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:02<00:00, 34.22it/s, loss=0.69, v_num=sl3t, BTC_val_\u001b[A\n","Epoch 17:  99%|▉| 80/81 [00:02<00:00, 39.05it/s, loss=0.693, v_num=sl3t, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:02<00:00, 38.61it/s, loss=0.693, v_num=sl3t, BTC_val\u001b[A\n","Epoch 18:  99%|▉| 80/81 [00:02<00:00, 38.85it/s, loss=0.695, v_num=sl3t, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:02<00:00, 38.59it/s, loss=0.695, v_num=sl3t, BTC_val\u001b[A\n","Epoch 19:  99%|▉| 80/81 [00:02<00:00, 32.96it/s, loss=0.691, v_num=sl3t, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:02<00:00, 32.57it/s, loss=0.691, v_num=sl3t, BTC_val\u001b[A\n","Epoch 20:  99%|▉| 80/81 [00:02<00:00, 36.17it/s, loss=0.689, v_num=sl3t, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.691. Signaling Trainer to stop.\n","Epoch 20: 100%|█| 81/81 [00:02<00:00, 35.97it/s, loss=0.689, v_num=sl3t, BTC_val\n","Epoch 20: 100%|█| 81/81 [00:02<00:00, 35.92it/s, loss=0.689, v_num=sl3t, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 78.72it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.4193548262119293,\n"," 'BTC_test_f1': 0.29533159732818604,\n"," 'test_loss': 0.710812509059906}\n","--------------------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 65576\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010922-3cc6sl3t/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_010922-3cc6sl3t/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.33333\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.70306\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1680\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 51\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462213\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.5398\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.3528\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69123\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.55556\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.35714\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.68851\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.41935\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.29533\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.71081\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▄▅▅▄▃▄▅█▅▄▅▄▄▅▃▁▃▅▅▄▃▃▅▁▆▅▅▅▄▄▅▃▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▃▅▅▃▂▃▅█▄▃▄▃▂▄▂▁▂▃▄▄▂▂▃▁▃▃▃▃▂▂▃▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅▃▅▆▅▅▄▁▄▄▆▄▅▅▅▅▅▅▄▅▅▆▄█▄▅▅▄▅▅▄▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▃▅▁▄▄▆▂▃▆▄▅▇▃▇▆▇▆█▇▆▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▇█▇▇▆▄▄▄▄▄▃▅▅▂▁▁▁▂▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▆▂▃▄▃▂▁▃▂▁▃▂▁▁▂▂▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ██▄▁▆▆▂▅▂▃▂▂▃▄▂▄▄▃▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3cc6sl3t\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:10:28.686761: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm__multi_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1rkw3uss\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011027-1rkw3uss\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 195   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Training:   0%|                                          | 0/80 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0: 100%|█| 80/80 [00:01<00:00, 44.21it/s, loss=1.05, v_num=3uss, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.981\n","Epoch 0: 100%|█| 80/80 [00:01<00:00, 43.93it/s, loss=1.05, v_num=3uss, BTC_val_a\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 46.00it/s, loss=1.05, v_num=3uss, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 45.71it/s, loss=1.05, v_num=3uss, BTC_val_a\u001b[A\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 46.45it/s, loss=1.04, v_num=3uss, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 46.17it/s, loss=1.04, v_num=3uss, BTC_val_a\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:01<00:00, 46.22it/s, loss=1.03, v_num=3uss, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:01<00:00, 45.92it/s, loss=1.03, v_num=3uss, BTC_val_a\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 45.60it/s, loss=1.03, v_num=3uss, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 45.31it/s, loss=1.03, v_num=3uss, BTC_val_a\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:01<00:00, 45.54it/s, loss=1.05, v_num=3uss, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:01<00:00, 45.24it/s, loss=1.05, v_num=3uss, BTC_val_a\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:01<00:00, 45.73it/s, loss=1.02, v_num=3uss, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:01<00:00, 45.45it/s, loss=1.02, v_num=3uss, BTC_val_a\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:02<00:00, 38.43it/s, loss=1.02, v_num=3uss, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:02<00:00, 38.21it/s, loss=1.02, v_num=3uss, BTC_val_a\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:02<00:00, 38.68it/s, loss=0.984, v_num=3uss, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:02<00:00, 38.47it/s, loss=0.984, v_num=3uss, BTC_val_\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:01<00:00, 45.73it/s, loss=1, v_num=3uss, BTC_val_acc=\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:01<00:00, 45.43it/s, loss=1, v_num=3uss, BTC_val_acc=\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:01<00:00, 44.81it/s, loss=1.01, v_num=3uss, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:01<00:00, 44.52it/s, loss=1.01, v_num=3uss, BTC_val_\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:01<00:00, 45.80it/s, loss=1.02, v_num=3uss, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:01<00:00, 45.51it/s, loss=1.02, v_num=3uss, BTC_val_\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 12: 100%|█| 80/80 [00:01<00:00, 44.63it/s, loss=1.01, v_num=3uss, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:01<00:00, 44.34it/s, loss=1.01, v_num=3uss, BTC_val_\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:01<00:00, 44.93it/s, loss=1.03, v_num=3uss, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:01<00:00, 44.63it/s, loss=1.03, v_num=3uss, BTC_val_\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:01<00:00, 42.45it/s, loss=0.995, v_num=3uss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:01<00:00, 42.18it/s, loss=0.995, v_num=3uss, BTC_val\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:01<00:00, 43.77it/s, loss=1.03, v_num=3uss, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:01<00:00, 43.47it/s, loss=1.03, v_num=3uss, BTC_val_\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:01<00:00, 44.22it/s, loss=1.02, v_num=3uss, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:01<00:00, 43.90it/s, loss=1.02, v_num=3uss, BTC_val_\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:02<00:00, 37.29it/s, loss=1, v_num=3uss, BTC_val_acc\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:02<00:00, 36.86it/s, loss=1, v_num=3uss, BTC_val_acc\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:02<00:00, 31.72it/s, loss=1.02, v_num=3uss, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:02<00:00, 31.51it/s, loss=1.02, v_num=3uss, BTC_val_\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:02<00:00, 33.13it/s, loss=1.02, v_num=3uss, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:02<00:00, 32.98it/s, loss=1.02, v_num=3uss, BTC_val_\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:02<00:00, 30.15it/s, loss=1.04, v_num=3uss, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.981. Signaling Trainer to stop.\n","Epoch 20: 100%|█| 80/80 [00:02<00:00, 30.01it/s, loss=1.04, v_num=3uss, BTC_val_\n","Epoch 20: 100%|█| 80/80 [00:02<00:00, 29.98it/s, loss=1.04, v_num=3uss, BTC_val_\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 87.71it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.5483871102333069,\n"," 'BTC_test_f1': 0.23566308617591858,\n"," 'test_loss': 0.941138505935669}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 65761\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011027-1rkw3uss/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011027-1rkw3uss/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.18182\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.13358\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1659\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 49\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462276\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.49802\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.21888\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.01995\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.01299\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.54839\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.23566\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.94114\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▆▄▄▆▅▂▄▄█▇▁▅▆▅▇▃▄▅▁▄▃▃▆▅▅▅▃▅▅▅▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁█▃▅▅▅▂▃▃▆▆▁▄▅▅▆▃▃▄▁▃▃▃▅▄▄▅▃▅▅▄▅▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▂▄▄▃▄▆▆▆▃▁██▂▄▂▅▄▃█▅▅▆▂▃▇▄▆▄▄▄▂▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▅▇▇██▇▇█████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch █▄▅▃▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▃▅▃▃▃▂▂▃▂▃▁▂▁▂▂▂▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▆▇▆▅▇▄▇▅▇▇█▆▇▅▆▅▆▆▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1rkw3uss\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:11:33.562941: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm__multi_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/q7jhhfaz\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011132-q7jhhfaz\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 195   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 0:  99%|▉| 80/81 [00:01<00:00, 41.97it/s, loss=1.11, v_num=hfaz, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.087\n","Epoch 0: 100%|█| 81/81 [00:01<00:00, 41.02it/s, loss=1.11, v_num=hfaz, BTC_val_a\n","Epoch 1:  99%|▉| 80/81 [00:01<00:00, 42.73it/s, loss=1.12, v_num=hfaz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 81/81 [00:01<00:00, 42.31it/s, loss=1.12, v_num=hfaz, BTC_val_a\u001b[A\n","Epoch 2: 100%|█| 81/81 [00:01<00:00, 45.77it/s, loss=1.08, v_num=hfaz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 81/81 [00:01<00:00, 45.42it/s, loss=1.08, v_num=hfaz, BTC_val_a\u001b[A\n","Epoch 3:  99%|▉| 80/81 [00:01<00:00, 47.47it/s, loss=1.09, v_num=hfaz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:01<00:00, 47.06it/s, loss=1.09, v_num=hfaz, BTC_val_a\u001b[A\n","Epoch 4:  99%|▉| 80/81 [00:01<00:00, 40.22it/s, loss=1.11, v_num=hfaz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:02<00:00, 39.97it/s, loss=1.11, v_num=hfaz, BTC_val_a\u001b[A\n","Epoch 5:  99%|▉| 80/81 [00:02<00:00, 37.94it/s, loss=1.09, v_num=hfaz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:02<00:00, 37.64it/s, loss=1.09, v_num=hfaz, BTC_val_a\u001b[A\n","Epoch 6:  99%|▉| 80/81 [00:01<00:00, 41.11it/s, loss=1.1, v_num=hfaz, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:01<00:00, 40.68it/s, loss=1.1, v_num=hfaz, BTC_val_ac\u001b[A\n","Epoch 7:  99%|▉| 80/81 [00:01<00:00, 45.26it/s, loss=1.1, v_num=hfaz, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:01<00:00, 44.88it/s, loss=1.1, v_num=hfaz, BTC_val_ac\u001b[A\n","Epoch 8:  99%|▉| 80/81 [00:01<00:00, 45.81it/s, loss=1.1, v_num=hfaz, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:01<00:00, 45.35it/s, loss=1.1, v_num=hfaz, BTC_val_ac\u001b[A\n","Epoch 9:  99%|▉| 80/81 [00:01<00:00, 45.65it/s, loss=1.09, v_num=hfaz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:01<00:00, 45.24it/s, loss=1.09, v_num=hfaz, BTC_val_a\u001b[A\n","Epoch 10:  99%|▉| 80/81 [00:01<00:00, 44.92it/s, loss=1.09, v_num=hfaz, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:01<00:00, 44.46it/s, loss=1.09, v_num=hfaz, BTC_val_\u001b[A\n","Epoch 11:  99%|▉| 80/81 [00:01<00:00, 44.08it/s, loss=1.1, v_num=hfaz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:01<00:00, 43.74it/s, loss=1.1, v_num=hfaz, BTC_val_a\u001b[A\n","Epoch 12:  99%|▉| 80/81 [00:01<00:00, 42.46it/s, loss=1.11, v_num=hfaz, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:01<00:00, 41.99it/s, loss=1.11, v_num=hfaz, BTC_val_\u001b[A\n","Epoch 13:  99%|▉| 80/81 [00:02<00:00, 35.03it/s, loss=1.1, v_num=hfaz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:02<00:00, 34.83it/s, loss=1.1, v_num=hfaz, BTC_val_a\u001b[A\n","Epoch 14:  99%|▉| 80/81 [00:02<00:00, 34.27it/s, loss=1.1, v_num=hfaz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:02<00:00, 34.13it/s, loss=1.1, v_num=hfaz, BTC_val_a\u001b[A\n","Epoch 15:  99%|▉| 80/81 [00:01<00:00, 44.29it/s, loss=1.08, v_num=hfaz, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:01<00:00, 42.47it/s, loss=1.08, v_num=hfaz, BTC_val_\u001b[A\n","Epoch 16:  99%|▉| 80/81 [00:02<00:00, 36.61it/s, loss=1.09, v_num=hfaz, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:02<00:00, 36.40it/s, loss=1.09, v_num=hfaz, BTC_val_\u001b[A\n","Epoch 17:  99%|▉| 80/81 [00:01<00:00, 44.18it/s, loss=1.09, v_num=hfaz, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:01<00:00, 43.55it/s, loss=1.09, v_num=hfaz, BTC_val_\u001b[A\n","Epoch 18:  99%|▉| 80/81 [00:02<00:00, 33.08it/s, loss=1.1, v_num=hfaz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:02<00:00, 32.75it/s, loss=1.1, v_num=hfaz, BTC_val_a\u001b[A\n","Epoch 19:  99%|▉| 80/81 [00:02<00:00, 28.20it/s, loss=1.1, v_num=hfaz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:02<00:00, 28.01it/s, loss=1.1, v_num=hfaz, BTC_val_a\u001b[A\n","Epoch 20:  99%|▉| 80/81 [00:02<00:00, 33.36it/s, loss=1.11, v_num=hfaz, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.087. Signaling Trainer to stop.\n","Epoch 20: 100%|█| 81/81 [00:02<00:00, 33.22it/s, loss=1.11, v_num=hfaz, BTC_val_\n","Epoch 20: 100%|█| 81/81 [00:02<00:00, 33.17it/s, loss=1.11, v_num=hfaz, BTC_val_\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 101.07it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.25806450843811035,\n"," 'BTC_test_f1': 0.1367289274930954,\n"," 'test_loss': 1.1524884700775146}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 65944\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011132-q7jhhfaz/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011132-q7jhhfaz/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.2967\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.08042\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1680\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 51\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462343\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.3554\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.27011\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09526\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.11437\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.25806\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.13673\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.15249\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▅▆▅▅▄▂▄█▆▄▅▆▃▂▃█▂▄▃▁▃▇▇▅▃▄▃▇▅▃▆▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▆▇▅▅▂▂▄▇▅▅▅▅▃▁▃█▂▄▂▁▃▅▆█▃▄▃▇▅▃▃▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▄▄▄▄▄▅▄▁▃▄▅▄▄▃▅▁▄▃▃▄▄▂▃█▄▃▄▄▃▄▃▅▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▂▃▄▁▄▆▇▃▄▃▅▃█▃▅▆▆▃▄▃▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▆▇▆▅▆▅█▅▇▅▅▄▆▃▄▇▅▄▅▁▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▂▅▄▂▂▃▃▃▂▃▂▂▁▂▂▂▂▃▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ███████▁█████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ███████▁█████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▂▅▅▃▄▅█▄▃▆▇▆██▇▆▅▅▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/q7jhhfaz\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:12:38.247830: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_single_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3si4xqi8\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011236-3si4xqi8\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 130   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.7 K    Trainable params\n","0         Non-trainable params\n","75.7 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 79/80 [00:00<00:00, 88.76it/s, loss=0.626, v_num=xqi8, ETH_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.591\n","Epoch 0: 100%|█| 80/80 [00:00<00:00, 80.25it/s, loss=0.626, v_num=xqi8, ETH_val_\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 77.17it/s, loss=0.604, v_num=xqi8, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.015 >= min_delta = 0.003. New best score: 0.577\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 76.45it/s, loss=0.604, v_num=xqi8, ETH_val_\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 78.85it/s, loss=0.622, v_num=xqi8, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.047 >= min_delta = 0.003. New best score: 0.529\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 78.12it/s, loss=0.622, v_num=xqi8, ETH_val_\n","Epoch 3: 100%|█| 80/80 [00:01<00:00, 69.24it/s, loss=0.568, v_num=xqi8, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:01<00:00, 68.66it/s, loss=0.568, v_num=xqi8, ETH_val_\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 71.90it/s, loss=0.608, v_num=xqi8, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 71.32it/s, loss=0.608, v_num=xqi8, ETH_val_\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:01<00:00, 73.79it/s, loss=0.598, v_num=xqi8, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.522\n","Epoch 5: 100%|█| 80/80 [00:01<00:00, 72.81it/s, loss=0.598, v_num=xqi8, ETH_val_\n","Epoch 6: 100%|█| 80/80 [00:01<00:00, 66.07it/s, loss=0.595, v_num=xqi8, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.517\n","Epoch 6: 100%|█| 80/80 [00:01<00:00, 65.15it/s, loss=0.595, v_num=xqi8, ETH_val_\n","Epoch 7: 100%|█| 80/80 [00:01<00:00, 59.84it/s, loss=0.599, v_num=xqi8, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:01<00:00, 59.07it/s, loss=0.599, v_num=xqi8, ETH_val_\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:01<00:00, 71.82it/s, loss=0.581, v_num=xqi8, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:01<00:00, 71.27it/s, loss=0.581, v_num=xqi8, ETH_val_\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:00<00:00, 85.79it/s, loss=0.584, v_num=xqi8, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:00<00:00, 85.04it/s, loss=0.584, v_num=xqi8, ETH_val_\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:00<00:00, 89.36it/s, loss=0.649, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:00<00:00, 88.56it/s, loss=0.649, v_num=xqi8, ETH_val\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:00<00:00, 90.79it/s, loss=0.555, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:00<00:00, 90.00it/s, loss=0.555, v_num=xqi8, ETH_val\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:00<00:00, 87.21it/s, loss=0.571, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.033 >= min_delta = 0.003. New best score: 0.484\n","Epoch 12: 100%|█| 80/80 [00:00<00:00, 86.43it/s, loss=0.571, v_num=xqi8, ETH_val\n","Epoch 13: 100%|█| 80/80 [00:01<00:00, 66.99it/s, loss=0.554, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:01<00:00, 66.38it/s, loss=0.554, v_num=xqi8, ETH_val\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:01<00:00, 68.32it/s, loss=0.574, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:01<00:00, 67.81it/s, loss=0.574, v_num=xqi8, ETH_val\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:00<00:00, 85.39it/s, loss=0.571, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:00<00:00, 84.60it/s, loss=0.571, v_num=xqi8, ETH_val\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.478\n","Epoch 16: 100%|█| 80/80 [00:00<00:00, 86.73it/s, loss=0.565, v_num=xqi8, ETH_val\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:00<00:00, 85.89it/s, loss=0.565, v_num=xqi8, ETH_val\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:01<00:00, 76.86it/s, loss=0.567, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:01<00:00, 76.15it/s, loss=0.567, v_num=xqi8, ETH_val\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:00<00:00, 85.43it/s, loss=0.595, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:00<00:00, 84.68it/s, loss=0.595, v_num=xqi8, ETH_val\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:00<00:00, 90.19it/s, loss=0.572, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:00<00:00, 89.45it/s, loss=0.572, v_num=xqi8, ETH_val\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:00<00:00, 90.07it/s, loss=0.607, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:00<00:00, 89.26it/s, loss=0.607, v_num=xqi8, ETH_val\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:00<00:00, 89.82it/s, loss=0.584, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:00<00:00, 89.05it/s, loss=0.584, v_num=xqi8, ETH_val\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:00<00:00, 88.68it/s, loss=0.579, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:00<00:00, 87.90it/s, loss=0.579, v_num=xqi8, ETH_val\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 23: 100%|█| 80/80 [00:00<00:00, 90.38it/s, loss=0.603, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:00<00:00, 89.49it/s, loss=0.603, v_num=xqi8, ETH_val\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:00<00:00, 81.54it/s, loss=0.606, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:00<00:00, 80.86it/s, loss=0.606, v_num=xqi8, ETH_val\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:01<00:00, 70.23it/s, loss=0.555, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:01<00:00, 69.69it/s, loss=0.555, v_num=xqi8, ETH_val\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:01<00:00, 79.14it/s, loss=0.566, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:01<00:00, 78.07it/s, loss=0.566, v_num=xqi8, ETH_val\u001b[A\n","Epoch 27: 100%|█| 80/80 [00:01<00:00, 76.24it/s, loss=0.565, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 80/80 [00:01<00:00, 75.61it/s, loss=0.565, v_num=xqi8, ETH_val\u001b[A\n","Epoch 28: 100%|█| 80/80 [00:00<00:00, 81.66it/s, loss=0.53, v_num=xqi8, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 80/80 [00:00<00:00, 81.01it/s, loss=0.53, v_num=xqi8, ETH_val_\u001b[A\n","Epoch 29: 100%|█| 80/80 [00:00<00:00, 88.35it/s, loss=0.553, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 80/80 [00:00<00:00, 87.61it/s, loss=0.553, v_num=xqi8, ETH_val\u001b[A\n","Epoch 30: 100%|█| 80/80 [00:00<00:00, 89.26it/s, loss=0.566, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 80/80 [00:00<00:00, 88.42it/s, loss=0.566, v_num=xqi8, ETH_val\u001b[A\n","Epoch 31: 100%|█| 80/80 [00:00<00:00, 89.71it/s, loss=0.581, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 80/80 [00:00<00:00, 88.89it/s, loss=0.581, v_num=xqi8, ETH_val\u001b[A\n","Epoch 32: 100%|█| 80/80 [00:00<00:00, 84.91it/s, loss=0.589, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 80/80 [00:00<00:00, 84.11it/s, loss=0.589, v_num=xqi8, ETH_val\u001b[A\n","Epoch 33: 100%|█| 80/80 [00:00<00:00, 83.82it/s, loss=0.579, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 33: 100%|█| 80/80 [00:00<00:00, 82.93it/s, loss=0.579, v_num=xqi8, ETH_val\u001b[A\n","Epoch 34: 100%|█| 80/80 [00:00<00:00, 82.99it/s, loss=0.577, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 80/80 [00:00<00:00, 82.33it/s, loss=0.577, v_num=xqi8, ETH_val\u001b[A\n","Epoch 35: 100%|█| 80/80 [00:01<00:00, 78.29it/s, loss=0.539, v_num=xqi8, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 35: 100%|█| 80/80 [00:01<00:00, 77.60it/s, loss=0.539, v_num=xqi8, ETH_val\u001b[A\n","                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.478. Signaling Trainer to stop.\n","Epoch 35: 100%|█| 80/80 [00:01<00:00, 77.35it/s, loss=0.539, v_num=xqi8, ETH_val\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 114.98it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'ETH_test_acc': 0.7419354915618896,\n"," 'ETH_test_f1': 0.741647481918335,\n"," 'test_loss': 0.5086269378662109}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 66129\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011236-3si4xqi8/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011236-3si4xqi8/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.6875\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.68627\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.68277\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 35\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2844\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 45\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462401\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 128\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.71813\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.70306\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.55649\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.77778\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.775\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.47895\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.74194\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.74165\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.50863\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▅▅▅▄▁▃▃▅▅▃▅▅▃▃▁▅▄▅▃▅▂▂▃▄█▆▄▅▁▅▄▄▆▃▇▅▅▃▃▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▅▅▅▄▁▂▃▄▅▁▄▄▂▁▁▄▃▅▃▅▁▂▃▄█▆▃▄▁▅▄▄▆▃▇▅▅▃▂▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▃▃▂▆▇▄█▄▃▆▃▂▅▄▇▃▃▃▅▄█▅▄▄▁▂▄▂▆▃▄▃▂▃▁▂▂▇▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▅▆▇▅▇▆▇▇▆▆▇▆▇▇▇▇▇█▇▇▇██▇▇▇▇▇█▇▇▇▇█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▅▆▆▆▇▆▇▇▆▆▇▇▇▇▇▇▇█▇▇▆███▇█▇▇▇▇▇█▇█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▃▃▃▄▃▃▂▃▃▃▃▂▂▃▃▂▂▂▂▂▂▁▂▂▂▁▁▁▁▂▂▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▁▃▆▃▆▃▁▁▁▁▁█▁▆█▁▁▁▃▃▃▆▁▆▃▆▆▃▆▆▃▁▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▄▆▄▆▄▁▁▁▂▂█▂▆█▁▁▂▄▄▄▆▁▆▄▆▆▄▆▆▄▁▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▇▄▄▅▄▃▄▅▅▅▅▁▃▃▁▆▅▃▄▃▂▂▅▄▁▁▂▃▂▂▃▄▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_single_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3si4xqi8\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:13:31.377867: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_single_lstm_loss_weighted_binary_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/6arfgw3b\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011330-6arfgw3b\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 130   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.7 K    Trainable params\n","0         Non-trainable params\n","75.7 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n"],"name":"stdout"},{"output_type":"stream","text":["/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 80/81 [00:00<00:00, 90.62it/s, loss=0.703, v_num=gw3b, ETH_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.603\n","Epoch 0: 100%|█| 81/81 [00:00<00:00, 87.76it/s, loss=0.703, v_num=gw3b, ETH_val_\n","Epoch 1:  99%|▉| 80/81 [00:01<00:00, 79.59it/s, loss=0.709, v_num=gw3b, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 81/81 [00:01<00:00, 77.45it/s, loss=0.709, v_num=gw3b, ETH_val_\u001b[A\n","Epoch 2:  99%|▉| 80/81 [00:00<00:00, 88.78it/s, loss=0.696, v_num=gw3b, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.038 >= min_delta = 0.003. New best score: 0.565\n","Epoch 2: 100%|█| 81/81 [00:00<00:00, 86.33it/s, loss=0.696, v_num=gw3b, ETH_val_\n","Epoch 3:  99%|▉| 80/81 [00:00<00:00, 87.95it/s, loss=0.693, v_num=gw3b, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:00<00:00, 85.21it/s, loss=0.693, v_num=gw3b, ETH_val_\u001b[A\n","Epoch 4:  99%|▉| 80/81 [00:00<00:00, 80.35it/s, loss=0.699, v_num=gw3b, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:01<00:00, 78.08it/s, loss=0.699, v_num=gw3b, ETH_val_\u001b[A\n","Epoch 5:  99%|▉| 80/81 [00:00<00:00, 83.82it/s, loss=0.689, v_num=gw3b, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:00<00:00, 81.61it/s, loss=0.689, v_num=gw3b, ETH_val_\u001b[A\n","Epoch 6:  99%|▉| 80/81 [00:01<00:00, 72.93it/s, loss=0.698, v_num=gw3b, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:01<00:00, 70.08it/s, loss=0.698, v_num=gw3b, ETH_val_\u001b[A\n","Epoch 7:  99%|▉| 80/81 [00:01<00:00, 77.20it/s, loss=0.701, v_num=gw3b, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:01<00:00, 75.33it/s, loss=0.701, v_num=gw3b, ETH_val_\u001b[A\n","Epoch 8:  99%|▉| 80/81 [00:00<00:00, 83.69it/s, loss=0.692, v_num=gw3b, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:00<00:00, 81.61it/s, loss=0.692, v_num=gw3b, ETH_val_\u001b[A\n","Epoch 9:  99%|▉| 80/81 [00:00<00:00, 92.33it/s, loss=0.701, v_num=gw3b, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:00<00:00, 89.82it/s, loss=0.701, v_num=gw3b, ETH_val_\u001b[A\n","Epoch 10:  99%|▉| 80/81 [00:00<00:00, 92.13it/s, loss=0.692, v_num=gw3b, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:00<00:00, 89.79it/s, loss=0.692, v_num=gw3b, ETH_val\u001b[A\n","Epoch 11:  99%|▉| 80/81 [00:00<00:00, 92.76it/s, loss=0.694, v_num=gw3b, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:00<00:00, 90.29it/s, loss=0.694, v_num=gw3b, ETH_val\u001b[A\n","Epoch 12:  99%|▉| 80/81 [00:00<00:00, 92.13it/s, loss=0.695, v_num=gw3b, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:00<00:00, 89.78it/s, loss=0.695, v_num=gw3b, ETH_val\u001b[A\n","Epoch 13:  99%|▉| 80/81 [00:00<00:00, 90.13it/s, loss=0.698, v_num=gw3b, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:00<00:00, 87.51it/s, loss=0.698, v_num=gw3b, ETH_val\u001b[A\n","Epoch 14:  99%|▉| 80/81 [00:00<00:00, 91.94it/s, loss=0.695, v_num=gw3b, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:00<00:00, 89.32it/s, loss=0.695, v_num=gw3b, ETH_val\u001b[A\n","Epoch 15:  99%|▉| 80/81 [00:00<00:00, 89.68it/s, loss=0.69, v_num=gw3b, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:00<00:00, 82.14it/s, loss=0.69, v_num=gw3b, ETH_val_\u001b[A\n","Epoch 16:  99%|▉| 80/81 [00:00<00:00, 82.31it/s, loss=0.693, v_num=gw3b, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:01<00:00, 80.22it/s, loss=0.693, v_num=gw3b, ETH_val\u001b[A\n","Epoch 17:  99%|▉| 80/81 [00:00<00:00, 88.18it/s, loss=0.695, v_num=gw3b, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:00<00:00, 85.84it/s, loss=0.695, v_num=gw3b, ETH_val\u001b[A\n","Epoch 18:  99%|▉| 80/81 [00:01<00:00, 78.39it/s, loss=0.702, v_num=gw3b, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:01<00:00, 75.81it/s, loss=0.702, v_num=gw3b, ETH_val\u001b[A\n","Epoch 19:  99%|▉| 80/81 [00:01<00:00, 75.26it/s, loss=0.699, v_num=gw3b, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:01<00:00, 73.51it/s, loss=0.699, v_num=gw3b, ETH_val\u001b[A\n","Epoch 20:  99%|▉| 80/81 [00:00<00:00, 85.83it/s, loss=0.695, v_num=gw3b, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:00<00:00, 83.68it/s, loss=0.695, v_num=gw3b, ETH_val\u001b[A\n","Epoch 21:  99%|▉| 80/81 [00:00<00:00, 90.58it/s, loss=0.696, v_num=gw3b, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 81/81 [00:00<00:00, 88.28it/s, loss=0.696, v_num=gw3b, ETH_val\u001b[A\n","Epoch 22:  99%|▉| 80/81 [00:00<00:00, 89.74it/s, loss=0.694, v_num=gw3b, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.565. Signaling Trainer to stop.\n","Epoch 22: 100%|█| 81/81 [00:00<00:00, 87.24it/s, loss=0.694, v_num=gw3b, ETH_val\n","Epoch 22: 100%|█| 81/81 [00:00<00:00, 87.00it/s, loss=0.694, v_num=gw3b, ETH_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 147.78it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'ETH_test_acc': 0.6129032373428345,\n"," 'ETH_test_f1': 0.3793548345565796,\n"," 'test_loss': 0.6772454977035522}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 66324\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011330-6arfgw3b/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011330-6arfgw3b/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.4375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.417\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.70312\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 22\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1840\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 31\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462441\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 82\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.50906\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.49335\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69379\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.88889\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.47059\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.65843\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.37935\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.67725\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▃▃▃▄▄▂▃▅▃▃▂▃▃▄▃▅▃▁▄▅▅▆▅▁▄▅▃█▆▅▆▃▅▃▄▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▃▃▃▄▄▂▃▅▃▃▂▃▂▃▃▅▂▁▄▅▅▆▅▁▄▄▃█▆▅▆▂▅▃▄▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆█▆▃▂▆▂▂▅▄▆▄▄▃▅▄▄▆▅▃▃▃▃▄▃▄▅▁▂▂▃▇▃▄▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▂▁█▄▃▇▄▄▄▅▆▄▅▆▃▃▆▆▃▇▆▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▂▁▅▂▂█▄▅▃▅▆▃▅▆▅▃▇█▂█▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▂▄▄▁▂▂▂▂▁▂▂▁▂▁▁▁▂▁▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▃▆▁▄▇▃▄▆▆▃▆█▆█▇▆▇▆█▆▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_single_lstm_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/6arfgw3b\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:14:10.809025: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_single_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2bbz7omm\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011409-2bbz7omm\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 195   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.8 K    Trainable params\n","0         Non-trainable params\n","75.8 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 79/80 [00:01<00:00, 74.01it/s, loss=1.11, v_num=7omm, ETH_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.082\n","Epoch 0: 100%|█| 80/80 [00:01<00:00, 67.61it/s, loss=1.11, v_num=7omm, ETH_val_a\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 67.99it/s, loss=1.1, v_num=7omm, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 67.48it/s, loss=1.1, v_num=7omm, ETH_val_ac\u001b[A\n","Epoch 2: 100%|█| 80/80 [00:00<00:00, 80.71it/s, loss=1.09, v_num=7omm, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 80/80 [00:00<00:00, 80.06it/s, loss=1.09, v_num=7omm, ETH_val_a\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:00<00:00, 89.22it/s, loss=1.04, v_num=7omm, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.023 >= min_delta = 0.003. New best score: 1.060\n","Epoch 3: 100%|█| 80/80 [00:00<00:00, 88.33it/s, loss=1.04, v_num=7omm, ETH_val_a\n","Epoch 4: 100%|█| 80/80 [00:00<00:00, 81.74it/s, loss=0.993, v_num=7omm, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:00<00:00, 80.68it/s, loss=0.993, v_num=7omm, ETH_val_\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:01<00:00, 77.80it/s, loss=1.04, v_num=7omm, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 1.055\n","Epoch 5: 100%|█| 80/80 [00:01<00:00, 77.00it/s, loss=1.04, v_num=7omm, ETH_val_a\n","Epoch 6: 100%|█| 80/80 [00:01<00:00, 77.63it/s, loss=0.978, v_num=7omm, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.019 >= min_delta = 0.003. New best score: 1.036\n","Epoch 6: 100%|█| 80/80 [00:01<00:00, 76.96it/s, loss=0.978, v_num=7omm, ETH_val_\n","Epoch 7: 100%|█| 80/80 [00:01<00:00, 70.77it/s, loss=0.941, v_num=7omm, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:01<00:00, 69.81it/s, loss=0.941, v_num=7omm, ETH_val_\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:01<00:00, 62.06it/s, loss=0.999, v_num=7omm, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:01<00:00, 61.51it/s, loss=0.999, v_num=7omm, ETH_val_\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:01<00:00, 74.38it/s, loss=0.998, v_num=7omm, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:01<00:00, 73.77it/s, loss=0.998, v_num=7omm, ETH_val_\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:00<00:00, 86.21it/s, loss=0.945, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:00<00:00, 85.38it/s, loss=0.945, v_num=7omm, ETH_val\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:00<00:00, 88.23it/s, loss=0.918, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.052 >= min_delta = 0.003. New best score: 0.984\n","Epoch 11: 100%|█| 80/80 [00:00<00:00, 87.47it/s, loss=0.918, v_num=7omm, ETH_val\n","Epoch 12: 100%|█| 80/80 [00:00<00:00, 84.65it/s, loss=0.934, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:00<00:00, 83.90it/s, loss=0.934, v_num=7omm, ETH_val\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:01<00:00, 76.25it/s, loss=0.996, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:01<00:00, 75.66it/s, loss=0.996, v_num=7omm, ETH_val\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:00<00:00, 87.50it/s, loss=0.946, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:00<00:00, 86.67it/s, loss=0.946, v_num=7omm, ETH_val\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:00<00:00, 88.94it/s, loss=0.946, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:00<00:00, 88.13it/s, loss=0.946, v_num=7omm, ETH_val\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:00<00:00, 88.39it/s, loss=0.974, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:00<00:00, 87.49it/s, loss=0.974, v_num=7omm, ETH_val\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:00<00:00, 86.07it/s, loss=0.935, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:00<00:00, 85.33it/s, loss=0.935, v_num=7omm, ETH_val\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:00<00:00, 89.20it/s, loss=0.994, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:00<00:00, 88.44it/s, loss=0.994, v_num=7omm, ETH_val\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:00<00:00, 88.62it/s, loss=0.976, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:00<00:00, 87.85it/s, loss=0.976, v_num=7omm, ETH_val\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:00<00:00, 91.05it/s, loss=0.913, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:00<00:00, 90.19it/s, loss=0.913, v_num=7omm, ETH_val\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:00<00:00, 88.18it/s, loss=0.943, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:00<00:00, 87.36it/s, loss=0.943, v_num=7omm, ETH_val\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:01<00:00, 69.94it/s, loss=0.934, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:01<00:00, 69.37it/s, loss=0.934, v_num=7omm, ETH_val\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:01<00:00, 69.75it/s, loss=0.949, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:01<00:00, 69.26it/s, loss=0.949, v_num=7omm, ETH_val\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:00<00:00, 84.73it/s, loss=0.945, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:00<00:00, 84.03it/s, loss=0.945, v_num=7omm, ETH_val\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:00<00:00, 91.76it/s, loss=0.983, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:00<00:00, 90.93it/s, loss=0.983, v_num=7omm, ETH_val\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:00<00:00, 87.78it/s, loss=0.942, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:00<00:00, 86.99it/s, loss=0.942, v_num=7omm, ETH_val\u001b[A\n","Epoch 27: 100%|█| 80/80 [00:00<00:00, 90.15it/s, loss=0.892, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 80/80 [00:00<00:00, 89.36it/s, loss=0.892, v_num=7omm, ETH_val\u001b[A\n","Epoch 28: 100%|█| 80/80 [00:00<00:00, 87.03it/s, loss=1, v_num=7omm, ETH_val_acc\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 80/80 [00:00<00:00, 86.26it/s, loss=1, v_num=7omm, ETH_val_acc\u001b[A\n","Epoch 29: 100%|█| 80/80 [00:00<00:00, 89.36it/s, loss=0.944, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 80/80 [00:00<00:00, 88.57it/s, loss=0.944, v_num=7omm, ETH_val\u001b[A\n","Epoch 30: 100%|█| 80/80 [00:00<00:00, 88.20it/s, loss=0.961, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 80/80 [00:00<00:00, 87.42it/s, loss=0.961, v_num=7omm, ETH_val\u001b[A\n","Epoch 31: 100%|█| 80/80 [00:00<00:00, 86.67it/s, loss=0.898, v_num=7omm, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 80/80 [00:00<00:00, 85.89it/s, loss=0.898, v_num=7omm, ETH_val\u001b[A\n","                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.984. Signaling Trainer to stop.\n","Epoch 31: 100%|█| 80/80 [00:00<00:00, 85.60it/s, loss=0.898, v_num=7omm, ETH_val\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 126.58it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'ETH_test_acc': 0.5161290168762207,\n"," 'ETH_test_f1': 0.5179212093353271,\n"," 'test_loss': 0.7952907681465149}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 66470\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011409-2bbz7omm/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011409-2bbz7omm/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.4375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.419\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.94675\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 31\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2528\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 40\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462489\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 114\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.49089\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.45903\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.91355\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.33333\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.06603\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.51613\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.51792\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.79529\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▄▄▅▃▅▄▅▅▇▃▇▅▂▄▂▂▄▆▇▃▄▇▅█▅▁▆▄▄▇▆▃▅▇▆▅▆▅█▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▃▄▃▄▂▅▆▇▂▅▂▂▂▂▂▂▆▇▃▃▇▃█▅▁▆▃▃▆▆▂▄▇▅▄▅▅█▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▅▆▅▅▄▃▂▂▄▄▅█▄▇▆▄▄▂▅▄▁▅▃▄▆▂▄▄▃▁▅▃▁▂▃▂▂▂▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▁▁▃▄▄▅▆▆▇▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇██▇█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▁▁▃▄▅▅▆▆▇▇▇▆▇▇█▇▇▇▇▇▇█▇██▇██▇█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ██▇▆▄▄▃▃▃▃▂▂▃▃▂▂▂▂▂▂▁▂▂▂▁▂▂▁▁▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc █▁▄▄▁▄▆▃▄▃▆▃▄▆▆▄▄▄▆▄▄▄▁▄▆▄▆▄▄▁▁▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 █▁▅▅▁▅▇▄▅▄▇▄▅▇▇▅▅▅▇▅▅▅▁▅▇▅▇▅▅▁▁▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄▆▅▄▄▃▃▃▇▄▆▁▂▄▂▅▄▆▇▆▇▅▃▄▅▆▄▂█▅▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_single_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2bbz7omm\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:15:04.086735: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_single_lstm_loss_weighted_multi_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/rhannaj0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011502-rhannaj0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 195   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.8 K    Trainable params\n","0         Non-trainable params\n","75.8 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 0:  99%|▉| 80/81 [00:00<00:00, 91.85it/s, loss=1.12, v_num=naj0, ETH_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.077\n","Epoch 0: 100%|█| 81/81 [00:00<00:00, 88.96it/s, loss=1.12, v_num=naj0, ETH_val_a\n","Epoch 1:  99%|▉| 80/81 [00:01<00:00, 76.68it/s, loss=1.11, v_num=naj0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 81/81 [00:01<00:00, 74.77it/s, loss=1.11, v_num=naj0, ETH_val_a\u001b[A\n","Epoch 2:  99%|▉| 80/81 [00:00<00:00, 83.68it/s, loss=1.11, v_num=naj0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 81/81 [00:00<00:00, 81.80it/s, loss=1.11, v_num=naj0, ETH_val_a\u001b[A\n","Epoch 3:  99%|▉| 80/81 [00:00<00:00, 90.50it/s, loss=1.09, v_num=naj0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:00<00:00, 88.08it/s, loss=1.09, v_num=naj0, ETH_val_a\u001b[A\n","Epoch 4:  99%|▉| 80/81 [00:00<00:00, 90.75it/s, loss=1.1, v_num=naj0, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:00<00:00, 88.18it/s, loss=1.1, v_num=naj0, ETH_val_ac\u001b[A\n","Epoch 5:  99%|▉| 80/81 [00:00<00:00, 92.76it/s, loss=1.1, v_num=naj0, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:00<00:00, 90.15it/s, loss=1.1, v_num=naj0, ETH_val_ac\u001b[A\n","Epoch 6:  99%|▉| 80/81 [00:00<00:00, 90.00it/s, loss=1.11, v_num=naj0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:00<00:00, 87.67it/s, loss=1.11, v_num=naj0, ETH_val_a\u001b[A\n","Epoch 7:  99%|▉| 80/81 [00:00<00:00, 88.06it/s, loss=1.1, v_num=naj0, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:00<00:00, 85.90it/s, loss=1.1, v_num=naj0, ETH_val_ac\u001b[A\n","Epoch 8:  99%|▉| 80/81 [00:00<00:00, 91.54it/s, loss=1.1, v_num=naj0, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:00<00:00, 89.11it/s, loss=1.1, v_num=naj0, ETH_val_ac\u001b[A\n","Epoch 9:  99%|▉| 80/81 [00:00<00:00, 92.64it/s, loss=1.1, v_num=naj0, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:00<00:00, 90.01it/s, loss=1.1, v_num=naj0, ETH_val_ac\u001b[A\n","Epoch 10:  99%|▉| 80/81 [00:00<00:00, 88.77it/s, loss=1.11, v_num=naj0, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:00<00:00, 86.54it/s, loss=1.11, v_num=naj0, ETH_val_\u001b[A\n","Epoch 11:  99%|▉| 80/81 [00:00<00:00, 92.78it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:00<00:00, 90.16it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n","Epoch 12:  99%|▉| 80/81 [00:00<00:00, 93.41it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:00<00:00, 90.98it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n","Epoch 13:  99%|▉| 80/81 [00:00<00:00, 91.78it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:00<00:00, 89.42it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n","Epoch 14:  99%|▉| 80/81 [00:00<00:00, 92.70it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:00<00:00, 90.24it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n","Epoch 15:  99%|▉| 80/81 [00:00<00:00, 90.87it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:00<00:00, 82.92it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n","Epoch 16:  99%|▉| 80/81 [00:00<00:00, 93.03it/s, loss=1.11, v_num=naj0, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:00<00:00, 90.54it/s, loss=1.11, v_num=naj0, ETH_val_\u001b[A\n","Epoch 17:  99%|▉| 80/81 [00:00<00:00, 91.56it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:00<00:00, 87.83it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n","Epoch 18:  99%|▉| 80/81 [00:00<00:00, 88.70it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:00<00:00, 86.34it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n","Epoch 19:  99%|▉| 80/81 [00:00<00:00, 93.11it/s, loss=1.11, v_num=naj0, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:00<00:00, 90.58it/s, loss=1.11, v_num=naj0, ETH_val_\u001b[A\n","Epoch 20:  99%|▉| 80/81 [00:00<00:00, 93.08it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:00<00:00, 90.60it/s, loss=1.1, v_num=naj0, ETH_val_a\u001b[A\n","                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.077. Signaling Trainer to stop.\n","Epoch 20: 100%|█| 81/81 [00:00<00:00, 90.27it/s, loss=1.1, v_num=naj0, ETH_val_a\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 141.71it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'ETH_test_acc': 0.4193548262119293,\n"," 'ETH_test_f1': 0.19648092985153198,\n"," 'test_loss': 1.124062180519104}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 66637\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011502-rhannaj0/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011502-rhannaj0/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.1875\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.19024\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.11129\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1680\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 28\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462530\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.33964\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.30714\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09908\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.2381\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.11437\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.41935\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19648\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.12406\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▆▅▄▂▅▂▂▆▁▅▁▄▁▁▁▃▃▃▂▂▂▅▂█▃▃▄▂▁▂▃▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▅▆▅▁▆▃▅█▂▆▁▆▁▁▁▅▃▃▂▃▄▇▃█▄▅▅▃▂▄▃▄▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▄▁▃▆▆▆▇▄█▃▇▅▆▆▇▄▅▅▅▇▆▄▅▄▅▆▅▅▇▇▆▇▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▄▁▃█▅▂▁▂▄▅▅▄▄▃▂▅▅▅▅▂▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▃▃▅▇█▃▁▄▅▇▇▅▅▅▄▇▅▇█▃▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▄▁▂▃▃▃▂▁▂▂▂▂▂▁▂▁▁▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc █▁██▁▁▁▃▁▁▁▆▁▁▁█▁▁▁▁█\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▅▁▅█▁▁▁▄▁▁▁▆▁▁▁▅▁▁▁▁▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁█▂▄▅▅▆▄▅▅▄▃▅▅▅▄▆▅▅▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_single_lstm_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/rhannaj0\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:15:40.917349: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/27tapgej\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011539-27tapgej\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 130   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0: 100%|█| 80/80 [00:01<00:00, 40.03it/s, loss=0.683, v_num=pgej, ETH_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.658\n","Epoch 0: 100%|█| 80/80 [00:02<00:00, 39.79it/s, loss=0.683, v_num=pgej, ETH_val_\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 41.82it/s, loss=0.637, v_num=pgej, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.101 >= min_delta = 0.003. New best score: 0.557\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 41.57it/s, loss=0.637, v_num=pgej, ETH_val_\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 40.66it/s, loss=0.595, v_num=pgej, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.548\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 40.42it/s, loss=0.595, v_num=pgej, ETH_val_\n","Epoch 3: 100%|█| 80/80 [00:01<00:00, 40.80it/s, loss=0.581, v_num=pgej, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.030 >= min_delta = 0.003. New best score: 0.518\n","Epoch 3: 100%|█| 80/80 [00:01<00:00, 40.54it/s, loss=0.581, v_num=pgej, ETH_val_\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 40.28it/s, loss=0.611, v_num=pgej, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:02<00:00, 39.98it/s, loss=0.611, v_num=pgej, ETH_val_\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.501\n","Epoch 5: 100%|█| 80/80 [00:02<00:00, 38.84it/s, loss=0.593, v_num=pgej, ETH_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:02<00:00, 38.62it/s, loss=0.593, v_num=pgej, ETH_val_\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:02<00:00, 36.97it/s, loss=0.599, v_num=pgej, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.033 >= min_delta = 0.003. New best score: 0.468\n","Epoch 6: 100%|█| 80/80 [00:02<00:00, 36.72it/s, loss=0.599, v_num=pgej, ETH_val_\n","Epoch 7: 100%|█| 80/80 [00:03<00:00, 26.16it/s, loss=0.553, v_num=pgej, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:03<00:00, 26.06it/s, loss=0.553, v_num=pgej, ETH_val_\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 0.465\n","Epoch 8: 100%|█| 80/80 [00:02<00:00, 34.01it/s, loss=0.606, v_num=pgej, ETH_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:02<00:00, 33.83it/s, loss=0.606, v_num=pgej, ETH_val_\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:02<00:00, 34.38it/s, loss=0.574, v_num=pgej, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:02<00:00, 34.22it/s, loss=0.574, v_num=pgej, ETH_val_\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:02<00:00, 28.44it/s, loss=0.565, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 0.451\n","Epoch 10: 100%|█| 80/80 [00:02<00:00, 28.30it/s, loss=0.565, v_num=pgej, ETH_val\n","Epoch 11: 100%|█| 80/80 [00:02<00:00, 33.05it/s, loss=0.573, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:02<00:00, 32.82it/s, loss=0.573, v_num=pgej, ETH_val\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:01<00:00, 40.35it/s, loss=0.571, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:01<00:00, 40.05it/s, loss=0.571, v_num=pgej, ETH_val\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:01<00:00, 41.59it/s, loss=0.579, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:01<00:00, 41.29it/s, loss=0.579, v_num=pgej, ETH_val\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:01<00:00, 40.72it/s, loss=0.605, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:01<00:00, 40.39it/s, loss=0.605, v_num=pgej, ETH_val\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:02<00:00, 30.46it/s, loss=0.56, v_num=pgej, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:02<00:00, 30.33it/s, loss=0.56, v_num=pgej, ETH_val_\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:02<00:00, 28.79it/s, loss=0.583, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:02<00:00, 28.59it/s, loss=0.583, v_num=pgej, ETH_val\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:02<00:00, 30.13it/s, loss=0.574, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:02<00:00, 29.97it/s, loss=0.574, v_num=pgej, ETH_val\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:01<00:00, 41.32it/s, loss=0.537, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:01<00:00, 41.04it/s, loss=0.537, v_num=pgej, ETH_val\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:02<00:00, 29.57it/s, loss=0.613, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:02<00:00, 29.44it/s, loss=0.613, v_num=pgej, ETH_val\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:02<00:00, 35.90it/s, loss=0.58, v_num=pgej, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:02<00:00, 35.72it/s, loss=0.58, v_num=pgej, ETH_val_\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:02<00:00, 30.07it/s, loss=0.578, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:02<00:00, 29.92it/s, loss=0.578, v_num=pgej, ETH_val\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:02<00:00, 39.17it/s, loss=0.565, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:02<00:00, 38.94it/s, loss=0.565, v_num=pgej, ETH_val\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:02<00:00, 38.69it/s, loss=0.532, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:02<00:00, 38.44it/s, loss=0.532, v_num=pgej, ETH_val\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:02<00:00, 31.93it/s, loss=0.562, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:02<00:00, 31.76it/s, loss=0.562, v_num=pgej, ETH_val\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:02<00:00, 39.14it/s, loss=0.574, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:02<00:00, 38.93it/s, loss=0.574, v_num=pgej, ETH_val\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:02<00:00, 29.20it/s, loss=0.553, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:02<00:00, 29.06it/s, loss=0.553, v_num=pgej, ETH_val\u001b[A\n","Epoch 27: 100%|█| 80/80 [00:02<00:00, 37.54it/s, loss=0.534, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 80/80 [00:02<00:00, 37.06it/s, loss=0.534, v_num=pgej, ETH_val\u001b[A\n","Epoch 28: 100%|█| 80/80 [00:02<00:00, 28.82it/s, loss=0.559, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 80/80 [00:02<00:00, 28.69it/s, loss=0.559, v_num=pgej, ETH_val\u001b[A\n","Epoch 29: 100%|█| 80/80 [00:01<00:00, 41.97it/s, loss=0.553, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 80/80 [00:01<00:00, 41.61it/s, loss=0.553, v_num=pgej, ETH_val\u001b[A\n","Epoch 30: 100%|█| 80/80 [00:02<00:00, 37.06it/s, loss=0.531, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.020 >= min_delta = 0.003. New best score: 0.431\n","Epoch 30: 100%|█| 80/80 [00:02<00:00, 36.80it/s, loss=0.531, v_num=pgej, ETH_val\n","Epoch 31: 100%|█| 80/80 [00:02<00:00, 32.38it/s, loss=0.592, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.073 >= min_delta = 0.003. New best score: 0.359\n","Epoch 31: 100%|█| 80/80 [00:02<00:00, 32.22it/s, loss=0.592, v_num=pgej, ETH_val\n","Epoch 32: 100%|█| 80/80 [00:02<00:00, 32.58it/s, loss=0.552, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 80/80 [00:02<00:00, 32.39it/s, loss=0.552, v_num=pgej, ETH_val\u001b[A\n","Epoch 33: 100%|█| 80/80 [00:02<00:00, 34.89it/s, loss=0.553, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 33: 100%|█| 80/80 [00:02<00:00, 34.67it/s, loss=0.553, v_num=pgej, ETH_val\u001b[A\n","Epoch 34: 100%|█| 80/80 [00:02<00:00, 30.26it/s, loss=0.587, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 80/80 [00:02<00:00, 30.12it/s, loss=0.587, v_num=pgej, ETH_val\u001b[A\n","Epoch 35: 100%|█| 80/80 [00:02<00:00, 34.26it/s, loss=0.581, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 35: 100%|█| 80/80 [00:02<00:00, 34.07it/s, loss=0.581, v_num=pgej, ETH_val\u001b[A\n","Epoch 36: 100%|█| 80/80 [00:02<00:00, 38.29it/s, loss=0.558, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 36: 100%|█| 80/80 [00:02<00:00, 38.08it/s, loss=0.558, v_num=pgej, ETH_val\u001b[A\n","Epoch 37: 100%|█| 80/80 [00:02<00:00, 36.69it/s, loss=0.519, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 37: 100%|█| 80/80 [00:02<00:00, 36.47it/s, loss=0.519, v_num=pgej, ETH_val\u001b[A\n","Epoch 38: 100%|█| 80/80 [00:02<00:00, 35.28it/s, loss=0.518, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 38: 100%|█| 80/80 [00:02<00:00, 35.10it/s, loss=0.518, v_num=pgej, ETH_val\u001b[A\n","Epoch 39: 100%|█| 80/80 [00:02<00:00, 30.28it/s, loss=0.575, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 39: 100%|█| 80/80 [00:02<00:00, 30.09it/s, loss=0.575, v_num=pgej, ETH_val\u001b[A\n","Epoch 40: 100%|█| 80/80 [00:02<00:00, 34.20it/s, loss=0.554, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 40: 100%|█| 80/80 [00:02<00:00, 33.99it/s, loss=0.554, v_num=pgej, ETH_val\u001b[A\n","Epoch 41: 100%|█| 80/80 [00:02<00:00, 34.68it/s, loss=0.551, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 41: 100%|█| 80/80 [00:02<00:00, 34.47it/s, loss=0.551, v_num=pgej, ETH_val\u001b[A\n","Epoch 42: 100%|█| 80/80 [00:02<00:00, 36.45it/s, loss=0.558, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 42: 100%|█| 80/80 [00:02<00:00, 36.25it/s, loss=0.558, v_num=pgej, ETH_val\u001b[A\n","Epoch 43: 100%|█| 80/80 [00:02<00:00, 35.17it/s, loss=0.544, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 43: 100%|█| 80/80 [00:02<00:00, 34.89it/s, loss=0.544, v_num=pgej, ETH_val\u001b[A\n","Epoch 44: 100%|█| 80/80 [00:02<00:00, 36.29it/s, loss=0.504, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 44: 100%|█| 80/80 [00:02<00:00, 36.09it/s, loss=0.504, v_num=pgej, ETH_val\u001b[A\n","Epoch 45: 100%|█| 80/80 [00:02<00:00, 39.42it/s, loss=0.537, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 45: 100%|█| 80/80 [00:02<00:00, 39.17it/s, loss=0.537, v_num=pgej, ETH_val\u001b[A\n","Epoch 46: 100%|█| 80/80 [00:02<00:00, 35.24it/s, loss=0.546, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 46: 100%|█| 80/80 [00:02<00:00, 35.00it/s, loss=0.546, v_num=pgej, ETH_val\u001b[A\n","Epoch 47: 100%|█| 80/80 [00:02<00:00, 32.20it/s, loss=0.493, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 47: 100%|█| 80/80 [00:02<00:00, 32.03it/s, loss=0.493, v_num=pgej, ETH_val\u001b[A\n","Epoch 48: 100%|█| 80/80 [00:02<00:00, 34.30it/s, loss=0.438, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 48: 100%|█| 80/80 [00:02<00:00, 34.09it/s, loss=0.438, v_num=pgej, ETH_val\u001b[A\n","Epoch 49: 100%|█| 80/80 [00:02<00:00, 28.12it/s, loss=0.463, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 49: 100%|█| 80/80 [00:02<00:00, 28.00it/s, loss=0.463, v_num=pgej, ETH_val\u001b[A\n","Epoch 50: 100%|█| 80/80 [00:02<00:00, 38.91it/s, loss=0.486, v_num=pgej, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 50: 100%|█| 80/80 [00:02<00:00, 38.64it/s, loss=0.486, v_num=pgej, ETH_val\u001b[A\n","Epoch 51: 100%|█| 80/80 [00:02<00:00, 32.49it/s, loss=0.47, v_num=pgej, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.359. Signaling Trainer to stop.\n","Epoch 51: 100%|█| 80/80 [00:02<00:00, 32.22it/s, loss=0.47, v_num=pgej, ETH_val_\n","Epoch 51: 100%|█| 80/80 [00:02<00:00, 32.17it/s, loss=0.47, v_num=pgej, ETH_val_\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 68.84it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'ETH_test_acc': 0.6774193644523621,\n"," 'ETH_test_f1': 0.6737711429595947,\n"," 'test_loss': 0.5348719358444214}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 66771\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011539-27tapgej/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011539-27tapgej/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.6875\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.68627\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.48098\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 51\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4108\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 130\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462669\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 186\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.76089\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.74942\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.48915\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.77778\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.775\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.57336\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.67742\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.67377\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.53487\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▂▄▆▅▅▂▃▅▅▄▄▅▅▄▅▆▇▄▄▇▅▆▅▇▇▅▅▆█▆▅▅▇▅▇▄▅▁▇▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▃▄▆▅▅▂▃▅▅▄▄▅▅▃▅▆▇▄▄▇▅▆▅▇▇▅▅▆█▆▅▅▇▅▇▄▅▁▇▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▅▃▄▄▇▅▃▄▅█▄▄▇▄▃▃▆▄▂▅▃▄▃▂▅▆▃▁▄▆▃▂▃▂▅▃▇▂▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▃▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▃▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▅▄▄▅▄▄▄▄▄▄▄▄▃▃▃▄▃▃▃▃▃▃▃▃▃▃▂▂▃▂▂▂▂▂▂▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▄▄▄▇▅▇▅▇▇▅▂▄▅▂▅▇▄▂▅▂▂▁▄▇▇▅▇▂▁▅▇█▂▇▂▇▅▅▂▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▄▄▄▇▅▇▅▇▇▅▂▄▅▂▅▇▄▂▅▂▂▁▄▇▇▅▇▂▁▅▇█▂▇▂▇▅▅▂▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▆▅▅▄▄▃▄▃▅▅▄▄▆▄▄▅▆▄▅▅▅▄▃▁▃▂▄▆▃▃▂▆▃▆▃▅▅▇▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/27tapgej\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:18:04.102915: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_loss_weighted_binary_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2j95iuew\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011802-2j95iuew\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 130   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:   0%|                                           | 0/81 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 80/81 [00:01<00:00, 43.07it/s, loss=0.692, v_num=iuew, ETH_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.710\n","Epoch 0: 100%|█| 81/81 [00:01<00:00, 42.40it/s, loss=0.692, v_num=iuew, ETH_val_\n","Epoch 1:  99%|▉| 80/81 [00:02<00:00, 36.02it/s, loss=0.71, v_num=iuew, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.018 >= min_delta = 0.003. New best score: 0.692\n","Epoch 1: 100%|█| 81/81 [00:02<00:00, 35.72it/s, loss=0.71, v_num=iuew, ETH_val_a\n","Epoch 2:  99%|▉| 80/81 [00:01<00:00, 41.70it/s, loss=0.695, v_num=iuew, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 0.681\n","Epoch 2: 100%|█| 81/81 [00:01<00:00, 41.09it/s, loss=0.695, v_num=iuew, ETH_val_\n","Epoch 3:  99%|▉| 80/81 [00:01<00:00, 40.57it/s, loss=0.689, v_num=iuew, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:02<00:00, 40.03it/s, loss=0.689, v_num=iuew, ETH_val_\u001b[A\n","Epoch 4:  99%|▉| 80/81 [00:02<00:00, 35.81it/s, loss=0.69, v_num=iuew, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.025 >= min_delta = 0.003. New best score: 0.656\n","Epoch 4: 100%|█| 81/81 [00:02<00:00, 35.59it/s, loss=0.69, v_num=iuew, ETH_val_a\n","Epoch 5:  99%|▉| 80/81 [00:02<00:00, 39.07it/s, loss=0.7, v_num=iuew, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:02<00:00, 38.72it/s, loss=0.7, v_num=iuew, ETH_val_ac\u001b[A\n","Epoch 6:  99%|▉| 80/81 [00:02<00:00, 39.71it/s, loss=0.697, v_num=iuew, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.027 >= min_delta = 0.003. New best score: 0.629\n","Epoch 6: 100%|█| 81/81 [00:02<00:00, 39.17it/s, loss=0.697, v_num=iuew, ETH_val_\n","Epoch 7:  99%|▉| 80/81 [00:02<00:00, 31.61it/s, loss=0.698, v_num=iuew, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:02<00:00, 31.30it/s, loss=0.698, v_num=iuew, ETH_val_\u001b[A\n","Epoch 8:  99%|▉| 80/81 [00:02<00:00, 28.73it/s, loss=0.698, v_num=iuew, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:02<00:00, 28.59it/s, loss=0.698, v_num=iuew, ETH_val_\u001b[A\n","Epoch 9:  99%|▉| 80/81 [00:02<00:00, 33.41it/s, loss=0.698, v_num=iuew, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:02<00:00, 33.26it/s, loss=0.698, v_num=iuew, ETH_val_\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.622\n","Epoch 10:  99%|▉| 80/81 [00:02<00:00, 37.30it/s, loss=0.694, v_num=iuew, ETH_val\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:02<00:00, 36.88it/s, loss=0.694, v_num=iuew, ETH_val\u001b[A\n","Epoch 11:  99%|▉| 80/81 [00:02<00:00, 33.53it/s, loss=0.695, v_num=iuew, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:02<00:00, 33.40it/s, loss=0.695, v_num=iuew, ETH_val\u001b[A\n","Epoch 12:  99%|▉| 80/81 [00:02<00:00, 30.21it/s, loss=0.694, v_num=iuew, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:02<00:00, 30.02it/s, loss=0.694, v_num=iuew, ETH_val\u001b[A\n","Epoch 13:  99%|▉| 80/81 [00:02<00:00, 34.49it/s, loss=0.697, v_num=iuew, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:02<00:00, 34.16it/s, loss=0.697, v_num=iuew, ETH_val\u001b[A\n","Epoch 14:  99%|▉| 80/81 [00:02<00:00, 37.93it/s, loss=0.691, v_num=iuew, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:02<00:00, 37.39it/s, loss=0.691, v_num=iuew, ETH_val\u001b[A\n","Epoch 15:  99%|▉| 80/81 [00:02<00:00, 34.94it/s, loss=0.692, v_num=iuew, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:02<00:00, 33.90it/s, loss=0.692, v_num=iuew, ETH_val\u001b[A\n","Epoch 16:  99%|▉| 80/81 [00:02<00:00, 39.89it/s, loss=0.697, v_num=iuew, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:02<00:00, 39.58it/s, loss=0.697, v_num=iuew, ETH_val\u001b[A\n","Epoch 17:  99%|▉| 80/81 [00:02<00:00, 36.76it/s, loss=0.7, v_num=iuew, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:02<00:00, 36.55it/s, loss=0.7, v_num=iuew, ETH_val_a\u001b[A\n","Epoch 18:  99%|▉| 80/81 [00:02<00:00, 32.06it/s, loss=0.695, v_num=iuew, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:02<00:00, 31.82it/s, loss=0.695, v_num=iuew, ETH_val\u001b[A\n","Epoch 19:  99%|▉| 80/81 [00:02<00:00, 38.79it/s, loss=0.693, v_num=iuew, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:02<00:00, 38.38it/s, loss=0.693, v_num=iuew, ETH_val\u001b[A\n","Epoch 20:  99%|▉| 80/81 [00:02<00:00, 35.50it/s, loss=0.689, v_num=iuew, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:02<00:00, 35.20it/s, loss=0.689, v_num=iuew, ETH_val\u001b[A\n","Epoch 21:  99%|▉| 80/81 [00:02<00:00, 27.60it/s, loss=0.702, v_num=iuew, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 81/81 [00:02<00:00, 27.55it/s, loss=0.702, v_num=iuew, ETH_val\u001b[A\n","Epoch 22:  99%|▉| 80/81 [00:02<00:00, 38.49it/s, loss=0.697, v_num=iuew, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 81/81 [00:02<00:00, 38.17it/s, loss=0.697, v_num=iuew, ETH_val\u001b[A\n","Epoch 23:  99%|▉| 80/81 [00:02<00:00, 28.60it/s, loss=0.698, v_num=iuew, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 81/81 [00:03<00:00, 26.70it/s, loss=0.698, v_num=iuew, ETH_val\u001b[A\n","Epoch 24:  99%|▉| 80/81 [00:02<00:00, 31.31it/s, loss=0.692, v_num=iuew, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 81/81 [00:02<00:00, 31.22it/s, loss=0.692, v_num=iuew, ETH_val\u001b[A\n","Epoch 25:  99%|▉| 80/81 [00:02<00:00, 35.34it/s, loss=0.694, v_num=iuew, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 81/81 [00:02<00:00, 35.18it/s, loss=0.694, v_num=iuew, ETH_val\u001b[A\n","Epoch 26:  99%|▉| 80/81 [00:02<00:00, 26.96it/s, loss=0.692, v_num=iuew, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 81/81 [00:03<00:00, 26.85it/s, loss=0.692, v_num=iuew, ETH_val\u001b[A\n","Epoch 27:  99%|▉| 80/81 [00:03<00:00, 26.45it/s, loss=0.694, v_num=iuew, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 81/81 [00:03<00:00, 26.27it/s, loss=0.694, v_num=iuew, ETH_val\u001b[A\n","Epoch 28:  99%|▉| 80/81 [00:02<00:00, 27.71it/s, loss=0.69, v_num=iuew, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 81/81 [00:02<00:00, 27.58it/s, loss=0.69, v_num=iuew, ETH_val_\u001b[A\n","Epoch 29:  99%|▉| 80/81 [00:02<00:00, 34.90it/s, loss=0.694, v_num=iuew, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 81/81 [00:02<00:00, 34.70it/s, loss=0.694, v_num=iuew, ETH_val\u001b[A\n","                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.622. Signaling Trainer to stop.\n","Epoch 29: 100%|█| 81/81 [00:02<00:00, 34.64it/s, loss=0.694, v_num=iuew, ETH_val\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 94.47it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'ETH_test_acc': 0.6129032373428345,\n"," 'ETH_test_f1': 0.3793548345565796,\n"," 'test_loss': 0.6798999905586243}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 67132\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011802-2j95iuew/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011802-2j95iuew/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.6\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.58333\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.68506\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 29\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2400\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 81\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462763\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 108\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.48385\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.45631\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69572\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.88889\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.47059\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.688\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.37935\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.6799\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▅▄▅▄▆▃▃▅▆▅▅▆▃▅▅▅▂▅▅▆▅▄▄▇▂▆▃▇▅▅▆▃▆▅▄▅█▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▃▃▆▄▇▂▃▆▇▆▅▅▄▆▅▅▂▄▃▇▄▄▄█▂▇▄█▆▄▇▂▄▅▄▅█▆▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▅▆▃▆▄▄▅▄▄▃▃▁▇▂▃▄▆▅▃▄▃▄▅▃▅▄▄▃▄▅▂▄▃▃▄▄▃▄▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▆▂▆▂▇▄▅▃▃▅▆▅▄▃▄▇▅▂▂█▆▄▄▄▃▃█▄▅▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▃▃▆▃▇▅▆▂▆▃▅▅▅▂▆█▇▂▂▃▇▅▄▁▁▂▆▅▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▄▄▂▃▃▃▂▂▂▂▂▃▂▁▂▂▂▁▁▂▂▂▂▂▁▂▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▆████████████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▇████████████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▇▆▆▄▄▂▅▃▁▅▄▅▅▅▅▅▅▅▅▆▆▅▅▆▆▆▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2j95iuew\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:19:39.235906: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/38lcyiu0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011937-38lcyiu0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 195   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Training: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 0: 100%|█| 80/80 [00:02<00:00, 39.57it/s, loss=1.11, v_num=yiu0, ETH_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.092\n","Epoch 0: 100%|█| 80/80 [00:02<00:00, 39.33it/s, loss=1.11, v_num=yiu0, ETH_val_a\n","Epoch 1: 100%|█| 80/80 [00:02<00:00, 39.80it/s, loss=1.11, v_num=yiu0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.024 >= min_delta = 0.003. New best score: 1.068\n","Epoch 1: 100%|█| 80/80 [00:02<00:00, 39.54it/s, loss=1.11, v_num=yiu0, ETH_val_a\n","Epoch 2: 100%|█| 80/80 [00:02<00:00, 32.97it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 80/80 [00:02<00:00, 32.78it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:02<00:00, 38.22it/s, loss=1.11, v_num=yiu0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:02<00:00, 37.98it/s, loss=1.11, v_num=yiu0, ETH_val_a\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:02<00:00, 30.70it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:02<00:00, 30.54it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:02<00:00, 31.83it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:02<00:00, 31.69it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:02<00:00, 39.82it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:02<00:00, 39.59it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:02<00:00, 39.56it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:02<00:00, 39.33it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:01<00:00, 40.35it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:01<00:00, 40.11it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:02<00:00, 36.53it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:02<00:00, 36.33it/s, loss=1.1, v_num=yiu0, ETH_val_ac\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:02<00:00, 39.97it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:02<00:00, 39.74it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:02<00:00, 33.63it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:02<00:00, 33.42it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:02<00:00, 31.64it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:02<00:00, 31.48it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:02<00:00, 38.15it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:02<00:00, 37.92it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:02<00:00, 38.51it/s, loss=1.11, v_num=yiu0, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:02<00:00, 38.26it/s, loss=1.11, v_num=yiu0, ETH_val_\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:02<00:00, 29.28it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:02<00:00, 29.16it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:02<00:00, 29.67it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:02<00:00, 29.49it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:02<00:00, 35.41it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:02<00:00, 35.19it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:02<00:00, 36.54it/s, loss=1.09, v_num=yiu0, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:02<00:00, 36.34it/s, loss=1.09, v_num=yiu0, ETH_val_\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:02<00:00, 28.98it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:02<00:00, 28.86it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:02<00:00, 26.76it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:03<00:00, 26.63it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:03<00:00, 25.94it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:03<00:00, 25.85it/s, loss=1.1, v_num=yiu0, ETH_val_a\u001b[A\n","                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.068. Signaling Trainer to stop.\n","Epoch 21: 100%|█| 80/80 [00:03<00:00, 25.81it/s, loss=1.1, v_num=yiu0, ETH_val_a\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 91.08it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'ETH_test_acc': 0.4838709533214569,\n"," 'ETH_test_f1': 0.2169238030910492,\n"," 'test_loss': 1.1012004613876343}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 67370\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011937-38lcyiu0/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_011937-38lcyiu0/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.25\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.17172\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.11422\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 21\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1738\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 61\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462838\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 78\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.42993\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.29497\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09807\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.66667\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.26667\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.08673\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.48387\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.21692\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.1012\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▆▆█▇▇▂▃▅▁▃▂▃▅▆▅▃▁▇▆▃▂▅█▃▂▅█▅▃▃█▇▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▄▅▆█▅▂▅▄▂▃▂▃▄▅▂▅▁▇▅▄▃▂▆▄▃▃▅▄▂▃▅▆▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▁▄▇▃▃▆▆▅▇▆▆▆▄▅▅▄▆▄▅▅▆▆▃▅▇▅▅▄▅▅▄▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▂▃▃▁▁▃▅▄▇▄▃▆▆▆▇▇▆█▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▃▁▄▅▁▃▂▆▄▆▅▄██▆▆▅▁▃▆▃▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▃▄▃▃▂▂▂▂▂▂▂▁▂▁▂▂▁▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁█▁███████▅███████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▇▁▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▁█▄▄▄▄▄▄▄▆▄▄▄▄▅▅▄▅▄▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/38lcyiu0\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:20:53.070186: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_loss_weighted_multi_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1n4w9wbk\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012051-1n4w9wbk\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 195   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 80/81 [00:01<00:00, 41.04it/s, loss=1.11, v_num=9wbk, ETH_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.066\n","Epoch 0: 100%|█| 81/81 [00:01<00:00, 40.65it/s, loss=1.11, v_num=9wbk, ETH_val_a\n","Epoch 1:  99%|▉| 80/81 [00:01<00:00, 41.54it/s, loss=1.12, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 81/81 [00:01<00:00, 41.22it/s, loss=1.12, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 2:  99%|▉| 80/81 [00:01<00:00, 41.56it/s, loss=1.12, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 81/81 [00:01<00:00, 41.22it/s, loss=1.12, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 3:  99%|▉| 80/81 [00:02<00:00, 37.40it/s, loss=1.1, v_num=9wbk, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:02<00:00, 37.15it/s, loss=1.1, v_num=9wbk, ETH_val_ac\u001b[A\n","Epoch 4:  99%|▉| 80/81 [00:01<00:00, 41.56it/s, loss=1.1, v_num=9wbk, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:01<00:00, 41.18it/s, loss=1.1, v_num=9wbk, ETH_val_ac\u001b[A\n","Epoch 5:  99%|▉| 80/81 [00:02<00:00, 38.52it/s, loss=1.11, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:02<00:00, 38.26it/s, loss=1.11, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 6:  99%|▉| 80/81 [00:02<00:00, 35.46it/s, loss=1.1, v_num=9wbk, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:02<00:00, 35.18it/s, loss=1.1, v_num=9wbk, ETH_val_ac\u001b[A\n","Epoch 7:  99%|▉| 80/81 [00:01<00:00, 41.75it/s, loss=1.11, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:01<00:00, 41.25it/s, loss=1.11, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 8:  99%|▉| 80/81 [00:01<00:00, 42.02it/s, loss=1.11, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:01<00:00, 41.47it/s, loss=1.11, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 9:  99%|▉| 80/81 [00:01<00:00, 40.62it/s, loss=1.1, v_num=9wbk, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:02<00:00, 40.09it/s, loss=1.1, v_num=9wbk, ETH_val_ac\u001b[A\n","Epoch 10:  99%|▉| 80/81 [00:02<00:00, 27.55it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:02<00:00, 27.48it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 11:  99%|▉| 80/81 [00:02<00:00, 31.90it/s, loss=1.09, v_num=9wbk, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:02<00:00, 31.76it/s, loss=1.09, v_num=9wbk, ETH_val_\u001b[A\n","Metric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 1.058\n","Epoch 12:  99%|▉| 80/81 [00:02<00:00, 34.82it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:02<00:00, 34.58it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 13:  99%|▉| 80/81 [00:02<00:00, 31.02it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:02<00:00, 30.80it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 14:  99%|▉| 80/81 [00:02<00:00, 33.42it/s, loss=1.11, v_num=9wbk, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:02<00:00, 33.14it/s, loss=1.11, v_num=9wbk, ETH_val_\u001b[A\n","Epoch 15:  99%|▉| 80/81 [00:02<00:00, 29.12it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:02<00:00, 28.45it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 16:  99%|▉| 80/81 [00:02<00:00, 30.45it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:02<00:00, 30.16it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 17:  99%|▉| 80/81 [00:01<00:00, 40.14it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:02<00:00, 39.61it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 18:  99%|▉| 80/81 [00:02<00:00, 33.19it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:02<00:00, 33.02it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 19:  99%|▉| 80/81 [00:02<00:00, 37.66it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:02<00:00, 37.34it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 20:  99%|▉| 80/81 [00:02<00:00, 37.04it/s, loss=1.09, v_num=9wbk, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:02<00:00, 36.76it/s, loss=1.09, v_num=9wbk, ETH_val_\u001b[A\n","Epoch 21:  99%|▉| 80/81 [00:02<00:00, 37.40it/s, loss=1.09, v_num=9wbk, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 81/81 [00:02<00:00, 37.02it/s, loss=1.09, v_num=9wbk, ETH_val_\u001b[A\n","Epoch 22:  99%|▉| 80/81 [00:02<00:00, 38.37it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 81/81 [00:02<00:00, 38.12it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 23:  99%|▉| 80/81 [00:03<00:00, 26.52it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 81/81 [00:03<00:00, 26.45it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 24:  99%|▉| 80/81 [00:02<00:00, 34.42it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 81/81 [00:02<00:00, 34.25it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 25:  99%|▉| 80/81 [00:02<00:00, 37.93it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 81/81 [00:02<00:00, 37.69it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 26:  99%|▉| 80/81 [00:02<00:00, 37.65it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 81/81 [00:02<00:00, 37.41it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 27:  99%|▉| 80/81 [00:02<00:00, 36.02it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 81/81 [00:02<00:00, 35.53it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 28:  99%|▉| 80/81 [00:02<00:00, 36.00it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 81/81 [00:02<00:00, 35.69it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 29:  99%|▉| 80/81 [00:02<00:00, 34.26it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 81/81 [00:02<00:00, 33.78it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 30:  99%|▉| 80/81 [00:02<00:00, 34.72it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 81/81 [00:02<00:00, 34.41it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Epoch 31:  99%|▉| 80/81 [00:02<00:00, 37.70it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 81/81 [00:02<00:00, 37.39it/s, loss=1.1, v_num=9wbk, ETH_val_a\u001b[A\n","                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.058. Signaling Trainer to stop.\n","Epoch 31: 100%|█| 81/81 [00:02<00:00, 37.33it/s, loss=1.1, v_num=9wbk, ETH_val_a\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 91.59it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'ETH_test_acc': 0.4193548262119293,\n"," 'ETH_test_f1': 0.19648092985153198,\n"," 'test_loss': 1.093916893005371}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 67582\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012051-1n4w9wbk/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012051-1n4w9wbk/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.36667\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.11474\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 31\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2560\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 82\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462933\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 115\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.32939\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.30131\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.10063\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.2381\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.09221\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.41935\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19648\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.09392\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▅▅▅▃▅▃▆▄▄▄▅▂▆▆▅▅▄█▁▄▂▄▄▅█▆▇▆▅▄▅█▄▅▅▅▅▆▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▄▅▅▃▄▃▆▄▃▄▅▂▃▆▅▅▄█▁▄▃▄▃▄▆▆▇▇▅▄▆▆▄▆▅▄▄▅▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▄█▇▄▆█▄▇▅▅▄▇▁▃▃▃▇▄▆▄▆▅▅▅▃▅▃▄▆▄▆▄▆▅▅▅▆▄▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▅▄▄▄▃▄▂▁▅▃▇▇▄▄▅▆▃▆▅▅▅▇█▆▅▂▄█▇▆▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▃▆▃▂▄▁▂▄▆▅▅█▄▆▅▄▅█▆▅▄▅▇▆▆▃▅▆█▃▃▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▅▅▄▄▃▄▂▂▂▂▂▃▂▂▂▂▂▂▂▁▃▂▃▂▃▁▂▂▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc █▁▄▄████▃▁█████████▁▁▁▁█▁███▄▄██\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 █▁▅▅████▆▁█████████▁▁▁▁█▁███▅▅██\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▂█▆▅▅▅▅▅▅▅▃▁▄▄▄▄▄▄▄▅▅█▆▄▅▄▅▅▅▄▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1n4w9wbk\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:22:29.091323: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_single_lstm__binary_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/xcvozi1q\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012227-xcvozi1q\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 130   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.7 K    Trainable params\n","0         Non-trainable params\n","75.7 K    Total params\n","0.303     Total estimated model params size (MB)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:   0%|                                           | 0/80 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 79/80 [00:01<00:00, 77.46it/s, loss=0.59, v_num=zi1q, ETH_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 0: 100%|█| 80/80 [00:01<00:00, 70.67it/s, loss=0.59, v_num=zi1q, ETH_val_a\u001b[A\n","                                                                                \u001b[AMetric val_loss improved. New best score: 0.535\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 1: 100%|█| 80/80 [00:01<00:00, 72.50it/s, loss=0.534, v_num=zi1q, ETH_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.048 >= min_delta = 0.003. New best score: 0.487\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 71.90it/s, loss=0.534, v_num=zi1q, ETH_val_\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 71.47it/s, loss=0.562, v_num=zi1q, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 70.77it/s, loss=0.562, v_num=zi1q, ETH_val_\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:01<00:00, 64.50it/s, loss=0.604, v_num=zi1q, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:01<00:00, 63.90it/s, loss=0.604, v_num=zi1q, ETH_val_\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 66.98it/s, loss=0.616, v_num=zi1q, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 66.51it/s, loss=0.616, v_num=zi1q, ETH_val_\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:01<00:00, 75.60it/s, loss=0.558, v_num=zi1q, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:01<00:00, 74.78it/s, loss=0.558, v_num=zi1q, ETH_val_\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:01<00:00, 65.99it/s, loss=0.617, v_num=zi1q, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:01<00:00, 65.26it/s, loss=0.617, v_num=zi1q, ETH_val_\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:01<00:00, 65.08it/s, loss=0.596, v_num=zi1q, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:01<00:00, 64.39it/s, loss=0.596, v_num=zi1q, ETH_val_\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:00<00:00, 80.32it/s, loss=0.535, v_num=zi1q, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:01<00:00, 79.67it/s, loss=0.535, v_num=zi1q, ETH_val_\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:01<00:00, 71.42it/s, loss=0.556, v_num=zi1q, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.026 >= min_delta = 0.003. New best score: 0.461\n","Epoch 9: 100%|█| 80/80 [00:01<00:00, 70.85it/s, loss=0.556, v_num=zi1q, ETH_val_\n","Epoch 10: 100%|█| 80/80 [00:00<00:00, 83.54it/s, loss=0.583, v_num=zi1q, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:00<00:00, 82.53it/s, loss=0.583, v_num=zi1q, ETH_val\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:01<00:00, 66.51it/s, loss=0.587, v_num=zi1q, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:01<00:00, 66.00it/s, loss=0.587, v_num=zi1q, ETH_val\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:01<00:00, 77.15it/s, loss=0.607, v_num=zi1q, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:01<00:00, 76.08it/s, loss=0.607, v_num=zi1q, ETH_val\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:01<00:00, 68.72it/s, loss=0.606, v_num=zi1q, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:01<00:00, 68.22it/s, loss=0.606, v_num=zi1q, ETH_val\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:01<00:00, 60.10it/s, loss=0.605, v_num=zi1q, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:01<00:00, 59.60it/s, loss=0.605, v_num=zi1q, ETH_val\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:01<00:00, 73.69it/s, loss=0.58, v_num=zi1q, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:01<00:00, 73.09it/s, loss=0.58, v_num=zi1q, ETH_val_\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:01<00:00, 69.56it/s, loss=0.582, v_num=zi1q, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:01<00:00, 69.07it/s, loss=0.582, v_num=zi1q, ETH_val\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:00<00:00, 84.38it/s, loss=0.581, v_num=zi1q, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:00<00:00, 83.48it/s, loss=0.581, v_num=zi1q, ETH_val\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:01<00:00, 70.61it/s, loss=0.577, v_num=zi1q, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:01<00:00, 70.07it/s, loss=0.577, v_num=zi1q, ETH_val\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:00<00:00, 82.36it/s, loss=0.537, v_num=zi1q, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:00<00:00, 81.38it/s, loss=0.537, v_num=zi1q, ETH_val\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:01<00:00, 70.70it/s, loss=0.585, v_num=zi1q, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:01<00:00, 70.14it/s, loss=0.585, v_num=zi1q, ETH_val\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:00<00:00, 82.81it/s, loss=0.554, v_num=zi1q, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:00<00:00, 81.92it/s, loss=0.554, v_num=zi1q, ETH_val\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:01<00:00, 74.54it/s, loss=0.55, v_num=zi1q, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:01<00:00, 73.99it/s, loss=0.55, v_num=zi1q, ETH_val_\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:01<00:00, 78.29it/s, loss=0.582, v_num=zi1q, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:01<00:00, 77.54it/s, loss=0.582, v_num=zi1q, ETH_val\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:01<00:00, 64.08it/s, loss=0.549, v_num=zi1q, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:01<00:00, 63.47it/s, loss=0.549, v_num=zi1q, ETH_val\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:01<00:00, 67.73it/s, loss=0.574, v_num=zi1q, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:01<00:00, 66.99it/s, loss=0.574, v_num=zi1q, ETH_val\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:00<00:00, 83.10it/s, loss=0.536, v_num=zi1q, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:00<00:00, 82.42it/s, loss=0.536, v_num=zi1q, ETH_val\u001b[A\n","Epoch 27: 100%|█| 80/80 [00:01<00:00, 69.91it/s, loss=0.543, v_num=zi1q, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 80/80 [00:01<00:00, 69.35it/s, loss=0.543, v_num=zi1q, ETH_val\u001b[A\n","Epoch 28: 100%|█| 80/80 [00:00<00:00, 85.85it/s, loss=0.583, v_num=zi1q, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 80/80 [00:00<00:00, 85.11it/s, loss=0.583, v_num=zi1q, ETH_val\u001b[A\n","Epoch 29: 100%|█| 80/80 [00:01<00:00, 69.56it/s, loss=0.55, v_num=zi1q, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.461. Signaling Trainer to stop.\n","Epoch 29: 100%|█| 80/80 [00:01<00:00, 69.04it/s, loss=0.55, v_num=zi1q, ETH_val_\n","Epoch 29: 100%|█| 80/80 [00:01<00:00, 68.84it/s, loss=0.55, v_num=zi1q, ETH_val_\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 111.16it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'ETH_test_acc': 0.7096773982048035,\n"," 'ETH_test_f1': 0.7085039019584656,\n"," 'test_loss': 0.5299782752990723}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 67846\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012227-xcvozi1q/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012227-xcvozi1q/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.9375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.92271\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.42581\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 29\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2370\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 42\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621462989\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 107\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.70942\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.69617\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.56198\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.77778\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.775\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.48858\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.70968\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.7085\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.52998\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▃▇▃▃▆▂▇▆▅▅▅▄▂▅▁▆▄▂▄▃▄▃▆▂▄▄▃▂▅▂▂▇▇▃▅▂▅▆▂█\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▄▇▃▃▆▁▇▆▅▅▅▄▂▅▂▆▄▂▄▄▄▂▆▂▄▄▂▃▅▂▂▇▇▃▅▃▅▆▃█\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅▁▄▅▂▅▁▃▃▂▄▄▇▃▅▄▄▅▄█▄▇▂▅▅▅▅▆▂▄▆▁▃█▃▅▄▃▅▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▆▅▆▆▆▆▇▇▇▆▇▇▆▆▇▇▇▇▇█▇▇█▇▇█▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▇▅▆▆▆▇▇▇▇▆▇▇▆▇█▇▇▇▇██▇█▇██▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▃▄▃▄▃▃▃▂▁▃▂▂▃▂▂▃▃▂▂▂▂▂▁▂▁▁▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▅▆▅▆▅▁▆▃▅█▅██▆▆█▆▅▅▅▆█▆▅▅▆▆▅▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▅▆▅▆▅▁▆▂▅█▅██▆▆█▆▅▅▅▆█▆▅▅▆▆▅▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▃▆▇█▆▄▆▅▁▆▄▅▅▄▅▃▃▅▂▄▁▄▂▂▃▅▄▃▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_single_lstm__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/xcvozi1q\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:23:19.572980: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_single_lstm__binary_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/10m6n47o\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012318-10m6n47o\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 130   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.7 K    Trainable params\n","0         Non-trainable params\n","75.7 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 80/81 [00:01<00:00, 68.46it/s, loss=0.691, v_num=n47o, ETH_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.950\n","Epoch 0: 100%|█| 81/81 [00:01<00:00, 67.00it/s, loss=0.691, v_num=n47o, ETH_val_\n","Epoch 1:  99%|▉| 80/81 [00:01<00:00, 75.97it/s, loss=0.718, v_num=n47o, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.162 >= min_delta = 0.003. New best score: 0.788\n","Epoch 1: 100%|█| 81/81 [00:01<00:00, 73.86it/s, loss=0.718, v_num=n47o, ETH_val_\n","Epoch 2: 100%|█| 81/81 [00:01<00:00, 73.95it/s, loss=0.723, v_num=n47o, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.068 >= min_delta = 0.003. New best score: 0.720\n","Epoch 2: 100%|█| 81/81 [00:01<00:00, 73.30it/s, loss=0.723, v_num=n47o, ETH_val_\n","Epoch 3: 100%|█| 81/81 [00:01<00:00, 65.06it/s, loss=0.7, v_num=n47o, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:01<00:00, 64.37it/s, loss=0.7, v_num=n47o, ETH_val_ac\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:01<00:00, 71.95it/s, loss=0.694, v_num=n47o, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.024 >= min_delta = 0.003. New best score: 0.696\n","Epoch 4: 100%|█| 81/81 [00:01<00:00, 71.11it/s, loss=0.694, v_num=n47o, ETH_val_\n","Epoch 5: 100%|█| 81/81 [00:01<00:00, 68.70it/s, loss=0.698, v_num=n47o, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:01<00:00, 68.19it/s, loss=0.698, v_num=n47o, ETH_val_\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:01<00:00, 74.12it/s, loss=0.692, v_num=n47o, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.025 >= min_delta = 0.003. New best score: 0.671\n","Epoch 6: 100%|█| 81/81 [00:01<00:00, 73.25it/s, loss=0.692, v_num=n47o, ETH_val_\n","Epoch 7: 100%|█| 81/81 [00:01<00:00, 64.97it/s, loss=0.688, v_num=n47o, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 0.660\n","Epoch 7: 100%|█| 81/81 [00:01<00:00, 64.52it/s, loss=0.688, v_num=n47o, ETH_val_\n","Epoch 8: 100%|█| 81/81 [00:01<00:00, 55.59it/s, loss=0.691, v_num=n47o, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:01<00:00, 55.20it/s, loss=0.691, v_num=n47o, ETH_val_\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:01<00:00, 71.43it/s, loss=0.695, v_num=n47o, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:01<00:00, 70.72it/s, loss=0.695, v_num=n47o, ETH_val_\u001b[A\n","Epoch 10:  99%|▉| 80/81 [00:01<00:00, 59.43it/s, loss=0.697, v_num=n47o, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:01<00:00, 58.47it/s, loss=0.697, v_num=n47o, ETH_val\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.049 >= min_delta = 0.003. New best score: 0.611\n","Epoch 11:  99%|▉| 80/81 [00:01<00:00, 64.93it/s, loss=0.699, v_num=n47o, ETH_val\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:01<00:00, 63.34it/s, loss=0.699, v_num=n47o, ETH_val\u001b[A\n","Epoch 12:  99%|▉| 80/81 [00:01<00:00, 71.46it/s, loss=0.695, v_num=n47o, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:01<00:00, 69.85it/s, loss=0.695, v_num=n47o, ETH_val\u001b[A\n","Epoch 13:  99%|▉| 80/81 [00:01<00:00, 54.89it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:01<00:00, 54.23it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n","Epoch 14:  99%|▉| 80/81 [00:01<00:00, 78.95it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:01<00:00, 76.58it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n","Epoch 15:  99%|▉| 80/81 [00:01<00:00, 78.32it/s, loss=0.691, v_num=n47o, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:01<00:00, 76.42it/s, loss=0.691, v_num=n47o, ETH_val\u001b[A\n","Epoch 16:  99%|▉| 80/81 [00:01<00:00, 54.16it/s, loss=0.691, v_num=n47o, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:01<00:00, 53.49it/s, loss=0.691, v_num=n47o, ETH_val\u001b[A\n","Epoch 17:  99%|▉| 80/81 [00:01<00:00, 77.75it/s, loss=0.695, v_num=n47o, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:01<00:00, 75.40it/s, loss=0.695, v_num=n47o, ETH_val\u001b[A\n","Epoch 18:  99%|▉| 80/81 [00:01<00:00, 60.83it/s, loss=0.691, v_num=n47o, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:01<00:00, 59.88it/s, loss=0.691, v_num=n47o, ETH_val\u001b[A\n","Epoch 19:  99%|▉| 80/81 [00:01<00:00, 74.63it/s, loss=0.696, v_num=n47o, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:01<00:00, 72.49it/s, loss=0.696, v_num=n47o, ETH_val\u001b[A\n","Epoch 20:  99%|▉| 80/81 [00:00<00:00, 81.17it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:01<00:00, 79.17it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n","Epoch 21:  99%|▉| 80/81 [00:01<00:00, 57.49it/s, loss=0.696, v_num=n47o, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 81/81 [00:01<00:00, 56.68it/s, loss=0.696, v_num=n47o, ETH_val\u001b[A\n","Epoch 22:  99%|▉| 80/81 [00:00<00:00, 87.95it/s, loss=0.692, v_num=n47o, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 81/81 [00:00<00:00, 85.03it/s, loss=0.692, v_num=n47o, ETH_val\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 23:  99%|▉| 80/81 [00:01<00:00, 57.91it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 81/81 [00:01<00:00, 57.15it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n","Epoch 24:  99%|▉| 80/81 [00:01<00:00, 79.18it/s, loss=0.693, v_num=n47o, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 81/81 [00:01<00:00, 76.80it/s, loss=0.693, v_num=n47o, ETH_val\u001b[A\n","Epoch 25:  99%|▉| 80/81 [00:01<00:00, 79.61it/s, loss=0.692, v_num=n47o, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 81/81 [00:01<00:00, 77.81it/s, loss=0.692, v_num=n47o, ETH_val\u001b[A\n","Epoch 26:  99%|▉| 80/81 [00:01<00:00, 68.33it/s, loss=0.693, v_num=n47o, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 81/81 [00:01<00:00, 66.80it/s, loss=0.693, v_num=n47o, ETH_val\u001b[A\n","Epoch 27:  99%|▉| 80/81 [00:01<00:00, 74.08it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 81/81 [00:01<00:00, 71.85it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n","Epoch 28:  99%|▉| 80/81 [00:01<00:00, 66.42it/s, loss=0.693, v_num=n47o, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 81/81 [00:01<00:00, 64.99it/s, loss=0.693, v_num=n47o, ETH_val\u001b[A\n","Epoch 29:  99%|▉| 80/81 [00:00<00:00, 82.80it/s, loss=0.695, v_num=n47o, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 81/81 [00:01<00:00, 80.24it/s, loss=0.695, v_num=n47o, ETH_val\u001b[A\n","Epoch 30:  99%|▉| 80/81 [00:01<00:00, 72.64it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.611. Signaling Trainer to stop.\n","Epoch 30: 100%|█| 81/81 [00:01<00:00, 70.96it/s, loss=0.694, v_num=n47o, ETH_val\n","Epoch 30: 100%|█| 81/81 [00:01<00:00, 70.76it/s, loss=0.694, v_num=n47o, ETH_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 126.79it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'ETH_test_acc': 0.6129032373428345,\n"," 'ETH_test_f1': 0.3793548345565796,\n"," 'test_loss': 0.6854448914527893}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 68005\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012318-10m6n47o/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012318-10m6n47o/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.4375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.30435\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.68893\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 30\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2480\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 46\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463044\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 111\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.50591\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.39304\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69312\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.88889\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.47059\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.68576\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.37935\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.68544\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▃▃▄▃▆▃▃▅▄▄▆▆▃▃▄▄▃▅▅▃▃▇▄▃▄▆▃▄▃█▅▁▃█▄▆▄▆▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▃▃▄▄▆▃▃▂▄▄▄▇▂▃▄▄▃▅▅▁▄▇▃▄▄▅▃▄▃█▄▁▁▆▄▃▂▃▄▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▄▂▅▄▆▂▃▃▃▂▂▄▄▃▃▄▃▃▄▄▁▄▄▄▃▄▄▄▂▃▅▅▂▃▂▃▂▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch █▄▆▆▄▆▅▆▆█▇▂▄▇▅▆▇▆▃▆▅▅▆▅▃▇▁▇▆▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch █▆▇▆▇▆▆▇▆▆▅▆▄▇▆▇▆▅▆▃▅▅▄▃▆▄▄▂▁▂▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ▅█▆▄▃▃▃▁▃▂▂▃▂▁▂▂▁▂▂▂▂▂▁▂▂▁▂▁▁▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▁▁▁▁▁███▁███████▁▃████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▁▁▁▁███▁███████▁▅████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▅▃▃▃▃▂▂▂▃▁▂▃▂▂▃▂▃▃▃▃▃▂▃▃▃▃▂▂▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_single_lstm__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/10m6n47o\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:24:15.590597: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_single_lstm__multi_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2pc295sf\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012412-2pc295sf\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 195   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.8 K    Trainable params\n","0         Non-trainable params\n","75.8 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 0:  99%|▉| 79/80 [00:01<00:00, 66.17it/s, loss=1.08, v_num=95sf, ETH_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.977\n","Epoch 0: 100%|█| 80/80 [00:01<00:00, 60.54it/s, loss=1.08, v_num=95sf, ETH_val_a\n","Epoch 1:  99%|▉| 79/80 [00:01<00:00, 69.16it/s, loss=1.07, v_num=95sf, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 67.02it/s, loss=1.07, v_num=95sf, ETH_val_a\u001b[A\n","Epoch 2:  99%|▉| 79/80 [00:00<00:00, 79.58it/s, loss=1.05, v_num=95sf, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.018 >= min_delta = 0.003. New best score: 0.959\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 77.83it/s, loss=1.05, v_num=95sf, ETH_val_a\n","Epoch 3:  99%|▉| 79/80 [00:01<00:00, 76.48it/s, loss=1, v_num=95sf, ETH_val_acc=\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 0.949\n","Epoch 3: 100%|█| 80/80 [00:01<00:00, 73.84it/s, loss=1, v_num=95sf, ETH_val_acc=\n","Epoch 4:  99%|▉| 79/80 [00:01<00:00, 64.77it/s, loss=0.955, v_num=95sf, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.034 >= min_delta = 0.003. New best score: 0.915\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 62.99it/s, loss=0.955, v_num=95sf, ETH_val_\n","Epoch 5:  99%|▉| 79/80 [00:01<00:00, 62.79it/s, loss=0.952, v_num=95sf, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:01<00:00, 61.50it/s, loss=0.952, v_num=95sf, ETH_val_\u001b[A\n","Epoch 6:  99%|▉| 79/80 [00:01<00:00, 71.56it/s, loss=0.92, v_num=95sf, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.047 >= min_delta = 0.003. New best score: 0.867\n","Epoch 6: 100%|█| 80/80 [00:01<00:00, 69.43it/s, loss=0.92, v_num=95sf, ETH_val_a\n","Epoch 7:  99%|▉| 79/80 [00:01<00:00, 72.27it/s, loss=0.965, v_num=95sf, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:01<00:00, 70.37it/s, loss=0.965, v_num=95sf, ETH_val_\u001b[A\n","Epoch 8:  99%|▉| 79/80 [00:01<00:00, 68.47it/s, loss=0.95, v_num=95sf, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:01<00:00, 66.80it/s, loss=0.95, v_num=95sf, ETH_val_a\u001b[A\n","Epoch 9:  99%|▉| 79/80 [00:01<00:00, 77.25it/s, loss=0.929, v_num=95sf, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:01<00:00, 74.76it/s, loss=0.929, v_num=95sf, ETH_val_\u001b[A\n","Epoch 10:  99%|▉| 79/80 [00:01<00:00, 63.79it/s, loss=0.928, v_num=95sf, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:01<00:00, 62.56it/s, loss=0.928, v_num=95sf, ETH_val\u001b[A\n","Epoch 11:  99%|▉| 79/80 [00:01<00:00, 71.58it/s, loss=0.935, v_num=95sf, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:01<00:00, 69.41it/s, loss=0.935, v_num=95sf, ETH_val\u001b[A\n","Epoch 12:  99%|▉| 79/80 [00:01<00:00, 73.14it/s, loss=0.92, v_num=95sf, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:01<00:00, 71.19it/s, loss=0.92, v_num=95sf, ETH_val_\u001b[A\n","Epoch 13:  99%|▉| 79/80 [00:00<00:00, 81.08it/s, loss=0.908, v_num=95sf, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:01<00:00, 78.11it/s, loss=0.908, v_num=95sf, ETH_val\u001b[A\n","Epoch 14:  99%|▉| 79/80 [00:01<00:00, 70.59it/s, loss=0.912, v_num=95sf, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:01<00:00, 69.27it/s, loss=0.912, v_num=95sf, ETH_val\u001b[A\n","Epoch 15:  99%|▉| 79/80 [00:01<00:00, 76.55it/s, loss=0.944, v_num=95sf, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:01<00:00, 74.37it/s, loss=0.944, v_num=95sf, ETH_val\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:01<00:00, 76.25it/s, loss=0.886, v_num=95sf, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:01<00:00, 75.67it/s, loss=0.886, v_num=95sf, ETH_val\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:01<00:00, 65.89it/s, loss=0.897, v_num=95sf, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:01<00:00, 65.40it/s, loss=0.897, v_num=95sf, ETH_val\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:00<00:00, 84.49it/s, loss=0.9, v_num=95sf, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:00<00:00, 83.40it/s, loss=0.9, v_num=95sf, ETH_val_a\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:01<00:00, 70.54it/s, loss=0.937, v_num=95sf, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:01<00:00, 69.98it/s, loss=0.937, v_num=95sf, ETH_val\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:01<00:00, 53.48it/s, loss=0.904, v_num=95sf, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:01<00:00, 53.08it/s, loss=0.904, v_num=95sf, ETH_val\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:01<00:00, 71.61it/s, loss=0.911, v_num=95sf, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:01<00:00, 71.09it/s, loss=0.911, v_num=95sf, ETH_val\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:01<00:00, 74.16it/s, loss=0.965, v_num=95sf, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:01<00:00, 73.35it/s, loss=0.965, v_num=95sf, ETH_val\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:00<00:00, 80.22it/s, loss=0.911, v_num=95sf, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:01<00:00, 79.57it/s, loss=0.911, v_num=95sf, ETH_val\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:01<00:00, 72.41it/s, loss=0.933, v_num=95sf, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:01<00:00, 71.85it/s, loss=0.933, v_num=95sf, ETH_val\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:01<00:00, 73.96it/s, loss=0.919, v_num=95sf, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:01<00:00, 73.33it/s, loss=0.919, v_num=95sf, ETH_val\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:01<00:00, 65.86it/s, loss=0.915, v_num=95sf, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:01<00:00, 65.42it/s, loss=0.915, v_num=95sf, ETH_val\u001b[A\n","                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.867. Signaling Trainer to stop.\n","Epoch 26: 100%|█| 80/80 [00:01<00:00, 65.24it/s, loss=0.915, v_num=95sf, ETH_val\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 136.75it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'ETH_test_acc': 0.4838709533214569,\n"," 'ETH_test_f1': 0.31950849294662476,\n"," 'test_loss': 0.8143577575683594}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 68206\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012412-2pc295sf/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012412-2pc295sf/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.58378\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.78695\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 26\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2133\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 41\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463093\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 96\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.53523\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.46465\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.91559\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.33333\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.89928\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.48387\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.31951\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.81436\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▄█▁▅▂▄▅▆█▅▅▄▇▅█▂▃▅▄▆▅▆▂▃▄▅▄▅▆▆▆▄▆▆▅▇▆▄▆▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▂▃▁▂▂▂▄▃▃▄▄▄▅▄▆▂▅▅▂▇▇█▄▃▃▅▄▆▇▄▇▄▅█▄█▇▆▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅▃▆▄▆▅▃▄▃▄▃▅▁▃▁█▃▃▄▃▂▂▅▅▂▃▂▂▃▂▃▃▂▃▃▂▃▃▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▃▂▃▄▅▄▅▆▅▆▅▆▅▆▅▅▆▆█▇▆▆▇▇▆█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▂▂▁▂▃▃▄▄▆▆▆▆▆▇▇▇▇▇▇██▇▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▇▆▄▄▃▃▂▃▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▆▆▆▆▃▃█▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▂▂▂▂▁▄█▃▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▇▇▆▅▄▅▁▃▅▅█▄▅▇▅▆▇▅▂▇▄▇▂▄▇▄▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_single_lstm__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2pc295sf\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:25:04.083015: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_single_lstm__multi_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/tmlbsttl\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012502-tmlbsttl\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 195   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.8 K    Trainable params\n","0         Non-trainable params\n","75.8 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:   0%|                                           | 0/81 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 80/81 [00:01<00:00, 78.69it/s, loss=1.09, v_num=sttl, ETH_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.018\n","Epoch 0: 100%|█| 81/81 [00:01<00:00, 76.80it/s, loss=1.09, v_num=sttl, ETH_val_a\n","Epoch 1:  99%|▉| 80/81 [00:01<00:00, 70.08it/s, loss=1.13, v_num=sttl, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 81/81 [00:01<00:00, 67.98it/s, loss=1.13, v_num=sttl, ETH_val_a\u001b[A\n","Epoch 2:  99%|▉| 80/81 [00:00<00:00, 84.11it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 81/81 [00:00<00:00, 82.01it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n","Epoch 3:  99%|▉| 80/81 [00:01<00:00, 70.29it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:01<00:00, 68.77it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n","Epoch 4:  99%|▉| 80/81 [00:00<00:00, 91.64it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:00<00:00, 88.84it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n","Epoch 5:  99%|▉| 80/81 [00:01<00:00, 71.59it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:01<00:00, 70.11it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n","Epoch 6:  99%|▉| 80/81 [00:00<00:00, 88.50it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:00<00:00, 84.91it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n","Epoch 7:  99%|▉| 80/81 [00:01<00:00, 68.63it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:01<00:00, 66.86it/s, loss=1.11, v_num=sttl, ETH_val_a\u001b[A\n","Epoch 8:  99%|▉| 80/81 [00:01<00:00, 72.60it/s, loss=1.1, v_num=sttl, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:01<00:00, 70.18it/s, loss=1.1, v_num=sttl, ETH_val_ac\u001b[A\n","Epoch 9:  99%|▉| 80/81 [00:01<00:00, 77.80it/s, loss=1.1, v_num=sttl, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:01<00:00, 75.76it/s, loss=1.1, v_num=sttl, ETH_val_ac\u001b[A\n","Epoch 10:  99%|▉| 80/81 [00:01<00:00, 79.17it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:01<00:00, 76.93it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n","Epoch 11:  99%|▉| 80/81 [00:01<00:00, 79.80it/s, loss=1.11, v_num=sttl, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:01<00:00, 77.99it/s, loss=1.11, v_num=sttl, ETH_val_\u001b[A\n","Epoch 12:  99%|▉| 80/81 [00:01<00:00, 75.80it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:01<00:00, 73.65it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n","Epoch 13:  99%|▉| 80/81 [00:00<00:00, 83.06it/s, loss=1.09, v_num=sttl, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:00<00:00, 81.17it/s, loss=1.09, v_num=sttl, ETH_val_\u001b[A\n","Epoch 14:  99%|▉| 80/81 [00:01<00:00, 75.73it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:01<00:00, 73.47it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n","Epoch 15:  99%|▉| 80/81 [00:00<00:00, 84.64it/s, loss=1.11, v_num=sttl, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:00<00:00, 82.64it/s, loss=1.11, v_num=sttl, ETH_val_\u001b[A\n","Epoch 16:  99%|▉| 80/81 [00:01<00:00, 71.73it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:01<00:00, 70.04it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n","Epoch 17:  99%|▉| 80/81 [00:00<00:00, 80.14it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:01<00:00, 77.72it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n","Epoch 18:  99%|▉| 80/81 [00:01<00:00, 70.37it/s, loss=1.11, v_num=sttl, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:01<00:00, 68.98it/s, loss=1.11, v_num=sttl, ETH_val_\u001b[A\n","Epoch 19:  99%|▉| 80/81 [00:01<00:00, 75.49it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:01<00:00, 73.00it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n","Epoch 20:  99%|▉| 80/81 [00:01<00:00, 69.51it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.018. Signaling Trainer to stop.\n","Epoch 20: 100%|█| 81/81 [00:01<00:00, 68.09it/s, loss=1.1, v_num=sttl, ETH_val_a\n","Epoch 20: 100%|█| 81/81 [00:01<00:00, 67.91it/s, loss=1.1, v_num=sttl, ETH_val_a\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 120.29it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'ETH_test_acc': 0.4193548262119293,\n"," 'ETH_test_f1': 0.19648092985153198,\n"," 'test_loss': 1.105859398841858}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 68369\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012502-tmlbsttl/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012502-tmlbsttl/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.4375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.33333\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.08555\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1680\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 32\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463134\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.35619\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.24744\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.10005\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.29293\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.0909\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.41935\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19648\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.10586\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▂▄▅▄▄▂▂▁▂▄▄▄▅▂▃▃▄▃▄▄▃▆▃▅▄▅▄▅▃▆▄█▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▃▆▆▅▅▃▂▁▂▅▆▅▇▂▄▃▅▃▄▆▄█▄▄▆▇▄▆▄▆▆▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▃▂▄▄▅▅█▄▄▄▃▃▄▄▅▃▄▃▃▄▃▄▄▃▃▃▃▃▃▃▁▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch █▇▅▆▃▃▆▄▆▆▆█▅▆▆▆▆▄▁▅█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▇█▇█▄▅▆▃▅▄▄▃▆█▃▆▄▁▂▆▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▄▂▅▆▃▄▂▂▂▂▁▂▂▂▁▂▃▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ███▁▆▃█▄▃█▄▃▄▄█▄▃▄▄█▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▅▅▅▁█▄▅▅▄▅▆▄▅▆▅▆▄▆▅▅▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▁▅█▅▆▄▅▆▄▅▆▆▆▅▅▆▆▆▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_single_lstm__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/tmlbsttl\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:25:43.788125: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm__binary_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3dhx48n4\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012542-3dhx48n4\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 130   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 79/80 [00:01<00:00, 40.86it/s, loss=0.706, v_num=48n4, ETH_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.692\n","Epoch 0: 100%|█| 80/80 [00:02<00:00, 38.85it/s, loss=0.706, v_num=48n4, ETH_val_\n","Epoch 1: 100%|█| 80/80 [00:02<00:00, 30.52it/s, loss=0.655, v_num=48n4, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.085 >= min_delta = 0.003. New best score: 0.607\n","Epoch 1: 100%|█| 80/80 [00:02<00:00, 30.37it/s, loss=0.655, v_num=48n4, ETH_val_\n","Epoch 2: 100%|█| 80/80 [00:02<00:00, 31.95it/s, loss=0.584, v_num=48n4, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.108 >= min_delta = 0.003. New best score: 0.498\n","Epoch 2: 100%|█| 80/80 [00:02<00:00, 31.80it/s, loss=0.584, v_num=48n4, ETH_val_\n","Epoch 3: 100%|█| 80/80 [00:02<00:00, 39.66it/s, loss=0.605, v_num=48n4, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:02<00:00, 39.41it/s, loss=0.605, v_num=48n4, ETH_val_\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:02<00:00, 28.46it/s, loss=0.601, v_num=48n4, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:02<00:00, 28.32it/s, loss=0.601, v_num=48n4, ETH_val_\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:02<00:00, 29.74it/s, loss=0.636, v_num=48n4, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:02<00:00, 29.62it/s, loss=0.636, v_num=48n4, ETH_val_\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:02<00:00, 34.24it/s, loss=0.586, v_num=48n4, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.045 >= min_delta = 0.003. New best score: 0.453\n","Epoch 6: 100%|█| 80/80 [00:02<00:00, 34.07it/s, loss=0.586, v_num=48n4, ETH_val_\n","Epoch 7: 100%|█| 80/80 [00:02<00:00, 34.52it/s, loss=0.589, v_num=48n4, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:02<00:00, 34.35it/s, loss=0.589, v_num=48n4, ETH_val_\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:02<00:00, 32.47it/s, loss=0.599, v_num=48n4, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:02<00:00, 32.28it/s, loss=0.599, v_num=48n4, ETH_val_\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:02<00:00, 26.85it/s, loss=0.541, v_num=48n4, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:02<00:00, 26.75it/s, loss=0.541, v_num=48n4, ETH_val_\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 10: 100%|█| 80/80 [00:02<00:00, 31.51it/s, loss=0.589, v_num=48n4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:02<00:00, 31.36it/s, loss=0.589, v_num=48n4, ETH_val\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:03<00:00, 25.12it/s, loss=0.573, v_num=48n4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:03<00:00, 25.03it/s, loss=0.573, v_num=48n4, ETH_val\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:02<00:00, 31.07it/s, loss=0.592, v_num=48n4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:02<00:00, 30.74it/s, loss=0.592, v_num=48n4, ETH_val\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:02<00:00, 27.88it/s, loss=0.56, v_num=48n4, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:02<00:00, 27.74it/s, loss=0.56, v_num=48n4, ETH_val_\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:02<00:00, 28.62it/s, loss=0.578, v_num=48n4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:02<00:00, 28.51it/s, loss=0.578, v_num=48n4, ETH_val\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:02<00:00, 29.95it/s, loss=0.54, v_num=48n4, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:02<00:00, 29.80it/s, loss=0.54, v_num=48n4, ETH_val_\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:02<00:00, 28.39it/s, loss=0.573, v_num=48n4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:02<00:00, 28.28it/s, loss=0.573, v_num=48n4, ETH_val\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:02<00:00, 35.25it/s, loss=0.605, v_num=48n4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:02<00:00, 35.07it/s, loss=0.605, v_num=48n4, ETH_val\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:02<00:00, 38.56it/s, loss=0.543, v_num=48n4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:02<00:00, 38.36it/s, loss=0.543, v_num=48n4, ETH_val\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:02<00:00, 36.81it/s, loss=0.553, v_num=48n4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:02<00:00, 36.61it/s, loss=0.553, v_num=48n4, ETH_val\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:02<00:00, 32.18it/s, loss=0.536, v_num=48n4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:02<00:00, 32.04it/s, loss=0.536, v_num=48n4, ETH_val\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:02<00:00, 36.10it/s, loss=0.547, v_num=48n4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:02<00:00, 35.88it/s, loss=0.547, v_num=48n4, ETH_val\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:02<00:00, 39.58it/s, loss=0.537, v_num=48n4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:02<00:00, 39.35it/s, loss=0.537, v_num=48n4, ETH_val\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:02<00:00, 37.79it/s, loss=0.549, v_num=48n4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:02<00:00, 37.58it/s, loss=0.549, v_num=48n4, ETH_val\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:02<00:00, 37.26it/s, loss=0.55, v_num=48n4, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:02<00:00, 36.98it/s, loss=0.55, v_num=48n4, ETH_val_\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:02<00:00, 29.32it/s, loss=0.584, v_num=48n4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:02<00:00, 29.20it/s, loss=0.584, v_num=48n4, ETH_val\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:02<00:00, 34.70it/s, loss=0.515, v_num=48n4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.453. Signaling Trainer to stop.\n","Epoch 26: 100%|█| 80/80 [00:02<00:00, 34.53it/s, loss=0.515, v_num=48n4, ETH_val\n","Epoch 26: 100%|█| 80/80 [00:02<00:00, 34.48it/s, loss=0.515, v_num=48n4, ETH_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 94.54it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'ETH_test_acc': 0.7096773982048035,\n"," 'ETH_test_f1': 0.7096773982048035,\n"," 'test_loss': 0.5478140115737915}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 68509\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012542-3dhx48n4/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012542-3dhx48n4/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.70909\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.45342\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 26\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2133\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 76\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463218\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 96\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.72526\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.71134\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.54515\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.66667\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.64935\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.49525\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.70968\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.70968\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.54781\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▆▁▄▄▅▂▂▄▂▅▅▆▇▇▅▅▆▅▅▇▆▅▄▅▅▅▇▅▆▅▅▆▇█▅▆▆▆▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▆▁▃▃▅▂▂▄▂▅▄▅▇▇▄▄▆▅▅▇▆▄▄▄▅▅▇▄▆▅▅▆▇█▅▆▆▆▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▇▄▆▅▇▆▅▇▆▅▅▄▃▄▄▃▅▃▂▃▆▅█▃▄▁▄▃▄▄▂▃▁▅▃▂▃▁▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▂▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▄▃▃▂▃▂▃▂▃▃▂▂▂▂▂▃▂▂▂▂▂▂▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▃▆█▃▃▆▆██▃▃▃▃▆▃▃▃▃▅▃▅▅▅▅▃▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▃▇█▃▃▇▇██▃▃▄▃▇▃▃▃▃▅▃▅▅▅▅▃▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▆▂▂▄▃▁▂▂▁▂▄▃▆▂▄▄▆▄▂▅▂▂▄▂▆▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3dhx48n4\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:27:13.816308: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm__binary_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2dhg4mr4\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012712-2dhg4mr4\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"],"name":"stdout"},{"output_type":"stream","text":["\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 130   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 80/81 [00:01<00:00, 41.08it/s, loss=0.731, v_num=4mr4, ETH_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.688\n","Epoch 0: 100%|█| 81/81 [00:01<00:00, 40.56it/s, loss=0.731, v_num=4mr4, ETH_val_\n","Epoch 1:  99%|▉| 80/81 [00:01<00:00, 42.52it/s, loss=0.697, v_num=4mr4, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 0.667\n","Epoch 1: 100%|█| 81/81 [00:01<00:00, 42.01it/s, loss=0.697, v_num=4mr4, ETH_val_\n","Epoch 2:  99%|▉| 80/81 [00:01<00:00, 40.88it/s, loss=0.686, v_num=4mr4, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.048 >= min_delta = 0.003. New best score: 0.620\n","Epoch 2: 100%|█| 81/81 [00:02<00:00, 40.40it/s, loss=0.686, v_num=4mr4, ETH_val_\n","Epoch 3:  99%|▉| 80/81 [00:02<00:00, 39.80it/s, loss=0.706, v_num=4mr4, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:02<00:00, 39.29it/s, loss=0.706, v_num=4mr4, ETH_val_\u001b[A\n","Epoch 4:  99%|▉| 80/81 [00:01<00:00, 41.21it/s, loss=0.695, v_num=4mr4, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:01<00:00, 40.71it/s, loss=0.695, v_num=4mr4, ETH_val_\u001b[A\n","Epoch 5:  99%|▉| 80/81 [00:02<00:00, 39.19it/s, loss=0.708, v_num=4mr4, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:02<00:00, 38.72it/s, loss=0.708, v_num=4mr4, ETH_val_\u001b[A\n","Epoch 6:  99%|▉| 80/81 [00:01<00:00, 41.40it/s, loss=0.697, v_num=4mr4, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:01<00:00, 40.91it/s, loss=0.697, v_num=4mr4, ETH_val_\u001b[A\n","Epoch 7:  99%|▉| 80/81 [00:02<00:00, 30.87it/s, loss=0.699, v_num=4mr4, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:02<00:00, 30.66it/s, loss=0.699, v_num=4mr4, ETH_val_\u001b[A\n","Epoch 8:  99%|▉| 80/81 [00:02<00:00, 35.48it/s, loss=0.69, v_num=4mr4, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:02<00:00, 35.30it/s, loss=0.69, v_num=4mr4, ETH_val_a\u001b[A\n","Epoch 9:  99%|▉| 80/81 [00:01<00:00, 40.39it/s, loss=0.694, v_num=4mr4, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:02<00:00, 40.09it/s, loss=0.694, v_num=4mr4, ETH_val_\u001b[A\n","Epoch 10:  99%|▉| 80/81 [00:01<00:00, 41.28it/s, loss=0.689, v_num=4mr4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:01<00:00, 40.94it/s, loss=0.689, v_num=4mr4, ETH_val\u001b[A\n","Epoch 11:  99%|▉| 80/81 [00:01<00:00, 40.61it/s, loss=0.698, v_num=4mr4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:02<00:00, 40.28it/s, loss=0.698, v_num=4mr4, ETH_val\u001b[A\n","Epoch 12:  99%|▉| 80/81 [00:02<00:00, 34.45it/s, loss=0.691, v_num=4mr4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:02<00:00, 34.28it/s, loss=0.691, v_num=4mr4, ETH_val\u001b[A\n","Epoch 13:  99%|▉| 80/81 [00:01<00:00, 41.14it/s, loss=0.694, v_num=4mr4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:01<00:00, 40.84it/s, loss=0.694, v_num=4mr4, ETH_val\u001b[A\n","Epoch 14:  99%|▉| 80/81 [00:02<00:00, 38.54it/s, loss=0.691, v_num=4mr4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:02<00:00, 38.24it/s, loss=0.691, v_num=4mr4, ETH_val\u001b[A\n","Epoch 15:  99%|▉| 80/81 [00:02<00:00, 31.79it/s, loss=0.695, v_num=4mr4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:02<00:00, 30.80it/s, loss=0.695, v_num=4mr4, ETH_val\u001b[A\n","Epoch 16:  99%|▉| 80/81 [00:01<00:00, 40.39it/s, loss=0.697, v_num=4mr4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:02<00:00, 39.90it/s, loss=0.697, v_num=4mr4, ETH_val\u001b[A\n","Epoch 17:  99%|▉| 80/81 [00:02<00:00, 33.48it/s, loss=0.694, v_num=4mr4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:02<00:00, 33.28it/s, loss=0.694, v_num=4mr4, ETH_val\u001b[A\n","Epoch 18:  99%|▉| 80/81 [00:02<00:00, 39.87it/s, loss=0.696, v_num=4mr4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:02<00:00, 39.41it/s, loss=0.696, v_num=4mr4, ETH_val\u001b[A\n","Epoch 19:  99%|▉| 80/81 [00:02<00:00, 37.67it/s, loss=0.693, v_num=4mr4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:02<00:00, 37.43it/s, loss=0.693, v_num=4mr4, ETH_val\u001b[A\n","Epoch 20:  99%|▉| 80/81 [00:01<00:00, 40.30it/s, loss=0.694, v_num=4mr4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:02<00:00, 39.97it/s, loss=0.694, v_num=4mr4, ETH_val\u001b[A\n","Epoch 21:  99%|▉| 80/81 [00:02<00:00, 38.05it/s, loss=0.691, v_num=4mr4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 81/81 [00:02<00:00, 37.81it/s, loss=0.691, v_num=4mr4, ETH_val\u001b[A\n","Epoch 22:  99%|▉| 80/81 [00:01<00:00, 40.95it/s, loss=0.695, v_num=4mr4, ETH_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 81/81 [00:01<00:00, 40.58it/s, loss=0.695, v_num=4mr4, ETH_val\u001b[A\n","                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.620. Signaling Trainer to stop.\n","Epoch 22: 100%|█| 81/81 [00:02<00:00, 40.50it/s, loss=0.695, v_num=4mr4, ETH_val\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 93.09it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'ETH_test_acc': 0.6129032373428345,\n"," 'ETH_test_f1': 0.3793548345565796,\n"," 'test_loss': 0.6768962144851685}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 68737\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012712-2dhg4mr4/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012712-2dhg4mr4/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.56364\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.673\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 22\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1840\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 58\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463290\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 82\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.50591\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.40438\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69444\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.88889\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.47059\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.67953\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.37935\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.6769\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▄▃▇▇▄▂▅▁▄▄▃▄▄▂▇█▆▄▂▂▆▅▄▆▇▃▅▃▂▆▃▆▃▄▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▄▃▇▇▃▃▃▁▄▄▃▅▄▂▆█▆▄▃▂▅▅▄▆▄▃▄▃▂▅▃▆▂▃▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅█▁▃▅▆▃▆▄▄▅▃▅▄▃▄▄▄▆▄▃▄▄▅▃▅▅▄▅▄▄▄▄▄▄▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▂██▁▅▆▄▆▂▅▃▄▅█▄▄▆▂▄▄▂▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▆██▅▅▆▆▆▅▅▅▄▅▆▅▂▇▂▁▆▂▅▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▃▃▆▁▂▂▂▂▂▁▂▁▂▂▁▁▂▁▁▁▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁██████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁██████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▆▁▄▆▅▄▂▃▃▃▃▄▆▆▆▂▆▇▇▆▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2dhg4mr4\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:28:24.651716: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm__multi_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/19c4lpom\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012823-19c4lpom\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 195   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0: 100%|█| 80/80 [00:02<00:00, 40.00it/s, loss=1.04, v_num=lpom, ETH_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.853\n","Epoch 0: 100%|█| 80/80 [00:02<00:00, 39.75it/s, loss=1.04, v_num=lpom, ETH_val_a\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 41.34it/s, loss=1.09, v_num=lpom, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 41.10it/s, loss=1.09, v_num=lpom, ETH_val_a\u001b[A\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 42.11it/s, loss=1.04, v_num=lpom, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 41.80it/s, loss=1.04, v_num=lpom, ETH_val_a\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:02<00:00, 36.40it/s, loss=1.05, v_num=lpom, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:02<00:00, 36.22it/s, loss=1.05, v_num=lpom, ETH_val_a\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 42.03it/s, loss=1.06, v_num=lpom, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 41.77it/s, loss=1.06, v_num=lpom, ETH_val_a\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:02<00:00, 31.32it/s, loss=1.04, v_num=lpom, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:02<00:00, 31.17it/s, loss=1.04, v_num=lpom, ETH_val_a\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:02<00:00, 35.84it/s, loss=1.02, v_num=lpom, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:02<00:00, 35.66it/s, loss=1.02, v_num=lpom, ETH_val_a\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:02<00:00, 30.62it/s, loss=1.03, v_num=lpom, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:02<00:00, 30.46it/s, loss=1.03, v_num=lpom, ETH_val_a\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:02<00:00, 31.25it/s, loss=1.08, v_num=lpom, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:02<00:00, 31.09it/s, loss=1.08, v_num=lpom, ETH_val_a\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:02<00:00, 35.11it/s, loss=1.05, v_num=lpom, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:02<00:00, 34.93it/s, loss=1.05, v_num=lpom, ETH_val_a\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:02<00:00, 35.60it/s, loss=1.03, v_num=lpom, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:02<00:00, 35.42it/s, loss=1.03, v_num=lpom, ETH_val_\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:02<00:00, 26.92it/s, loss=1.02, v_num=lpom, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:02<00:00, 26.79it/s, loss=1.02, v_num=lpom, ETH_val_\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:01<00:00, 40.49it/s, loss=1.05, v_num=lpom, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:01<00:00, 40.19it/s, loss=1.05, v_num=lpom, ETH_val_\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:02<00:00, 33.67it/s, loss=1.03, v_num=lpom, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:02<00:00, 33.50it/s, loss=1.03, v_num=lpom, ETH_val_\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:01<00:00, 41.00it/s, loss=1.05, v_num=lpom, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:01<00:00, 40.75it/s, loss=1.05, v_num=lpom, ETH_val_\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:02<00:00, 38.55it/s, loss=1.05, v_num=lpom, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:02<00:00, 38.32it/s, loss=1.05, v_num=lpom, ETH_val_\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:02<00:00, 28.49it/s, loss=1.06, v_num=lpom, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:02<00:00, 28.38it/s, loss=1.06, v_num=lpom, ETH_val_\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:02<00:00, 29.92it/s, loss=1.03, v_num=lpom, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:02<00:00, 29.75it/s, loss=1.03, v_num=lpom, ETH_val_\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:02<00:00, 30.72it/s, loss=1, v_num=lpom, ETH_val_acc\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:02<00:00, 30.58it/s, loss=1, v_num=lpom, ETH_val_acc\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:02<00:00, 34.30it/s, loss=1.05, v_num=lpom, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:02<00:00, 33.90it/s, loss=1.05, v_num=lpom, ETH_val_\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:02<00:00, 29.16it/s, loss=1.05, v_num=lpom, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:02<00:00, 29.00it/s, loss=1.05, v_num=lpom, ETH_val_Monitored metric val_loss did not improve in the last 20 records. Best score: 0.853. Signaling Trainer to stop.\n","\n","Epoch 20: 100%|█| 80/80 [00:02<00:00, 28.96it/s, loss=1.05, v_num=lpom, ETH_val_\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 81.14it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'ETH_test_acc': 0.4838709533214569,\n"," 'ETH_test_f1': 0.2169238030910492,\n"," 'test_loss': 1.0823054313659668}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 68968\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012823-19c4lpom/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012823-19c4lpom/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.24\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.97029\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1659\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 58\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463361\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.4806\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.21312\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.03947\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.66667\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.26667\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.93892\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.48387\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.21692\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.08231\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▅▆▇▅▆▆▇▆▆█▆▅▅▅▆▇█▆▂▅▆▂▅▇▇▃▃▅▅▇▁▃▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▆▇█▃▄▄▄▄▄▅▄▄▃▃▄▄▅▄▂▃▄▂▃▄▄▃▃▃▃▄▁▃▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▄▅▃▆▂▃▄▅▄▃▃▅▄▅▃▁▂▅▇▄▅▃▅▅▁▆▆▆▄▂█▄▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▅▆▅▇▇▇▇█▆███████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch █▃▂▃▂▅▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▂▃▃▂▂▂▂▂▁▂▂▁▂▂▂▂▂▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▆▆▆▇▅▆▅█▅▄▅▆▆▆▆▅▅▄▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/19c4lpom\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:29:36.631732: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm__multi_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3f5p6x2i\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012935-3f5p6x2i\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 195   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 0:  99%|▉| 80/81 [00:01<00:00, 42.90it/s, loss=1.12, v_num=6x2i, ETH_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.023\n","Epoch 0: 100%|█| 81/81 [00:01<00:00, 42.37it/s, loss=1.12, v_num=6x2i, ETH_val_a\n","Epoch 1:  99%|▉| 80/81 [00:01<00:00, 41.73it/s, loss=1.09, v_num=6x2i, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 81/81 [00:01<00:00, 41.12it/s, loss=1.09, v_num=6x2i, ETH_val_a\u001b[A\n","Epoch 2:  99%|▉| 80/81 [00:01<00:00, 43.15it/s, loss=1.09, v_num=6x2i, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 81/81 [00:01<00:00, 42.63it/s, loss=1.09, v_num=6x2i, ETH_val_a\u001b[A\n","Epoch 3:  99%|▉| 80/81 [00:02<00:00, 35.39it/s, loss=1.13, v_num=6x2i, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:02<00:00, 35.21it/s, loss=1.13, v_num=6x2i, ETH_val_a\u001b[A\n","Epoch 4:  99%|▉| 80/81 [00:02<00:00, 34.78it/s, loss=1.11, v_num=6x2i, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:02<00:00, 34.54it/s, loss=1.11, v_num=6x2i, ETH_val_a\u001b[A\n","Epoch 5:  99%|▉| 80/81 [00:02<00:00, 36.34it/s, loss=1.1, v_num=6x2i, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:02<00:00, 36.12it/s, loss=1.1, v_num=6x2i, ETH_val_ac\u001b[A\n","Epoch 6:  99%|▉| 80/81 [00:02<00:00, 37.94it/s, loss=1.11, v_num=6x2i, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:02<00:00, 37.58it/s, loss=1.11, v_num=6x2i, ETH_val_a\u001b[A\n","Epoch 7:  99%|▉| 80/81 [00:02<00:00, 36.70it/s, loss=1.11, v_num=6x2i, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:02<00:00, 36.40it/s, loss=1.11, v_num=6x2i, ETH_val_a\u001b[A\n","Epoch 8:  99%|▉| 80/81 [00:01<00:00, 42.11it/s, loss=1.1, v_num=6x2i, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:01<00:00, 41.59it/s, loss=1.1, v_num=6x2i, ETH_val_ac\u001b[A\n","Epoch 9:  99%|▉| 80/81 [00:01<00:00, 43.12it/s, loss=1.1, v_num=6x2i, ETH_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:01<00:00, 42.64it/s, loss=1.1, v_num=6x2i, ETH_val_ac\u001b[A\n","Epoch 10:  99%|▉| 80/81 [00:01<00:00, 42.10it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:01<00:00, 41.71it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n","Epoch 11:  99%|▉| 80/81 [00:01<00:00, 40.45it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:02<00:00, 40.16it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n","Epoch 12:  99%|▉| 80/81 [00:02<00:00, 35.77it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:02<00:00, 35.46it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n","Epoch 13:  99%|▉| 80/81 [00:02<00:00, 31.37it/s, loss=1.12, v_num=6x2i, ETH_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:02<00:00, 31.17it/s, loss=1.12, v_num=6x2i, ETH_val_\u001b[A\n","Epoch 14:  99%|▉| 80/81 [00:02<00:00, 30.94it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:02<00:00, 30.83it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n","Epoch 15:  99%|▉| 80/81 [00:02<00:00, 35.95it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:02<00:00, 34.84it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n","Epoch 16:  99%|▉| 80/81 [00:01<00:00, 40.18it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:02<00:00, 39.90it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n","Epoch 17:  99%|▉| 80/81 [00:02<00:00, 39.36it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:02<00:00, 39.10it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n","Epoch 18:  99%|▉| 80/81 [00:02<00:00, 29.56it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:02<00:00, 29.40it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n","Epoch 19:  99%|▉| 80/81 [00:02<00:00, 31.29it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:02<00:00, 31.15it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n","Epoch 20:  99%|▉| 80/81 [00:02<00:00, 38.78it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.023. Signaling Trainer to stop.\n","Epoch 20: 100%|█| 81/81 [00:02<00:00, 38.54it/s, loss=1.1, v_num=6x2i, ETH_val_a\n","Epoch 20: 100%|█| 81/81 [00:02<00:00, 38.48it/s, loss=1.1, v_num=6x2i, ETH_val_a\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 92.94it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'ETH_test_acc': 0.4193548262119293,\n"," 'ETH_test_f1': 0.19648092985153198,\n"," 'test_loss': 1.0810121297836304}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 69160\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012935-3f5p6x2i/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_012935-3f5p6x2i/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.3125\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.23148\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.08112\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1680\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 55\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463430\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.33255\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.24784\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.0983\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.2381\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.09688\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.41935\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19648\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.08101\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▅█▇▅▅▃▅▃▆▅▅▅▅▆▅▅▃▅▅▇▅▄▆▃▃▄▅▇▅▄▅▁▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▅█▅▄▄▃▄▃▆▄▅▅▅▅▄▄▃▅▄▆▄▃▆▂▃▃▄▆▅▃▄▁▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▁▁▁▅▄▅█▃▆▄▄▅▆▅▄▅▆▅▂▆▅▃▄▅▅▄▄▄▄▅▇▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▇▇▁▆▃▆▃▃█▅█▅▅▅▅▄▇▆▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▂█▇▅▇▅▅▅▅▆▅▆▆▅▆▅▇▅▃▅▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▄▅▂▃▃▃▃▂▁▂▂▂▂▂▁▁▁▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ██▁███▁████▆██▁▁▁█▃██\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▇▇▁▇▇▇▁▇▇▇▇▇██▁▁▁▇▅█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▃█▅▆▅█▅▆▆▆▆▆▆▆▇▆▆▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3f5p6x2i\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:30:45.690738: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_single_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1wawr371\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013044-1wawr371\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 130   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.7 K    Trainable params\n","0         Non-trainable params\n","75.7 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 72/73 [00:00<00:00, 74.13it/s, loss=0.604, v_num=r371, LTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.536\n","Epoch 0: 100%|█| 73/73 [00:01<00:00, 68.08it/s, loss=0.604, v_num=r371, LTC_val_\n","Epoch 1:  99%|▉| 72/73 [00:00<00:00, 78.55it/s, loss=0.611, v_num=r371, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 73/73 [00:00<00:00, 76.10it/s, loss=0.611, v_num=r371, LTC_val_\u001b[A\n","Epoch 2:  99%|▉| 72/73 [00:01<00:00, 71.11it/s, loss=0.575, v_num=r371, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.035 >= min_delta = 0.003. New best score: 0.501\n","Epoch 2: 100%|█| 73/73 [00:01<00:00, 69.45it/s, loss=0.575, v_num=r371, LTC_val_\n","Epoch 3:  99%|▉| 72/73 [00:00<00:00, 77.76it/s, loss=0.601, v_num=r371, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 73/73 [00:00<00:00, 75.44it/s, loss=0.601, v_num=r371, LTC_val_\u001b[A\n","Epoch 4:  99%|▉| 72/73 [00:01<00:00, 70.76it/s, loss=0.569, v_num=r371, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 73/73 [00:01<00:00, 69.14it/s, loss=0.569, v_num=r371, LTC_val_\u001b[A\n","Epoch 5:  99%|▉| 72/73 [00:00<00:00, 79.18it/s, loss=0.589, v_num=r371, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 73/73 [00:00<00:00, 76.97it/s, loss=0.589, v_num=r371, LTC_val_\u001b[A\n","Epoch 6:  99%|▉| 72/73 [00:00<00:00, 73.55it/s, loss=0.557, v_num=r371, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 73/73 [00:01<00:00, 72.13it/s, loss=0.557, v_num=r371, LTC_val_\u001b[A\n","Epoch 7:  99%|▉| 72/73 [00:00<00:00, 86.58it/s, loss=0.582, v_num=r371, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 73/73 [00:00<00:00, 83.90it/s, loss=0.582, v_num=r371, LTC_val_\u001b[A\n","Epoch 8:  99%|▉| 72/73 [00:00<00:00, 73.65it/s, loss=0.552, v_num=r371, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 73/73 [00:01<00:00, 72.26it/s, loss=0.552, v_num=r371, LTC_val_\u001b[A\n","Epoch 9:  99%|▉| 72/73 [00:00<00:00, 89.44it/s, loss=0.537, v_num=r371, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 73/73 [00:00<00:00, 86.97it/s, loss=0.537, v_num=r371, LTC_val_\u001b[A\n","Epoch 10:  99%|▉| 72/73 [00:01<00:00, 69.43it/s, loss=0.566, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 73/73 [00:01<00:00, 67.96it/s, loss=0.566, v_num=r371, LTC_val\u001b[A\n","Epoch 11:  99%|▉| 72/73 [00:00<00:00, 85.54it/s, loss=0.573, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 73/73 [00:00<00:00, 83.34it/s, loss=0.573, v_num=r371, LTC_val\u001b[A\n","Epoch 12:  99%|▉| 72/73 [00:00<00:00, 72.03it/s, loss=0.581, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 73/73 [00:01<00:00, 70.22it/s, loss=0.581, v_num=r371, LTC_val\u001b[A\n","Epoch 13:  99%|▉| 72/73 [00:00<00:00, 87.69it/s, loss=0.591, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 73/73 [00:00<00:00, 85.26it/s, loss=0.591, v_num=r371, LTC_val\u001b[A\n","Epoch 14:  99%|▉| 72/73 [00:01<00:00, 68.77it/s, loss=0.546, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 73/73 [00:01<00:00, 66.93it/s, loss=0.546, v_num=r371, LTC_val\u001b[A\n","Epoch 15:  99%|▉| 72/73 [00:00<00:00, 82.91it/s, loss=0.526, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 73/73 [00:00<00:00, 80.88it/s, loss=0.526, v_num=r371, LTC_val\u001b[A\n","Epoch 16:  99%|▉| 72/73 [00:00<00:00, 74.56it/s, loss=0.567, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 0.487\n","Epoch 16: 100%|█| 73/73 [00:01<00:00, 72.74it/s, loss=0.567, v_num=r371, LTC_val\n","Epoch 17:  99%|▉| 72/73 [00:00<00:00, 80.42it/s, loss=0.616, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 73/73 [00:00<00:00, 78.18it/s, loss=0.616, v_num=r371, LTC_val\u001b[A\n","Epoch 18:  99%|▉| 72/73 [00:01<00:00, 67.85it/s, loss=0.591, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 73/73 [00:01<00:00, 66.12it/s, loss=0.591, v_num=r371, LTC_val\u001b[A\n","Epoch 19:  99%|▉| 72/73 [00:00<00:00, 81.44it/s, loss=0.533, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 73/73 [00:00<00:00, 79.59it/s, loss=0.533, v_num=r371, LTC_val\u001b[A\n","Epoch 20:  99%|▉| 72/73 [00:00<00:00, 74.71it/s, loss=0.588, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 73/73 [00:01<00:00, 72.67it/s, loss=0.588, v_num=r371, LTC_val\u001b[A\n","Epoch 21:  99%|▉| 72/73 [00:00<00:00, 83.72it/s, loss=0.564, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 73/73 [00:00<00:00, 81.58it/s, loss=0.564, v_num=r371, LTC_val\u001b[A\n","Epoch 22:  99%|▉| 72/73 [00:00<00:00, 75.70it/s, loss=0.565, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 73/73 [00:00<00:00, 73.62it/s, loss=0.565, v_num=r371, LTC_val\u001b[A\n","Epoch 23:  99%|▉| 72/73 [00:01<00:00, 71.04it/s, loss=0.503, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 73/73 [00:01<00:00, 69.60it/s, loss=0.503, v_num=r371, LTC_val\u001b[A\n","Epoch 24:  99%|▉| 72/73 [00:00<00:00, 76.09it/s, loss=0.555, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 73/73 [00:00<00:00, 73.71it/s, loss=0.555, v_num=r371, LTC_val\u001b[A\n","Epoch 25:  99%|▉| 72/73 [00:00<00:00, 84.71it/s, loss=0.552, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 73/73 [00:00<00:00, 82.60it/s, loss=0.552, v_num=r371, LTC_val\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 26:  99%|▉| 72/73 [00:00<00:00, 79.44it/s, loss=0.566, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 73/73 [00:00<00:00, 77.14it/s, loss=0.566, v_num=r371, LTC_val\u001b[A\n","Epoch 27:  99%|▉| 72/73 [00:01<00:00, 70.92it/s, loss=0.582, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 73/73 [00:01<00:00, 69.65it/s, loss=0.582, v_num=r371, LTC_val\u001b[A\n","Epoch 28:  99%|▉| 72/73 [00:01<00:00, 55.03it/s, loss=0.519, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 73/73 [00:01<00:00, 54.13it/s, loss=0.519, v_num=r371, LTC_val\u001b[A\n","Epoch 29:  99%|▉| 72/73 [00:00<00:00, 88.10it/s, loss=0.566, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 73/73 [00:00<00:00, 85.80it/s, loss=0.566, v_num=r371, LTC_val\u001b[A\n","Epoch 30:  99%|▉| 72/73 [00:01<00:00, 71.10it/s, loss=0.571, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 73/73 [00:01<00:00, 69.30it/s, loss=0.571, v_num=r371, LTC_val\u001b[A\n","Epoch 31:  99%|▉| 72/73 [00:00<00:00, 87.30it/s, loss=0.539, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 73/73 [00:00<00:00, 84.82it/s, loss=0.539, v_num=r371, LTC_val\u001b[A\n","Epoch 32:  99%|▉| 72/73 [00:00<00:00, 75.52it/s, loss=0.543, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 73/73 [00:00<00:00, 73.57it/s, loss=0.543, v_num=r371, LTC_val\u001b[A\n","Epoch 33:  99%|▉| 72/73 [00:00<00:00, 84.75it/s, loss=0.565, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 33: 100%|█| 73/73 [00:00<00:00, 82.79it/s, loss=0.565, v_num=r371, LTC_valMetric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 0.472\n","\n","Epoch 34:  99%|▉| 72/73 [00:00<00:00, 74.48it/s, loss=0.579, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 73/73 [00:01<00:00, 72.38it/s, loss=0.579, v_num=r371, LTC_val\u001b[A\n","Epoch 35:  99%|▉| 72/73 [00:01<00:00, 71.04it/s, loss=0.581, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 35: 100%|█| 73/73 [00:01<00:00, 69.35it/s, loss=0.581, v_num=r371, LTC_val\u001b[A\n","Epoch 36:  99%|▉| 72/73 [00:00<00:00, 72.22it/s, loss=0.577, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 36: 100%|█| 73/73 [00:01<00:00, 70.29it/s, loss=0.577, v_num=r371, LTC_val\u001b[A\n","Epoch 37:  99%|▉| 72/73 [00:00<00:00, 84.09it/s, loss=0.556, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 37: 100%|█| 73/73 [00:00<00:00, 82.01it/s, loss=0.556, v_num=r371, LTC_val\u001b[A\n","Epoch 38:  99%|▉| 72/73 [00:01<00:00, 63.62it/s, loss=0.545, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.035 >= min_delta = 0.003. New best score: 0.438\n","Epoch 38: 100%|█| 73/73 [00:01<00:00, 62.17it/s, loss=0.545, v_num=r371, LTC_val\n","Epoch 39:  99%|▉| 72/73 [00:00<00:00, 83.28it/s, loss=0.603, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 39: 100%|█| 73/73 [00:00<00:00, 80.70it/s, loss=0.603, v_num=r371, LTC_val\u001b[A\n","Epoch 40:  99%|▉| 72/73 [00:01<00:00, 65.26it/s, loss=0.534, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 40: 100%|█| 73/73 [00:01<00:00, 63.49it/s, loss=0.534, v_num=r371, LTC_val\u001b[A\n","Epoch 41:  99%|▉| 72/73 [00:00<00:00, 80.19it/s, loss=0.536, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 41: 100%|█| 73/73 [00:00<00:00, 78.31it/s, loss=0.536, v_num=r371, LTC_val\u001b[A\n","Epoch 42:  99%|▉| 72/73 [00:00<00:00, 73.43it/s, loss=0.572, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 42: 100%|█| 73/73 [00:01<00:00, 71.60it/s, loss=0.572, v_num=r371, LTC_val\u001b[A\n","Epoch 43:  99%|▉| 72/73 [00:00<00:00, 86.01it/s, loss=0.596, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.430\n","Epoch 43: 100%|█| 73/73 [00:00<00:00, 83.25it/s, loss=0.596, v_num=r371, LTC_val\n","Epoch 44:  99%|▉| 72/73 [00:01<00:00, 66.78it/s, loss=0.533, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 44: 100%|█| 73/73 [00:01<00:00, 64.93it/s, loss=0.533, v_num=r371, LTC_val\u001b[A\n","Epoch 45:  99%|▉| 72/73 [00:00<00:00, 76.22it/s, loss=0.557, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 45: 100%|█| 73/73 [00:00<00:00, 74.26it/s, loss=0.557, v_num=r371, LTC_val\u001b[A\n","Epoch 46:  99%|▉| 72/73 [00:01<00:00, 58.71it/s, loss=0.526, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 46: 100%|█| 73/73 [00:01<00:00, 57.57it/s, loss=0.526, v_num=r371, LTC_val\u001b[A\n","Epoch 47:  99%|▉| 72/73 [00:00<00:00, 82.58it/s, loss=0.54, v_num=r371, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 47: 100%|█| 73/73 [00:00<00:00, 79.93it/s, loss=0.54, v_num=r371, LTC_val_\u001b[A\n","Epoch 48:  99%|▉| 72/73 [00:01<00:00, 70.15it/s, loss=0.572, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 48: 100%|█| 73/73 [00:01<00:00, 68.82it/s, loss=0.572, v_num=r371, LTC_val\u001b[A\n","Epoch 49:  99%|▉| 72/73 [00:00<00:00, 77.99it/s, loss=0.579, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 49: 100%|█| 73/73 [00:00<00:00, 75.83it/s, loss=0.579, v_num=r371, LTC_val\u001b[A\n","Epoch 50:  99%|▉| 72/73 [00:01<00:00, 71.23it/s, loss=0.552, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 50: 100%|█| 73/73 [00:01<00:00, 69.92it/s, loss=0.552, v_num=r371, LTC_val\u001b[A\n","Epoch 51:  99%|▉| 72/73 [00:00<00:00, 89.57it/s, loss=0.581, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 0.420\n","Epoch 51: 100%|█| 73/73 [00:00<00:00, 87.17it/s, loss=0.581, v_num=r371, LTC_val\n","Epoch 52:  99%|▉| 72/73 [00:01<00:00, 52.62it/s, loss=0.558, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 52: 100%|█| 73/73 [00:01<00:00, 51.78it/s, loss=0.558, v_num=r371, LTC_val\u001b[A\n","Epoch 53:  99%|▉| 72/73 [00:01<00:00, 59.87it/s, loss=0.564, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 53: 100%|█| 73/73 [00:01<00:00, 58.78it/s, loss=0.564, v_num=r371, LTC_val\u001b[A\n","Epoch 54:  99%|▉| 72/73 [00:00<00:00, 82.72it/s, loss=0.538, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 54: 100%|█| 73/73 [00:00<00:00, 80.61it/s, loss=0.538, v_num=r371, LTC_val\u001b[A\n","Epoch 55:  99%|▉| 72/73 [00:00<00:00, 79.78it/s, loss=0.529, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 55: 100%|█| 73/73 [00:00<00:00, 77.52it/s, loss=0.529, v_num=r371, LTC_val\u001b[A\n","Epoch 56:  99%|▉| 72/73 [00:01<00:00, 61.18it/s, loss=0.556, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 56: 100%|█| 73/73 [00:01<00:00, 60.19it/s, loss=0.556, v_num=r371, LTC_val\u001b[A\n","Epoch 57:  99%|▉| 72/73 [00:01<00:00, 69.40it/s, loss=0.557, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 57: 100%|█| 73/73 [00:01<00:00, 67.85it/s, loss=0.557, v_num=r371, LTC_val\u001b[A\n","Epoch 58:  99%|▉| 72/73 [00:00<00:00, 81.24it/s, loss=0.585, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 58: 100%|█| 73/73 [00:00<00:00, 78.88it/s, loss=0.585, v_num=r371, LTC_val\u001b[A\n","Epoch 59:  99%|▉| 72/73 [00:00<00:00, 74.17it/s, loss=0.54, v_num=r371, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 59: 100%|█| 73/73 [00:01<00:00, 72.28it/s, loss=0.54, v_num=r371, LTC_val_\u001b[A\n","Epoch 60:  99%|▉| 72/73 [00:00<00:00, 74.51it/s, loss=0.563, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 60: 100%|█| 73/73 [00:01<00:00, 72.95it/s, loss=0.563, v_num=r371, LTC_val\u001b[A\n","Epoch 61:  99%|▉| 72/73 [00:01<00:00, 54.24it/s, loss=0.569, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.415\n","Epoch 61: 100%|█| 73/73 [00:01<00:00, 53.53it/s, loss=0.569, v_num=r371, LTC_val\n","Epoch 62:  99%|▉| 72/73 [00:00<00:00, 87.76it/s, loss=0.513, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 62: 100%|█| 73/73 [00:00<00:00, 84.86it/s, loss=0.513, v_num=r371, LTC_val\u001b[A\n","Epoch 63:  99%|▉| 72/73 [00:01<00:00, 59.10it/s, loss=0.532, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 63: 100%|█| 73/73 [00:01<00:00, 57.94it/s, loss=0.532, v_num=r371, LTC_val\u001b[A\n","Epoch 64:  99%|▉| 72/73 [00:00<00:00, 85.63it/s, loss=0.522, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 64: 100%|█| 73/73 [00:00<00:00, 83.02it/s, loss=0.522, v_num=r371, LTC_val\u001b[A\n","Epoch 65:  99%|▉| 72/73 [00:01<00:00, 64.04it/s, loss=0.528, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 65: 100%|█| 73/73 [00:01<00:00, 62.99it/s, loss=0.528, v_num=r371, LTC_val\u001b[A\n","Epoch 66:  99%|▉| 72/73 [00:00<00:00, 85.46it/s, loss=0.545, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 66: 100%|█| 73/73 [00:00<00:00, 82.66it/s, loss=0.545, v_num=r371, LTC_val\u001b[A\n","Epoch 67:  99%|▉| 72/73 [00:01<00:00, 64.80it/s, loss=0.559, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 67: 100%|█| 73/73 [00:01<00:00, 63.73it/s, loss=0.559, v_num=r371, LTC_val\u001b[A\n","Epoch 68:  99%|▉| 72/73 [00:00<00:00, 84.99it/s, loss=0.542, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 68: 100%|█| 73/73 [00:00<00:00, 82.48it/s, loss=0.542, v_num=r371, LTC_val\u001b[A\n","Epoch 69:  99%|▉| 72/73 [00:01<00:00, 69.36it/s, loss=0.503, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 69: 100%|█| 73/73 [00:01<00:00, 68.06it/s, loss=0.503, v_num=r371, LTC_val\u001b[A\n","Epoch 70:  99%|▉| 72/73 [00:00<00:00, 72.78it/s, loss=0.53, v_num=r371, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.410\n","Epoch 70: 100%|█| 73/73 [00:01<00:00, 70.83it/s, loss=0.53, v_num=r371, LTC_val_\n","Epoch 71:  99%|▉| 72/73 [00:01<00:00, 69.90it/s, loss=0.543, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 71: 100%|█| 73/73 [00:01<00:00, 68.17it/s, loss=0.543, v_num=r371, LTC_val\u001b[A\n","Epoch 72:  99%|▉| 72/73 [00:01<00:00, 62.15it/s, loss=0.589, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 72: 100%|█| 73/73 [00:01<00:00, 60.68it/s, loss=0.589, v_num=r371, LTC_val\u001b[A\n","Epoch 73:  99%|▉| 72/73 [00:01<00:00, 70.30it/s, loss=0.536, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 73: 100%|█| 73/73 [00:01<00:00, 68.97it/s, loss=0.536, v_num=r371, LTC_val\u001b[A\n","Epoch 74:  99%|▉| 72/73 [00:01<00:00, 70.32it/s, loss=0.525, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 74: 100%|█| 73/73 [00:01<00:00, 68.20it/s, loss=0.525, v_num=r371, LTC_val\u001b[A\n","Epoch 75:  99%|▉| 72/73 [00:00<00:00, 81.44it/s, loss=0.529, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 75: 100%|█| 73/73 [00:00<00:00, 79.46it/s, loss=0.529, v_num=r371, LTC_val\u001b[A\n","Epoch 76:  99%|▉| 72/73 [00:01<00:00, 64.59it/s, loss=0.558, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 76: 100%|█| 73/73 [00:01<00:00, 63.23it/s, loss=0.558, v_num=r371, LTC_val\u001b[A\n","Epoch 77:  99%|▉| 72/73 [00:00<00:00, 85.76it/s, loss=0.55, v_num=r371, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 77: 100%|█| 73/73 [00:00<00:00, 83.61it/s, loss=0.55, v_num=r371, LTC_val_\u001b[A\n","Epoch 78:  99%|▉| 72/73 [00:01<00:00, 69.59it/s, loss=0.553, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 78: 100%|█| 73/73 [00:01<00:00, 67.80it/s, loss=0.553, v_num=r371, LTC_val\u001b[A\n","Epoch 79:  99%|▉| 72/73 [00:00<00:00, 74.95it/s, loss=0.565, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 79: 100%|█| 73/73 [00:00<00:00, 73.30it/s, loss=0.565, v_num=r371, LTC_val\u001b[A\n","Epoch 80:  99%|▉| 72/73 [00:00<00:00, 72.70it/s, loss=0.532, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 80: 100%|█| 73/73 [00:01<00:00, 70.67it/s, loss=0.532, v_num=r371, LTC_val\u001b[A\n","Epoch 81:  99%|▉| 72/73 [00:00<00:00, 83.41it/s, loss=0.543, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 81: 100%|█| 73/73 [00:00<00:00, 81.35it/s, loss=0.543, v_num=r371, LTC_val\u001b[A\n","Epoch 82:  99%|▉| 72/73 [00:01<00:00, 68.04it/s, loss=0.478, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 82: 100%|█| 73/73 [00:01<00:00, 66.23it/s, loss=0.478, v_num=r371, LTC_val\u001b[A\n","Epoch 83:  99%|▉| 72/73 [00:00<00:00, 80.02it/s, loss=0.545, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 83: 100%|█| 73/73 [00:00<00:00, 77.07it/s, loss=0.545, v_num=r371, LTC_val\u001b[A\n","Epoch 84:  99%|▉| 72/73 [00:01<00:00, 45.86it/s, loss=0.521, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 84: 100%|█| 73/73 [00:01<00:00, 45.50it/s, loss=0.521, v_num=r371, LTC_val\u001b[A\n","Epoch 85:  99%|▉| 72/73 [00:00<00:00, 81.03it/s, loss=0.538, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 85: 100%|█| 73/73 [00:00<00:00, 78.36it/s, loss=0.538, v_num=r371, LTC_val\u001b[A\n","Epoch 86:  99%|▉| 72/73 [00:01<00:00, 70.81it/s, loss=0.576, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 86: 100%|█| 73/73 [00:01<00:00, 69.53it/s, loss=0.576, v_num=r371, LTC_val\u001b[A\n","Epoch 87:  99%|▉| 72/73 [00:00<00:00, 83.97it/s, loss=0.52, v_num=r371, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 87: 100%|█| 73/73 [00:00<00:00, 81.21it/s, loss=0.52, v_num=r371, LTC_val_\u001b[A\n","Epoch 88:  99%|▉| 72/73 [00:01<00:00, 68.58it/s, loss=0.497, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 88: 100%|█| 73/73 [00:01<00:00, 67.38it/s, loss=0.497, v_num=r371, LTC_val\u001b[A\n","Epoch 89:  99%|▉| 72/73 [00:01<00:00, 69.58it/s, loss=0.506, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 89: 100%|█| 73/73 [00:01<00:00, 68.04it/s, loss=0.506, v_num=r371, LTC_val\u001b[A\n","Epoch 90:  99%|▉| 72/73 [00:00<00:00, 78.12it/s, loss=0.542, v_num=r371, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.410. Signaling Trainer to stop.\n","Epoch 90: 100%|█| 73/73 [00:00<00:00, 76.42it/s, loss=0.542, v_num=r371, LTC_val\n","Epoch 90: 100%|█| 73/73 [00:00<00:00, 76.19it/s, loss=0.542, v_num=r371, LTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 130.61it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'LTC_test_acc': 0.75,\n"," 'LTC_test_f1': 0.7414525747299194,\n"," 'test_loss': 0.43861886858940125}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 69362\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013044-1wawr371/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013044-1wawr371/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 1.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 1.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.26427\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 90\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 6552\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 102\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463546\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 313\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.74826\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.73695\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.51494\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.61905\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.42779\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.74145\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.43862\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▅▅▃█▅▂▆▇▅▅▃▆▄█▅▅▄▅▅▅▂▅█▆▁█▄▅▅▆▇▆▅▅▅▄▆▃▅▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▄▅▃█▅▁▆▇▅▅▃▆▃█▅▄▄▄▄▅▂▄█▆▁█▄▅▅▆▇▆▅▅▅▃▆▃▄▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▅▆▁▆▅▂▃▄▄▅▃▆▁▆▇▅▄▇▃▅▃▂▂█▁▄▆▃▄▃▃▄▄▃▅▂█▅▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▅▅▆▆▆▆▆▆▇▆▆▆▇▆▇▆▇▆▆▆▆▇▇▇▆▇▆▇▇▇▇█▇▇▇▇▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▅▅▆▇▆▆▆▆▇▆▆▆▇▆▇▆▇▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▄▄▃▄▄▄▄▃▃▃▄▃▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▃▆▆▁▁▆▃▆▃▃▃▁▃▃▃▃▃▃▃▆▃▆▆▆▆▆▆▆▆▃▆▆█▆▆▆▆▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▃▅▅▁▁▅▃▅▃▃▃▁▃▃▃▃▃▃▃▅▃▅▅▆▆▆▆▆▆▃▆▆█▅▆▆▅▅▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▇▅▆█▇▅▅▄▆▇▆▆▇▅▆▅▆▄▂▁▄▂▂▃▃▂▃▂▃▃▂▄▁▂▁▂▁▁▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_single_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1wawr371\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:32:36.867484: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_single_lstm_loss_weighted_binary_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2ub8061i\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013235-2ub8061i\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 130   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.7 K    Trainable params\n","0         Non-trainable params\n","75.7 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 73/74 [00:00<00:00, 74.80it/s, loss=0.72, v_num=061i, LTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.697\n","Epoch 0: 100%|█| 74/74 [00:01<00:00, 72.68it/s, loss=0.72, v_num=061i, LTC_val_a\n","Epoch 1:  99%|▉| 73/74 [00:00<00:00, 73.39it/s, loss=0.704, v_num=061i, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 74/74 [00:01<00:00, 71.15it/s, loss=0.704, v_num=061i, LTC_val_\u001b[A\n","Epoch 2:  99%|▉| 73/74 [00:00<00:00, 75.93it/s, loss=0.719, v_num=061i, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 74/74 [00:01<00:00, 73.50it/s, loss=0.719, v_num=061i, LTC_val_\u001b[A\n","Epoch 3:  99%|▉| 73/74 [00:00<00:00, 78.17it/s, loss=0.693, v_num=061i, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 74/74 [00:00<00:00, 76.12it/s, loss=0.693, v_num=061i, LTC_val_\u001b[A\n","Epoch 4:  99%|▉| 73/74 [00:00<00:00, 75.44it/s, loss=0.693, v_num=061i, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 74/74 [00:01<00:00, 72.50it/s, loss=0.693, v_num=061i, LTC_val_\u001b[A\n","Epoch 5:  99%|▉| 73/74 [00:00<00:00, 75.71it/s, loss=0.703, v_num=061i, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 0.693\n","Epoch 5: 100%|█| 74/74 [00:01<00:00, 73.91it/s, loss=0.703, v_num=061i, LTC_val_\n","Epoch 6:  99%|▉| 73/74 [00:00<00:00, 74.50it/s, loss=0.697, v_num=061i, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 74/74 [00:01<00:00, 72.35it/s, loss=0.697, v_num=061i, LTC_val_\u001b[A\n","Epoch 7:  99%|▉| 73/74 [00:01<00:00, 62.44it/s, loss=0.708, v_num=061i, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 74/74 [00:01<00:00, 61.11it/s, loss=0.708, v_num=061i, LTC_val_\u001b[A\n","Epoch 8:  99%|▉| 73/74 [00:01<00:00, 72.45it/s, loss=0.691, v_num=061i, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 74/74 [00:01<00:00, 70.02it/s, loss=0.691, v_num=061i, LTC_val_\u001b[A\n","Epoch 9:  99%|▉| 73/74 [00:00<00:00, 84.47it/s, loss=0.698, v_num=061i, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 74/74 [00:00<00:00, 82.35it/s, loss=0.698, v_num=061i, LTC_val_\u001b[A\n","Epoch 10:  99%|▉| 73/74 [00:00<00:00, 75.72it/s, loss=0.694, v_num=061i, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 74/74 [00:01<00:00, 73.41it/s, loss=0.694, v_num=061i, LTC_val\u001b[A\n","Epoch 11:  99%|▉| 73/74 [00:00<00:00, 73.62it/s, loss=0.686, v_num=061i, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 74/74 [00:01<00:00, 71.87it/s, loss=0.686, v_num=061i, LTC_val\u001b[A\n","Epoch 12:  99%|▉| 73/74 [00:01<00:00, 71.73it/s, loss=0.694, v_num=061i, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 74/74 [00:01<00:00, 69.82it/s, loss=0.694, v_num=061i, LTC_val\u001b[A\n","Epoch 13:  99%|▉| 73/74 [00:00<00:00, 85.38it/s, loss=0.693, v_num=061i, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 74/74 [00:00<00:00, 83.28it/s, loss=0.693, v_num=061i, LTC_val\u001b[A\n","Epoch 14:  99%|▉| 73/74 [00:01<00:00, 67.15it/s, loss=0.691, v_num=061i, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 74/74 [00:01<00:00, 65.61it/s, loss=0.691, v_num=061i, LTC_val\u001b[A\n","Epoch 15:  99%|▉| 73/74 [00:00<00:00, 78.58it/s, loss=0.696, v_num=061i, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 74/74 [00:00<00:00, 76.89it/s, loss=0.696, v_num=061i, LTC_val\u001b[A\n","Epoch 16:  99%|▉| 73/74 [00:01<00:00, 71.45it/s, loss=0.698, v_num=061i, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 74/74 [00:01<00:00, 69.78it/s, loss=0.698, v_num=061i, LTC_val\u001b[A\n","Epoch 17:  99%|▉| 73/74 [00:00<00:00, 85.67it/s, loss=0.691, v_num=061i, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 74/74 [00:00<00:00, 83.51it/s, loss=0.691, v_num=061i, LTC_val\u001b[A\n","Epoch 18:  99%|▉| 73/74 [00:00<00:00, 76.22it/s, loss=0.693, v_num=061i, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 74/74 [00:01<00:00, 73.76it/s, loss=0.693, v_num=061i, LTC_val\u001b[A\n","Epoch 19:  99%|▉| 73/74 [00:00<00:00, 82.19it/s, loss=0.69, v_num=061i, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 74/74 [00:00<00:00, 80.20it/s, loss=0.69, v_num=061i, LTC_val_\u001b[A\n","Epoch 20:  99%|▉| 73/74 [00:01<00:00, 71.44it/s, loss=0.691, v_num=061i, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 74/74 [00:01<00:00, 69.80it/s, loss=0.691, v_num=061i, LTC_val\u001b[A\n","Epoch 21:  99%|▉| 73/74 [00:00<00:00, 79.22it/s, loss=0.695, v_num=061i, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 74/74 [00:00<00:00, 77.37it/s, loss=0.695, v_num=061i, LTC_val\u001b[A\n","Epoch 22:  99%|▉| 73/74 [00:00<00:00, 75.96it/s, loss=0.69, v_num=061i, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 74/74 [00:01<00:00, 73.95it/s, loss=0.69, v_num=061i, LTC_val_\u001b[A\n","Epoch 23:  99%|▉| 73/74 [00:00<00:00, 81.23it/s, loss=0.697, v_num=061i, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 74/74 [00:00<00:00, 79.32it/s, loss=0.697, v_num=061i, LTC_val\u001b[A\n","Epoch 24:  99%|▉| 73/74 [00:00<00:00, 79.29it/s, loss=0.69, v_num=061i, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 74/74 [00:00<00:00, 76.86it/s, loss=0.69, v_num=061i, LTC_val_\u001b[A\n","Epoch 25:  99%|▉| 73/74 [00:01<00:00, 68.97it/s, loss=0.691, v_num=061i, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.693. Signaling Trainer to stop.\n","Epoch 25: 100%|█| 74/74 [00:01<00:00, 67.71it/s, loss=0.691, v_num=061i, LTC_val\n","Epoch 25: 100%|█| 74/74 [00:01<00:00, 67.53it/s, loss=0.691, v_num=061i, LTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 148.05it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'LTC_test_acc': 0.5,\n"," 'LTC_test_f1': 0.3272727429866791,\n"," 'test_loss': 0.6927607655525208}\n","--------------------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 69633\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013235-2ub8061i/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013235-2ub8061i/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.73333\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.6755\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 25\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1898\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 35\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463590\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 89\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.53282\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.51489\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69464\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.33333\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.69479\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.32727\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.69276\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▃▄▄▅▇█▄▁▅▃▁▆▇▃▆▁▃▄▂▂▆▁▂█▁▄▅▇▃▂▂▂▆▁▅▆█\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▃▄▃▅▇▃▄▂▅▃▂▆▆▃▆▂▁▄▁▁▅▂▃█▁▃▅▇▃▃▂▃▆▂▅▆█\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅▇▅▆▃▁▄▇▅▄▇▄▄▅▄▆▅▅▆▆▄▆█▂▇▆▄▃▅▄▅▆▄▅▄▄▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▃▆▁▃▆▃▅▅▄▆▅▅▄▄█▆▅▅▆▃▅▂▅▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▄▆▁▁▂▄▆▄▅▆▆▅▅▂▇▆▁▄▅▄▅▃▆▆▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▇▅▂▄▂▃▂▂▂▂▂▂▁▂▃▂▂▂▂▂▁▂▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ███████████████▁███▁██████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▃▃▃▃▃▃▃▃▃▇▃▃▃▃▃▁▃▃▃▁▃█▃▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▃▄▃▃▂▁▂▂▂▁▁▂▅▂▄▂▇▂▁▄█▂▆▃▃▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_single_lstm_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2ub8061i\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:33:21.378120: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_single_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3glxbifx\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013319-3glxbifx\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 195   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.8 K    Trainable params\n","0         Non-trainable params\n","75.8 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 72/73 [00:01<00:00, 71.32it/s, loss=1.14, v_num=bifx, LTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.047\n","Epoch 0: 100%|█| 73/73 [00:01<00:00, 65.20it/s, loss=1.14, v_num=bifx, LTC_val_a\n","Epoch 1:  99%|▉| 72/73 [00:00<00:00, 78.04it/s, loss=1.1, v_num=bifx, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 73/73 [00:00<00:00, 76.08it/s, loss=1.1, v_num=bifx, LTC_val_ac\u001b[A\n","Epoch 2:  99%|▉| 72/73 [00:01<00:00, 71.59it/s, loss=1.09, v_num=bifx, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 73/73 [00:01<00:00, 69.75it/s, loss=1.09, v_num=bifx, LTC_val_a\u001b[A\n","Epoch 3:  99%|▉| 72/73 [00:00<00:00, 86.59it/s, loss=1.08, v_num=bifx, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.067 >= min_delta = 0.003. New best score: 0.979\n","Epoch 3: 100%|█| 73/73 [00:00<00:00, 84.16it/s, loss=1.08, v_num=bifx, LTC_val_a\n","Epoch 4:  99%|▉| 72/73 [00:01<00:00, 71.15it/s, loss=1.05, v_num=bifx, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 73/73 [00:01<00:00, 69.41it/s, loss=1.05, v_num=bifx, LTC_val_a\u001b[A\n","Epoch 5:  99%|▉| 72/73 [00:00<00:00, 73.06it/s, loss=0.988, v_num=bifx, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 73/73 [00:01<00:00, 71.41it/s, loss=0.988, v_num=bifx, LTC_val_\u001b[A\n","Epoch 6:  99%|▉| 72/73 [00:01<00:00, 66.68it/s, loss=1, v_num=bifx, LTC_val_acc=\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.976\n","Epoch 6: 100%|█| 73/73 [00:01<00:00, 65.17it/s, loss=1, v_num=bifx, LTC_val_acc=\n","Epoch 7:  99%|▉| 72/73 [00:00<00:00, 78.32it/s, loss=0.972, v_num=bifx, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 73/73 [00:00<00:00, 76.19it/s, loss=0.972, v_num=bifx, LTC_val_\u001b[A\n","Epoch 8:  99%|▉| 72/73 [00:01<00:00, 67.60it/s, loss=0.991, v_num=bifx, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 73/73 [00:01<00:00, 66.04it/s, loss=0.991, v_num=bifx, LTC_val_\u001b[A\n","Epoch 9:  99%|▉| 72/73 [00:00<00:00, 82.67it/s, loss=0.998, v_num=bifx, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 73/73 [00:00<00:00, 80.73it/s, loss=0.998, v_num=bifx, LTC_val_\u001b[A\n","Epoch 10:  99%|▉| 72/73 [00:01<00:00, 71.26it/s, loss=0.979, v_num=bifx, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 73/73 [00:01<00:00, 69.70it/s, loss=0.979, v_num=bifx, LTC_val\u001b[A\n","Epoch 11:  99%|▉| 72/73 [00:00<00:00, 87.68it/s, loss=0.988, v_num=bifx, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 73/73 [00:00<00:00, 85.45it/s, loss=0.988, v_num=bifx, LTC_val\u001b[A\n","Epoch 12:  99%|▉| 72/73 [00:00<00:00, 74.85it/s, loss=0.99, v_num=bifx, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.036 >= min_delta = 0.003. New best score: 0.939\n","Epoch 12: 100%|█| 73/73 [00:01<00:00, 72.89it/s, loss=0.99, v_num=bifx, LTC_val_\n","Epoch 13:  99%|▉| 72/73 [00:00<00:00, 84.04it/s, loss=1, v_num=bifx, LTC_val_acc\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 73/73 [00:00<00:00, 81.45it/s, loss=1, v_num=bifx, LTC_val_acc\u001b[A\n","Epoch 14:  99%|▉| 72/73 [00:01<00:00, 67.88it/s, loss=0.969, v_num=bifx, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 73/73 [00:01<00:00, 66.29it/s, loss=0.969, v_num=bifx, LTC_val\u001b[A\n","Epoch 15:  99%|▉| 72/73 [00:00<00:00, 84.96it/s, loss=0.938, v_num=bifx, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 73/73 [00:00<00:00, 81.70it/s, loss=0.938, v_num=bifx, LTC_val\u001b[A\n","Epoch 16:  99%|▉| 72/73 [00:00<00:00, 75.53it/s, loss=0.96, v_num=bifx, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 73/73 [00:00<00:00, 73.03it/s, loss=0.96, v_num=bifx, LTC_val_\u001b[A\n","Epoch 17:  99%|▉| 72/73 [00:00<00:00, 74.30it/s, loss=0.961, v_num=bifx, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 73/73 [00:01<00:00, 72.88it/s, loss=0.961, v_num=bifx, LTC_val\u001b[A\n","Epoch 18:  99%|▉| 72/73 [00:00<00:00, 74.19it/s, loss=0.935, v_num=bifx, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 73/73 [00:01<00:00, 72.35it/s, loss=0.935, v_num=bifx, LTC_val\u001b[A\n","Epoch 19:  99%|▉| 72/73 [00:00<00:00, 85.18it/s, loss=0.96, v_num=bifx, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 73/73 [00:00<00:00, 83.17it/s, loss=0.96, v_num=bifx, LTC_val_\u001b[A\n","Epoch 20:  99%|▉| 72/73 [00:01<00:00, 64.97it/s, loss=0.947, v_num=bifx, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 73/73 [00:01<00:00, 63.61it/s, loss=0.947, v_num=bifx, LTC_val\u001b[A\n","Epoch 21:  99%|▉| 72/73 [00:00<00:00, 85.59it/s, loss=0.957, v_num=bifx, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 73/73 [00:00<00:00, 83.55it/s, loss=0.957, v_num=bifx, LTC_val\u001b[A\n","Epoch 22:  99%|▉| 72/73 [00:00<00:00, 73.61it/s, loss=0.997, v_num=bifx, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 73/73 [00:01<00:00, 71.69it/s, loss=0.997, v_num=bifx, LTC_val\u001b[A\n","Epoch 23:  99%|▉| 72/73 [00:00<00:00, 84.36it/s, loss=0.958, v_num=bifx, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 73/73 [00:00<00:00, 82.22it/s, loss=0.958, v_num=bifx, LTC_val\u001b[A\n","Epoch 24:  99%|▉| 72/73 [00:00<00:00, 72.83it/s, loss=0.939, v_num=bifx, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 73/73 [00:01<00:00, 70.93it/s, loss=0.939, v_num=bifx, LTC_val\u001b[A\n","Epoch 25:  99%|▉| 72/73 [00:00<00:00, 79.81it/s, loss=0.966, v_num=bifx, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 73/73 [00:00<00:00, 77.73it/s, loss=0.966, v_num=bifx, LTC_val\u001b[A\n","Epoch 26:  99%|▉| 72/73 [00:00<00:00, 76.77it/s, loss=0.96, v_num=bifx, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 73/73 [00:00<00:00, 74.65it/s, loss=0.96, v_num=bifx, LTC_val_\u001b[A\n","Epoch 27:  99%|▉| 72/73 [00:00<00:00, 75.01it/s, loss=1.01, v_num=bifx, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 73/73 [00:00<00:00, 73.28it/s, loss=1.01, v_num=bifx, LTC_val_\u001b[A\n","Epoch 28:  99%|▉| 72/73 [00:00<00:00, 78.08it/s, loss=0.961, v_num=bifx, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 73/73 [00:00<00:00, 75.77it/s, loss=0.961, v_num=bifx, LTC_val\u001b[A\n","Epoch 29:  99%|▉| 72/73 [00:01<00:00, 66.73it/s, loss=0.931, v_num=bifx, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 73/73 [00:01<00:00, 65.49it/s, loss=0.931, v_num=bifx, LTC_val\u001b[A\n","Epoch 30:  99%|▉| 72/73 [00:00<00:00, 75.28it/s, loss=0.997, v_num=bifx, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 73/73 [00:01<00:00, 72.82it/s, loss=0.997, v_num=bifx, LTC_val\u001b[A\n","Epoch 31:  99%|▉| 72/73 [00:00<00:00, 78.56it/s, loss=0.912, v_num=bifx, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 73/73 [00:00<00:00, 76.55it/s, loss=0.912, v_num=bifx, LTC_val\u001b[A\n","Epoch 32:  99%|▉| 72/73 [00:00<00:00, 74.95it/s, loss=1, v_num=bifx, LTC_val_acc\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.939. Signaling Trainer to stop.\n","Epoch 32: 100%|█| 73/73 [00:01<00:00, 72.92it/s, loss=1, v_num=bifx, LTC_val_acc\n","Epoch 32: 100%|█| 73/73 [00:01<00:00, 72.64it/s, loss=1, v_num=bifx, LTC_val_acc\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 105.73it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'LTC_test_acc': 0.6785714030265808,\n"," 'LTC_test_f1': 0.6618326306343079,\n"," 'test_loss': 0.8116836547851562}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 69817\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013319-3glxbifx/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013319-3glxbifx/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.62222\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.75376\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 32\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2376\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 42\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463641\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 113\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.47049\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.44906\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.96406\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.39524\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.04748\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.67857\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.66183\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.81168\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▅▅▂▁▂▂▂▆▄▄▂▂▄▅▂▃▇▂▄▅▄▃▄▂▅▆▂▅▆▇█▇▃▆▅▄▄▄▂▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▄▅▂▁▃▂▃▆▄▄▃▃▃▅▂▃▇▃▄▄▃▃▄▃▅▆▂▆▆██▇▃▇▅▄▄▄▃▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▄▄▅▆▆▅▅▃▄▄▅▅▃▃▅▄▁▆▂▁▄▅█▆▃▁▅▁▃▁▂▂▅▁▃▆▂▅▄▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▃▃▁▃▃▄▄▄▅▆▆▆▆▅▇▇▇▇▆▇▆▇▇▆▇▇▇▆▇▇▇█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▂▁▁▃▄▄▅▄▅▆▆▆▇▆▇▇▇▇▆█▇██▆▇▇▇▇▇▇██▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▇▇▅▄▃▃▃▃▂▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▆▅▃█▅▆█▅▅▁█▅█▆▃█▆▅█▆▅█▃▆▁▆▃▃▅█▅█▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▅▄▃█▄▆█▅▄▁█▄█▆▃█▆▅█▆▅█▃▆▁▆▃▃▄█▄█▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄▅▅▂▃▂▂▆█▅▃▅▁▂▇▅▄▅▆▄▆▆▇▄▆▄█▄▃▆▇▂▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_single_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3glxbifx\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:34:11.075424: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_single_lstm_loss_weighted_multi_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2xwo69me\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013409-2xwo69me\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 195   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.8 K    Trainable params\n","0         Non-trainable params\n","75.8 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:   0%|                                           | 0/74 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 73/74 [00:00<00:00, 79.62it/s, loss=1.12, v_num=69me, LTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.112\n","Epoch 0: 100%|█| 74/74 [00:00<00:00, 76.98it/s, loss=1.12, v_num=69me, LTC_val_a\n","Epoch 1:  99%|▉| 73/74 [00:00<00:00, 74.11it/s, loss=1.13, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 1.095\n","Epoch 1: 100%|█| 74/74 [00:01<00:00, 71.69it/s, loss=1.13, v_num=69me, LTC_val_a\n","Epoch 2:  99%|▉| 73/74 [00:00<00:00, 74.04it/s, loss=1.12, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 1.091\n","Epoch 2: 100%|█| 74/74 [00:01<00:00, 71.92it/s, loss=1.12, v_num=69me, LTC_val_a\n","Epoch 3:  99%|▉| 73/74 [00:01<00:00, 72.47it/s, loss=1.11, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 74/74 [00:01<00:00, 70.83it/s, loss=1.11, v_num=69me, LTC_val_a\u001b[A\n","Epoch 4:  99%|▉| 73/74 [00:01<00:00, 69.23it/s, loss=1.1, v_num=69me, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 1.081\n","Epoch 4: 100%|█| 74/74 [00:01<00:00, 67.50it/s, loss=1.1, v_num=69me, LTC_val_ac\n","Epoch 5:  99%|▉| 73/74 [00:00<00:00, 76.90it/s, loss=1.11, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 74/74 [00:00<00:00, 74.99it/s, loss=1.11, v_num=69me, LTC_val_a\u001b[A\n","Epoch 6:  99%|▉| 73/74 [00:01<00:00, 67.28it/s, loss=1.11, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 74/74 [00:01<00:00, 65.53it/s, loss=1.11, v_num=69me, LTC_val_a\u001b[A\n","Epoch 7:  99%|▉| 73/74 [00:00<00:00, 83.66it/s, loss=1.1, v_num=69me, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 74/74 [00:00<00:00, 80.09it/s, loss=1.1, v_num=69me, LTC_val_ac\u001b[A\n","Epoch 8:  99%|▉| 73/74 [00:01<00:00, 65.61it/s, loss=1.11, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 74/74 [00:01<00:00, 64.13it/s, loss=1.11, v_num=69me, LTC_val_a\u001b[A\n","Epoch 9:  99%|▉| 73/74 [00:01<00:00, 71.77it/s, loss=1.1, v_num=69me, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 74/74 [00:01<00:00, 70.05it/s, loss=1.1, v_num=69me, LTC_val_ac\u001b[A\n","Epoch 10:  99%|▉| 73/74 [00:01<00:00, 70.65it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 74/74 [00:01<00:00, 69.07it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Epoch 11:  99%|▉| 73/74 [00:00<00:00, 87.37it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 74/74 [00:00<00:00, 85.14it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Epoch 12:  99%|▉| 73/74 [00:01<00:00, 72.00it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 74/74 [00:01<00:00, 70.11it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Epoch 13:  99%|▉| 73/74 [00:00<00:00, 88.27it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 74/74 [00:00<00:00, 85.89it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Epoch 14:  99%|▉| 73/74 [00:01<00:00, 72.65it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 74/74 [00:01<00:00, 71.02it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Epoch 15:  99%|▉| 73/74 [00:00<00:00, 79.06it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 74/74 [00:00<00:00, 77.26it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Epoch 16:  99%|▉| 73/74 [00:01<00:00, 60.05it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 74/74 [00:01<00:00, 59.17it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Epoch 17:  99%|▉| 73/74 [00:00<00:00, 88.39it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 74/74 [00:00<00:00, 86.16it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Epoch 18:  99%|▉| 73/74 [00:01<00:00, 69.46it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 74/74 [00:01<00:00, 68.02it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Epoch 19:  99%|▉| 73/74 [00:00<00:00, 79.44it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 74/74 [00:00<00:00, 77.45it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Epoch 20:  99%|▉| 73/74 [00:01<00:00, 70.83it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 74/74 [00:01<00:00, 69.37it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Epoch 21:  99%|▉| 73/74 [00:00<00:00, 88.80it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 74/74 [00:00<00:00, 86.57it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Epoch 22:  99%|▉| 73/74 [00:01<00:00, 69.28it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 74/74 [00:01<00:00, 67.77it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Epoch 23:  99%|▉| 73/74 [00:00<00:00, 89.03it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 74/74 [00:00<00:00, 86.63it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Epoch 24:  99%|▉| 73/74 [00:01<00:00, 71.87it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.081. Signaling Trainer to stop.\n","Epoch 24: 100%|█| 74/74 [00:01<00:00, 69.92it/s, loss=1.1, v_num=69me, LTC_val_a\n","Epoch 24: 100%|█| 74/74 [00:01<00:00, 69.71it/s, loss=1.1, v_num=69me, LTC_val_a\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n"],"name":"stdout"},{"output_type":"stream","text":["Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 109.56it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'LTC_test_acc': 0.25,\n"," 'LTC_test_f1': 0.13151928782463074,\n"," 'test_loss': 1.1005421876907349}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 69978\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013409-2xwo69me/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013409-2xwo69me/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.25877\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.09018\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 24\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1825\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 34\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463683\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 86\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.32211\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.27431\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09972\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.125\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.07407\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.10292\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.25\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.13152\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.10054\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▆█▆▂▅▆▅▇▃▄▅▅▆▄▁▆▅▅▂▅█▆▄▇▅▅▅▆▅▅▃█▆▇▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▆█▆▂▅▆▄▇▃▄▅▆▆▃▁▆▄▅▂▄▇▅▄▇▄▅▆▆▅▆▃█▇▇▆▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅▁▄█▄▃▅▄▃▄▃▄▃▄▆▃▄▄▅▄▄▄▄▄▃▄▄▄▄▃▄▄▄▄▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▅▆▅▇▅▆▂▅▅▄▅█▃▄▆▄▃▄▂▅▅█▁▇▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▄▆▅▅▅▅▂▆▅▄▅▇▃▄▇▄▃▃▃▅▅█▁▆▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▅▃▃▂▄▂▂▂▃▁▂▂▁▂▂▂▂▁▂▁▂▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▆▆▆▆█▆▁▆▆▆▃▆▆▆▆▆▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▄▄▄▄█▄▁▄▄▄▃▄▄▄▄▄▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▄▃▅▁▄▅▆▇▆▅▆▅▆▆▆▆▇▆▆▆▆▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_single_lstm_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2xwo69me\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:34:54.039441: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/233c0o7p\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013452-233c0o7p\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 130   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0: 100%|█| 73/73 [00:01<00:00, 40.04it/s, loss=0.681, v_num=0o7p, LTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.618\n","Epoch 0: 100%|█| 73/73 [00:01<00:00, 39.77it/s, loss=0.681, v_num=0o7p, LTC_val_\n","Epoch 1:  99%|▉| 72/73 [00:01<00:00, 42.28it/s, loss=0.646, v_num=0o7p, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 73/73 [00:01<00:00, 41.75it/s, loss=0.646, v_num=0o7p, LTC_val_\u001b[A\n","Epoch 2:  99%|▉| 72/73 [00:02<00:00, 34.73it/s, loss=0.631, v_num=0o7p, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 73/73 [00:02<00:00, 34.50it/s, loss=0.631, v_num=0o7p, LTC_val_\u001b[A\n","Epoch 3:  99%|▉| 72/73 [00:01<00:00, 42.84it/s, loss=0.608, v_num=0o7p, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.089 >= min_delta = 0.003. New best score: 0.529\n","Epoch 3: 100%|█| 73/73 [00:01<00:00, 42.54it/s, loss=0.608, v_num=0o7p, LTC_val_\n","Epoch 4:  99%|▉| 72/73 [00:01<00:00, 40.13it/s, loss=0.619, v_num=0o7p, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 73/73 [00:01<00:00, 39.79it/s, loss=0.619, v_num=0o7p, LTC_val_\u001b[A\n","Epoch 5:  99%|▉| 72/73 [00:02<00:00, 32.42it/s, loss=0.555, v_num=0o7p, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 73/73 [00:02<00:00, 32.24it/s, loss=0.555, v_num=0o7p, LTC_val_\u001b[A\n","Epoch 6:  99%|▉| 72/73 [00:01<00:00, 42.65it/s, loss=0.591, v_num=0o7p, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 73/73 [00:01<00:00, 42.25it/s, loss=0.591, v_num=0o7p, LTC_val_\u001b[A\n","Epoch 7:  99%|▉| 72/73 [00:01<00:00, 40.99it/s, loss=0.606, v_num=0o7p, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 73/73 [00:01<00:00, 40.73it/s, loss=0.606, v_num=0o7p, LTC_val_\u001b[A\n","Epoch 8:  99%|▉| 72/73 [00:01<00:00, 40.90it/s, loss=0.581, v_num=0o7p, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 73/73 [00:01<00:00, 40.62it/s, loss=0.581, v_num=0o7p, LTC_val_\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 9:  99%|▉| 72/73 [00:01<00:00, 41.02it/s, loss=0.548, v_num=0o7p, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 73/73 [00:01<00:00, 40.74it/s, loss=0.548, v_num=0o7p, LTC_val_\u001b[A\n","Epoch 10:  99%|▉| 72/73 [00:01<00:00, 39.72it/s, loss=0.601, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 73/73 [00:01<00:00, 39.45it/s, loss=0.601, v_num=0o7p, LTC_val\u001b[A\n","Epoch 11:  99%|▉| 72/73 [00:02<00:00, 35.54it/s, loss=0.584, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 73/73 [00:02<00:00, 35.41it/s, loss=0.584, v_num=0o7p, LTC_val\u001b[A\n","Epoch 12:  99%|▉| 72/73 [00:01<00:00, 37.51it/s, loss=0.574, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 73/73 [00:01<00:00, 37.32it/s, loss=0.574, v_num=0o7p, LTC_val\u001b[A\n","Epoch 13:  99%|▉| 72/73 [00:01<00:00, 41.33it/s, loss=0.596, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.516\n","Epoch 13: 100%|█| 73/73 [00:01<00:00, 40.92it/s, loss=0.596, v_num=0o7p, LTC_val\n","Epoch 14:  99%|▉| 72/73 [00:01<00:00, 40.18it/s, loss=0.549, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 73/73 [00:01<00:00, 39.74it/s, loss=0.549, v_num=0o7p, LTC_val\u001b[A\n","Epoch 15:  99%|▉| 72/73 [00:02<00:00, 29.34it/s, loss=0.571, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 73/73 [00:02<00:00, 29.31it/s, loss=0.571, v_num=0o7p, LTC_val\u001b[A\n","Epoch 16:  99%|▉| 72/73 [00:01<00:00, 40.35it/s, loss=0.59, v_num=0o7p, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 73/73 [00:01<00:00, 40.01it/s, loss=0.59, v_num=0o7p, LTC_val_\u001b[A\n","Epoch 17:  99%|▉| 72/73 [00:01<00:00, 37.88it/s, loss=0.585, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 73/73 [00:01<00:00, 37.53it/s, loss=0.585, v_num=0o7p, LTC_val\u001b[A\n","Epoch 18:  99%|▉| 72/73 [00:02<00:00, 30.81it/s, loss=0.567, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 73/73 [00:02<00:00, 30.40it/s, loss=0.567, v_num=0o7p, LTC_val\u001b[A\n","Epoch 19:  99%|▉| 72/73 [00:02<00:00, 27.75it/s, loss=0.541, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 73/73 [00:02<00:00, 27.69it/s, loss=0.541, v_num=0o7p, LTC_val\u001b[A\n","Epoch 20:  99%|▉| 72/73 [00:02<00:00, 29.81it/s, loss=0.633, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 73/73 [00:02<00:00, 27.14it/s, loss=0.633, v_num=0o7p, LTC_val\u001b[A\n","Epoch 21:  99%|▉| 72/73 [00:02<00:00, 28.81it/s, loss=0.583, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 73/73 [00:02<00:00, 28.77it/s, loss=0.583, v_num=0o7p, LTC_val\u001b[A\n","Epoch 22:  99%|▉| 72/73 [00:02<00:00, 26.71it/s, loss=0.54, v_num=0o7p, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 0.495\n","Epoch 22: 100%|█| 73/73 [00:02<00:00, 26.59it/s, loss=0.54, v_num=0o7p, LTC_val_\n","Epoch 23:  99%|▉| 72/73 [00:02<00:00, 28.62it/s, loss=0.531, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 73/73 [00:02<00:00, 28.52it/s, loss=0.531, v_num=0o7p, LTC_val\u001b[A\n","Epoch 24:  99%|▉| 72/73 [00:02<00:00, 29.95it/s, loss=0.589, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 73/73 [00:02<00:00, 29.86it/s, loss=0.589, v_num=0o7p, LTC_val\u001b[A\n","Epoch 25:  99%|▉| 72/73 [00:02<00:00, 27.17it/s, loss=0.507, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 73/73 [00:02<00:00, 27.15it/s, loss=0.507, v_num=0o7p, LTC_val\u001b[A\n","Epoch 26:  99%|▉| 72/73 [00:01<00:00, 39.75it/s, loss=0.545, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 73/73 [00:01<00:00, 39.51it/s, loss=0.545, v_num=0o7p, LTC_val\u001b[A\n","Epoch 27:  99%|▉| 72/73 [00:01<00:00, 37.84it/s, loss=0.598, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 73/73 [00:01<00:00, 37.65it/s, loss=0.598, v_num=0o7p, LTC_val\u001b[A\n","Epoch 28:  99%|▉| 72/73 [00:01<00:00, 38.26it/s, loss=0.589, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 73/73 [00:01<00:00, 38.03it/s, loss=0.589, v_num=0o7p, LTC_val\u001b[A\n","Epoch 29:  99%|▉| 72/73 [00:01<00:00, 37.93it/s, loss=0.516, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 73/73 [00:01<00:00, 37.66it/s, loss=0.516, v_num=0o7p, LTC_val\u001b[A\n","Epoch 30:  99%|▉| 72/73 [00:01<00:00, 37.92it/s, loss=0.558, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 73/73 [00:01<00:00, 37.74it/s, loss=0.558, v_num=0o7p, LTC_val\u001b[A\n","Epoch 31:  99%|▉| 72/73 [00:02<00:00, 30.31it/s, loss=0.537, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 73/73 [00:02<00:00, 30.25it/s, loss=0.537, v_num=0o7p, LTC_val\u001b[A\n","Epoch 32:  99%|▉| 72/73 [00:02<00:00, 25.10it/s, loss=0.527, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 73/73 [00:02<00:00, 25.05it/s, loss=0.527, v_num=0o7p, LTC_val\u001b[A\n","Epoch 33:  99%|▉| 72/73 [00:02<00:00, 29.59it/s, loss=0.536, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 0.481\n","Epoch 33: 100%|█| 73/73 [00:02<00:00, 29.57it/s, loss=0.536, v_num=0o7p, LTC_val\n","Epoch 34:  99%|▉| 72/73 [00:02<00:00, 28.99it/s, loss=0.53, v_num=0o7p, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 73/73 [00:02<00:00, 28.74it/s, loss=0.53, v_num=0o7p, LTC_val_\u001b[A\n","Epoch 35:  99%|▉| 72/73 [00:02<00:00, 30.45it/s, loss=0.551, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 35: 100%|█| 73/73 [00:02<00:00, 30.32it/s, loss=0.551, v_num=0o7p, LTC_val\u001b[A\n","Epoch 36:  99%|▉| 72/73 [00:02<00:00, 35.46it/s, loss=0.587, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 36: 100%|█| 73/73 [00:02<00:00, 35.11it/s, loss=0.587, v_num=0o7p, LTC_val\u001b[A\n","Epoch 37:  99%|▉| 72/73 [00:01<00:00, 38.74it/s, loss=0.534, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 37: 100%|█| 73/73 [00:01<00:00, 38.40it/s, loss=0.534, v_num=0o7p, LTC_val\u001b[A\n","Epoch 38:  99%|▉| 72/73 [00:02<00:00, 33.51it/s, loss=0.531, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 38: 100%|█| 73/73 [00:02<00:00, 33.32it/s, loss=0.531, v_num=0o7p, LTC_val\u001b[A\n","Epoch 39:  99%|▉| 72/73 [00:02<00:00, 35.64it/s, loss=0.575, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 39: 100%|█| 73/73 [00:02<00:00, 35.19it/s, loss=0.575, v_num=0o7p, LTC_val\u001b[A\n","Epoch 40:  99%|▉| 72/73 [00:02<00:00, 33.32it/s, loss=0.498, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.047 >= min_delta = 0.003. New best score: 0.434\n","Epoch 40: 100%|█| 73/73 [00:02<00:00, 33.05it/s, loss=0.498, v_num=0o7p, LTC_val\n","Epoch 41:  99%|▉| 72/73 [00:02<00:00, 31.59it/s, loss=0.56, v_num=0o7p, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 41: 100%|█| 73/73 [00:02<00:00, 31.45it/s, loss=0.56, v_num=0o7p, LTC_val_\u001b[A\n","Epoch 42:  99%|▉| 72/73 [00:02<00:00, 26.75it/s, loss=0.51, v_num=0o7p, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 42: 100%|█| 73/73 [00:02<00:00, 26.73it/s, loss=0.51, v_num=0o7p, LTC_val_\u001b[A\n","Epoch 43:  99%|▉| 72/73 [00:02<00:00, 33.29it/s, loss=0.527, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 43: 100%|█| 73/73 [00:02<00:00, 33.17it/s, loss=0.527, v_num=0o7p, LTC_val\u001b[A\n","Epoch 44:  99%|▉| 72/73 [00:02<00:00, 33.97it/s, loss=0.551, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.038 >= min_delta = 0.003. New best score: 0.396\n","Epoch 44: 100%|█| 73/73 [00:02<00:00, 33.81it/s, loss=0.551, v_num=0o7p, LTC_val\n","Epoch 45:  99%|▉| 72/73 [00:02<00:00, 28.14it/s, loss=0.539, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 45: 100%|█| 73/73 [00:02<00:00, 27.13it/s, loss=0.539, v_num=0o7p, LTC_val\u001b[A\n","Epoch 46:  99%|▉| 72/73 [00:02<00:00, 31.22it/s, loss=0.488, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 46: 100%|█| 73/73 [00:02<00:00, 31.17it/s, loss=0.488, v_num=0o7p, LTC_val\u001b[A\n","Epoch 47:  99%|▉| 72/73 [00:01<00:00, 39.46it/s, loss=0.512, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 47: 100%|█| 73/73 [00:01<00:00, 38.91it/s, loss=0.512, v_num=0o7p, LTC_val\u001b[A\n","Epoch 48:  99%|▉| 72/73 [00:01<00:00, 39.79it/s, loss=0.53, v_num=0o7p, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 48: 100%|█| 73/73 [00:01<00:00, 39.43it/s, loss=0.53, v_num=0o7p, LTC_val_\u001b[A\n","Epoch 49:  99%|▉| 72/73 [00:01<00:00, 40.49it/s, loss=0.542, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 49: 100%|█| 73/73 [00:01<00:00, 40.00it/s, loss=0.542, v_num=0o7p, LTC_val\u001b[A\n","Epoch 50:  99%|▉| 72/73 [00:01<00:00, 40.31it/s, loss=0.473, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 50: 100%|█| 73/73 [00:01<00:00, 39.90it/s, loss=0.473, v_num=0o7p, LTC_val\u001b[A\n","Epoch 51:  99%|▉| 72/73 [00:01<00:00, 40.53it/s, loss=0.473, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 51: 100%|█| 73/73 [00:01<00:00, 40.12it/s, loss=0.473, v_num=0o7p, LTC_val\u001b[A\n","Epoch 52:  99%|▉| 72/73 [00:01<00:00, 39.66it/s, loss=0.538, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 52: 100%|█| 73/73 [00:01<00:00, 39.29it/s, loss=0.538, v_num=0o7p, LTC_val\u001b[A\n","Epoch 53:  99%|▉| 72/73 [00:01<00:00, 40.63it/s, loss=0.494, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 53: 100%|█| 73/73 [00:01<00:00, 40.25it/s, loss=0.494, v_num=0o7p, LTC_val\u001b[A\n","Epoch 54:  99%|▉| 72/73 [00:02<00:00, 35.93it/s, loss=0.51, v_num=0o7p, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 54: 100%|█| 73/73 [00:02<00:00, 35.63it/s, loss=0.51, v_num=0o7p, LTC_val_\u001b[A\n","Epoch 55:  99%|▉| 72/73 [00:02<00:00, 30.88it/s, loss=0.503, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 55: 100%|█| 73/73 [00:02<00:00, 30.73it/s, loss=0.503, v_num=0o7p, LTC_val\u001b[A\n","Epoch 56:  99%|▉| 72/73 [00:02<00:00, 35.20it/s, loss=0.486, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n","Epoch 56: 100%|█| 73/73 [00:02<00:00, 32.62it/s, loss=0.486, v_num=0o7p, LTC_val\u001b[A\n","Epoch 57:  99%|▉| 72/73 [00:02<00:00, 28.08it/s, loss=0.487, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.392\n","Epoch 57: 100%|█| 73/73 [00:02<00:00, 27.94it/s, loss=0.487, v_num=0o7p, LTC_val\n","Epoch 58:  99%|▉| 72/73 [00:02<00:00, 29.70it/s, loss=0.508, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 58: 100%|█| 73/73 [00:02<00:00, 29.46it/s, loss=0.508, v_num=0o7p, LTC_val\u001b[A\n","Epoch 59:  99%|▉| 72/73 [00:01<00:00, 40.09it/s, loss=0.446, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 59: 100%|█| 73/73 [00:01<00:00, 39.83it/s, loss=0.446, v_num=0o7p, LTC_val\u001b[A\n","Epoch 60:  99%|▉| 72/73 [00:02<00:00, 31.90it/s, loss=0.481, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 60: 100%|█| 73/73 [00:02<00:00, 31.79it/s, loss=0.481, v_num=0o7p, LTC_val\u001b[A\n","Epoch 61:  99%|▉| 72/73 [00:02<00:00, 32.69it/s, loss=0.54, v_num=0o7p, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n","Epoch 61: 100%|█| 73/73 [00:02<00:00, 30.99it/s, loss=0.54, v_num=0o7p, LTC_val_\u001b[A\n","Epoch 62:  99%|▉| 72/73 [00:02<00:00, 30.64it/s, loss=0.458, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.388\n","Epoch 62: 100%|█| 73/73 [00:02<00:00, 30.50it/s, loss=0.458, v_num=0o7p, LTC_val\n","Epoch 63:  99%|▉| 72/73 [00:02<00:00, 32.82it/s, loss=0.436, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 63: 100%|█| 73/73 [00:02<00:00, 32.63it/s, loss=0.436, v_num=0o7p, LTC_val\u001b[A\n","Epoch 64:  99%|▉| 72/73 [00:02<00:00, 33.40it/s, loss=0.385, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.033 >= min_delta = 0.003. New best score: 0.355\n","Epoch 64: 100%|█| 73/73 [00:02<00:00, 33.31it/s, loss=0.385, v_num=0o7p, LTC_val\n","Epoch 65:  99%|▉| 72/73 [00:03<00:00, 22.93it/s, loss=0.496, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 65: 100%|█| 73/73 [00:03<00:00, 22.94it/s, loss=0.496, v_num=0o7p, LTC_val\u001b[A\n","Epoch 66:  99%|▉| 72/73 [00:01<00:00, 40.08it/s, loss=0.486, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 66: 100%|█| 73/73 [00:01<00:00, 39.64it/s, loss=0.486, v_num=0o7p, LTC_val\u001b[A\n","Epoch 67:  99%|▉| 72/73 [00:01<00:00, 37.19it/s, loss=0.395, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 67: 100%|█| 73/73 [00:01<00:00, 36.93it/s, loss=0.395, v_num=0o7p, LTC_val\u001b[A\n","Epoch 68:  99%|▉| 72/73 [00:02<00:00, 33.11it/s, loss=0.461, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 68: 100%|█| 73/73 [00:02<00:00, 32.90it/s, loss=0.461, v_num=0o7p, LTC_val\u001b[A\n","Epoch 69:  99%|▉| 72/73 [00:02<00:00, 31.43it/s, loss=0.384, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 69: 100%|█| 73/73 [00:02<00:00, 31.23it/s, loss=0.384, v_num=0o7p, LTC_val\u001b[A\n","Epoch 70:  99%|▉| 72/73 [00:02<00:00, 30.85it/s, loss=0.408, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 70: 100%|█| 73/73 [00:02<00:00, 30.76it/s, loss=0.408, v_num=0o7p, LTC_val\u001b[A\n","Epoch 71:  99%|▉| 72/73 [00:01<00:00, 39.27it/s, loss=0.429, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 71: 100%|█| 73/73 [00:01<00:00, 38.91it/s, loss=0.429, v_num=0o7p, LTC_val\u001b[A\n","Epoch 72:  99%|▉| 72/73 [00:01<00:00, 39.36it/s, loss=0.466, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 72: 100%|█| 73/73 [00:01<00:00, 38.88it/s, loss=0.466, v_num=0o7p, LTC_val\u001b[A\n","Epoch 73:  99%|▉| 72/73 [00:01<00:00, 40.72it/s, loss=0.393, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 73: 100%|█| 73/73 [00:01<00:00, 40.28it/s, loss=0.393, v_num=0o7p, LTC_val\u001b[A\n","Epoch 74:  99%|▉| 72/73 [00:01<00:00, 40.01it/s, loss=0.356, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 74: 100%|█| 73/73 [00:01<00:00, 39.44it/s, loss=0.356, v_num=0o7p, LTC_val\u001b[A\n","Epoch 75:  99%|▉| 72/73 [00:01<00:00, 40.25it/s, loss=0.396, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 75: 100%|█| 73/73 [00:01<00:00, 39.85it/s, loss=0.396, v_num=0o7p, LTC_val\u001b[A\n","Epoch 76:  99%|▉| 72/73 [00:01<00:00, 40.03it/s, loss=0.384, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 76: 100%|█| 73/73 [00:01<00:00, 39.52it/s, loss=0.384, v_num=0o7p, LTC_val\u001b[A\n","Epoch 77:  99%|▉| 72/73 [00:01<00:00, 40.59it/s, loss=0.376, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 77: 100%|█| 73/73 [00:01<00:00, 40.17it/s, loss=0.376, v_num=0o7p, LTC_val\u001b[A\n","Epoch 78:  99%|▉| 72/73 [00:01<00:00, 40.08it/s, loss=0.356, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 78: 100%|█| 73/73 [00:01<00:00, 39.79it/s, loss=0.356, v_num=0o7p, LTC_val\u001b[A\n","Epoch 79:  99%|▉| 72/73 [00:01<00:00, 39.86it/s, loss=0.368, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 79: 100%|█| 73/73 [00:01<00:00, 39.56it/s, loss=0.368, v_num=0o7p, LTC_val\u001b[A\n","Epoch 80:  99%|▉| 72/73 [00:01<00:00, 39.61it/s, loss=0.381, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 80: 100%|█| 73/73 [00:01<00:00, 39.26it/s, loss=0.381, v_num=0o7p, LTC_val\u001b[A\n","Epoch 81:  99%|▉| 72/73 [00:01<00:00, 39.22it/s, loss=0.371, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 81: 100%|█| 73/73 [00:01<00:00, 38.94it/s, loss=0.371, v_num=0o7p, LTC_val\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 82:  99%|▉| 72/73 [00:01<00:00, 39.32it/s, loss=0.387, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 82: 100%|█| 73/73 [00:01<00:00, 39.03it/s, loss=0.387, v_num=0o7p, LTC_val\u001b[A\n","Epoch 83:  99%|▉| 72/73 [00:01<00:00, 40.13it/s, loss=0.367, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 83: 100%|█| 73/73 [00:01<00:00, 38.64it/s, loss=0.367, v_num=0o7p, LTC_val\u001b[A\n","Epoch 84:  99%|▉| 72/73 [00:01<00:00, 39.46it/s, loss=0.369, v_num=0o7p, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 84: 100%|█| 73/73 [00:01<00:00, 39.16it/s, loss=0.369, v_num=0o7p, LTC_val\u001b[A\n","                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.355. Signaling Trainer to stop.\n","Epoch 84: 100%|█| 73/73 [00:01<00:00, 39.09it/s, loss=0.369, v_num=0o7p, LTC_val\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 82.68it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'LTC_test_acc': 0.75,\n"," 'LTC_test_f1': 0.7463489174842834,\n"," 'test_loss': 0.48873159289360046}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 70139\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013452-233c0o7p/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013452-233c0o7p/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 1.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 1.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.16663\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 84\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 6120\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 189\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463881\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 292\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.81337\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.80411\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.3506\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.42625\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.74635\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.48873\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▃▂▄▂▃▃▆▄▄▃▂▂▄▄▃▆▆▃▂▃▅▆▁▄▂▄▄▄▇▂▇▄▆▂▅▆▄▆▄█\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▃▂▄▂▃▂▆▄▄▃▂▂▄▄▃▆▆▃▂▃▅▆▁▃▂▄▄▄▇▂▇▃▆▁▄▆▄▆▄█\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆█▅▇▆▆▃▆▅▅▆█▅▅▅▄▄▄▅▅▆▄▆▆▄▄▅▆▂▅▅▄▂▇▃▃▅▃▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▅▅▆▅▅▆▅▅▅▆▆▆▆▆▆▆▅▆▆▆▆▆▆▆▇▇▇▆▇▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▅▅▆▅▅▆▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▆▇▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▆▆▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▄▅▄▄▄▄▄▃▃▃▂▃▃▂▂▂▂▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc █▁█▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 █▁█▄▄▄▄▄▄▄▄▄▄▄▄▄█▄▄▄████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ██▆▆▆▆▆▅▅▇▅▆▅▅▅▅▆▅▆▃▄▃▂▃▂▂▂▃▂▂▁▂▄▂▂▃▄▄▄▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/233c0o7p\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:38:17.138439: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_loss_weighted_binary_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/6fbrw9ap\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013815-6fbrw9ap\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 130   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 73/74 [00:01<00:00, 41.35it/s, loss=0.708, v_num=w9ap, LTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.703\n","Epoch 0: 100%|█| 74/74 [00:01<00:00, 40.76it/s, loss=0.708, v_num=w9ap, LTC_val_\n","Epoch 1:  99%|▉| 73/74 [00:01<00:00, 41.28it/s, loss=0.702, v_num=w9ap, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 0.692\n","Epoch 1: 100%|█| 74/74 [00:01<00:00, 41.02it/s, loss=0.702, v_num=w9ap, LTC_val_\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 2:  99%|▉| 73/74 [00:01<00:00, 37.53it/s, loss=0.703, v_num=w9ap, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 74/74 [00:01<00:00, 37.22it/s, loss=0.703, v_num=w9ap, LTC_val_\u001b[A\n","Epoch 3:  99%|▉| 73/74 [00:01<00:00, 38.73it/s, loss=0.698, v_num=w9ap, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 74/74 [00:01<00:00, 38.46it/s, loss=0.698, v_num=w9ap, LTC_val_\u001b[A\n","Epoch 4:  99%|▉| 73/74 [00:01<00:00, 38.58it/s, loss=0.707, v_num=w9ap, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 74/74 [00:01<00:00, 38.35it/s, loss=0.707, v_num=w9ap, LTC_val_\u001b[A\n","Epoch 5:  99%|▉| 73/74 [00:02<00:00, 31.64it/s, loss=0.692, v_num=w9ap, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 74/74 [00:02<00:00, 31.58it/s, loss=0.692, v_num=w9ap, LTC_val_\u001b[A\n","Epoch 6:  99%|▉| 73/74 [00:02<00:00, 27.48it/s, loss=0.699, v_num=w9ap, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 74/74 [00:02<00:00, 27.43it/s, loss=0.699, v_num=w9ap, LTC_val_\u001b[A\n","Epoch 7:  99%|▉| 73/74 [00:01<00:00, 40.83it/s, loss=0.704, v_num=w9ap, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 74/74 [00:01<00:00, 40.51it/s, loss=0.704, v_num=w9ap, LTC_val_\u001b[A\n","Epoch 8:  99%|▉| 73/74 [00:01<00:00, 41.02it/s, loss=0.698, v_num=w9ap, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 74/74 [00:01<00:00, 40.75it/s, loss=0.698, v_num=w9ap, LTC_val_\u001b[A\n","Epoch 9:  99%|▉| 73/74 [00:01<00:00, 38.94it/s, loss=0.693, v_num=w9ap, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 74/74 [00:01<00:00, 38.72it/s, loss=0.693, v_num=w9ap, LTC_val_\u001b[A\n","Epoch 10:  99%|▉| 73/74 [00:01<00:00, 40.22it/s, loss=0.701, v_num=w9ap, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 74/74 [00:01<00:00, 39.65it/s, loss=0.701, v_num=w9ap, LTC_val\u001b[A\n","Epoch 11:  99%|▉| 73/74 [00:02<00:00, 27.13it/s, loss=0.693, v_num=w9ap, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 74/74 [00:02<00:00, 27.06it/s, loss=0.693, v_num=w9ap, LTC_val\u001b[A\n","Epoch 12:  99%|▉| 73/74 [00:02<00:00, 35.35it/s, loss=0.695, v_num=w9ap, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 74/74 [00:02<00:00, 35.05it/s, loss=0.695, v_num=w9ap, LTC_val\u001b[A\n","Epoch 13:  99%|▉| 73/74 [00:01<00:00, 41.23it/s, loss=0.692, v_num=w9ap, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 74/74 [00:01<00:00, 40.79it/s, loss=0.692, v_num=w9ap, LTC_val\u001b[A\n","Epoch 14:  99%|▉| 73/74 [00:01<00:00, 38.12it/s, loss=0.692, v_num=w9ap, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 74/74 [00:01<00:00, 37.80it/s, loss=0.692, v_num=w9ap, LTC_val\u001b[A\n","Epoch 15:  99%|▉| 73/74 [00:01<00:00, 39.79it/s, loss=0.699, v_num=w9ap, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 74/74 [00:01<00:00, 38.08it/s, loss=0.699, v_num=w9ap, LTC_val\u001b[A\n","Epoch 16:  99%|▉| 73/74 [00:02<00:00, 26.05it/s, loss=0.697, v_num=w9ap, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 74/74 [00:02<00:00, 26.03it/s, loss=0.697, v_num=w9ap, LTC_val\u001b[A\n","Epoch 17:  99%|▉| 73/74 [00:01<00:00, 42.67it/s, loss=0.699, v_num=w9ap, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 74/74 [00:01<00:00, 42.14it/s, loss=0.699, v_num=w9ap, LTC_val\u001b[A\n","Epoch 18:  99%|▉| 73/74 [00:01<00:00, 42.88it/s, loss=0.698, v_num=w9ap, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 74/74 [00:01<00:00, 42.40it/s, loss=0.698, v_num=w9ap, LTC_val\u001b[A\n","Epoch 19:  99%|▉| 73/74 [00:01<00:00, 42.15it/s, loss=0.697, v_num=w9ap, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 74/74 [00:01<00:00, 41.58it/s, loss=0.697, v_num=w9ap, LTC_val\u001b[A\n","Epoch 20:  99%|▉| 73/74 [00:01<00:00, 42.58it/s, loss=0.694, v_num=w9ap, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 74/74 [00:01<00:00, 42.05it/s, loss=0.694, v_num=w9ap, LTC_val\u001b[A\n","Epoch 21:  99%|▉| 73/74 [00:01<00:00, 39.49it/s, loss=0.694, v_num=w9ap, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.692. Signaling Trainer to stop.\n","Epoch 21: 100%|█| 74/74 [00:01<00:00, 39.22it/s, loss=0.694, v_num=w9ap, LTC_val\n","Epoch 21: 100%|█| 74/74 [00:01<00:00, 39.15it/s, loss=0.694, v_num=w9ap, LTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 72.16it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'LTC_test_acc': 0.5,\n"," 'LTC_test_f1': 0.3272727429866791,\n"," 'test_loss': 0.6933034062385559}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 70606\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013815-6fbrw9ap/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013815-6fbrw9ap/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.5625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.54656\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.68455\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 21\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1606\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 53\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621463948\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 76\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.48359\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.47013\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69423\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.33333\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.69308\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.32727\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.6933\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▆▁▆▆▃▁▅▂▂▂█▇▇▃▅▂▃▆▅▅▂▂▅▇▂▃▃▅▁▅█▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▅▁▅▅▁▁▄▂▂▂▇▆▇▃▄▂▂▄▅▄▂▂▄▆▂▁▃▄▁▄█▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▁█▃▄▄▆▄▄▄▄▃▃▃▃▃▃▃▃▄▃▄▄▃▃▃▃▄▃▄▄▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▆▄▆▁▅█▂▆▆▂▄▆▄▇▅▅▅▅▆▅▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▆▅▅▁▁▆▃█▇▄▄▆▄▇▆▆▅▄▅▅▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▄▄▃▁▄▂▂▃▂▁▁▂▂▂▂▂▁▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ██████▁██████████▁████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ██████▁██████████▁████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▁▁▂▂▂▂▂▂▂▁▁▁▂▂▂▁▁▁▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/6fbrw9ap\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:39:26.143843: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2i1habeu\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013924-2i1habeu\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 195   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:   0%|                                           | 0/73 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 72/73 [00:01<00:00, 43.13it/s, loss=1.13, v_num=abeu, LTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.054\n","Epoch 0: 100%|█| 73/73 [00:01<00:00, 40.55it/s, loss=1.13, v_num=abeu, LTC_val_a\n","Epoch 1:  99%|▉| 72/73 [00:01<00:00, 43.70it/s, loss=1.1, v_num=abeu, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 73/73 [00:01<00:00, 43.19it/s, loss=1.1, v_num=abeu, LTC_val_ac\u001b[A\n","Epoch 2:  99%|▉| 72/73 [00:01<00:00, 42.48it/s, loss=1.1, v_num=abeu, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 73/73 [00:01<00:00, 42.19it/s, loss=1.1, v_num=abeu, LTC_val_ac\u001b[A\n","Epoch 3:  99%|▉| 72/73 [00:02<00:00, 35.75it/s, loss=1.11, v_num=abeu, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 73/73 [00:02<00:00, 35.60it/s, loss=1.11, v_num=abeu, LTC_val_a\u001b[A\n","Epoch 4:  99%|▉| 72/73 [00:01<00:00, 42.13it/s, loss=1.11, v_num=abeu, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 73/73 [00:01<00:00, 41.82it/s, loss=1.11, v_num=abeu, LTC_val_a\u001b[A\n","Epoch 5:  99%|▉| 72/73 [00:01<00:00, 41.50it/s, loss=1.11, v_num=abeu, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 73/73 [00:01<00:00, 41.24it/s, loss=1.11, v_num=abeu, LTC_val_a\u001b[A\n","Epoch 6:  99%|▉| 72/73 [00:02<00:00, 35.66it/s, loss=1.11, v_num=abeu, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 73/73 [00:02<00:00, 35.54it/s, loss=1.11, v_num=abeu, LTC_val_a\u001b[A\n","Epoch 7:  99%|▉| 72/73 [00:01<00:00, 41.72it/s, loss=1.1, v_num=abeu, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 73/73 [00:01<00:00, 41.47it/s, loss=1.1, v_num=abeu, LTC_val_ac\u001b[A\n","Epoch 8:  99%|▉| 72/73 [00:01<00:00, 38.60it/s, loss=1.1, v_num=abeu, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 73/73 [00:01<00:00, 38.26it/s, loss=1.1, v_num=abeu, LTC_val_ac\u001b[A\n","Epoch 9:  99%|▉| 72/73 [00:02<00:00, 35.16it/s, loss=1.1, v_num=abeu, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 73/73 [00:02<00:00, 35.04it/s, loss=1.1, v_num=abeu, LTC_val_ac\u001b[A\n","Epoch 10:  99%|▉| 72/73 [00:01<00:00, 41.66it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 73/73 [00:01<00:00, 41.34it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n","Epoch 11:  99%|▉| 72/73 [00:01<00:00, 38.22it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 73/73 [00:01<00:00, 37.78it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n","Epoch 12:  99%|▉| 72/73 [00:01<00:00, 41.50it/s, loss=1.11, v_num=abeu, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 73/73 [00:01<00:00, 40.98it/s, loss=1.11, v_num=abeu, LTC_val_\u001b[A\n","Epoch 13:  99%|▉| 72/73 [00:02<00:00, 34.21it/s, loss=1.11, v_num=abeu, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 73/73 [00:02<00:00, 34.02it/s, loss=1.11, v_num=abeu, LTC_val_\u001b[A\n","Epoch 14:  99%|▉| 72/73 [00:01<00:00, 39.22it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 73/73 [00:01<00:00, 38.88it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n","Epoch 15:  99%|▉| 72/73 [00:02<00:00, 35.61it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 73/73 [00:02<00:00, 35.30it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n","Epoch 16:  99%|▉| 72/73 [00:01<00:00, 39.02it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 73/73 [00:01<00:00, 38.68it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n","Epoch 17:  99%|▉| 72/73 [00:01<00:00, 38.72it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 73/73 [00:01<00:00, 38.34it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n","Epoch 18:  99%|▉| 72/73 [00:01<00:00, 39.25it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 73/73 [00:01<00:00, 38.90it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n","Epoch 19:  99%|▉| 72/73 [00:01<00:00, 41.94it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 73/73 [00:01<00:00, 41.42it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n","Epoch 20:  99%|▉| 72/73 [00:01<00:00, 41.65it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.054. Signaling Trainer to stop.\n","Epoch 20: 100%|█| 73/73 [00:01<00:00, 41.17it/s, loss=1.1, v_num=abeu, LTC_val_a\n","Epoch 20: 100%|█| 73/73 [00:01<00:00, 41.09it/s, loss=1.1, v_num=abeu, LTC_val_a\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 77.21it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'LTC_test_acc': 0.1785714328289032,\n"," 'LTC_test_f1': 0.0981685072183609,\n"," 'test_loss': 1.1191810369491577}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 70789\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013924-2i1habeu/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_013924-2i1habeu/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.5625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.3619\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.107\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1512\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 48\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464012\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 72\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.40538\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.29441\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.0996\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.18182\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.10387\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.17857\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.09817\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.11918\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▃▁▃▆▃▂▃▅▅▄▃▂▂▅▄▄▅█▃▆▂▅▅▇▅▅▂▆▃▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▃▁▄▆▃▁▃▄▆▃▃▃▂▄▂▃▄█▄▄▂▅▄▃▂▅▂▅▂▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ██▅▅▃▄▇▄▂▄▆▅▅▇▁▃▃▂▄▃▅▄▃▂▄▃▄▃▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▃▃▅▁▄▅▅▄▄▆▆▅▆▅▇▆██▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▂▄▄▁▃▅▅▃▄▆▅▅▅▆▄▄▃▂█▆▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▄▄▃▃▃▄▃▁▂▂▃▃▂▂▂▂▁▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▆▆█▅▆▅▆▇▇▆▇▆▇▆▇▆▆▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2i1habeu\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:40:28.177213: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_loss_weighted_multi_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/xlclrwnp\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014026-xlclrwnp\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 195   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0: 100%|█| 74/74 [00:01<00:00, 42.13it/s, loss=1.13, v_num=rwnp, LTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.034\n","Epoch 0: 100%|█| 74/74 [00:01<00:00, 41.87it/s, loss=1.13, v_num=rwnp, LTC_val_a\n","Epoch 1:  99%|▉| 73/74 [00:01<00:00, 43.59it/s, loss=1.11, v_num=rwnp, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 74/74 [00:01<00:00, 42.93it/s, loss=1.11, v_num=rwnp, LTC_val_a\u001b[A\n","Epoch 2:  99%|▉| 73/74 [00:01<00:00, 42.17it/s, loss=1.11, v_num=rwnp, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 74/74 [00:01<00:00, 41.86it/s, loss=1.11, v_num=rwnp, LTC_val_a\u001b[A\n","Epoch 3:  99%|▉| 73/74 [00:01<00:00, 39.44it/s, loss=1.11, v_num=rwnp, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 74/74 [00:01<00:00, 38.94it/s, loss=1.11, v_num=rwnp, LTC_val_a\u001b[A\n","Epoch 4:  99%|▉| 73/74 [00:02<00:00, 35.10it/s, loss=1.1, v_num=rwnp, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 74/74 [00:02<00:00, 34.94it/s, loss=1.1, v_num=rwnp, LTC_val_ac\u001b[A\n","Epoch 5:  99%|▉| 73/74 [00:01<00:00, 41.94it/s, loss=1.11, v_num=rwnp, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 74/74 [00:01<00:00, 41.65it/s, loss=1.11, v_num=rwnp, LTC_val_a\u001b[A\n","Epoch 6:  99%|▉| 73/74 [00:01<00:00, 41.57it/s, loss=1.1, v_num=rwnp, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 74/74 [00:01<00:00, 41.30it/s, loss=1.1, v_num=rwnp, LTC_val_ac\u001b[A\n","Epoch 7:  99%|▉| 73/74 [00:02<00:00, 35.49it/s, loss=1.1, v_num=rwnp, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 74/74 [00:02<00:00, 35.34it/s, loss=1.1, v_num=rwnp, LTC_val_ac\u001b[A\n","Epoch 8:  99%|▉| 73/74 [00:01<00:00, 41.45it/s, loss=1.1, v_num=rwnp, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 74/74 [00:01<00:00, 41.18it/s, loss=1.1, v_num=rwnp, LTC_val_ac\u001b[A\n","Epoch 9:  99%|▉| 73/74 [00:01<00:00, 40.62it/s, loss=1.1, v_num=rwnp, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 74/74 [00:01<00:00, 40.39it/s, loss=1.1, v_num=rwnp, LTC_val_ac\u001b[A\n","Epoch 10:  99%|▉| 73/74 [00:02<00:00, 31.56it/s, loss=1.09, v_num=rwnp, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 74/74 [00:02<00:00, 31.32it/s, loss=1.09, v_num=rwnp, LTC_val_\u001b[A\n","Epoch 11:  99%|▉| 73/74 [00:02<00:00, 28.89it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 74/74 [00:02<00:00, 28.79it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 12:  99%|▉| 73/74 [00:02<00:00, 30.11it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 74/74 [00:02<00:00, 29.98it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n","Epoch 13:  99%|▉| 73/74 [00:01<00:00, 41.06it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 74/74 [00:01<00:00, 40.62it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n","Epoch 14:  99%|▉| 73/74 [00:01<00:00, 40.74it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 74/74 [00:01<00:00, 40.33it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n","Epoch 15:  99%|▉| 73/74 [00:01<00:00, 41.46it/s, loss=1.09, v_num=rwnp, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 74/74 [00:01<00:00, 39.70it/s, loss=1.09, v_num=rwnp, LTC_val_\u001b[A\n","Epoch 16:  99%|▉| 73/74 [00:02<00:00, 34.73it/s, loss=1.11, v_num=rwnp, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 74/74 [00:02<00:00, 34.46it/s, loss=1.11, v_num=rwnp, LTC_val_\u001b[A\n","Epoch 17:  99%|▉| 73/74 [00:02<00:00, 29.16it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 74/74 [00:02<00:00, 29.06it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n","Epoch 18:  99%|▉| 73/74 [00:01<00:00, 41.86it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 74/74 [00:01<00:00, 41.38it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n","Epoch 19:  99%|▉| 73/74 [00:01<00:00, 37.26it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 74/74 [00:02<00:00, 36.94it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n","Epoch 20:  99%|▉| 73/74 [00:01<00:00, 39.28it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.034. Signaling Trainer to stop.\n","Epoch 20: 100%|█| 74/74 [00:01<00:00, 38.88it/s, loss=1.1, v_num=rwnp, LTC_val_a\n","Epoch 20: 100%|█| 74/74 [00:01<00:00, 38.80it/s, loss=1.1, v_num=rwnp, LTC_val_a\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 73.71it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'LTC_test_acc': 0.25,\n"," 'LTC_test_f1': 0.13151928782463074,\n"," 'test_loss': 1.1264402866363525}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 70978\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014026-xlclrwnp/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014026-xlclrwnp/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.3125\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.31111\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.11554\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1533\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 51\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464077\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 72\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.35406\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.32887\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09848\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.22222\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.08551\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.25\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.13152\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.12644\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▂▅▁▂▅▅▇▄▃▃▂▄▁▄█▃▂▁▄▂▅▄▁▁▃▅▅▂▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▂▆▁▂▅▅▇▃▃▃▂▄▁▃█▂▂▁▃▂▄▄▁▂▃▄▅▂▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▂▂▄▅▃▁▁▃▄▆▅▃▅▄▂▄▅▆▄▅▃▄▄▅▅▅▃▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▄▅▆▁█▆▆█▅▆▃▅▄▄▆▄▅▄▇▆█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▄▅▁█▆▆▇▅▇▃▃▄▂▆▄▄▃▄▅█\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▄▄▂▂▂▁▂▂▂▂▂▂▁▁▂▁▁▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁███▁████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁███▁████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▅▅▆██▇▇▇▇▇█▇▇██▇█▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/xlclrwnp\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:41:32.689629: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_single_lstm__binary_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2cbnmwbm\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014131-2cbnmwbm\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 130   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.7 K    Trainable params\n","0         Non-trainable params\n","75.7 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 0:  99%|▉| 72/73 [00:00<00:00, 85.38it/s, loss=0.619, v_num=mwbm, LTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.511\n","Epoch 0: 100%|█| 73/73 [00:00<00:00, 82.29it/s, loss=0.619, v_num=mwbm, LTC_val_\n","Epoch 1:  99%|▉| 72/73 [00:01<00:00, 63.90it/s, loss=0.625, v_num=mwbm, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 73/73 [00:01<00:00, 62.62it/s, loss=0.625, v_num=mwbm, LTC_val_\u001b[A\n","Epoch 2:  99%|▉| 72/73 [00:00<00:00, 84.71it/s, loss=0.627, v_num=mwbm, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 73/73 [00:00<00:00, 81.97it/s, loss=0.627, v_num=mwbm, LTC_val_\u001b[A\n","Epoch 3:  99%|▉| 72/73 [00:00<00:00, 75.05it/s, loss=0.555, v_num=mwbm, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 73/73 [00:00<00:00, 73.42it/s, loss=0.555, v_num=mwbm, LTC_val_\u001b[A\n","Epoch 4:  99%|▉| 72/73 [00:00<00:00, 91.00it/s, loss=0.596, v_num=mwbm, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 73/73 [00:00<00:00, 88.61it/s, loss=0.596, v_num=mwbm, LTC_val_\u001b[A\n","Epoch 5:  99%|▉| 72/73 [00:00<00:00, 76.03it/s, loss=0.59, v_num=mwbm, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 73/73 [00:00<00:00, 74.17it/s, loss=0.59, v_num=mwbm, LTC_val_a\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.494\n","Epoch 6:  99%|▉| 72/73 [00:00<00:00, 78.68it/s, loss=0.553, v_num=mwbm, LTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 73/73 [00:00<00:00, 75.00it/s, loss=0.553, v_num=mwbm, LTC_val_\u001b[A\n","Epoch 7:  99%|▉| 72/73 [00:01<00:00, 66.43it/s, loss=0.589, v_num=mwbm, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 73/73 [00:01<00:00, 65.10it/s, loss=0.589, v_num=mwbm, LTC_val_\u001b[A\n","Epoch 8:  99%|▉| 72/73 [00:00<00:00, 87.27it/s, loss=0.614, v_num=mwbm, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 73/73 [00:00<00:00, 84.92it/s, loss=0.614, v_num=mwbm, LTC_val_\u001b[A\n","Epoch 9:  99%|▉| 72/73 [00:00<00:00, 73.57it/s, loss=0.582, v_num=mwbm, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 73/73 [00:01<00:00, 71.82it/s, loss=0.582, v_num=mwbm, LTC_val_\u001b[A\n","Epoch 10:  99%|▉| 72/73 [00:00<00:00, 88.98it/s, loss=0.565, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 73/73 [00:00<00:00, 86.65it/s, loss=0.565, v_num=mwbm, LTC_val\u001b[A\n","Epoch 11:  99%|▉| 72/73 [00:00<00:00, 74.64it/s, loss=0.546, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 73/73 [00:01<00:00, 72.83it/s, loss=0.546, v_num=mwbm, LTC_val\u001b[A\n","Epoch 12:  99%|▉| 72/73 [00:00<00:00, 84.99it/s, loss=0.592, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 73/73 [00:00<00:00, 82.70it/s, loss=0.592, v_num=mwbm, LTC_val\u001b[A\n","Epoch 13:  99%|▉| 72/73 [00:00<00:00, 76.55it/s, loss=0.543, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 73/73 [00:00<00:00, 74.12it/s, loss=0.543, v_num=mwbm, LTC_val\u001b[A\n","Epoch 14:  99%|▉| 72/73 [00:00<00:00, 82.26it/s, loss=0.541, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 73/73 [00:00<00:00, 80.23it/s, loss=0.541, v_num=mwbm, LTC_val\u001b[A\n","Epoch 15:  99%|▉| 72/73 [00:00<00:00, 79.67it/s, loss=0.575, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 73/73 [00:00<00:00, 77.14it/s, loss=0.575, v_num=mwbm, LTC_val\u001b[A\n","Epoch 16:  99%|▉| 72/73 [00:00<00:00, 77.99it/s, loss=0.561, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 73/73 [00:00<00:00, 76.21it/s, loss=0.561, v_num=mwbm, LTC_val\u001b[A\n","Epoch 17:  99%|▉| 72/73 [00:01<00:00, 70.12it/s, loss=0.53, v_num=mwbm, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.477\n","Epoch 17: 100%|█| 73/73 [00:01<00:00, 67.27it/s, loss=0.53, v_num=mwbm, LTC_val_\n","Epoch 18:  99%|▉| 72/73 [00:01<00:00, 55.47it/s, loss=0.575, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 73/73 [00:01<00:00, 54.35it/s, loss=0.575, v_num=mwbm, LTC_val\u001b[A\n","Epoch 19:  99%|▉| 72/73 [00:01<00:00, 64.82it/s, loss=0.575, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 73/73 [00:01<00:00, 63.61it/s, loss=0.575, v_num=mwbm, LTC_val\u001b[A\n","Epoch 20:  99%|▉| 72/73 [00:00<00:00, 86.67it/s, loss=0.561, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.473\n","Epoch 20: 100%|█| 73/73 [00:00<00:00, 84.48it/s, loss=0.561, v_num=mwbm, LTC_val\n","Epoch 21:  99%|▉| 72/73 [00:01<00:00, 69.70it/s, loss=0.588, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 73/73 [00:01<00:00, 68.24it/s, loss=0.588, v_num=mwbm, LTC_val\u001b[A\n","Epoch 22:  99%|▉| 72/73 [00:00<00:00, 79.50it/s, loss=0.558, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 73/73 [00:00<00:00, 77.28it/s, loss=0.558, v_num=mwbm, LTC_val\u001b[A\n","Epoch 23:  99%|▉| 72/73 [00:01<00:00, 69.91it/s, loss=0.59, v_num=mwbm, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 73/73 [00:01<00:00, 68.51it/s, loss=0.59, v_num=mwbm, LTC_val_\u001b[A\n","Epoch 24:  99%|▉| 72/73 [00:00<00:00, 87.79it/s, loss=0.54, v_num=mwbm, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 73/73 [00:00<00:00, 85.44it/s, loss=0.54, v_num=mwbm, LTC_val_\u001b[A\n","Epoch 25:  99%|▉| 72/73 [00:00<00:00, 72.41it/s, loss=0.572, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 73/73 [00:01<00:00, 70.73it/s, loss=0.572, v_num=mwbm, LTC_val\u001b[A\n","Epoch 26:  99%|▉| 72/73 [00:00<00:00, 82.68it/s, loss=0.588, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 73/73 [00:00<00:00, 80.83it/s, loss=0.588, v_num=mwbm, LTC_val\u001b[A\n","Epoch 27:  99%|▉| 72/73 [00:00<00:00, 74.38it/s, loss=0.57, v_num=mwbm, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 73/73 [00:01<00:00, 72.44it/s, loss=0.57, v_num=mwbm, LTC_val_\u001b[A\n","Epoch 28:  99%|▉| 72/73 [00:00<00:00, 84.96it/s, loss=0.563, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 0.463\n","Epoch 28: 100%|█| 73/73 [00:00<00:00, 82.72it/s, loss=0.563, v_num=mwbm, LTC_val\n","Epoch 29:  99%|▉| 72/73 [00:00<00:00, 72.45it/s, loss=0.579, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 73/73 [00:01<00:00, 68.97it/s, loss=0.579, v_num=mwbm, LTC_val\u001b[A\n","Epoch 30:  99%|▉| 72/73 [00:00<00:00, 75.56it/s, loss=0.574, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 73/73 [00:00<00:00, 73.75it/s, loss=0.574, v_num=mwbm, LTC_val\u001b[A\n","                                                                                Metric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.450\n","Epoch 31:  99%|▉| 72/73 [00:01<00:00, 60.05it/s, loss=0.567, v_num=mwbm, LTC_val\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 73/73 [00:01<00:00, 58.97it/s, loss=0.567, v_num=mwbm, LTC_val\u001b[A\n","Epoch 32:  99%|▉| 72/73 [00:00<00:00, 78.41it/s, loss=0.585, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 73/73 [00:01<00:00, 69.05it/s, loss=0.585, v_num=mwbm, LTC_val\u001b[A\n","Epoch 33:  99%|▉| 72/73 [00:01<00:00, 55.29it/s, loss=0.584, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 33: 100%|█| 73/73 [00:01<00:00, 54.60it/s, loss=0.584, v_num=mwbm, LTC_val\u001b[A\n","Epoch 34:  99%|▉| 72/73 [00:00<00:00, 81.14it/s, loss=0.554, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 73/73 [00:00<00:00, 78.83it/s, loss=0.554, v_num=mwbm, LTC_val\u001b[A\n","Epoch 35:  99%|▉| 72/73 [00:00<00:00, 75.58it/s, loss=0.535, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 35: 100%|█| 73/73 [00:00<00:00, 73.95it/s, loss=0.535, v_num=mwbm, LTC_val\u001b[A\n","Epoch 36:  99%|▉| 72/73 [00:00<00:00, 84.32it/s, loss=0.565, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 36: 100%|█| 73/73 [00:00<00:00, 81.39it/s, loss=0.565, v_num=mwbm, LTC_val\u001b[A\n","Epoch 37:  99%|▉| 72/73 [00:01<00:00, 67.50it/s, loss=0.587, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 37: 100%|█| 73/73 [00:01<00:00, 66.01it/s, loss=0.587, v_num=mwbm, LTC_val\u001b[A\n","Epoch 38:  99%|▉| 72/73 [00:01<00:00, 58.33it/s, loss=0.588, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.038 >= min_delta = 0.003. New best score: 0.412\n","Epoch 38: 100%|█| 73/73 [00:01<00:00, 57.24it/s, loss=0.588, v_num=mwbm, LTC_val\n","Epoch 39:  99%|▉| 72/73 [00:01<00:00, 66.98it/s, loss=0.594, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 39: 100%|█| 73/73 [00:01<00:00, 65.52it/s, loss=0.594, v_num=mwbm, LTC_val\u001b[A\n","Epoch 40:  99%|▉| 72/73 [00:01<00:00, 68.04it/s, loss=0.564, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 40: 100%|█| 73/73 [00:01<00:00, 66.04it/s, loss=0.564, v_num=mwbm, LTC_val\u001b[A\n","Epoch 41:  99%|▉| 72/73 [00:00<00:00, 82.95it/s, loss=0.554, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 41: 100%|█| 73/73 [00:00<00:00, 80.85it/s, loss=0.554, v_num=mwbm, LTC_val\u001b[A\n","Epoch 42:  99%|▉| 72/73 [00:01<00:00, 54.32it/s, loss=0.566, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 42: 100%|█| 73/73 [00:01<00:00, 53.74it/s, loss=0.566, v_num=mwbm, LTC_val\u001b[A\n","Epoch 43:  99%|▉| 72/73 [00:00<00:00, 89.01it/s, loss=0.618, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 43: 100%|█| 73/73 [00:00<00:00, 86.49it/s, loss=0.618, v_num=mwbm, LTC_val\u001b[A\n","Epoch 44:  99%|▉| 72/73 [00:01<00:00, 67.33it/s, loss=0.589, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 44: 100%|█| 73/73 [00:01<00:00, 64.87it/s, loss=0.589, v_num=mwbm, LTC_val\u001b[A\n","Epoch 45:  99%|▉| 72/73 [00:01<00:00, 70.16it/s, loss=0.57, v_num=mwbm, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 45: 100%|█| 73/73 [00:01<00:00, 68.62it/s, loss=0.57, v_num=mwbm, LTC_val_\u001b[A\n","Epoch 46:  99%|▉| 72/73 [00:01<00:00, 60.27it/s, loss=0.536, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 46: 100%|█| 73/73 [00:01<00:00, 59.40it/s, loss=0.536, v_num=mwbm, LTC_val\u001b[A\n","Epoch 47:  99%|▉| 72/73 [00:00<00:00, 80.72it/s, loss=0.56, v_num=mwbm, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 47: 100%|█| 73/73 [00:00<00:00, 78.17it/s, loss=0.56, v_num=mwbm, LTC_val_\u001b[A\n","Epoch 48:  99%|▉| 72/73 [00:01<00:00, 68.17it/s, loss=0.554, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 48: 100%|█| 73/73 [00:01<00:00, 66.98it/s, loss=0.554, v_num=mwbm, LTC_val\u001b[A\n","Epoch 49:  99%|▉| 72/73 [00:01<00:00, 67.96it/s, loss=0.52, v_num=mwbm, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 49: 100%|█| 73/73 [00:01<00:00, 66.18it/s, loss=0.52, v_num=mwbm, LTC_val_\u001b[A\n","Epoch 50:  99%|▉| 72/73 [00:01<00:00, 63.48it/s, loss=0.57, v_num=mwbm, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 50: 100%|█| 73/73 [00:01<00:00, 62.59it/s, loss=0.57, v_num=mwbm, LTC_val_\u001b[A\n","Epoch 51:  99%|▉| 72/73 [00:01<00:00, 62.96it/s, loss=0.526, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 51: 100%|█| 73/73 [00:01<00:00, 61.67it/s, loss=0.526, v_num=mwbm, LTC_val\u001b[A\n","Epoch 52:  99%|▉| 72/73 [00:00<00:00, 88.48it/s, loss=0.54, v_num=mwbm, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 52: 100%|█| 73/73 [00:00<00:00, 86.28it/s, loss=0.54, v_num=mwbm, LTC_val_\u001b[A\n","Epoch 53:  99%|▉| 72/73 [00:00<00:00, 74.84it/s, loss=0.606, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 53: 100%|█| 73/73 [00:01<00:00, 72.81it/s, loss=0.606, v_num=mwbm, LTC_val\u001b[A\n","Epoch 54:  99%|▉| 72/73 [00:00<00:00, 85.54it/s, loss=0.522, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 54: 100%|█| 73/73 [00:00<00:00, 83.43it/s, loss=0.522, v_num=mwbm, LTC_val\u001b[A\n","Epoch 55:  99%|▉| 72/73 [00:01<00:00, 66.53it/s, loss=0.534, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 55: 100%|█| 73/73 [00:01<00:00, 65.16it/s, loss=0.534, v_num=mwbm, LTC_val\u001b[A\n","Epoch 56:  99%|▉| 72/73 [00:00<00:00, 75.31it/s, loss=0.569, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 56: 100%|█| 73/73 [00:00<00:00, 73.76it/s, loss=0.569, v_num=mwbm, LTC_val\u001b[A\n","Epoch 57:  99%|▉| 72/73 [00:01<00:00, 68.96it/s, loss=0.549, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.408\n","Epoch 57: 100%|█| 73/73 [00:01<00:00, 67.12it/s, loss=0.549, v_num=mwbm, LTC_val\n","Epoch 58:  99%|▉| 72/73 [00:00<00:00, 77.94it/s, loss=0.533, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 0.404\n","Epoch 58: 100%|█| 73/73 [00:00<00:00, 75.99it/s, loss=0.533, v_num=mwbm, LTC_val\n","Epoch 59:  99%|▉| 72/73 [00:01<00:00, 63.28it/s, loss=0.512, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 0.392\n","Epoch 59: 100%|█| 73/73 [00:01<00:00, 61.93it/s, loss=0.512, v_num=mwbm, LTC_val\n","Epoch 60:  99%|▉| 72/73 [00:00<00:00, 75.58it/s, loss=0.539, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 60: 100%|█| 73/73 [00:00<00:00, 73.13it/s, loss=0.539, v_num=mwbm, LTC_val\u001b[A\n","Epoch 61:  99%|▉| 72/73 [00:01<00:00, 65.34it/s, loss=0.516, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 61: 100%|█| 73/73 [00:01<00:00, 63.90it/s, loss=0.516, v_num=mwbm, LTC_val\u001b[A\n","Epoch 62:  99%|▉| 72/73 [00:00<00:00, 75.75it/s, loss=0.542, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 62: 100%|█| 73/73 [00:00<00:00, 73.84it/s, loss=0.542, v_num=mwbm, LTC_val\u001b[A\n","Epoch 63:  99%|▉| 72/73 [00:01<00:00, 71.57it/s, loss=0.566, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 63: 100%|█| 73/73 [00:01<00:00, 69.83it/s, loss=0.566, v_num=mwbm, LTC_val\u001b[A\n","Epoch 64:  99%|▉| 72/73 [00:00<00:00, 79.95it/s, loss=0.55, v_num=mwbm, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 64: 100%|█| 73/73 [00:00<00:00, 77.47it/s, loss=0.55, v_num=mwbm, LTC_val_\u001b[A\n","Epoch 65:  99%|▉| 72/73 [00:01<00:00, 64.71it/s, loss=0.516, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 65: 100%|█| 73/73 [00:01<00:00, 63.60it/s, loss=0.516, v_num=mwbm, LTC_val\u001b[A\n","Epoch 66:  99%|▉| 72/73 [00:00<00:00, 83.84it/s, loss=0.555, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 66: 100%|█| 73/73 [00:00<00:00, 81.29it/s, loss=0.555, v_num=mwbm, LTC_val\u001b[A\n","Epoch 67:  99%|▉| 72/73 [00:01<00:00, 65.94it/s, loss=0.512, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 67: 100%|█| 73/73 [00:01<00:00, 64.92it/s, loss=0.512, v_num=mwbm, LTC_val\u001b[A\n","Epoch 68:  99%|▉| 72/73 [00:00<00:00, 85.58it/s, loss=0.519, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.388\n","Epoch 68: 100%|█| 73/73 [00:00<00:00, 82.56it/s, loss=0.519, v_num=mwbm, LTC_val\n","Epoch 69:  99%|▉| 72/73 [00:01<00:00, 67.86it/s, loss=0.535, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 69: 100%|█| 73/73 [00:01<00:00, 66.34it/s, loss=0.535, v_num=mwbm, LTC_val\u001b[A\n","Epoch 70:  99%|▉| 72/73 [00:00<00:00, 72.28it/s, loss=0.519, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 70: 100%|█| 73/73 [00:01<00:00, 69.94it/s, loss=0.519, v_num=mwbm, LTC_val\u001b[A\n","Epoch 71:  99%|▉| 72/73 [00:01<00:00, 67.12it/s, loss=0.543, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 71: 100%|█| 73/73 [00:01<00:00, 65.92it/s, loss=0.543, v_num=mwbm, LTC_val\u001b[A\n","Epoch 72:  99%|▉| 72/73 [00:00<00:00, 81.23it/s, loss=0.518, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 72: 100%|█| 73/73 [00:00<00:00, 78.83it/s, loss=0.518, v_num=mwbm, LTC_val\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 73:  99%|▉| 72/73 [00:00<00:00, 77.22it/s, loss=0.499, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 73: 100%|█| 73/73 [00:00<00:00, 75.68it/s, loss=0.499, v_num=mwbm, LTC_val\u001b[A\n","Epoch 74:  99%|▉| 72/73 [00:00<00:00, 83.13it/s, loss=0.537, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 74: 100%|█| 73/73 [00:00<00:00, 74.82it/s, loss=0.537, v_num=mwbm, LTC_val\u001b[A\n","Epoch 75:  99%|▉| 72/73 [00:00<00:00, 74.44it/s, loss=0.565, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 75: 100%|█| 73/73 [00:01<00:00, 72.87it/s, loss=0.565, v_num=mwbm, LTC_val\u001b[A\n","Epoch 76:  99%|▉| 72/73 [00:01<00:00, 71.41it/s, loss=0.511, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 76: 100%|█| 73/73 [00:01<00:00, 69.63it/s, loss=0.511, v_num=mwbm, LTC_val\u001b[A\n","Epoch 77:  99%|▉| 72/73 [00:00<00:00, 72.68it/s, loss=0.531, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 77: 100%|█| 73/73 [00:01<00:00, 71.38it/s, loss=0.531, v_num=mwbm, LTC_val\u001b[A\n","Epoch 78:  99%|▉| 72/73 [00:00<00:00, 81.70it/s, loss=0.604, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 78: 100%|█| 73/73 [00:00<00:00, 79.31it/s, loss=0.604, v_num=mwbm, LTC_val\u001b[A\n","Epoch 79:  99%|▉| 72/73 [00:01<00:00, 60.49it/s, loss=0.52, v_num=mwbm, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 79: 100%|█| 73/73 [00:01<00:00, 59.51it/s, loss=0.52, v_num=mwbm, LTC_val_\u001b[A\n","Epoch 80:  99%|▉| 72/73 [00:01<00:00, 71.06it/s, loss=0.546, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 80: 100%|█| 73/73 [00:01<00:00, 69.22it/s, loss=0.546, v_num=mwbm, LTC_val\u001b[A\n","Epoch 81:  99%|▉| 72/73 [00:01<00:00, 64.25it/s, loss=0.495, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.381\n","Epoch 81: 100%|█| 73/73 [00:01<00:00, 63.20it/s, loss=0.495, v_num=mwbm, LTC_val\n","Epoch 82:  99%|▉| 72/73 [00:01<00:00, 70.50it/s, loss=0.554, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 82: 100%|█| 73/73 [00:01<00:00, 62.39it/s, loss=0.554, v_num=mwbm, LTC_val\u001b[A\n","Epoch 83:  99%|▉| 72/73 [00:01<00:00, 66.91it/s, loss=0.531, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 83: 100%|█| 73/73 [00:01<00:00, 65.16it/s, loss=0.531, v_num=mwbm, LTC_val\u001b[A\n","Epoch 84:  99%|▉| 72/73 [00:01<00:00, 64.25it/s, loss=0.477, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 84: 100%|█| 73/73 [00:01<00:00, 63.25it/s, loss=0.477, v_num=mwbm, LTC_val\u001b[A\n","Epoch 85:  99%|▉| 72/73 [00:00<00:00, 83.68it/s, loss=0.543, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 85: 100%|█| 73/73 [00:00<00:00, 81.12it/s, loss=0.543, v_num=mwbm, LTC_val\u001b[A\n","Epoch 86:  99%|▉| 72/73 [00:00<00:00, 73.34it/s, loss=0.523, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 86: 100%|█| 73/73 [00:01<00:00, 71.95it/s, loss=0.523, v_num=mwbm, LTC_val\u001b[A\n","Epoch 87:  99%|▉| 72/73 [00:00<00:00, 72.69it/s, loss=0.542, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 87: 100%|█| 73/73 [00:01<00:00, 70.85it/s, loss=0.542, v_num=mwbm, LTC_val\u001b[A\n","Epoch 88:  99%|▉| 72/73 [00:01<00:00, 58.60it/s, loss=0.558, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 88: 100%|█| 73/73 [00:01<00:00, 57.82it/s, loss=0.558, v_num=mwbm, LTC_val\u001b[A\n","Epoch 89:  99%|▉| 72/73 [00:00<00:00, 79.97it/s, loss=0.519, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 89: 100%|█| 73/73 [00:00<00:00, 77.39it/s, loss=0.519, v_num=mwbm, LTC_val\u001b[A\n","Epoch 90:  99%|▉| 72/73 [00:00<00:00, 72.03it/s, loss=0.555, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 90: 100%|█| 73/73 [00:01<00:00, 70.56it/s, loss=0.555, v_num=mwbm, LTC_val\u001b[A\n","Epoch 91:  99%|▉| 72/73 [00:00<00:00, 81.84it/s, loss=0.506, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 91: 100%|█| 73/73 [00:00<00:00, 79.33it/s, loss=0.506, v_num=mwbm, LTC_val\u001b[A\n","Epoch 92:  99%|▉| 72/73 [00:01<00:00, 71.28it/s, loss=0.483, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 92: 100%|█| 73/73 [00:01<00:00, 69.93it/s, loss=0.483, v_num=mwbm, LTC_val\u001b[A\n","Epoch 93:  99%|▉| 72/73 [00:00<00:00, 80.82it/s, loss=0.536, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 93: 100%|█| 73/73 [00:00<00:00, 78.60it/s, loss=0.536, v_num=mwbm, LTC_val\u001b[A\n","Epoch 94:  99%|▉| 72/73 [00:01<00:00, 57.82it/s, loss=0.513, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 94: 100%|█| 73/73 [00:01<00:00, 56.95it/s, loss=0.513, v_num=mwbm, LTC_val\u001b[A\n","Epoch 95:  99%|▉| 72/73 [00:01<00:00, 65.95it/s, loss=0.44, v_num=mwbm, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 95: 100%|█| 73/73 [00:01<00:00, 64.58it/s, loss=0.44, v_num=mwbm, LTC_val_\u001b[A\n","Epoch 96:  99%|▉| 72/73 [00:00<00:00, 75.89it/s, loss=0.536, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 96: 100%|█| 73/73 [00:00<00:00, 74.31it/s, loss=0.536, v_num=mwbm, LTC_val\u001b[A\n","Epoch 97:  99%|▉| 72/73 [00:00<00:00, 73.13it/s, loss=0.536, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 97: 100%|█| 73/73 [00:01<00:00, 70.69it/s, loss=0.536, v_num=mwbm, LTC_val\u001b[A\n","Epoch 98:  99%|▉| 72/73 [00:00<00:00, 80.51it/s, loss=0.518, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 98: 100%|█| 73/73 [00:00<00:00, 78.76it/s, loss=0.518, v_num=mwbm, LTC_val\u001b[A\n","Epoch 99:  99%|▉| 72/73 [00:00<00:00, 74.15it/s, loss=0.527, v_num=mwbm, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 99: 100%|█| 73/73 [00:01<00:00, 72.11it/s, loss=0.527, v_num=mwbm, LTC_val\u001b[A\n","Epoch 100:  99%|▉| 72/73 [00:00<00:00, 82.51it/s, loss=0.488, v_num=mwbm, LTC_va\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 100: 100%|█| 73/73 [00:00<00:00, 80.53it/s, loss=0.488, v_num=mwbm, LTC_va\u001b[A\n","Epoch 101:  99%|▉| 72/73 [00:01<00:00, 63.38it/s, loss=0.489, v_num=mwbm, LTC_va\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.381. Signaling Trainer to stop.\n","Epoch 101: 100%|█| 73/73 [00:01<00:00, 61.85it/s, loss=0.489, v_num=mwbm, LTC_va\n","Epoch 101: 100%|█| 73/73 [00:01<00:00, 61.68it/s, loss=0.489, v_num=mwbm, LTC_va\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 109.54it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'LTC_test_acc': 0.75,\n"," 'LTC_test_f1': 0.7414525747299194,\n"," 'test_loss': 0.46684202551841736}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 71155\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014131-2cbnmwbm/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014131-2cbnmwbm/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.43983\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 101\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 7344\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 114\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464205\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 350\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.74826\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.73441\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.49645\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.875\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.87302\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.39826\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.74145\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.46684\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▆▄▇▆▅▄▆▆█▇▆▆▆▆▃▇▆▆▇▅▄▆▄▇▆▃▅▆▄▁▆▃▅█▅▄▆▆▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▆▃▇▅▅▄▆▆█▇▆▆▆▆▃▇▆▆▇▅▄▆▄▇▆▃▅▆▃▁▆▃▅█▅▄▆▆▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▃▄▃▃▄▄▅▃▂▃▃▄▃▂▇▁▂▃▂▅▅▃▄▃▃▅▄▃▄▇▃█▄▁▆▃▃▂▃▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▅▅▅▅▆▆▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆▇▇▆▆▇▇▆▇▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▆▇▇▆▇▇▇▇▇▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▁▂▁▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▃▁▃▃▃▃▁▃▆▃▃▃▃▃▃█▃▆▆▆▃▆▆█▆█▆▆█████▆██████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▃▁▃▃▃▃▁▃▅▃▃▃▃▃▃█▃▆▅▅▃▆▆█▆█▅▅█████▅██████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▆█▅▆▆▆▆▆▄▆▅▄▆▆▅▂▄▅▂▃▃▂▃▁▄▃▂▃▂▃▂▁▃▂▂▂▂▁▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_single_lstm__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2cbnmwbm\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:43:36.390804: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_single_lstm__binary_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2e4itpvr\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014334-2e4itpvr\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 130   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.7 K    Trainable params\n","0         Non-trainable params\n","75.7 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 73/74 [00:00<00:00, 86.99it/s, loss=0.697, v_num=tpvr, LTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.715\n","Epoch 0: 100%|█| 74/74 [00:00<00:00, 84.64it/s, loss=0.697, v_num=tpvr, LTC_val_\n","Epoch 1:  99%|▉| 73/74 [00:01<00:00, 69.42it/s, loss=0.698, v_num=tpvr, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.708\n","Epoch 1: 100%|█| 74/74 [00:01<00:00, 67.77it/s, loss=0.698, v_num=tpvr, LTC_val_\n","Epoch 2:  99%|▉| 73/74 [00:00<00:00, 80.08it/s, loss=0.701, v_num=tpvr, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 0.694\n","Epoch 2: 100%|█| 74/74 [00:00<00:00, 77.86it/s, loss=0.701, v_num=tpvr, LTC_val_\n","Epoch 3:  99%|▉| 73/74 [00:01<00:00, 69.03it/s, loss=0.7, v_num=tpvr, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 74/74 [00:01<00:00, 66.76it/s, loss=0.7, v_num=tpvr, LTC_val_ac\u001b[A\n","Epoch 4:  99%|▉| 73/74 [00:00<00:00, 82.14it/s, loss=0.701, v_num=tpvr, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 74/74 [00:00<00:00, 79.92it/s, loss=0.701, v_num=tpvr, LTC_val_\u001b[A\n","Epoch 5:  99%|▉| 73/74 [00:01<00:00, 69.80it/s, loss=0.697, v_num=tpvr, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 74/74 [00:01<00:00, 68.30it/s, loss=0.697, v_num=tpvr, LTC_val_\u001b[A\n","Epoch 6:  99%|▉| 73/74 [00:00<00:00, 87.91it/s, loss=0.693, v_num=tpvr, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 74/74 [00:00<00:00, 85.29it/s, loss=0.693, v_num=tpvr, LTC_val_\u001b[A\n","Epoch 7:  99%|▉| 73/74 [00:01<00:00, 65.65it/s, loss=0.694, v_num=tpvr, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 74/74 [00:01<00:00, 64.06it/s, loss=0.694, v_num=tpvr, LTC_val_\u001b[A\n","Epoch 8:  99%|▉| 73/74 [00:00<00:00, 80.84it/s, loss=0.699, v_num=tpvr, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 74/74 [00:00<00:00, 78.58it/s, loss=0.699, v_num=tpvr, LTC_val_\u001b[A\n","Epoch 9:  99%|▉| 73/74 [00:01<00:00, 66.63it/s, loss=0.696, v_num=tpvr, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 74/74 [00:01<00:00, 65.17it/s, loss=0.696, v_num=tpvr, LTC_val_\u001b[A\n","Epoch 10:  99%|▉| 73/74 [00:00<00:00, 75.64it/s, loss=0.694, v_num=tpvr, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 74/74 [00:01<00:00, 73.73it/s, loss=0.694, v_num=tpvr, LTC_val\u001b[A\n","Epoch 11:  99%|▉| 73/74 [00:01<00:00, 68.19it/s, loss=0.696, v_num=tpvr, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 74/74 [00:01<00:00, 66.68it/s, loss=0.696, v_num=tpvr, LTC_val\u001b[A\n","Epoch 12:  99%|▉| 73/74 [00:00<00:00, 79.95it/s, loss=0.692, v_num=tpvr, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 74/74 [00:00<00:00, 77.61it/s, loss=0.692, v_num=tpvr, LTC_val\u001b[A\n","Epoch 13:  99%|▉| 73/74 [00:01<00:00, 65.15it/s, loss=0.692, v_num=tpvr, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 74/74 [00:01<00:00, 64.15it/s, loss=0.692, v_num=tpvr, LTC_val\u001b[A\n","Epoch 14:  99%|▉| 73/74 [00:00<00:00, 84.12it/s, loss=0.694, v_num=tpvr, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 74/74 [00:00<00:00, 81.65it/s, loss=0.694, v_num=tpvr, LTC_val\u001b[A\n","Epoch 15:  99%|▉| 73/74 [00:00<00:00, 74.91it/s, loss=0.696, v_num=tpvr, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 74/74 [00:01<00:00, 73.34it/s, loss=0.696, v_num=tpvr, LTC_val\u001b[A\n","Epoch 16:  99%|▉| 73/74 [00:00<00:00, 78.57it/s, loss=0.696, v_num=tpvr, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 74/74 [00:00<00:00, 76.24it/s, loss=0.696, v_num=tpvr, LTC_val\u001b[A\n","Epoch 17:  99%|▉| 73/74 [00:01<00:00, 57.40it/s, loss=0.694, v_num=tpvr, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 74/74 [00:01<00:00, 56.37it/s, loss=0.694, v_num=tpvr, LTC_val\u001b[A\n","Epoch 18:  99%|▉| 73/74 [00:01<00:00, 68.39it/s, loss=0.691, v_num=tpvr, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 74/74 [00:01<00:00, 66.61it/s, loss=0.691, v_num=tpvr, LTC_val\u001b[A\n","Epoch 19:  99%|▉| 73/74 [00:00<00:00, 78.04it/s, loss=0.698, v_num=tpvr, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 74/74 [00:00<00:00, 76.29it/s, loss=0.698, v_num=tpvr, LTC_val\u001b[A\n","Epoch 20:  99%|▉| 73/74 [00:01<00:00, 53.89it/s, loss=0.696, v_num=tpvr, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 74/74 [00:01<00:00, 53.30it/s, loss=0.696, v_num=tpvr, LTC_val\u001b[A\n","Epoch 21:  99%|▉| 73/74 [00:00<00:00, 89.61it/s, loss=0.691, v_num=tpvr, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 74/74 [00:00<00:00, 87.09it/s, loss=0.691, v_num=tpvr, LTC_val\u001b[A\n","Epoch 22:  99%|▉| 73/74 [00:01<00:00, 54.58it/s, loss=0.691, v_num=tpvr, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.694. Signaling Trainer to stop.\n","Epoch 22: 100%|█| 74/74 [00:01<00:00, 53.98it/s, loss=0.691, v_num=tpvr, LTC_val\n","Epoch 22: 100%|█| 74/74 [00:01<00:00, 53.84it/s, loss=0.691, v_num=tpvr, LTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 147.95it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'LTC_test_acc': 0.5,\n"," 'LTC_test_f1': 0.32692310214042664,\n"," 'test_loss': 0.6933276057243347}\n","--------------------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 71483\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014334-2e4itpvr/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014334-2e4itpvr/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.4375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.37662\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.69875\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 22\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1679\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 34\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464248\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 79\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.51554\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.48434\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.6934\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.33333\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.69318\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.32692\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.69333\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▃▆▂▆█▃▅▅▃▂▅▅▁▇▇▅▆▇▆▄▃▃█▇▄▅▄▇▃▃█▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▂▆▂▅█▃▄▅▃▂▅▅▁▇▆▅▆▅▆▄▃▂▇▇▄▅▄▇▃▃█▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▃▄▂▁▇▃▃▃▄▂▂▄▂▂▂▂▂▂▃▃▂▂▂▁▂▂▂▃▃▂▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▄▄▅▃▄▂▁▄▁▃▆▄▂▂▁▃▅▂█▂▅▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▅▆▇▅▃▆▅▆▂▅▃▇▅▄▂▁▇▂█▆▂▅▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▄▅▃▃▃▂▃▂▁▂▂▃▂▂▂▂▁▂▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▆▂▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_single_lstm__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2e4itpvr\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:44:22.809361: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_single_lstm__multi_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/53y9o27n\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014421-53y9o27n\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 195   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.8 K    Trainable params\n","0         Non-trainable params\n","75.8 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 72/73 [00:00<00:00, 81.71it/s, loss=1.05, v_num=o27n, LTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 0: 100%|█| 73/73 [00:00<00:00, 73.82it/s, loss=1.05, v_num=o27n, LTC_val_a\u001b[A\n","                                                                                \u001b[AMetric val_loss improved. New best score: 1.132\n","Epoch 1:  99%|▉| 72/73 [00:01<00:00, 70.81it/s, loss=1.07, v_num=o27n, LTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.015 >= min_delta = 0.003. New best score: 1.117\n","Epoch 1: 100%|█| 73/73 [00:01<00:00, 68.95it/s, loss=1.07, v_num=o27n, LTC_val_a\n","Epoch 2:  99%|▉| 72/73 [00:00<00:00, 79.40it/s, loss=1.03, v_num=o27n, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 73/73 [00:00<00:00, 76.75it/s, loss=1.03, v_num=o27n, LTC_val_a\u001b[A\n","Epoch 3:  99%|▉| 72/73 [00:01<00:00, 70.23it/s, loss=1.02, v_num=o27n, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 1.111\n","Epoch 3: 100%|█| 73/73 [00:01<00:00, 68.52it/s, loss=1.02, v_num=o27n, LTC_val_a\n","Epoch 4:  99%|▉| 72/73 [00:00<00:00, 78.39it/s, loss=1.04, v_num=o27n, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 73/73 [00:00<00:00, 76.27it/s, loss=1.04, v_num=o27n, LTC_val_a\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.029 >= min_delta = 0.003. New best score: 1.082\n","Epoch 5:  99%|▉| 72/73 [00:01<00:00, 70.80it/s, loss=1, v_num=o27n, LTC_val_acc=\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.050 >= min_delta = 0.003. New best score: 1.032\n","Epoch 5: 100%|█| 73/73 [00:01<00:00, 69.10it/s, loss=1, v_num=o27n, LTC_val_acc=\n","Epoch 6:  99%|▉| 72/73 [00:00<00:00, 73.13it/s, loss=0.985, v_num=o27n, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 73/73 [00:01<00:00, 71.23it/s, loss=0.985, v_num=o27n, LTC_val_\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 1.023\n","Epoch 7:  99%|▉| 72/73 [00:01<00:00, 70.54it/s, loss=0.963, v_num=o27n, LTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 73/73 [00:01<00:00, 68.99it/s, loss=0.963, v_num=o27n, LTC_val_\u001b[A\n","Epoch 8:  99%|▉| 72/73 [00:01<00:00, 70.40it/s, loss=0.961, v_num=o27n, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 1.002\n","Epoch 8: 100%|█| 73/73 [00:01<00:00, 68.64it/s, loss=0.961, v_num=o27n, LTC_val_\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 9:  99%|▉| 72/73 [00:00<00:00, 74.94it/s, loss=0.976, v_num=o27n, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 73/73 [00:00<00:00, 73.13it/s, loss=0.976, v_num=o27n, LTC_val_\u001b[A\n","Epoch 10:  99%|▉| 72/73 [00:01<00:00, 68.89it/s, loss=0.91, v_num=o27n, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 73/73 [00:01<00:00, 66.56it/s, loss=0.91, v_num=o27n, LTC_val_\u001b[A\n","Epoch 11:  99%|▉| 72/73 [00:00<00:00, 74.55it/s, loss=0.945, v_num=o27n, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 73/73 [00:00<00:00, 73.07it/s, loss=0.945, v_num=o27n, LTC_val\u001b[A\n","Epoch 12:  99%|▉| 72/73 [00:00<00:00, 75.98it/s, loss=0.957, v_num=o27n, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 73/73 [00:00<00:00, 73.67it/s, loss=0.957, v_num=o27n, LTC_val\u001b[A\n","Epoch 13:  99%|▉| 72/73 [00:00<00:00, 76.48it/s, loss=0.952, v_num=o27n, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 73/73 [00:00<00:00, 74.77it/s, loss=0.952, v_num=o27n, LTC_val\u001b[A\n","Epoch 14:  99%|▉| 72/73 [00:00<00:00, 77.94it/s, loss=0.927, v_num=o27n, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 73/73 [00:01<00:00, 63.54it/s, loss=0.927, v_num=o27n, LTC_val\u001b[A\n","Epoch 15:  99%|▉| 72/73 [00:00<00:00, 74.50it/s, loss=0.939, v_num=o27n, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 73/73 [00:01<00:00, 73.00it/s, loss=0.939, v_num=o27n, LTC_val\u001b[A\n","Epoch 16:  99%|▉| 72/73 [00:00<00:00, 73.42it/s, loss=0.968, v_num=o27n, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 73/73 [00:01<00:00, 71.13it/s, loss=0.968, v_num=o27n, LTC_val\u001b[A\n","Epoch 17:  99%|▉| 72/73 [00:00<00:00, 84.79it/s, loss=0.924, v_num=o27n, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 73/73 [00:00<00:00, 82.79it/s, loss=0.924, v_num=o27n, LTC_val\u001b[A\n","Epoch 18:  99%|▉| 72/73 [00:00<00:00, 76.29it/s, loss=0.931, v_num=o27n, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 73/73 [00:00<00:00, 74.25it/s, loss=0.931, v_num=o27n, LTC_val\u001b[A\n","Epoch 19:  99%|▉| 72/73 [00:00<00:00, 80.89it/s, loss=0.944, v_num=o27n, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 73/73 [00:00<00:00, 78.96it/s, loss=0.944, v_num=o27n, LTC_val\u001b[A\n","Epoch 20:  99%|▉| 72/73 [00:00<00:00, 77.21it/s, loss=0.935, v_num=o27n, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 73/73 [00:00<00:00, 75.08it/s, loss=0.935, v_num=o27n, LTC_val\u001b[A\n","Epoch 21:  99%|▉| 72/73 [00:01<00:00, 69.42it/s, loss=0.934, v_num=o27n, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 73/73 [00:01<00:00, 68.17it/s, loss=0.934, v_num=o27n, LTC_val\u001b[A\n","Epoch 22:  99%|▉| 72/73 [00:00<00:00, 75.19it/s, loss=0.907, v_num=o27n, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 73/73 [00:01<00:00, 72.89it/s, loss=0.907, v_num=o27n, LTC_val\u001b[A\n","Epoch 23:  99%|▉| 72/73 [00:00<00:00, 80.34it/s, loss=0.991, v_num=o27n, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 73/73 [00:00<00:00, 78.53it/s, loss=0.991, v_num=o27n, LTC_val\u001b[A\n","Epoch 24:  99%|▉| 72/73 [00:00<00:00, 76.97it/s, loss=0.946, v_num=o27n, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 73/73 [00:00<00:00, 74.72it/s, loss=0.946, v_num=o27n, LTC_val\u001b[A\n","Epoch 25:  99%|▉| 72/73 [00:00<00:00, 80.09it/s, loss=0.924, v_num=o27n, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 73/73 [00:00<00:00, 78.32it/s, loss=0.924, v_num=o27n, LTC_val\u001b[A\n","Epoch 26:  99%|▉| 72/73 [00:01<00:00, 66.41it/s, loss=0.937, v_num=o27n, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 73/73 [00:01<00:00, 64.87it/s, loss=0.937, v_num=o27n, LTC_val\u001b[A\n","Epoch 27:  99%|▉| 72/73 [00:00<00:00, 80.42it/s, loss=0.914, v_num=o27n, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 73/73 [00:00<00:00, 78.52it/s, loss=0.914, v_num=o27n, LTC_val\u001b[A\n","Epoch 28:  99%|▉| 72/73 [00:00<00:00, 77.88it/s, loss=0.919, v_num=o27n, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.002. Signaling Trainer to stop.\n","Epoch 28: 100%|█| 73/73 [00:00<00:00, 75.61it/s, loss=0.919, v_num=o27n, LTC_val\n","Epoch 28: 100%|█| 73/73 [00:00<00:00, 75.15it/s, loss=0.919, v_num=o27n, LTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 104.40it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'LTC_test_acc': 0.5,\n"," 'LTC_test_f1': 0.3047619163990021,\n"," 'test_loss': 0.8802793622016907}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 71654\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014421-53y9o27n/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014421-53y9o27n/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.34444\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.90984\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 28\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2088\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 38\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464299\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 99\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.5191\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.39949\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.92335\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.13992\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.30476\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.88028\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▅▅▄▆▆█▅▅▄▅▅▁▅▅▄▇▄▅▅▄▆▆▅▃▆▅▅▃▅▆▆▅▄▃▅▃▆▇▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▄▄▃▄▄▅▄▂▃▂▄▁▂▄▃▄▃▅▄▂▅▆▅▂▅▃▅▃▃█▅▂▃▃▇▂▅▆▃▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅█▇▄▆▄▇▅▇▅▆▇▄▅█▃▇▄▅█▆▄▅▆▅▅▆▅▃█▂▄▆█▆▆▃▁▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▃▄▅▄▅▅▅▅▆▆▆▆▆▆▇█▆▇▇▇▇█▇▆███▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▂▂▂▁▂▁▁▂▄▄▄▅▆▅▆▆▇▆▆▆█▆▇██▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▇▇▇▆▅▅▄▃▃▂▃▂▂▃▂▁▂▂▂▂▁▁▃▁▁▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▅▅▅▅▅▅▅▁█▅█▅▅▅▁▅▁▅▁▅▅█▅██▅▁▁▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▂▂▂▂▂▂▂▁▇▅█▅▅▅▃▅▃▅▃▅▅█▅██▆▃▃▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▄▅▄▃▂▂▂▁▃▃▄▄▃▅▂▆▂▄▃▂▄▃▂▂▂█▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_single_lstm__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/53y9o27n\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:45:10.316975: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_single_lstm__multi_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/23me2100\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014507-23me2100\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 195   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.8 K    Trainable params\n","0         Non-trainable params\n","75.8 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 73/74 [00:00<00:00, 79.10it/s, loss=1.11, v_num=2100, LTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.083\n","Epoch 0: 100%|█| 74/74 [00:00<00:00, 77.01it/s, loss=1.11, v_num=2100, LTC_val_a\n","Epoch 1:  99%|▉| 73/74 [00:01<00:00, 70.31it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 1.077\n","Epoch 1: 100%|█| 74/74 [00:01<00:00, 68.13it/s, loss=1.11, v_num=2100, LTC_val_a\n","Epoch 2:  99%|▉| 73/74 [00:00<00:00, 81.50it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 74/74 [00:00<00:00, 79.50it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n","Epoch 3:  99%|▉| 73/74 [00:01<00:00, 71.55it/s, loss=1.1, v_num=2100, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 74/74 [00:01<00:00, 69.67it/s, loss=1.1, v_num=2100, LTC_val_ac\u001b[A\n","Epoch 4:  99%|▉| 73/74 [00:00<00:00, 79.51it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 74/74 [00:00<00:00, 77.66it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n","Epoch 5:  99%|▉| 73/74 [00:00<00:00, 74.76it/s, loss=1.1, v_num=2100, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 74/74 [00:01<00:00, 72.69it/s, loss=1.1, v_num=2100, LTC_val_ac\u001b[A\n","Epoch 6:  99%|▉| 73/74 [00:00<00:00, 84.97it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 74/74 [00:00<00:00, 82.58it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n","Epoch 7:  99%|▉| 73/74 [00:00<00:00, 73.95it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 74/74 [00:01<00:00, 71.90it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n","Epoch 8:  99%|▉| 73/74 [00:00<00:00, 81.74it/s, loss=1.09, v_num=2100, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 74/74 [00:00<00:00, 79.67it/s, loss=1.09, v_num=2100, LTC_val_a\u001b[A\n","Epoch 9:  99%|▉| 73/74 [00:00<00:00, 79.06it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 74/74 [00:00<00:00, 76.38it/s, loss=1.11, v_num=2100, LTC_val_a\u001b[A\n","Epoch 10:  99%|▉| 73/74 [00:00<00:00, 75.50it/s, loss=1.11, v_num=2100, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 74/74 [00:01<00:00, 73.71it/s, loss=1.11, v_num=2100, LTC_val_\u001b[A\n","Epoch 11:  99%|▉| 73/74 [00:00<00:00, 78.46it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 74/74 [00:00<00:00, 75.85it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n","Epoch 12:  99%|▉| 73/74 [00:00<00:00, 82.20it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 74/74 [00:00<00:00, 80.32it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n","Epoch 13:  99%|▉| 73/74 [00:00<00:00, 83.20it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 74/74 [00:00<00:00, 80.74it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n","Epoch 14:  99%|▉| 73/74 [00:00<00:00, 77.90it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 74/74 [00:00<00:00, 76.20it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n","Epoch 15:  99%|▉| 73/74 [00:00<00:00, 82.64it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 74/74 [00:00<00:00, 80.06it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n","Epoch 16:  99%|▉| 73/74 [00:01<00:00, 71.16it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 74/74 [00:01<00:00, 69.78it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n","Epoch 17:  99%|▉| 73/74 [00:00<00:00, 83.62it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 74/74 [00:00<00:00, 81.10it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n","Epoch 18:  99%|▉| 73/74 [00:00<00:00, 76.01it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 74/74 [00:00<00:00, 74.09it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n","Epoch 19:  99%|▉| 73/74 [00:00<00:00, 74.95it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 74/74 [00:01<00:00, 72.69it/s, loss=1.1, v_num=2100, LTC_val_a\u001b[A\n","Epoch 20:  99%|▉| 73/74 [00:01<00:00, 70.85it/s, loss=1.09, v_num=2100, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 74/74 [00:01<00:00, 69.38it/s, loss=1.09, v_num=2100, LTC_val_\u001b[A\n","Epoch 21:  99%|▉| 73/74 [00:00<00:00, 82.53it/s, loss=1.09, v_num=2100, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.077. Signaling Trainer to stop.\n","Epoch 21: 100%|█| 74/74 [00:00<00:00, 79.92it/s, loss=1.09, v_num=2100, LTC_val_\n","Epoch 21: 100%|█| 74/74 [00:00<00:00, 79.59it/s, loss=1.09, v_num=2100, LTC_val_\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 107.11it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'LTC_test_acc': 0.25,\n"," 'LTC_test_f1': 0.13151928782463074,\n"," 'test_loss': 1.1257140636444092}\n","--------------------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 71818\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014507-23me2100/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014507-23me2100/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.49697\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.07673\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 21\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1606\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 32\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464339\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 76\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.35579\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.32371\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09512\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.18182\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.09773\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.25\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.13152\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.12571\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▄▃█▅▄▆▅▅▁▁▄▄▂▃▆▃▂▄▅▂▆▇▃▂▃▆▃▅▅▃▁▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▄▃█▅▄▄▄▄▂▁▅▄▃▃▆▃▂▄▆▂▆█▃▂▃▅▃▄▆▂▁▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▄▄▁▁▅▂▆▁█▆▅▅▇▄▅▆▄▄▃▅▂▄█▄▄▃▄▄▄▄▇▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▁▃▂▅▆▂▄▆▃▆▇▆▅█▆▆▂▄▅█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▁▃▄▅▆▁▄▇▆▇▅▆▅█▇▄▁▂▁█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▅▄▅▃▄▄▂▃▄▂▂▃▂▂▂▃▃▃▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▁▁▁▁██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▂▁▅▁▇▆█▆▄▅▃▄▅▂▃▂▂▂▄▂▄▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_single_lstm__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/23me2100\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:45:49.023290: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm__binary_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3kuodfw5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014547-3kuodfw5\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 130   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 72/73 [00:01<00:00, 41.98it/s, loss=0.689, v_num=dfw5, LTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.641\n","Epoch 0: 100%|█| 73/73 [00:01<00:00, 39.96it/s, loss=0.689, v_num=dfw5, LTC_val_\n","Epoch 1:  99%|▉| 72/73 [00:01<00:00, 40.23it/s, loss=0.647, v_num=dfw5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.632\n","Epoch 1: 100%|█| 73/73 [00:01<00:00, 40.00it/s, loss=0.647, v_num=dfw5, LTC_val_\n","Epoch 2:  99%|▉| 72/73 [00:01<00:00, 37.78it/s, loss=0.59, v_num=dfw5, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.067 >= min_delta = 0.003. New best score: 0.564\n","Epoch 2: 100%|█| 73/73 [00:01<00:00, 37.53it/s, loss=0.59, v_num=dfw5, LTC_val_a\n","Epoch 3:  99%|▉| 72/73 [00:01<00:00, 38.81it/s, loss=0.613, v_num=dfw5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.035 >= min_delta = 0.003. New best score: 0.529\n","Epoch 3: 100%|█| 73/73 [00:01<00:00, 38.43it/s, loss=0.613, v_num=dfw5, LTC_val_\n","Epoch 4:  99%|▉| 72/73 [00:02<00:00, 34.31it/s, loss=0.587, v_num=dfw5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 73/73 [00:02<00:00, 34.11it/s, loss=0.587, v_num=dfw5, LTC_val_\u001b[A\n","Epoch 5:  99%|▉| 72/73 [00:01<00:00, 41.48it/s, loss=0.636, v_num=dfw5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 73/73 [00:01<00:00, 40.97it/s, loss=0.636, v_num=dfw5, LTC_val_\u001b[A\n","Epoch 6:  99%|▉| 72/73 [00:01<00:00, 42.46it/s, loss=0.57, v_num=dfw5, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 73/73 [00:01<00:00, 41.96it/s, loss=0.57, v_num=dfw5, LTC_val_a\u001b[A\n","Epoch 7:  99%|▉| 72/73 [00:01<00:00, 42.70it/s, loss=0.593, v_num=dfw5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 73/73 [00:01<00:00, 42.29it/s, loss=0.593, v_num=dfw5, LTC_val_\u001b[A\n","Epoch 8:  99%|▉| 72/73 [00:01<00:00, 39.41it/s, loss=0.568, v_num=dfw5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 73/73 [00:01<00:00, 39.01it/s, loss=0.568, v_num=dfw5, LTC_val_\u001b[A\n","Epoch 9:  99%|▉| 72/73 [00:02<00:00, 34.52it/s, loss=0.58, v_num=dfw5, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 9: 100%|█| 73/73 [00:02<00:00, 34.32it/s, loss=0.58, v_num=dfw5, LTC_val_a\u001b[A\n","Epoch 10:  99%|▉| 72/73 [00:01<00:00, 40.30it/s, loss=0.639, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 73/73 [00:01<00:00, 39.91it/s, loss=0.639, v_num=dfw5, LTC_val\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 0.508\n","Epoch 11:  99%|▉| 72/73 [00:02<00:00, 28.45it/s, loss=0.604, v_num=dfw5, LTC_val\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 73/73 [00:02<00:00, 28.36it/s, loss=0.604, v_num=dfw5, LTC_val\u001b[A\n","Epoch 12:  99%|▉| 72/73 [00:02<00:00, 28.59it/s, loss=0.602, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 73/73 [00:02<00:00, 28.46it/s, loss=0.602, v_num=dfw5, LTC_val\u001b[A\n","Epoch 13:  99%|▉| 72/73 [00:01<00:00, 41.04it/s, loss=0.599, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 73/73 [00:01<00:00, 40.80it/s, loss=0.599, v_num=dfw5, LTC_val\u001b[A\n","Epoch 14:  99%|▉| 72/73 [00:01<00:00, 40.44it/s, loss=0.586, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 73/73 [00:01<00:00, 40.08it/s, loss=0.586, v_num=dfw5, LTC_val\u001b[A\n","Epoch 15:  99%|▉| 72/73 [00:01<00:00, 38.17it/s, loss=0.608, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 73/73 [00:01<00:00, 37.89it/s, loss=0.608, v_num=dfw5, LTC_val\u001b[A\n","Epoch 16:  99%|▉| 72/73 [00:01<00:00, 41.84it/s, loss=0.564, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 73/73 [00:01<00:00, 41.28it/s, loss=0.564, v_num=dfw5, LTC_val\u001b[A\n","Epoch 17:  99%|▉| 72/73 [00:01<00:00, 42.61it/s, loss=0.625, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 73/73 [00:01<00:00, 42.05it/s, loss=0.625, v_num=dfw5, LTC_val\u001b[A\n","Epoch 18:  99%|▉| 72/73 [00:01<00:00, 37.48it/s, loss=0.577, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.503\n","Epoch 18: 100%|█| 73/73 [00:01<00:00, 36.92it/s, loss=0.577, v_num=dfw5, LTC_val\n","Epoch 19:  99%|▉| 72/73 [00:01<00:00, 36.61it/s, loss=0.571, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 73/73 [00:02<00:00, 36.24it/s, loss=0.571, v_num=dfw5, LTC_val\u001b[A\n","Epoch 20:  99%|▉| 72/73 [00:02<00:00, 30.82it/s, loss=0.541, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 73/73 [00:02<00:00, 30.64it/s, loss=0.541, v_num=dfw5, LTC_val\u001b[A\n","Epoch 21:  99%|▉| 72/73 [00:02<00:00, 29.73it/s, loss=0.555, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 73/73 [00:02<00:00, 29.65it/s, loss=0.555, v_num=dfw5, LTC_val\u001b[A\n","Epoch 22:  99%|▉| 72/73 [00:02<00:00, 30.10it/s, loss=0.524, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 73/73 [00:02<00:00, 30.04it/s, loss=0.524, v_num=dfw5, LTC_val\u001b[A\n","Epoch 23:  99%|▉| 72/73 [00:02<00:00, 35.32it/s, loss=0.568, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 73/73 [00:02<00:00, 35.07it/s, loss=0.568, v_num=dfw5, LTC_val\u001b[A\n","Epoch 24:  99%|▉| 72/73 [00:01<00:00, 40.15it/s, loss=0.616, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.499\n","Epoch 24: 100%|█| 73/73 [00:01<00:00, 39.86it/s, loss=0.616, v_num=dfw5, LTC_val\n","Epoch 25:  99%|▉| 72/73 [00:02<00:00, 35.03it/s, loss=0.519, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.019 >= min_delta = 0.003. New best score: 0.480\n","Epoch 25: 100%|█| 73/73 [00:02<00:00, 34.68it/s, loss=0.519, v_num=dfw5, LTC_val\n","Epoch 26:  99%|▉| 72/73 [00:01<00:00, 36.20it/s, loss=0.592, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.028 >= min_delta = 0.003. New best score: 0.452\n","Epoch 26: 100%|█| 73/73 [00:02<00:00, 35.80it/s, loss=0.592, v_num=dfw5, LTC_val\n","Epoch 27:  99%|▉| 72/73 [00:02<00:00, 31.66it/s, loss=0.557, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 73/73 [00:02<00:00, 31.34it/s, loss=0.557, v_num=dfw5, LTC_val\u001b[A\n","Metric val_loss improved by 0.015 >= min_delta = 0.003. New best score: 0.437\n","Epoch 28:  99%|▉| 72/73 [00:02<00:00, 32.06it/s, loss=0.541, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 73/73 [00:02<00:00, 31.89it/s, loss=0.541, v_num=dfw5, LTC_val\u001b[A\n","Epoch 29:  99%|▉| 72/73 [00:02<00:00, 32.16it/s, loss=0.546, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 73/73 [00:02<00:00, 31.98it/s, loss=0.546, v_num=dfw5, LTC_val\u001b[A\n","Epoch 30:  99%|▉| 72/73 [00:01<00:00, 38.18it/s, loss=0.551, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.419\n","Epoch 30: 100%|█| 73/73 [00:01<00:00, 37.79it/s, loss=0.551, v_num=dfw5, LTC_val\n","Epoch 31:  99%|▉| 72/73 [00:02<00:00, 34.20it/s, loss=0.556, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 73/73 [00:02<00:00, 33.94it/s, loss=0.556, v_num=dfw5, LTC_val\u001b[A\n","Epoch 32:  99%|▉| 72/73 [00:02<00:00, 35.16it/s, loss=0.541, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 73/73 [00:02<00:00, 35.01it/s, loss=0.541, v_num=dfw5, LTC_val\u001b[A\n","Epoch 33:  99%|▉| 72/73 [00:02<00:00, 35.83it/s, loss=0.576, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 33: 100%|█| 73/73 [00:02<00:00, 35.64it/s, loss=0.576, v_num=dfw5, LTC_val\u001b[A\n","Epoch 34:  99%|▉| 72/73 [00:02<00:00, 32.74it/s, loss=0.58, v_num=dfw5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 73/73 [00:02<00:00, 32.65it/s, loss=0.58, v_num=dfw5, LTC_val_\u001b[A\n","Epoch 35:  99%|▉| 72/73 [00:01<00:00, 37.65it/s, loss=0.565, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 35: 100%|█| 73/73 [00:01<00:00, 37.38it/s, loss=0.565, v_num=dfw5, LTC_val\u001b[A\n","Epoch 36:  99%|▉| 72/73 [00:02<00:00, 35.00it/s, loss=0.583, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 36: 100%|█| 73/73 [00:02<00:00, 34.82it/s, loss=0.583, v_num=dfw5, LTC_val\u001b[A\n","Epoch 37:  99%|▉| 72/73 [00:01<00:00, 36.03it/s, loss=0.508, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 37: 100%|█| 73/73 [00:02<00:00, 35.85it/s, loss=0.508, v_num=dfw5, LTC_val\u001b[A\n","Epoch 38:  99%|▉| 72/73 [00:02<00:00, 32.25it/s, loss=0.54, v_num=dfw5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 38: 100%|█| 73/73 [00:02<00:00, 32.14it/s, loss=0.54, v_num=dfw5, LTC_val_\u001b[A\n","Epoch 39:  99%|▉| 72/73 [00:02<00:00, 35.15it/s, loss=0.558, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 39: 100%|█| 73/73 [00:02<00:00, 34.98it/s, loss=0.558, v_num=dfw5, LTC_val\u001b[A\n","Epoch 40:  99%|▉| 72/73 [00:02<00:00, 32.67it/s, loss=0.549, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 40: 100%|█| 73/73 [00:02<00:00, 32.49it/s, loss=0.549, v_num=dfw5, LTC_val\u001b[A\n","Epoch 41:  99%|▉| 72/73 [00:01<00:00, 37.67it/s, loss=0.502, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 41: 100%|█| 73/73 [00:01<00:00, 37.25it/s, loss=0.502, v_num=dfw5, LTC_val\u001b[A\n","Epoch 42:  99%|▉| 72/73 [00:02<00:00, 35.22it/s, loss=0.555, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 42: 100%|█| 73/73 [00:02<00:00, 34.88it/s, loss=0.555, v_num=dfw5, LTC_val\u001b[A\n","Epoch 43:  99%|▉| 72/73 [00:02<00:00, 35.17it/s, loss=0.491, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 43: 100%|█| 73/73 [00:02<00:00, 34.92it/s, loss=0.491, v_num=dfw5, LTC_val\u001b[A\n","Epoch 44:  99%|▉| 72/73 [00:01<00:00, 36.17it/s, loss=0.558, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 44: 100%|█| 73/73 [00:02<00:00, 35.92it/s, loss=0.558, v_num=dfw5, LTC_val\u001b[A\n","Epoch 45:  99%|▉| 72/73 [00:02<00:00, 32.39it/s, loss=0.566, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 45: 100%|█| 73/73 [00:02<00:00, 32.23it/s, loss=0.566, v_num=dfw5, LTC_val\u001b[A\n","Epoch 46:  99%|▉| 72/73 [00:02<00:00, 34.53it/s, loss=0.512, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 46: 100%|█| 73/73 [00:02<00:00, 34.31it/s, loss=0.512, v_num=dfw5, LTC_val\u001b[A\n","Epoch 47:  99%|▉| 72/73 [00:02<00:00, 34.21it/s, loss=0.566, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 47: 100%|█| 73/73 [00:02<00:00, 33.85it/s, loss=0.566, v_num=dfw5, LTC_val\u001b[A\n","Epoch 48:  99%|▉| 72/73 [00:01<00:00, 39.87it/s, loss=0.529, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 48: 100%|█| 73/73 [00:01<00:00, 39.53it/s, loss=0.529, v_num=dfw5, LTC_val\u001b[A\n","Epoch 49:  99%|▉| 72/73 [00:02<00:00, 27.68it/s, loss=0.485, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.034 >= min_delta = 0.003. New best score: 0.385\n","Epoch 49: 100%|█| 73/73 [00:02<00:00, 27.62it/s, loss=0.485, v_num=dfw5, LTC_val\n","Epoch 50:  99%|▉| 72/73 [00:01<00:00, 37.15it/s, loss=0.544, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 50: 100%|█| 73/73 [00:01<00:00, 36.63it/s, loss=0.544, v_num=dfw5, LTC_val\u001b[A\n","Epoch 51:  99%|▉| 72/73 [00:01<00:00, 37.01it/s, loss=0.512, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 51: 100%|█| 73/73 [00:01<00:00, 36.78it/s, loss=0.512, v_num=dfw5, LTC_val\u001b[A\n","Epoch 52:  99%|▉| 72/73 [00:01<00:00, 37.93it/s, loss=0.51, v_num=dfw5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 52: 100%|█| 73/73 [00:01<00:00, 37.65it/s, loss=0.51, v_num=dfw5, LTC_val_\u001b[A\n","Epoch 53:  99%|▉| 72/73 [00:01<00:00, 38.01it/s, loss=0.526, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 53: 100%|█| 73/73 [00:01<00:00, 37.68it/s, loss=0.526, v_num=dfw5, LTC_val\u001b[A\n","Epoch 54:  99%|▉| 72/73 [00:01<00:00, 37.46it/s, loss=0.467, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 54: 100%|█| 73/73 [00:01<00:00, 37.21it/s, loss=0.467, v_num=dfw5, LTC_val\u001b[A\n","Epoch 55:  99%|▉| 72/73 [00:02<00:00, 35.15it/s, loss=0.49, v_num=dfw5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 55: 100%|█| 73/73 [00:02<00:00, 34.96it/s, loss=0.49, v_num=dfw5, LTC_val_\u001b[A\n","Epoch 56:  99%|▉| 72/73 [00:01<00:00, 37.14it/s, loss=0.469, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.015 >= min_delta = 0.003. New best score: 0.370\n","Epoch 56: 100%|█| 73/73 [00:01<00:00, 36.87it/s, loss=0.469, v_num=dfw5, LTC_val\n","Epoch 57:  99%|▉| 72/73 [00:02<00:00, 32.16it/s, loss=0.463, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 57: 100%|█| 73/73 [00:02<00:00, 31.99it/s, loss=0.463, v_num=dfw5, LTC_val\u001b[A\n","Epoch 58:  99%|▉| 72/73 [00:02<00:00, 32.53it/s, loss=0.502, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 58: 100%|█| 73/73 [00:02<00:00, 32.42it/s, loss=0.502, v_num=dfw5, LTC_val\u001b[A\n","Epoch 59:  99%|▉| 72/73 [00:02<00:00, 35.54it/s, loss=0.493, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 59: 100%|█| 73/73 [00:02<00:00, 35.39it/s, loss=0.493, v_num=dfw5, LTC_val\u001b[A\n","Epoch 60:  99%|▉| 72/73 [00:02<00:00, 29.82it/s, loss=0.45, v_num=dfw5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 60: 100%|█| 73/73 [00:02<00:00, 29.72it/s, loss=0.45, v_num=dfw5, LTC_val_\u001b[A\n","Epoch 61:  99%|▉| 72/73 [00:01<00:00, 36.48it/s, loss=0.47, v_num=dfw5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 61: 100%|█| 73/73 [00:02<00:00, 36.22it/s, loss=0.47, v_num=dfw5, LTC_val_\u001b[A\n","Epoch 62:  99%|▉| 72/73 [00:02<00:00, 31.77it/s, loss=0.467, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 62: 100%|█| 73/73 [00:02<00:00, 31.50it/s, loss=0.467, v_num=dfw5, LTC_val\u001b[A\n","Epoch 63:  99%|▉| 72/73 [00:01<00:00, 37.18it/s, loss=0.441, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 63: 100%|█| 73/73 [00:01<00:00, 36.82it/s, loss=0.441, v_num=dfw5, LTC_val\u001b[A\n","Epoch 64:  99%|▉| 72/73 [00:01<00:00, 39.78it/s, loss=0.443, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 64: 100%|█| 73/73 [00:01<00:00, 39.27it/s, loss=0.443, v_num=dfw5, LTC_val\u001b[A\n","Epoch 65:  99%|▉| 72/73 [00:01<00:00, 36.73it/s, loss=0.47, v_num=dfw5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 65: 100%|█| 73/73 [00:02<00:00, 36.44it/s, loss=0.47, v_num=dfw5, LTC_val_\u001b[A\n","Epoch 66:  99%|▉| 72/73 [00:02<00:00, 35.97it/s, loss=0.457, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 66: 100%|█| 73/73 [00:02<00:00, 35.63it/s, loss=0.457, v_num=dfw5, LTC_val\u001b[A\n","Epoch 67:  99%|▉| 72/73 [00:02<00:00, 32.56it/s, loss=0.371, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 67: 100%|█| 73/73 [00:02<00:00, 32.39it/s, loss=0.371, v_num=dfw5, LTC_val\u001b[A\n","Epoch 68:  99%|▉| 72/73 [00:02<00:00, 30.80it/s, loss=0.407, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 68: 100%|█| 73/73 [00:02<00:00, 30.69it/s, loss=0.407, v_num=dfw5, LTC_val\u001b[A\n","Epoch 69:  99%|▉| 72/73 [00:02<00:00, 32.86it/s, loss=0.386, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 69: 100%|█| 73/73 [00:02<00:00, 32.74it/s, loss=0.386, v_num=dfw5, LTC_val\u001b[A\n","Epoch 70:  99%|▉| 72/73 [00:02<00:00, 33.28it/s, loss=0.372, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 70: 100%|█| 73/73 [00:02<00:00, 33.19it/s, loss=0.372, v_num=dfw5, LTC_val\u001b[A\n","Epoch 71:  99%|▉| 72/73 [00:02<00:00, 29.67it/s, loss=0.357, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n","Epoch 71: 100%|█| 73/73 [00:02<00:00, 27.73it/s, loss=0.357, v_num=dfw5, LTC_val\u001b[A\n","Epoch 72:  99%|▉| 72/73 [00:02<00:00, 28.64it/s, loss=0.368, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 72: 100%|█| 73/73 [00:02<00:00, 28.58it/s, loss=0.368, v_num=dfw5, LTC_val\u001b[A\n","Epoch 73:  99%|▉| 72/73 [00:01<00:00, 40.66it/s, loss=0.368, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 73: 100%|█| 73/73 [00:01<00:00, 40.30it/s, loss=0.368, v_num=dfw5, LTC_val\u001b[A\n","Epoch 74:  99%|▉| 72/73 [00:01<00:00, 40.40it/s, loss=0.385, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 74: 100%|█| 73/73 [00:01<00:00, 39.97it/s, loss=0.385, v_num=dfw5, LTC_val\u001b[A\n","Epoch 75:  99%|▉| 72/73 [00:01<00:00, 40.29it/s, loss=0.357, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 75: 100%|█| 73/73 [00:01<00:00, 39.84it/s, loss=0.357, v_num=dfw5, LTC_val\u001b[A\n","Epoch 76:  99%|▉| 72/73 [00:02<00:00, 34.76it/s, loss=0.279, v_num=dfw5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 76: 100%|█| 73/73 [00:02<00:00, 34.22it/s, loss=0.279, v_num=dfw5, LTC_val\u001b[A\n","                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.370. Signaling Trainer to stop.\n","Epoch 76: 100%|█| 73/73 [00:02<00:00, 34.16it/s, loss=0.279, v_num=dfw5, LTC_val\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 51.36it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'LTC_test_acc': 0.7142857313156128,\n"," 'LTC_test_f1': 0.7088435888290405,\n"," 'test_loss': 0.5559489130973816}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 71970\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014547-3kuodfw5/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014547-3kuodfw5/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.6875\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.68627\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.62474\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 76\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 5544\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 170\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464517\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 264\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.82465\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.81569\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.36146\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.43753\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.71429\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.70884\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.55595\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▃▅▇▅▇▇▅▄▅▅▅▆▅▆▇▆▄▆▇▆▅▇▅▇▅▅▆▅▆▇▅█▅▆▇▃▆█▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▂▅▇▅▇▇▄▃▄▅▅▆▄▆▆▆▄▆▇▆▅▇▅▇▅▅▆▅▆▇▅█▄▆▇▃▆█▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▆▅▃▅▄▃▅▆▄▄▆▄▇▄▂▂▆▄▂▄▃▃▅▃▄▆▄▅▄▂▅▁▄▃▂▇▃▁▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▂▅▅▅▅▅▅▅▅▅▅▅▆▅▅▅▅▆▆▆▆▆▆▅▆▆▆▆▆▆▇▇▇▇▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▂▅▅▅▅▅▅▅▅▅▅▅▆▅▅▆▆▆▆▆▆▆▆▆▆▆▇▆▆▇▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▆▆▆▆▅▆▆▅▅▆▅▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▂▃▂▂▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▁█▁▁▁▁▁▁▁▁▁▁▅▅▁▅▅▁▅▅▅█▅▅▅▅▅▅█▁▅▅▁▁▅▅▅▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▄█▄▄▄▄▄▄▄▄▄▄▆▆▄▆▆▄▆▆▆█▆▆▆▆▆▆█▄▆▆▄▄▆▆▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ██▅▇▇█▆▅▆▅▄▄▅▄▃▅▃▂▄▃▄▃▃▅▂▂▅▃▃▁▃▂▂▅▃▂▂▃▄▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3kuodfw5\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:48:52.815743: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm__binary_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/23jfnnf5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014851-23jfnnf5\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 130   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 73/74 [00:01<00:00, 41.76it/s, loss=0.722, v_num=nnf5, LTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.702\n","Epoch 0: 100%|█| 74/74 [00:01<00:00, 41.08it/s, loss=0.722, v_num=nnf5, LTC_val_\n","Epoch 1:  99%|▉| 73/74 [00:02<00:00, 33.42it/s, loss=0.706, v_num=nnf5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 74/74 [00:02<00:00, 33.30it/s, loss=0.706, v_num=nnf5, LTC_val_\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.694\n","Epoch 2:  99%|▉| 73/74 [00:02<00:00, 30.33it/s, loss=0.695, v_num=nnf5, LTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 74/74 [00:02<00:00, 30.32it/s, loss=0.695, v_num=nnf5, LTC_val_\u001b[A\n","Epoch 3:  99%|▉| 73/74 [00:02<00:00, 31.25it/s, loss=0.714, v_num=nnf5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 74/74 [00:02<00:00, 31.08it/s, loss=0.714, v_num=nnf5, LTC_val_\u001b[A\n","Epoch 4:  99%|▉| 73/74 [00:01<00:00, 40.90it/s, loss=0.699, v_num=nnf5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 74/74 [00:01<00:00, 40.39it/s, loss=0.699, v_num=nnf5, LTC_val_\u001b[A\n","Epoch 5:  99%|▉| 73/74 [00:01<00:00, 41.90it/s, loss=0.693, v_num=nnf5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 74/74 [00:01<00:00, 41.63it/s, loss=0.693, v_num=nnf5, LTC_val_\u001b[A\n","Epoch 6:  99%|▉| 73/74 [00:01<00:00, 38.73it/s, loss=0.705, v_num=nnf5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 74/74 [00:01<00:00, 38.49it/s, loss=0.705, v_num=nnf5, LTC_val_\u001b[A\n","Epoch 7:  99%|▉| 73/74 [00:02<00:00, 35.18it/s, loss=0.693, v_num=nnf5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 74/74 [00:02<00:00, 35.06it/s, loss=0.693, v_num=nnf5, LTC_val_\u001b[A\n","Epoch 8:  99%|▉| 73/74 [00:02<00:00, 27.78it/s, loss=0.697, v_num=nnf5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 74/74 [00:02<00:00, 27.56it/s, loss=0.697, v_num=nnf5, LTC_val_\u001b[A\n","Epoch 9:  99%|▉| 73/74 [00:02<00:00, 28.94it/s, loss=0.696, v_num=nnf5, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 74/74 [00:02<00:00, 28.86it/s, loss=0.696, v_num=nnf5, LTC_val_\u001b[A\n","Epoch 10:  99%|▉| 73/74 [00:01<00:00, 38.01it/s, loss=0.697, v_num=nnf5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 74/74 [00:01<00:00, 37.76it/s, loss=0.697, v_num=nnf5, LTC_val\u001b[A\n","Epoch 11:  99%|▉| 73/74 [00:01<00:00, 41.52it/s, loss=0.694, v_num=nnf5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 74/74 [00:01<00:00, 41.05it/s, loss=0.694, v_num=nnf5, LTC_val\u001b[A\n","Epoch 12:  99%|▉| 73/74 [00:01<00:00, 39.14it/s, loss=0.688, v_num=nnf5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 74/74 [00:01<00:00, 38.76it/s, loss=0.688, v_num=nnf5, LTC_val\u001b[A\n","Epoch 13:  99%|▉| 73/74 [00:01<00:00, 38.47it/s, loss=0.694, v_num=nnf5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 74/74 [00:01<00:00, 38.15it/s, loss=0.694, v_num=nnf5, LTC_val\u001b[A\n","Epoch 14:  99%|▉| 73/74 [00:02<00:00, 30.78it/s, loss=0.698, v_num=nnf5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 74/74 [00:02<00:00, 30.58it/s, loss=0.698, v_num=nnf5, LTC_val\u001b[A\n","Epoch 15:  99%|▉| 73/74 [00:02<00:00, 27.86it/s, loss=0.695, v_num=nnf5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 74/74 [00:02<00:00, 27.27it/s, loss=0.695, v_num=nnf5, LTC_val\u001b[A\n","Epoch 16:  99%|▉| 73/74 [00:01<00:00, 38.69it/s, loss=0.696, v_num=nnf5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 74/74 [00:01<00:00, 38.32it/s, loss=0.696, v_num=nnf5, LTC_val\u001b[A\n","Epoch 17:  99%|▉| 73/74 [00:02<00:00, 34.38it/s, loss=0.698, v_num=nnf5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 74/74 [00:02<00:00, 34.18it/s, loss=0.698, v_num=nnf5, LTC_val\u001b[A\n","Epoch 18:  99%|▉| 73/74 [00:01<00:00, 39.67it/s, loss=0.692, v_num=nnf5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 74/74 [00:01<00:00, 39.43it/s, loss=0.692, v_num=nnf5, LTC_val\u001b[A\n","Epoch 19:  99%|▉| 73/74 [00:02<00:00, 35.30it/s, loss=0.691, v_num=nnf5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 74/74 [00:02<00:00, 35.11it/s, loss=0.691, v_num=nnf5, LTC_val\u001b[A\n","Epoch 20:  99%|▉| 73/74 [00:02<00:00, 33.51it/s, loss=0.699, v_num=nnf5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 74/74 [00:02<00:00, 33.24it/s, loss=0.699, v_num=nnf5, LTC_val\u001b[A\n","Epoch 21:  99%|▉| 73/74 [00:02<00:00, 36.43it/s, loss=0.695, v_num=nnf5, LTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.694. Signaling Trainer to stop.\n","Epoch 21: 100%|█| 74/74 [00:02<00:00, 36.11it/s, loss=0.695, v_num=nnf5, LTC_val\n","Epoch 21: 100%|█| 74/74 [00:02<00:00, 36.06it/s, loss=0.695, v_num=nnf5, LTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 74.06it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'LTC_test_acc': 0.5,\n"," 'LTC_test_f1': 0.32692310214042664,\n"," 'test_loss': 0.6930055618286133}\n","--------------------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 72428\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014851-23jfnnf5/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_014851-23jfnnf5/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.25\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.73672\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 21\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1606\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 56\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464587\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 76\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.51123\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.4794\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69236\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.33333\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.69448\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.32692\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.69301\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▇▇▂▇▆▅▂▆▆▃▆▆▅▆▇▃▆▇▇▄▁▁▄▃█▅▅▇▅▅█▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▇▇▂▇▅▄▂▆▆▂▆▃▅▆▇▃▆▇▄▄▂▂▄▃█▅▅▇▅▅█▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▁▂▆▆▆▄█▄▅▅▅▄▅▅▅▅▅▄▃▅▆▇▅▆▄▆▅▃▅▅▅█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▇▇▂▄▄▆▅▆▃▁▆█▂▇▆▂▄█▅▅▃▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▆▆▁▂▆▅▅▅▁▃▅▆▃▃▇▁▂█▅▆▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ▇▄█▅▃▃▃▂▃▂▁▁▂▂▁▁▂▁▁▁▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▁▂▂▃▄▂▂▁▁▂▁▁▂▁▁▁▁▁▁▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/23jfnnf5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:50:01.874256: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm__multi_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2dvmmjmf\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015000-2dvmmjmf\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 195   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 72/73 [00:01<00:00, 43.30it/s, loss=1.05, v_num=mjmf, LTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.280\n","Epoch 0: 100%|█| 73/73 [00:01<00:00, 41.01it/s, loss=1.05, v_num=mjmf, LTC_val_a\n","Epoch 1:  99%|▉| 72/73 [00:01<00:00, 42.64it/s, loss=1.09, v_num=mjmf, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.134 >= min_delta = 0.003. New best score: 1.146\n","Epoch 1: 100%|█| 73/73 [00:01<00:00, 42.35it/s, loss=1.09, v_num=mjmf, LTC_val_a\n","Epoch 2:  99%|▉| 72/73 [00:01<00:00, 39.77it/s, loss=1.04, v_num=mjmf, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 73/73 [00:01<00:00, 39.42it/s, loss=1.04, v_num=mjmf, LTC_val_a\u001b[A\n","Epoch 3:  99%|▉| 72/73 [00:01<00:00, 36.10it/s, loss=1.04, v_num=mjmf, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 73/73 [00:02<00:00, 35.99it/s, loss=1.04, v_num=mjmf, LTC_val_a\u001b[A\n","Epoch 4:  99%|▉| 72/73 [00:02<00:00, 29.69it/s, loss=1.03, v_num=mjmf, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 73/73 [00:02<00:00, 29.56it/s, loss=1.03, v_num=mjmf, LTC_val_a\u001b[A\n","Epoch 5:  99%|▉| 72/73 [00:02<00:00, 35.90it/s, loss=1.05, v_num=mjmf, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 1.134\n","Epoch 5: 100%|█| 73/73 [00:02<00:00, 35.61it/s, loss=1.05, v_num=mjmf, LTC_val_a\n","Epoch 6:  99%|▉| 72/73 [00:01<00:00, 41.46it/s, loss=1.06, v_num=mjmf, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 73/73 [00:01<00:00, 41.03it/s, loss=1.06, v_num=mjmf, LTC_val_a\u001b[A\n","Epoch 7:  99%|▉| 72/73 [00:02<00:00, 28.31it/s, loss=1.03, v_num=mjmf, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 73/73 [00:02<00:00, 28.22it/s, loss=1.03, v_num=mjmf, LTC_val_a\u001b[A\n","Epoch 8:  99%|▉| 72/73 [00:02<00:00, 34.80it/s, loss=1.05, v_num=mjmf, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 73/73 [00:02<00:00, 34.62it/s, loss=1.05, v_num=mjmf, LTC_val_a\u001b[A\n","Epoch 9:  99%|▉| 72/73 [00:01<00:00, 37.76it/s, loss=1.05, v_num=mjmf, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 73/73 [00:01<00:00, 37.37it/s, loss=1.05, v_num=mjmf, LTC_val_a\u001b[A\n","                                                                                Metric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 1.128\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 10:  99%|▉| 72/73 [00:01<00:00, 39.14it/s, loss=1.04, v_num=mjmf, LTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 73/73 [00:01<00:00, 38.72it/s, loss=1.04, v_num=mjmf, LTC_val_\u001b[A\n","Epoch 11:  99%|▉| 72/73 [00:01<00:00, 37.51it/s, loss=1.01, v_num=mjmf, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 73/73 [00:01<00:00, 37.16it/s, loss=1.01, v_num=mjmf, LTC_val_\u001b[A\n","Epoch 12:  99%|▉| 72/73 [00:02<00:00, 33.25it/s, loss=1, v_num=mjmf, LTC_val_acc\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 73/73 [00:02<00:00, 33.11it/s, loss=1, v_num=mjmf, LTC_val_acc\u001b[A\n","Epoch 13:  99%|▉| 72/73 [00:01<00:00, 42.05it/s, loss=1.02, v_num=mjmf, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 73/73 [00:01<00:00, 41.47it/s, loss=1.02, v_num=mjmf, LTC_val_\u001b[A\n","Epoch 14:  99%|▉| 72/73 [00:01<00:00, 42.34it/s, loss=1.04, v_num=mjmf, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 73/73 [00:01<00:00, 41.88it/s, loss=1.04, v_num=mjmf, LTC_val_\u001b[A\n","Epoch 15:  99%|▉| 72/73 [00:02<00:00, 35.92it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 73/73 [00:02<00:00, 33.30it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n","Epoch 16:  99%|▉| 72/73 [00:02<00:00, 27.20it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 73/73 [00:02<00:00, 27.19it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n","Epoch 17:  99%|▉| 72/73 [00:02<00:00, 30.56it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 73/73 [00:02<00:00, 30.52it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n","Epoch 18:  99%|▉| 72/73 [00:02<00:00, 25.85it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 73/73 [00:02<00:00, 25.80it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n","Epoch 19:  99%|▉| 72/73 [00:02<00:00, 29.54it/s, loss=1.05, v_num=mjmf, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 73/73 [00:02<00:00, 29.44it/s, loss=1.05, v_num=mjmf, LTC_val_\u001b[A\n","Epoch 20:  99%|▉| 72/73 [00:01<00:00, 38.50it/s, loss=1.04, v_num=mjmf, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 73/73 [00:01<00:00, 38.12it/s, loss=1.04, v_num=mjmf, LTC_val_\u001b[A\n","Epoch 21:  99%|▉| 72/73 [00:01<00:00, 41.36it/s, loss=1.06, v_num=mjmf, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 73/73 [00:01<00:00, 40.95it/s, loss=1.06, v_num=mjmf, LTC_val_\u001b[A\n","Epoch 22:  99%|▉| 72/73 [00:01<00:00, 41.08it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 73/73 [00:01<00:00, 40.71it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n","Epoch 23:  99%|▉| 72/73 [00:02<00:00, 31.78it/s, loss=1.01, v_num=mjmf, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 73/73 [00:02<00:00, 31.59it/s, loss=1.01, v_num=mjmf, LTC_val_\u001b[A\n","Epoch 24:  99%|▉| 72/73 [00:01<00:00, 37.88it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 73/73 [00:01<00:00, 37.55it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n","Epoch 25:  99%|▉| 72/73 [00:02<00:00, 30.06it/s, loss=1.05, v_num=mjmf, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 73/73 [00:02<00:00, 30.04it/s, loss=1.05, v_num=mjmf, LTC_val_\u001b[A\n","Epoch 26:  99%|▉| 72/73 [00:02<00:00, 30.39it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 73/73 [00:02<00:00, 30.34it/s, loss=1.03, v_num=mjmf, LTC_val_\u001b[A\n","Epoch 27:  99%|▉| 72/73 [00:01<00:00, 39.53it/s, loss=1.02, v_num=mjmf, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 73/73 [00:01<00:00, 39.30it/s, loss=1.02, v_num=mjmf, LTC_val_\u001b[A\n","Epoch 28:  99%|▉| 72/73 [00:02<00:00, 30.15it/s, loss=1.05, v_num=mjmf, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 73/73 [00:02<00:00, 30.12it/s, loss=1.05, v_num=mjmf, LTC_val_\u001b[A\n","Epoch 29:  99%|▉| 72/73 [00:02<00:00, 28.63it/s, loss=1.04, v_num=mjmf, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.128. Signaling Trainer to stop.\n","Epoch 29: 100%|█| 73/73 [00:02<00:00, 28.52it/s, loss=1.04, v_num=mjmf, LTC_val_\n","Epoch 29: 100%|█| 73/73 [00:02<00:00, 28.48it/s, loss=1.04, v_num=mjmf, LTC_val_\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 71.17it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'LTC_test_acc': 0.4642857015132904,\n"," 'LTC_test_f1': 0.21118013560771942,\n"," 'test_loss': 1.0434070825576782}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 72619\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015000-2dvmmjmf/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015000-2dvmmjmf/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.5625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.24\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.90547\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 29\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2160\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 73\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464673\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 103\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.48872\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.21657\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.03112\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.18182\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.14876\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.46429\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.21118\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.04341\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▂▄▃▂▄▃▅▂▄▂▄▃▅▁▄▅▄▅▅▂▅▃█▂▂▄▅▄▃▄▃▅▅▆▇▅▂▃▂▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▃▅▇▂▅▆▆▃▅▂▅▄▅▁▅▅▅▆▅▃▆▄█▃▃▅▅▅▄▅▄▅▆▆▇▆▃▄▂▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▂▅▆▄▆▆▅▅▆▆▅▄█▄▅▅▂▄▅▅▅▁▇▆▅▆▆▆▆▅▅▅▅▃▄▆▅▆▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▆▄▆█▇█▇█▇████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch █▇▅▄▅▃▃▂▂▃▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▃▄▂▂▃▂▂▃▂▁▂▂▂▁▂▂▂▁▁▁▂▁▂▁▁▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▂▃▄▂▁▁▂▂▁▃▂▃▂▂▂▂▁▂▂▁▂▂▂▁▂▂▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2dvmmjmf\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:51:27.832319: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm__multi_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2oujlg93\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015126-2oujlg93\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 195   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 73/74 [00:01<00:00, 41.87it/s, loss=1.11, v_num=lg93, LTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.035\n","Epoch 0: 100%|█| 74/74 [00:01<00:00, 41.35it/s, loss=1.11, v_num=lg93, LTC_val_a\n","Epoch 1:  99%|▉| 73/74 [00:01<00:00, 41.27it/s, loss=1.13, v_num=lg93, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 74/74 [00:01<00:00, 40.99it/s, loss=1.13, v_num=lg93, LTC_val_a\u001b[A\n","Epoch 2:  99%|▉| 73/74 [00:01<00:00, 41.52it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 74/74 [00:01<00:00, 41.24it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n","Epoch 3:  99%|▉| 73/74 [00:01<00:00, 36.97it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 74/74 [00:02<00:00, 36.79it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n","Epoch 4:  99%|▉| 73/74 [00:01<00:00, 41.23it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 74/74 [00:01<00:00, 40.95it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n","Epoch 5:  99%|▉| 73/74 [00:02<00:00, 31.78it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 74/74 [00:02<00:00, 31.73it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n","Epoch 6:  99%|▉| 73/74 [00:01<00:00, 40.87it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 74/74 [00:01<00:00, 40.62it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n","Epoch 7:  99%|▉| 73/74 [00:01<00:00, 40.92it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 74/74 [00:01<00:00, 40.61it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n","Epoch 8:  99%|▉| 73/74 [00:01<00:00, 42.08it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 74/74 [00:01<00:00, 41.76it/s, loss=1.1, v_num=lg93, LTC_val_ac\u001b[A\n","Epoch 9:  99%|▉| 73/74 [00:01<00:00, 40.68it/s, loss=1.11, v_num=lg93, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 74/74 [00:01<00:00, 40.42it/s, loss=1.11, v_num=lg93, LTC_val_a\u001b[A\n","Epoch 10:  99%|▉| 73/74 [00:01<00:00, 41.92it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 74/74 [00:01<00:00, 41.43it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n","Epoch 11:  99%|▉| 73/74 [00:01<00:00, 39.51it/s, loss=1.12, v_num=lg93, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 74/74 [00:01<00:00, 39.12it/s, loss=1.12, v_num=lg93, LTC_val_\u001b[A\n","Epoch 12:  99%|▉| 73/74 [00:02<00:00, 30.43it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 74/74 [00:02<00:00, 30.34it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n","Epoch 13:  99%|▉| 73/74 [00:01<00:00, 39.72it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 74/74 [00:01<00:00, 39.49it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n","Epoch 14:  99%|▉| 73/74 [00:01<00:00, 40.95it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 74/74 [00:01<00:00, 40.53it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n","Epoch 15:  99%|▉| 73/74 [00:01<00:00, 40.55it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 74/74 [00:01<00:00, 38.96it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n","Epoch 16:  99%|▉| 73/74 [00:01<00:00, 37.77it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 74/74 [00:01<00:00, 37.41it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n","Epoch 17:  99%|▉| 73/74 [00:01<00:00, 40.89it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 74/74 [00:01<00:00, 40.49it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n","Epoch 18:  99%|▉| 73/74 [00:02<00:00, 35.70it/s, loss=1.11, v_num=lg93, LTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 74/74 [00:02<00:00, 35.45it/s, loss=1.11, v_num=lg93, LTC_val_\u001b[A\n","Epoch 19:  99%|▉| 73/74 [00:01<00:00, 40.34it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 74/74 [00:01<00:00, 39.98it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n","Epoch 20:  99%|▉| 73/74 [00:01<00:00, 40.18it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.035. Signaling Trainer to stop.\n","Epoch 20: 100%|█| 74/74 [00:01<00:00, 39.76it/s, loss=1.1, v_num=lg93, LTC_val_a\n","Epoch 20: 100%|█| 74/74 [00:01<00:00, 39.69it/s, loss=1.1, v_num=lg93, LTC_val_a\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 72.61it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'LTC_test_acc': 0.25,\n"," 'LTC_test_f1': 0.13151928782463074,\n"," 'test_loss': 1.1257113218307495}\n","--------------------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 72830\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015126-2oujlg93/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015126-2oujlg93/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.58053\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.0785\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1533\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 49\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464735\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 72\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.33074\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.27981\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.10022\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.125\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.07407\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.11085\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.25\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.13152\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.12571\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▄▅▅▅▅▅▄▄▅▅▂▄▄▄▄▆█▄▄▁▄▅▄▃▄▄▅▅▂▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▃▅▃▅▅▅▃▄▅▅▂▄▄▃▅▆█▃▄▁▄▄▄▃▃▄▅▆▃▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▃▁▃▅▄▃▅▂▅▂▇█▅▄▅▂▁▄▆▇▄▃▃▅▆▅▂▄▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▇▆▆▃▃▂▅▃█▂▄▃▄▄▅▃▁▆▅▁▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▅▆▆▄▃▅▆▄█▅▆▃▄▅▅▂▅▁▆▂▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▄▄▃▃▂▂▂▃▁▃▂▂▂▂▂▁▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ██▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▇▇▂▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▆█▇▆▆▇▆▆▇▇▇▆▇▇▆▆▆▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2oujlg93\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:52:30.980778: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_single_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/yh1l3les\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015229-yh1l3les\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 130   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.7 K    Trainable params\n","0         Non-trainable params\n","75.7 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 79/80 [00:01<00:00, 42.00it/s, loss=0.589, v_num=3les, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.553\n","Epoch 0: 100%|█| 80/80 [00:01<00:00, 41.15it/s, loss=0.589, v_num=3les, BTC_val_\n","Epoch 1:  99%|▉| 79/80 [00:01<00:00, 43.65it/s, loss=0.646, v_num=3les, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 42.87it/s, loss=0.646, v_num=3les, BTC_val_\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.546\n","Epoch 2:  99%|▉| 79/80 [00:01<00:00, 43.90it/s, loss=0.605, v_num=3les, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 42.87it/s, loss=0.605, v_num=3les, BTC_val_\u001b[A\n","Epoch 3:  99%|▉| 79/80 [00:01<00:00, 42.12it/s, loss=0.55, v_num=3les, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.023 >= min_delta = 0.003. New best score: 0.523\n","Epoch 3: 100%|█| 80/80 [00:01<00:00, 41.14it/s, loss=0.55, v_num=3les, BTC_val_a\n","Epoch 4:  99%|▉| 79/80 [00:01<00:00, 41.32it/s, loss=0.573, v_num=3les, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 40.44it/s, loss=0.573, v_num=3les, BTC_val_\u001b[A\n","Epoch 5:  99%|▉| 79/80 [00:01<00:00, 44.53it/s, loss=0.581, v_num=3les, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:01<00:00, 43.46it/s, loss=0.581, v_num=3les, BTC_val_\u001b[A\n","Epoch 6:  99%|▉| 79/80 [00:02<00:00, 39.03it/s, loss=0.56, v_num=3les, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.506\n","Epoch 6: 100%|█| 80/80 [00:02<00:00, 37.38it/s, loss=0.56, v_num=3les, BTC_val_a\n","Epoch 7:  99%|▉| 79/80 [00:02<00:00, 38.93it/s, loss=0.579, v_num=3les, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:02<00:00, 38.12it/s, loss=0.579, v_num=3les, BTC_val_\u001b[A\n","Epoch 8:  99%|▉| 79/80 [00:01<00:00, 43.43it/s, loss=0.592, v_num=3les, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:01<00:00, 42.54it/s, loss=0.592, v_num=3les, BTC_val_\u001b[A\n","Epoch 9:  99%|▉| 79/80 [00:01<00:00, 44.00it/s, loss=0.562, v_num=3les, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:01<00:00, 42.94it/s, loss=0.562, v_num=3les, BTC_val_\u001b[A\n","Epoch 10:  99%|▉| 79/80 [00:02<00:00, 35.52it/s, loss=0.581, v_num=3les, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 10: 100%|█| 80/80 [00:02<00:00, 35.06it/s, loss=0.581, v_num=3les, BTC_val\u001b[A\n","Epoch 11:  99%|▉| 79/80 [00:01<00:00, 43.26it/s, loss=0.582, v_num=3les, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:01<00:00, 42.42it/s, loss=0.582, v_num=3les, BTC_val\u001b[A\n","Epoch 12:  99%|▉| 79/80 [00:01<00:00, 41.87it/s, loss=0.559, v_num=3les, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:01<00:00, 41.14it/s, loss=0.559, v_num=3les, BTC_val\u001b[A\n","Epoch 13:  99%|▉| 79/80 [00:02<00:00, 37.40it/s, loss=0.605, v_num=3les, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:02<00:00, 36.89it/s, loss=0.605, v_num=3les, BTC_val\u001b[A\n","Epoch 14:  99%|▉| 79/80 [00:01<00:00, 45.44it/s, loss=0.603, v_num=3les, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:01<00:00, 44.59it/s, loss=0.603, v_num=3les, BTC_val\u001b[A\n","Epoch 15:  99%|▉| 79/80 [00:01<00:00, 45.39it/s, loss=0.53, v_num=3les, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:01<00:00, 44.22it/s, loss=0.53, v_num=3les, BTC_val_\u001b[A\n","Epoch 16:  99%|▉| 79/80 [00:01<00:00, 42.51it/s, loss=0.574, v_num=3les, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:01<00:00, 41.48it/s, loss=0.574, v_num=3les, BTC_val\u001b[A\n","Epoch 17:  99%|▉| 79/80 [00:02<00:00, 37.58it/s, loss=0.59, v_num=3les, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:02<00:00, 36.88it/s, loss=0.59, v_num=3les, BTC_val_\u001b[A\n","Epoch 18:  99%|▉| 79/80 [00:02<00:00, 36.27it/s, loss=0.57, v_num=3les, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:02<00:00, 35.80it/s, loss=0.57, v_num=3les, BTC_val_\u001b[A\n","Epoch 19:  99%|▉| 79/80 [00:01<00:00, 44.23it/s, loss=0.533, v_num=3les, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:01<00:00, 43.36it/s, loss=0.533, v_num=3les, BTC_val\u001b[A\n","Epoch 20:  99%|▉| 79/80 [00:02<00:00, 36.52it/s, loss=0.593, v_num=3les, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:02<00:00, 36.02it/s, loss=0.593, v_num=3les, BTC_val\u001b[A\n","Epoch 21:  99%|▉| 79/80 [00:02<00:00, 36.00it/s, loss=0.581, v_num=3les, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:02<00:00, 35.30it/s, loss=0.581, v_num=3les, BTC_val\u001b[A\n","Epoch 22:  99%|▉| 79/80 [00:02<00:00, 36.85it/s, loss=0.596, v_num=3les, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:02<00:00, 36.31it/s, loss=0.596, v_num=3les, BTC_val\u001b[A\n","Epoch 23:  99%|▉| 79/80 [00:01<00:00, 43.85it/s, loss=0.567, v_num=3les, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:01<00:00, 43.07it/s, loss=0.567, v_num=3les, BTC_val\u001b[A\n","Epoch 24:  99%|▉| 79/80 [00:01<00:00, 40.51it/s, loss=0.559, v_num=3les, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:02<00:00, 39.89it/s, loss=0.559, v_num=3les, BTC_val\u001b[A\n","Epoch 25:  99%|▉| 79/80 [00:02<00:00, 37.66it/s, loss=0.606, v_num=3les, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:02<00:00, 37.15it/s, loss=0.606, v_num=3les, BTC_val\u001b[A\n","Epoch 26:  99%|▉| 79/80 [00:02<00:00, 37.29it/s, loss=0.586, v_num=3les, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.506. Signaling Trainer to stop.\n","Epoch 26: 100%|█| 80/80 [00:02<00:00, 36.61it/s, loss=0.586, v_num=3les, BTC_val\n","Epoch 26: 100%|█| 80/80 [00:02<00:00, 36.52it/s, loss=0.586, v_num=3les, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 60.19it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.6774193644523621,\n"," 'BTC_test_f1': 0.6673067212104797,\n"," 'ETH_test_acc': 0.7096773982048035,\n"," 'ETH_test_f1': 0.7085039019584656,\n"," 'test_loss': 0.5560200214385986}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 73060\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015229-yh1l3les/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015229-yh1l3les/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.54656\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.61175\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 26\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2133\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 63\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464812\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 96\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.72842\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.7141\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.71101\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.69858\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.56446\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.66667\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.66667\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.66667\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.64935\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.54846\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.67742\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.66731\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.70968\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.7085\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.55602\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▂▄▃▄▅▅▄▄▇▅▇▄▄▂▇▄▄█▃▃▁▆▆▃▆▄▆▂▅▂▂█▅▄▆█▆▆▂▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▂▄▃▄▅▅▄▄▇▅▇▄▄▂▇▂▄█▂▃▁▆▆▃▆▄▆▂▅▂▂█▅▄▆█▆▆▂▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step █▆▄▆▁▅▅▅▅▅▇▅▆▅▄▅▅▅▁▆▄▆▇▄▄▄▅▄▅▄▅▅▆▆▅▆▆▇▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step █▆▅▆▂▅▅▆▄▅▇▆▆▅▄▅▆▄▁▆▄▆▇▅▄▅▆▄▅▅▅▆▆▆▆▆▆▇▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅▆▆▆▅▄▄▄▄▅▁▃▃█▄▄▄▃█▄▇▂▃▆▅▄▄▆▄▅▆▃▄▃▄▃▄▃▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▅▇▅▆▆▆▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▅▆▄▅▆▆▇▇█▇▇▇▇▇▇▇▇▇▇█▇▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▅▆▅▆▆▆▇▆▇▆▇▆▇██▆█▇███▇▇▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▅▇▆▆▆▆▇▆▇▆▇▆▇██▆█▇███▇▇▇█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▃▃▂▂▂▂▃▂▂▂▃▂▂▂▂▂▂▁▂▁▂▁▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc █▁███████▁█████▁▁▁█▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 █▁███████▁█████▁▁▁█▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▁▃▃▁▃▃▃▃▁▁▁▃█▆▁▁▁▃▃▁▆▃▁▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▄▄▁▄▄▄▄▁▂▁▄█▆▁▁▁▄▄▁▆▄▁▄▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▇▇▄▅▃▁▅▄▄▃▃▂▆▄▄▄▇▄▂▅▄▅▇▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_single_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/yh1l3les\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:53:42.577030: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_single_lstm_loss_weighted_binary_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/dwrvgzys\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015341-dwrvgzys\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 130   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.7 K    Trainable params\n","0         Non-trainable params\n","75.7 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 80/81 [00:01<00:00, 44.80it/s, loss=0.688, v_num=gzys, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.666\n","Epoch 0: 100%|█| 81/81 [00:01<00:00, 43.45it/s, loss=0.688, v_num=gzys, BTC_val_\n","Epoch 1:  99%|▉| 80/81 [00:01<00:00, 45.00it/s, loss=0.703, v_num=gzys, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 81/81 [00:01<00:00, 43.77it/s, loss=0.703, v_num=gzys, BTC_val_\u001b[A\n","Epoch 2:  99%|▉| 80/81 [00:01<00:00, 45.36it/s, loss=0.699, v_num=gzys, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 81/81 [00:01<00:00, 43.93it/s, loss=0.699, v_num=gzys, BTC_val_\u001b[A\n","Epoch 3:  99%|▉| 80/81 [00:01<00:00, 45.58it/s, loss=0.7, v_num=gzys, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:01<00:00, 44.45it/s, loss=0.7, v_num=gzys, BTC_val_ac\u001b[A\n","Epoch 4:  99%|▉| 80/81 [00:01<00:00, 46.79it/s, loss=0.696, v_num=gzys, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:01<00:00, 45.51it/s, loss=0.696, v_num=gzys, BTC_val_\u001b[A\n","Epoch 5:  99%|▉| 80/81 [00:01<00:00, 45.54it/s, loss=0.695, v_num=gzys, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:01<00:00, 44.43it/s, loss=0.695, v_num=gzys, BTC_val_\u001b[A\n","Epoch 6:  99%|▉| 80/81 [00:01<00:00, 44.04it/s, loss=0.696, v_num=gzys, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:01<00:00, 43.10it/s, loss=0.696, v_num=gzys, BTC_val_\u001b[A\n","Epoch 7:  99%|▉| 80/81 [00:01<00:00, 45.08it/s, loss=0.702, v_num=gzys, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:01<00:00, 44.27it/s, loss=0.702, v_num=gzys, BTC_val_\u001b[A\n","Epoch 8:  99%|▉| 80/81 [00:01<00:00, 44.77it/s, loss=0.697, v_num=gzys, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:01<00:00, 43.93it/s, loss=0.697, v_num=gzys, BTC_val_\u001b[A\n","Epoch 9:  99%|▉| 80/81 [00:01<00:00, 44.40it/s, loss=0.692, v_num=gzys, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:01<00:00, 43.48it/s, loss=0.692, v_num=gzys, BTC_val_\u001b[A\n","Epoch 10:  99%|▉| 80/81 [00:01<00:00, 43.31it/s, loss=0.691, v_num=gzys, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:01<00:00, 42.44it/s, loss=0.691, v_num=gzys, BTC_val\u001b[A\n","Epoch 11:  99%|▉| 80/81 [00:02<00:00, 38.15it/s, loss=0.694, v_num=gzys, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:02<00:00, 37.64it/s, loss=0.694, v_num=gzys, BTC_val\u001b[A\n","Epoch 12:  99%|▉| 80/81 [00:01<00:00, 45.02it/s, loss=0.695, v_num=gzys, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:01<00:00, 44.20it/s, loss=0.695, v_num=gzys, BTC_val\u001b[A\n","Epoch 13:  99%|▉| 80/81 [00:01<00:00, 41.81it/s, loss=0.695, v_num=gzys, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:01<00:00, 41.12it/s, loss=0.695, v_num=gzys, BTC_val\u001b[A\n","Epoch 14:  99%|▉| 80/81 [00:01<00:00, 44.42it/s, loss=0.693, v_num=gzys, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:01<00:00, 43.55it/s, loss=0.693, v_num=gzys, BTC_val\u001b[A\n","Epoch 15:  99%|▉| 80/81 [00:01<00:00, 44.91it/s, loss=0.694, v_num=gzys, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:01<00:00, 44.05it/s, loss=0.694, v_num=gzys, BTC_val\u001b[A\n","Epoch 16:  99%|▉| 80/81 [00:02<00:00, 38.22it/s, loss=0.696, v_num=gzys, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:02<00:00, 37.70it/s, loss=0.696, v_num=gzys, BTC_val\u001b[A\n","Epoch 17:  99%|▉| 80/81 [00:01<00:00, 44.84it/s, loss=0.694, v_num=gzys, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:01<00:00, 44.03it/s, loss=0.694, v_num=gzys, BTC_val\u001b[A\n","Epoch 18:  99%|▉| 80/81 [00:01<00:00, 45.33it/s, loss=0.694, v_num=gzys, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:01<00:00, 43.81it/s, loss=0.694, v_num=gzys, BTC_val\u001b[A\n","Epoch 19:  99%|▉| 80/81 [00:01<00:00, 41.88it/s, loss=0.694, v_num=gzys, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:01<00:00, 40.87it/s, loss=0.694, v_num=gzys, BTC_val\u001b[A\n","Epoch 20:  99%|▉| 80/81 [00:02<00:00, 35.20it/s, loss=0.695, v_num=gzys, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.666. Signaling Trainer to stop.\n","Epoch 20: 100%|█| 81/81 [00:02<00:00, 34.74it/s, loss=0.695, v_num=gzys, BTC_val\n","Epoch 20: 100%|█| 81/81 [00:02<00:00, 34.69it/s, loss=0.695, v_num=gzys, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 78.67it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.4193548262119293,\n"," 'BTC_test_f1': 0.29533159732818604,\n"," 'ETH_test_acc': 0.6129032373428345,\n"," 'ETH_test_f1': 0.3793548345565796,\n"," 'test_loss': 0.6961109638214111}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 73286\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015341-dwrvgzys/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015341-dwrvgzys/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.4375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.37662\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.4375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.43529\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.69785\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1680\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 49\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464870\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.52955\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.47844\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.50512\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.45697\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69439\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.55556\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.35714\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.88889\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.47059\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.67389\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.41935\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.29533\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.37935\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.69611\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▂▅▃▅▄▆▅█▅▄▅▆▇▇█▆▃▁▅▅▂▃▅▃▅▅▂▄▄▃▆▆▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▂▅▃▅▃▆▅█▅▄▅▆▆▆█▆▂▁▃▄▂▃▃▂▅▄▂▄▄▃▆▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▆▅▆▆▆▅▃▃▄▄▃▃▅▆▅▃▃▃▅▂▄▃▃▃▅▁▁█▂▄▆▅▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▆▅▇▃▇▅▃▂▄▄▄▂▅▅▅▂▃▃▃▂▄▃▃▂▅▁▂█▂▄▆▃▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▄▇▅▁▃▃▅▂▆▆▄▄▃▂▂▄▇█▄▅▅▅▄▄▃▅▆▄▅▄▃▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▅▃▁█▄▆▅▂▄▆█▂▇▃█▁▂▆▅▄█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▂▄▃█▃█▇▃▅▆▂▁▇▃▆▂▂▇▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▇▅▄▅▁▆▅▄▅▄▅▅▅▂▄▇▁█▅▇▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▄▇▆▇▃▇▇▄▅▅▁▃▆▂▄▆▃█▆▇▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▄▃▃▂▃▃▂▂▁▁▁▂▁▁▂▁▁▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▅█▅▁▁▁█▅█▅█▅███▅▅▅▅▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▂█▂▁▁▁█▂█▂█▂███▂▂▂▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ███▁▁▁▇█▇█▇█▇▇▇██████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ███▁▁▁▇█▇█▇█▇▇▇██████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▂▂██▄▂▂▂▁▂▁▂▂▂▂▂▂▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_single_lstm_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/dwrvgzys\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:54:40.438672: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_single_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/c6dpd4yb\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015438-c6dpd4yb\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 195   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.8 K    Trainable params\n","0         Non-trainable params\n","75.8 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:   0%|                                           | 0/80 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 79/80 [00:01<00:00, 43.22it/s, loss=1.11, v_num=d4yb, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 0: 100%|█| 80/80 [00:01<00:00, 41.95it/s, loss=1.11, v_num=d4yb, BTC_val_aMetric val_loss improved. New best score: 1.086\n","\n","Epoch 1:  99%|▉| 79/80 [00:01<00:00, 44.06it/s, loss=1.08, v_num=d4yb, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 43.06it/s, loss=1.08, v_num=d4yb, BTC_val_a\u001b[A\n","Epoch 2:  99%|▉| 79/80 [00:01<00:00, 45.87it/s, loss=1.06, v_num=d4yb, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.024 >= min_delta = 0.003. New best score: 1.062\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 44.67it/s, loss=1.06, v_num=d4yb, BTC_val_a\n","Epoch 3:  99%|▉| 79/80 [00:01<00:00, 40.04it/s, loss=0.999, v_num=d4yb, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:02<00:00, 39.11it/s, loss=0.999, v_num=d4yb, BTC_val_\u001b[A\n","Epoch 4:  99%|▉| 79/80 [00:01<00:00, 42.70it/s, loss=0.994, v_num=d4yb, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 40.83it/s, loss=0.994, v_num=d4yb, BTC_val_\u001b[A\n","Epoch 5:  99%|▉| 79/80 [00:01<00:00, 44.96it/s, loss=0.999, v_num=d4yb, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:01<00:00, 43.83it/s, loss=0.999, v_num=d4yb, BTC_val_\u001b[A\n","Epoch 6:  99%|▉| 79/80 [00:01<00:00, 45.32it/s, loss=0.992, v_num=d4yb, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 1.058\n","Epoch 6: 100%|█| 80/80 [00:01<00:00, 44.18it/s, loss=0.992, v_num=d4yb, BTC_val_\n","Epoch 7:  99%|▉| 79/80 [00:01<00:00, 40.15it/s, loss=0.991, v_num=d4yb, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 7: 100%|█| 80/80 [00:02<00:00, 39.19it/s, loss=0.991, v_num=d4yb, BTC_val_\u001b[A\n","Epoch 8:  99%|▉| 79/80 [00:02<00:00, 38.51it/s, loss=0.955, v_num=d4yb, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:02<00:00, 37.76it/s, loss=0.955, v_num=d4yb, BTC_val_\u001b[A\n","Epoch 9:  99%|▉| 79/80 [00:01<00:00, 43.12it/s, loss=0.979, v_num=d4yb, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:01<00:00, 42.05it/s, loss=0.979, v_num=d4yb, BTC_val_\u001b[A\n","Epoch 10:  99%|▉| 79/80 [00:02<00:00, 39.35it/s, loss=0.939, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:02<00:00, 38.54it/s, loss=0.939, v_num=d4yb, BTC_val\u001b[A\n","Epoch 11:  99%|▉| 79/80 [00:01<00:00, 45.72it/s, loss=1.01, v_num=d4yb, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:01<00:00, 44.57it/s, loss=1.01, v_num=d4yb, BTC_val_\u001b[A\n","Epoch 12:  99%|▉| 79/80 [00:01<00:00, 42.44it/s, loss=0.949, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:01<00:00, 41.48it/s, loss=0.949, v_num=d4yb, BTC_val\u001b[A\n","Epoch 13:  99%|▉| 79/80 [00:02<00:00, 35.08it/s, loss=0.969, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:02<00:00, 34.42it/s, loss=0.969, v_num=d4yb, BTC_val\u001b[A\n","Epoch 14:  99%|▉| 79/80 [00:01<00:00, 45.59it/s, loss=0.95, v_num=d4yb, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:01<00:00, 44.50it/s, loss=0.95, v_num=d4yb, BTC_val_\u001b[A\n","Epoch 15:  99%|▉| 79/80 [00:02<00:00, 37.58it/s, loss=0.93, v_num=d4yb, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:02<00:00, 36.80it/s, loss=0.93, v_num=d4yb, BTC_val_\u001b[A\n","Epoch 16:  99%|▉| 79/80 [00:01<00:00, 45.53it/s, loss=1.01, v_num=d4yb, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:01<00:00, 44.31it/s, loss=1.01, v_num=d4yb, BTC_val_\u001b[A\n","Epoch 17:  99%|▉| 79/80 [00:01<00:00, 44.77it/s, loss=0.964, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:01<00:00, 43.75it/s, loss=0.964, v_num=d4yb, BTC_val\u001b[A\n","Epoch 18:  99%|▉| 79/80 [00:02<00:00, 35.99it/s, loss=0.968, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:02<00:00, 35.34it/s, loss=0.968, v_num=d4yb, BTC_val\u001b[A\n","Epoch 19:  99%|▉| 79/80 [00:01<00:00, 40.07it/s, loss=0.934, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:02<00:00, 39.17it/s, loss=0.934, v_num=d4yb, BTC_val\u001b[A\n","                                                                                Metric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 1.046\n","Epoch 20:  99%|▉| 79/80 [00:02<00:00, 39.13it/s, loss=0.984, v_num=d4yb, BTC_val\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:02<00:00, 38.21it/s, loss=0.984, v_num=d4yb, BTC_val\u001b[A\n","Epoch 21:  99%|▉| 79/80 [00:02<00:00, 36.89it/s, loss=0.918, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:02<00:00, 36.25it/s, loss=0.918, v_num=d4yb, BTC_val\u001b[A\n","Epoch 22:  99%|▉| 79/80 [00:02<00:00, 37.17it/s, loss=0.93, v_num=d4yb, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:02<00:00, 36.47it/s, loss=0.93, v_num=d4yb, BTC_val_\u001b[A\n","Epoch 23:  99%|▉| 79/80 [00:01<00:00, 44.19it/s, loss=0.922, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:01<00:00, 43.16it/s, loss=0.922, v_num=d4yb, BTC_val\u001b[A\n","Epoch 24:  99%|▉| 79/80 [00:02<00:00, 34.44it/s, loss=0.938, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:02<00:00, 34.03it/s, loss=0.938, v_num=d4yb, BTC_val\u001b[A\n","Epoch 25:  99%|▉| 79/80 [00:01<00:00, 42.69it/s, loss=0.952, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:01<00:00, 41.75it/s, loss=0.952, v_num=d4yb, BTC_val\u001b[A\n","Epoch 26:  99%|▉| 79/80 [00:02<00:00, 35.10it/s, loss=0.902, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:02<00:00, 34.69it/s, loss=0.902, v_num=d4yb, BTC_val\u001b[A\n","Epoch 27:  99%|▉| 79/80 [00:02<00:00, 36.44it/s, loss=0.926, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 80/80 [00:02<00:00, 35.96it/s, loss=0.926, v_num=d4yb, BTC_val\u001b[A\n","Epoch 28:  99%|▉| 79/80 [00:02<00:00, 34.66it/s, loss=0.908, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 80/80 [00:02<00:00, 34.25it/s, loss=0.908, v_num=d4yb, BTC_val\u001b[A\n","Epoch 29:  99%|▉| 79/80 [00:02<00:00, 38.19it/s, loss=0.946, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 80/80 [00:02<00:00, 37.58it/s, loss=0.946, v_num=d4yb, BTC_val\u001b[A\n","Epoch 30:  99%|▉| 79/80 [00:02<00:00, 37.68it/s, loss=0.927, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 80/80 [00:02<00:00, 36.94it/s, loss=0.927, v_num=d4yb, BTC_val\u001b[A\n","Epoch 31:  99%|▉| 79/80 [00:01<00:00, 44.79it/s, loss=0.916, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 80/80 [00:01<00:00, 43.79it/s, loss=0.916, v_num=d4yb, BTC_val\u001b[A\n","Epoch 32:  99%|▉| 79/80 [00:02<00:00, 34.84it/s, loss=0.915, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 80/80 [00:02<00:00, 34.21it/s, loss=0.915, v_num=d4yb, BTC_val\u001b[A\n","Epoch 33:  99%|▉| 79/80 [00:02<00:00, 37.60it/s, loss=0.902, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 33: 100%|█| 80/80 [00:02<00:00, 36.81it/s, loss=0.902, v_num=d4yb, BTC_val\u001b[A\n","Epoch 34:  99%|▉| 79/80 [00:01<00:00, 42.71it/s, loss=0.931, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 80/80 [00:01<00:00, 41.66it/s, loss=0.931, v_num=d4yb, BTC_val\u001b[A\n","Epoch 35:  99%|▉| 79/80 [00:01<00:00, 39.96it/s, loss=0.912, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 35: 100%|█| 80/80 [00:02<00:00, 39.15it/s, loss=0.912, v_num=d4yb, BTC_val\u001b[A\n","Epoch 36:  99%|▉| 79/80 [00:02<00:00, 38.62it/s, loss=0.9, v_num=d4yb, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 36: 100%|█| 80/80 [00:02<00:00, 37.87it/s, loss=0.9, v_num=d4yb, BTC_val_a\u001b[A\n","Epoch 37:  99%|▉| 79/80 [00:02<00:00, 31.25it/s, loss=0.916, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 37: 100%|█| 80/80 [00:02<00:00, 30.93it/s, loss=0.916, v_num=d4yb, BTC_val\u001b[A\n","Epoch 38:  99%|▉| 79/80 [00:02<00:00, 35.07it/s, loss=0.935, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 38: 100%|█| 80/80 [00:02<00:00, 34.61it/s, loss=0.935, v_num=d4yb, BTC_val\u001b[A\n","Epoch 39:  99%|▉| 79/80 [00:02<00:00, 37.99it/s, loss=0.919, v_num=d4yb, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.046. Signaling Trainer to stop.\n","Epoch 39: 100%|█| 80/80 [00:02<00:00, 37.34it/s, loss=0.919, v_num=d4yb, BTC_val\n","Epoch 39: 100%|█| 80/80 [00:02<00:00, 37.20it/s, loss=0.919, v_num=d4yb, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 76.29it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.4516128897666931,\n"," 'BTC_test_f1': 0.40244993567466736,\n"," 'ETH_test_acc': 0.6451612710952759,\n"," 'ETH_test_f1': 0.6586883068084717,\n"," 'test_loss': 0.8453616499900818}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 73466\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015438-c6dpd4yb/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015438-c6dpd4yb/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.39044\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.50672\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.94113\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 39\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3160\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 92\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621464970\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 143\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.4901\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.46269\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.49089\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.45836\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.92495\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.33333\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.33968\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.33333\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.07729\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.45161\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.40245\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.64516\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.65869\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.84536\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▆▅▆▆▄▆▃▃▅▇▅▆▄▇▄▅▆▆▃▄▅▄▇▆▅▆█▆▅▄▇▅▆█▁█▅▆▅▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▄▅▆▆▅▆▄▁▅▇▄▆▄▆▅▅▆▆▄▃▅▄▇▄▅▆█▆▅▄▆▅▇█▂█▅▅▅▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▄▆▇▃▂▄▂▃▅▆▅▄▄▇▇▃▆▆▇▆▇▁▁▃▄█▂▇▆▆▂▆▆▄▂▁▅█▇▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▄▆▄▃▂▄▂▂▅▆▅▄▄▇▆▂▆▅▆▅▆▁▁▄▄█▂▇▅▆▃▆▆▄▂▂▅█▇▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▆▆▆▆▆▇▆▅▆▄▅▆▁▄▃▂▂▄▄▄▆▄▅▃▁█▂▄▄▃▄▆▃▆▃▃▃▄▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▄▃▄▅▆▆▆▆▇▇▇▇█▇▇▇▇▇▇▇▇█▇█▇▇▇▇▇█▇▇██▇▇█▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▃▃▄▆▆▆▆▇▇█▇▇█▇▇█▇▇▇█▇█▇█▇███▇█▇▇█████▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▂▂▅▅▆▆▆▆▇▆▆▆▇▆▇▇▇▇▇▇▇█▆█▇▇▇▇▇█▇▇█▇▇▇▇█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▁▂▆▅▆▆▆▆▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇█▇▇█▇▇█▇██▇█▇▇█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▇▅▄▄▃▃▃▂▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▆▁▆▃▆▃▃▃▃▆▆▆▃▆█▃▁▆▁▃▆█▃▁▃▃▁▃▆▆▁▃▁▃▁▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▆▁▆▄▆▄▄▄▃▆▆▆▄▆█▄▁▆▂▄▆█▄▂▄▄▁▄▆▆▁▄▁▄▁▄▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▆▃▆▆▃▆█▃█▁▆█▆███▆█▆█▆▆▆▆▆▃▆██▆▆▆▃▆▃▆▃▆▆█\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▆▃▆▆▄▆█▄█▁▆█▆███▆█▆█▆▆▆▆▆▄▆██▆▆▆▄▆▄▆▄▆▆█\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄▄▂▂▄▄▂▂▃▅▃▂▃▅▂▅▅▃▆▁▃▂▅▄▂▅▂▄▄▆▇█▄▄▄▂▄▅█▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_single_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/c6dpd4yb\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:56:19.989003: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_single_lstm_loss_weighted_multi_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2edskyvr\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015618-2edskyvr\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 195   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.8 K    Trainable params\n","0         Non-trainable params\n","75.8 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:   0%|                                           | 0/81 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 80/81 [00:01<00:00, 42.70it/s, loss=1.12, v_num=kyvr, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.161\n","Epoch 0: 100%|█| 81/81 [00:01<00:00, 41.41it/s, loss=1.12, v_num=kyvr, BTC_val_a\n","Epoch 1:  99%|▉| 80/81 [00:01<00:00, 45.85it/s, loss=1.11, v_num=kyvr, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.059 >= min_delta = 0.003. New best score: 1.102\n","Epoch 1: 100%|█| 81/81 [00:01<00:00, 44.69it/s, loss=1.11, v_num=kyvr, BTC_val_a\n","Epoch 2:  99%|▉| 80/81 [00:01<00:00, 40.21it/s, loss=1.1, v_num=kyvr, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 81/81 [00:02<00:00, 39.09it/s, loss=1.1, v_num=kyvr, BTC_val_ac\u001b[A\n","Epoch 3:  99%|▉| 80/81 [00:01<00:00, 43.91it/s, loss=1.09, v_num=kyvr, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:01<00:00, 42.79it/s, loss=1.09, v_num=kyvr, BTC_val_a\u001b[A\n","Epoch 4:  99%|▉| 80/81 [00:01<00:00, 45.70it/s, loss=1.11, v_num=kyvr, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:01<00:00, 44.47it/s, loss=1.11, v_num=kyvr, BTC_val_a\u001b[A\n","Epoch 5:  99%|▉| 80/81 [00:01<00:00, 43.78it/s, loss=1.1, v_num=kyvr, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:01<00:00, 42.71it/s, loss=1.1, v_num=kyvr, BTC_val_ac\u001b[A\n","Epoch 6:  99%|▉| 80/81 [00:01<00:00, 42.88it/s, loss=1.1, v_num=kyvr, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:01<00:00, 41.91it/s, loss=1.1, v_num=kyvr, BTC_val_ac\u001b[A\n","Epoch 7:  99%|▉| 80/81 [00:02<00:00, 37.75it/s, loss=1.1, v_num=kyvr, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:02<00:00, 37.05it/s, loss=1.1, v_num=kyvr, BTC_val_ac\u001b[A\n","Epoch 8:  99%|▉| 80/81 [00:01<00:00, 43.73it/s, loss=1.1, v_num=kyvr, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:01<00:00, 42.73it/s, loss=1.1, v_num=kyvr, BTC_val_ac\u001b[A\n","Epoch 9:  99%|▉| 80/81 [00:01<00:00, 44.45it/s, loss=1.11, v_num=kyvr, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:01<00:00, 43.62it/s, loss=1.11, v_num=kyvr, BTC_val_a\u001b[A\n","Epoch 10:  99%|▉| 80/81 [00:01<00:00, 44.64it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:01<00:00, 43.84it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n","Epoch 11:  99%|▉| 80/81 [00:01<00:00, 44.52it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:01<00:00, 43.65it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n","Epoch 12:  99%|▉| 80/81 [00:02<00:00, 37.84it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:02<00:00, 37.32it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n","Epoch 13:  99%|▉| 80/81 [00:01<00:00, 43.99it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:01<00:00, 43.23it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n","Epoch 14:  99%|▉| 80/81 [00:01<00:00, 43.31it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:01<00:00, 42.56it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n","Epoch 15:  99%|▉| 80/81 [00:02<00:00, 37.49it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:02<00:00, 36.88it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n","Epoch 16:  99%|▉| 80/81 [00:02<00:00, 35.26it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:02<00:00, 34.67it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n","Epoch 17:  99%|▉| 80/81 [00:01<00:00, 42.14it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:01<00:00, 41.20it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n","Epoch 18:  99%|▉| 80/81 [00:01<00:00, 44.62it/s, loss=1.09, v_num=kyvr, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:01<00:00, 43.64it/s, loss=1.09, v_num=kyvr, BTC_val_\u001b[A\n","Epoch 19:  99%|▉| 80/81 [00:01<00:00, 44.05it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:01<00:00, 43.25it/s, loss=1.1, v_num=kyvr, BTC_val_a\u001b[A\n","Epoch 20:  99%|▉| 80/81 [00:02<00:00, 34.87it/s, loss=1.11, v_num=kyvr, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:02<00:00, 34.33it/s, loss=1.11, v_num=kyvr, BTC_val_\u001b[A\n","Epoch 21:  99%|▉| 80/81 [00:02<00:00, 36.31it/s, loss=1.09, v_num=kyvr, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.102. Signaling Trainer to stop.\n","Epoch 21: 100%|█| 81/81 [00:02<00:00, 35.63it/s, loss=1.09, v_num=kyvr, BTC_val_\n","Epoch 21: 100%|█| 81/81 [00:02<00:00, 35.57it/s, loss=1.09, v_num=kyvr, BTC_val_\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 57.06it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.25806450843811035,\n"," 'BTC_test_f1': 0.1367289274930954,\n"," 'ETH_test_acc': 0.4193548262119293,\n"," 'ETH_test_f1': 0.19648092985153198,\n"," 'test_loss': 1.1303094625473022}\n","--------------------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 73739\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015618-2edskyvr/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015618-2edskyvr/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.4375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.46801\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.3125\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.26573\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.07848\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 21\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1760\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 53\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621465031\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 79\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.36013\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.32367\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.34831\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.30734\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09644\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.55556\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.33333\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.14163\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.25806\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.13673\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.41935\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19648\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.13031\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▄▅▇▂▇▅▅▅▇▅▅▄▅▇▇▃▇▄▆▆▆▄▄█▄▅▅█▆▂▆▁▇▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▄▄▇▂▆▅▄▄▅▃▄▃▄▅▆▃▆▂▅▄▅▄▃█▄▄▃█▄▂▆▁▆▄▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▅▅▅▃▅▅▅▁▆▇▃▅▆▅▆▁▃▅▇▂▅▃▄▁▅▃▅▅▆▇▆█▆█▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▅▄▅▃▅▅▅▁▅▅▃▅▆▄▅▁▃▄▆▂▄▃▃▁▅▃▄▅▄▇▆█▅▇▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▃▁▂▄▃▄▂▇▂▂▄▃▃▂▃█▃▄▁▄▁▃▂▄▂▃▃▂▂▃▂▃▃▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▄▅▁▅▆▃█▃▅▄▄▅▄▅▄▆▇▆▅▇▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▄█▁█▇▂█▅▇▇▇▇▃▅▆▆▇█▅▆▆█\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▂▄▁▇▆█▆▃▄▃▆█▆▃▄▃█▅▅█▄▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▂▄▁▇▆█▃▅▅▄██▅▃▅▃▇▆▄▃▃▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▄▂▂▂▁▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▆▆▆▆▆▆▆▆▆▁▁▆▁▁█▆▆▁▆▆██\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▃▃▃▃▃▃▃▃▃▁▁▃▁▁█▃▃▁▃▃██\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▇▇▇▇▇▁▇▂▇▁▁▇▁▂█▇▇▁▇▅▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▄▄▄▄▄▁▄▃▄▁▁▄▁▃█▄▄▁▄▃▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▁▆▃▃▃▃▃▃▅▃▅▅█▂▄▄▄▄▃▃▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_single_lstm_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2edskyvr\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 01:57:20.915505: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1prx6act\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015719-1prx6act\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 130   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 79/80 [00:03<00:00, 21.45it/s, loss=0.664, v_num=6act, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.680\n","Epoch 0: 100%|█| 80/80 [00:03<00:00, 21.37it/s, loss=0.664, v_num=6act, BTC_val_\n","Epoch 1:  99%|▉| 79/80 [00:04<00:00, 19.57it/s, loss=0.606, v_num=6act, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.153 >= min_delta = 0.003. New best score: 0.527\n","Epoch 1: 100%|█| 80/80 [00:04<00:00, 19.49it/s, loss=0.606, v_num=6act, BTC_val_\n","Epoch 2:  99%|▉| 79/80 [00:04<00:00, 17.75it/s, loss=0.607, v_num=6act, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 80/80 [00:04<00:00, 17.71it/s, loss=0.607, v_num=6act, BTC_val_\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.520\n","Epoch 3:  99%|▉| 79/80 [00:04<00:00, 18.13it/s, loss=0.593, v_num=6act, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:04<00:00, 18.04it/s, loss=0.593, v_num=6act, BTC_val_\u001b[A\n","Epoch 4:  99%|▉| 79/80 [00:03<00:00, 20.94it/s, loss=0.584, v_num=6act, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:03<00:00, 20.79it/s, loss=0.584, v_num=6act, BTC_val_\u001b[A\n","Epoch 5:  99%|▉| 79/80 [00:03<00:00, 20.13it/s, loss=0.608, v_num=6act, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.515\n","Epoch 5: 100%|█| 80/80 [00:03<00:00, 20.00it/s, loss=0.608, v_num=6act, BTC_val_\n","Epoch 6:  99%|▉| 79/80 [00:03<00:00, 20.41it/s, loss=0.602, v_num=6act, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:03<00:00, 20.27it/s, loss=0.602, v_num=6act, BTC_val_\u001b[A\n","Epoch 7:  99%|▉| 79/80 [00:04<00:00, 18.92it/s, loss=0.561, v_num=6act, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.508\n","Epoch 7: 100%|█| 80/80 [00:04<00:00, 18.80it/s, loss=0.561, v_num=6act, BTC_val_\n","Epoch 8:  99%|▉| 79/80 [00:04<00:00, 18.98it/s, loss=0.563, v_num=6act, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:04<00:00, 18.87it/s, loss=0.563, v_num=6act, BTC_val_\u001b[A\n","Epoch 9:  99%|▉| 79/80 [00:04<00:00, 18.87it/s, loss=0.585, v_num=6act, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:04<00:00, 18.74it/s, loss=0.585, v_num=6act, BTC_val_\u001b[A\n","Epoch 10:  99%|▉| 79/80 [00:05<00:00, 15.29it/s, loss=0.551, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:05<00:00, 15.26it/s, loss=0.551, v_num=6act, BTC_val\u001b[A\n","Epoch 11:  99%|▉| 79/80 [00:04<00:00, 18.63it/s, loss=0.589, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:04<00:00, 18.53it/s, loss=0.589, v_num=6act, BTC_val\u001b[A\n","Epoch 12:  99%|▉| 79/80 [00:03<00:00, 21.18it/s, loss=0.547, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:03<00:00, 21.02it/s, loss=0.547, v_num=6act, BTC_val\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 0.495\n","Epoch 13:  99%|▉| 79/80 [00:04<00:00, 15.87it/s, loss=0.55, v_num=6act, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.030 >= min_delta = 0.003. New best score: 0.465\n","Epoch 13: 100%|█| 80/80 [00:05<00:00, 15.84it/s, loss=0.55, v_num=6act, BTC_val_\n","Epoch 14:  99%|▉| 79/80 [00:04<00:00, 18.84it/s, loss=0.574, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:04<00:00, 18.78it/s, loss=0.574, v_num=6act, BTC_val\u001b[A\n","Epoch 15:  99%|▉| 79/80 [00:03<00:00, 21.02it/s, loss=0.564, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:03<00:00, 20.91it/s, loss=0.564, v_num=6act, BTC_val\u001b[A\n","Epoch 16:  99%|▉| 79/80 [00:04<00:00, 19.40it/s, loss=0.616, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:04<00:00, 19.34it/s, loss=0.616, v_num=6act, BTC_val\u001b[A\n","Epoch 17:  99%|▉| 79/80 [00:04<00:00, 18.02it/s, loss=0.587, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:04<00:00, 17.93it/s, loss=0.587, v_num=6act, BTC_val\u001b[A\n","Epoch 18:  99%|▉| 79/80 [00:04<00:00, 16.17it/s, loss=0.565, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:04<00:00, 16.15it/s, loss=0.565, v_num=6act, BTC_val\u001b[A\n","Epoch 19:  99%|▉| 79/80 [00:04<00:00, 17.01it/s, loss=0.543, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:04<00:00, 17.00it/s, loss=0.543, v_num=6act, BTC_val\u001b[A\n","Epoch 20:  99%|▉| 79/80 [00:04<00:00, 18.63it/s, loss=0.55, v_num=6act, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:04<00:00, 18.52it/s, loss=0.55, v_num=6act, BTC_val_\u001b[A\n","Epoch 21:  99%|▉| 79/80 [00:04<00:00, 17.39it/s, loss=0.56, v_num=6act, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:04<00:00, 17.33it/s, loss=0.56, v_num=6act, BTC_val_\u001b[A\n","Epoch 22:  99%|▉| 79/80 [00:04<00:00, 18.09it/s, loss=0.537, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:04<00:00, 18.05it/s, loss=0.537, v_num=6act, BTC_val\u001b[A\n","Epoch 23:  99%|▉| 79/80 [00:04<00:00, 17.99it/s, loss=0.574, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:04<00:00, 17.95it/s, loss=0.574, v_num=6act, BTC_val\u001b[A\n","Epoch 24:  99%|▉| 79/80 [00:05<00:00, 15.10it/s, loss=0.561, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:05<00:00, 15.08it/s, loss=0.561, v_num=6act, BTC_val\u001b[A\n","Epoch 25:  99%|▉| 79/80 [00:04<00:00, 17.62it/s, loss=0.563, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:04<00:00, 17.58it/s, loss=0.563, v_num=6act, BTC_val\u001b[A\n","Epoch 26:  99%|▉| 79/80 [00:04<00:00, 18.11it/s, loss=0.549, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:04<00:00, 18.06it/s, loss=0.549, v_num=6act, BTC_val\u001b[A\n","Epoch 27:  99%|▉| 79/80 [00:04<00:00, 17.02it/s, loss=0.549, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 80/80 [00:04<00:00, 16.97it/s, loss=0.549, v_num=6act, BTC_val\u001b[A\n","Epoch 28:  99%|▉| 79/80 [00:04<00:00, 16.67it/s, loss=0.559, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 80/80 [00:04<00:00, 16.64it/s, loss=0.559, v_num=6act, BTC_val\u001b[A\n","Epoch 29:  99%|▉| 79/80 [00:04<00:00, 17.65it/s, loss=0.534, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 80/80 [00:04<00:00, 17.60it/s, loss=0.534, v_num=6act, BTC_val\u001b[A\n","Epoch 30:  99%|▉| 79/80 [00:04<00:00, 18.01it/s, loss=0.547, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 80/80 [00:04<00:00, 17.93it/s, loss=0.547, v_num=6act, BTC_val\u001b[A\n","Epoch 31:  99%|▉| 79/80 [00:04<00:00, 16.83it/s, loss=0.551, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 80/80 [00:04<00:00, 16.81it/s, loss=0.551, v_num=6act, BTC_val\u001b[A\n","Epoch 32:  99%|▉| 79/80 [00:04<00:00, 17.78it/s, loss=0.526, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 80/80 [00:04<00:00, 17.67it/s, loss=0.526, v_num=6act, BTC_val\u001b[A\n","Epoch 33:  99%|▉| 79/80 [00:04<00:00, 18.58it/s, loss=0.529, v_num=6act, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.465. Signaling Trainer to stop.\n","Epoch 33: 100%|█| 80/80 [00:04<00:00, 18.47it/s, loss=0.529, v_num=6act, BTC_val\n","Epoch 33: 100%|█| 80/80 [00:04<00:00, 18.44it/s, loss=0.529, v_num=6act, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 37.91it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.6451612710952759,\n"," 'BTC_test_f1': 0.6390261650085449,\n"," 'ETH_test_acc': 0.6774193644523621,\n"," 'ETH_test_f1': 0.6768433451652527,\n"," 'test_loss': 0.6043744683265686}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 73918\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015719-1prx6act/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_015719-1prx6act/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.6875\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.67611\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.875\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.87302\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.52106\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 33\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2686\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 159\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621465198\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 121\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.73317\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.72289\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.73317\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.71942\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.5358\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.77778\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.775\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.48338\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.64516\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.63903\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.67742\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.67684\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.60437\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▂▄▆▆▂▂▇▅▃▅▅▃▂▃▃▅▅▆▇▃▃▄▅▅▆▇▄▅█▃▁▄▅▆▇▅▅▇▃▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▂▄▆▆▂▂▇▅▃▄▆▂▂▃▂▅▅▆▇▃▃▄▅▆▆▇▄▅█▂▁▄▅▆▇▅▅▆▃▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▇▆▆▂▅▆▅▅▆▃▄▁▄▄▅▂▅▆▄▅▃▇▅▅▆▄▆█▃▃▂▅▇▄▃▅▄▂▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▂▇▆▆▃▆▆▅▆▆▄▄▂▅▁▅▃▆▅▄▆▄▇▆▅▆▄▆█▄▄▂▆▇▅▄▆▄▃▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▅▅▄█▅▃▅▆▄▅▇▆▇▆▅▆▄▃▆▅▆▃▃▄▂▅▃▁▆▅▇▃▁▄▅▄▄▇▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▆▇▇▇▇▇▇▇▇█▇▇▇▇▇▇██▇▇▇█▇████▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▅▇▇▇▆▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇█▇▇███▇▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▅▆▆▆▇▇▇█▇▇▇▇▇▇█▇▇▇▇▇▇██▇▇█▇▇███▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▆▇▆▇▇▇▇█▇▇▇▇▇▇█▇▇▇▇▇▇██▇█▇▇▇███▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▄▄▃▃▃▃▃▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁███▅███▅██▅██████▅████▅██▅██▅█▅▅█\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁███▆███▆██▆██████▆████▆██▆██▆█▆▆█\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▃▁▁▁█▁▆▁▆▆▁▆█▁▁▆▁█▆▁▃██▃▆█▃▁█▁█▆▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▄▁▁▁█▁▆▁▆▆▁▆█▁▁▆▁█▆▁▄██▄▆█▄▁█▁█▆▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▃▃▄▃▃▆▂▄▂▃▅▂▁▃▃▂▃▂▂▃▂▃▂▃▂▃▂▃▁▄▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1prx6act\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:00:18.030344: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_loss_weighted_binary_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2bqw3y4i\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020016-2bqw3y4i\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 130   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 80/81 [00:03<00:00, 22.72it/s, loss=0.713, v_num=3y4i, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.688\n","Epoch 0: 100%|█| 81/81 [00:03<00:00, 22.48it/s, loss=0.713, v_num=3y4i, BTC_val_\n","Epoch 1: 100%|█| 81/81 [00:03<00:00, 22.03it/s, loss=0.703, v_num=3y4i, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 81/81 [00:03<00:00, 21.90it/s, loss=0.703, v_num=3y4i, BTC_val_\u001b[A\n","Epoch 2: 100%|█| 81/81 [00:04<00:00, 20.13it/s, loss=0.696, v_num=3y4i, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 81/81 [00:04<00:00, 20.03it/s, loss=0.696, v_num=3y4i, BTC_val_\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:03<00:00, 21.61it/s, loss=0.691, v_num=3y4i, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:03<00:00, 21.50it/s, loss=0.691, v_num=3y4i, BTC_val_\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.681\n","Epoch 4: 100%|█| 81/81 [00:04<00:00, 19.28it/s, loss=0.696, v_num=3y4i, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:04<00:00, 19.19it/s, loss=0.696, v_num=3y4i, BTC_val_\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:03<00:00, 21.03it/s, loss=0.7, v_num=3y4i, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:03<00:00, 20.92it/s, loss=0.7, v_num=3y4i, BTC_val_ac\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:04<00:00, 16.66it/s, loss=0.695, v_num=3y4i, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:04<00:00, 16.57it/s, loss=0.695, v_num=3y4i, BTC_val_\u001b[A\n","Metric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.676\n","Epoch 7: 100%|█| 81/81 [00:04<00:00, 20.22it/s, loss=0.693, v_num=3y4i, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:04<00:00, 20.10it/s, loss=0.693, v_num=3y4i, BTC_val_\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:04<00:00, 20.05it/s, loss=0.697, v_num=3y4i, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:04<00:00, 19.93it/s, loss=0.697, v_num=3y4i, BTC_val_\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:04<00:00, 19.47it/s, loss=0.698, v_num=3y4i, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:04<00:00, 19.36it/s, loss=0.698, v_num=3y4i, BTC_val_\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:04<00:00, 19.95it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:04<00:00, 19.86it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:04<00:00, 19.42it/s, loss=0.696, v_num=3y4i, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:04<00:00, 19.21it/s, loss=0.696, v_num=3y4i, BTC_val\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:04<00:00, 19.12it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:04<00:00, 19.02it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:03<00:00, 20.53it/s, loss=0.697, v_num=3y4i, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:03<00:00, 20.43it/s, loss=0.697, v_num=3y4i, BTC_val\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:04<00:00, 18.46it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:04<00:00, 18.37it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:04<00:00, 18.61it/s, loss=0.693, v_num=3y4i, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:04<00:00, 18.49it/s, loss=0.693, v_num=3y4i, BTC_val\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:05<00:00, 14.80it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:05<00:00, 14.75it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:04<00:00, 16.90it/s, loss=0.692, v_num=3y4i, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:05<00:00, 15.80it/s, loss=0.692, v_num=3y4i, BTC_val\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:05<00:00, 16.16it/s, loss=0.693, v_num=3y4i, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:05<00:00, 16.09it/s, loss=0.693, v_num=3y4i, BTC_val\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:04<00:00, 17.02it/s, loss=0.693, v_num=3y4i, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:04<00:00, 16.94it/s, loss=0.693, v_num=3y4i, BTC_val\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:04<00:00, 18.65it/s, loss=0.696, v_num=3y4i, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:04<00:00, 18.56it/s, loss=0.696, v_num=3y4i, BTC_val\u001b[A\n","Epoch 21: 100%|█| 81/81 [00:04<00:00, 18.75it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 81/81 [00:04<00:00, 18.66it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n","Epoch 22: 100%|█| 81/81 [00:04<00:00, 19.42it/s, loss=0.691, v_num=3y4i, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 81/81 [00:04<00:00, 19.33it/s, loss=0.691, v_num=3y4i, BTC_val\u001b[A\n","Epoch 23: 100%|█| 81/81 [00:04<00:00, 17.49it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 81/81 [00:04<00:00, 17.41it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n","Epoch 24: 100%|█| 81/81 [00:04<00:00, 17.67it/s, loss=0.692, v_num=3y4i, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 81/81 [00:04<00:00, 17.55it/s, loss=0.692, v_num=3y4i, BTC_val\u001b[A\n","Epoch 25: 100%|█| 81/81 [00:04<00:00, 18.99it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 81/81 [00:04<00:00, 18.90it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n","Epoch 26: 100%|█| 81/81 [00:04<00:00, 17.13it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.676. Signaling Trainer to stop.\n","Epoch 26: 100%|█| 81/81 [00:04<00:00, 17.04it/s, loss=0.694, v_num=3y4i, BTC_val\n","Epoch 26: 100%|█| 81/81 [00:04<00:00, 17.02it/s, loss=0.694, v_num=3y4i, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 38.63it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.4193548262119293,\n"," 'BTC_test_f1': 0.29533159732818604,\n"," 'ETH_test_acc': 0.6129032373428345,\n"," 'ETH_test_f1': 0.3793548345565796,\n"," 'test_loss': 0.6955219507217407}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 74356\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020016-2bqw3y4i/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020016-2bqw3y4i/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.51515\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.25\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.69733\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 26\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2160\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 126\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621465342\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 97\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.52955\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.41539\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.49961\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.38942\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69413\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.55556\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.35714\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.88889\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.47059\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.68941\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.41935\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.29533\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.37935\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.69552\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▅▄▅▅▃▃▄▃▆▃▄▆▃▅█▁▄▄▃▂▃▄█▄▅▃▃▂▄▆▃▃▃▅▄▄▃▆▃▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▄▄▅▃▂▃▄▃▆▂▃▆▃▅█▁▄▄▂▂▃▄█▄▅▃▃▂▃▃▂▃▃▅▄▄▃▆▂▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▄▄▄▂▅▅▃▁▂▇▄▄▅▇▃▂▄▄▄▃▃▅▁▄▄▇▄▄▇█▄▇▃▅▄▇▃▆▄▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▄▃▅▃▃▆▃▁▂▆▅▅▅█▄▂▅▄▄▃▃▅▁▅▄▇▅▅▇█▅▇▃▅▄▇▃▃▅▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▄▆▄▅▇▄▇█▅▄▆▄▆▃▁█▆▅▇▆▆▅▆▅▄▅▆▅▄▃▆▅▅▅▆▅▅▃▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▄▅▇▄▂▂▅▇▁▄█▁█▅▃▄▄▅▁▇▄▅▂▄▂▁▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▅▆█▅▅▄▁▄▄▆▇▅▄█▆▆▅▇▅▃▄▇▅▅▃▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▆▃▇▅▆▅▄▇█▅▁█▆▂▅█▇▆▃▆█▄▇▇▄▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▆▄▇▅▇▆▂▄█▇▃█▄▅▆█▇▇▆▃▇▆▇▆▃▅▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▂▃▃▂▂▂▂▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc █████████████████▁███▁▁████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 █████████████████▁███▁▁████\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ██▇██████████████▁███▁▁████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ██▇██████████████▁███▁▁████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▆▇▃▆▃▁▆▅▄▅▅▆▆▆▆▆█▇▇▇██▇▇▇▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2bqw3y4i\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:02:41.235833: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/11yjrbef\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020239-11yjrbef\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 195   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:   0%|                                           | 0/80 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 0:  99%|▉| 79/80 [00:03<00:00, 21.45it/s, loss=1.11, v_num=rbef, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.098\n","Epoch 0: 100%|█| 80/80 [00:03<00:00, 21.36it/s, loss=1.11, v_num=rbef, BTC_val_a\n","Epoch 1:  99%|▉| 79/80 [00:03<00:00, 20.09it/s, loss=1.11, v_num=rbef, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 80/80 [00:03<00:00, 20.03it/s, loss=1.11, v_num=rbef, BTC_val_a\u001b[A\n","Epoch 2:  99%|▉| 79/80 [00:03<00:00, 21.84it/s, loss=1.11, v_num=rbef, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 80/80 [00:03<00:00, 21.75it/s, loss=1.11, v_num=rbef, BTC_val_a\u001b[A\n","Epoch 3:  99%|▉| 79/80 [00:04<00:00, 17.64it/s, loss=1.11, v_num=rbef, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:04<00:00, 17.60it/s, loss=1.11, v_num=rbef, BTC_val_a\u001b[A\n","Epoch 4:  99%|▉| 79/80 [00:04<00:00, 18.58it/s, loss=1.1, v_num=rbef, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:04<00:00, 18.44it/s, loss=1.1, v_num=rbef, BTC_val_ac\u001b[A\n","Epoch 5:  99%|▉| 79/80 [00:04<00:00, 16.59it/s, loss=1.1, v_num=rbef, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 1.095\n","Epoch 5: 100%|█| 80/80 [00:04<00:00, 16.53it/s, loss=1.1, v_num=rbef, BTC_val_ac\n","Epoch 6:  99%|▉| 79/80 [00:03<00:00, 20.49it/s, loss=1.11, v_num=rbef, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:03<00:00, 20.33it/s, loss=1.11, v_num=rbef, BTC_val_a\u001b[A\n","Epoch 7:  99%|▉| 79/80 [00:05<00:00, 14.70it/s, loss=1.1, v_num=rbef, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:05<00:00, 14.68it/s, loss=1.1, v_num=rbef, BTC_val_ac\u001b[A\n","Epoch 8:  99%|▉| 79/80 [00:04<00:00, 18.48it/s, loss=1.1, v_num=rbef, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:04<00:00, 18.40it/s, loss=1.1, v_num=rbef, BTC_val_ac\u001b[A\n","Epoch 9:  99%|▉| 79/80 [00:04<00:00, 18.13it/s, loss=1.1, v_num=rbef, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:04<00:00, 18.08it/s, loss=1.1, v_num=rbef, BTC_val_ac\u001b[A\n","Epoch 10:  99%|▉| 79/80 [00:05<00:00, 14.90it/s, loss=1.11, v_num=rbef, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:05<00:00, 14.87it/s, loss=1.11, v_num=rbef, BTC_val_\u001b[A\n","Epoch 11:  99%|▉| 79/80 [00:05<00:00, 15.18it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:05<00:00, 15.17it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Epoch 12:  99%|▉| 79/80 [00:05<00:00, 15.45it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:05<00:00, 15.42it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Epoch 13:  99%|▉| 79/80 [00:04<00:00, 15.83it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 1.092\n","Epoch 13: 100%|█| 80/80 [00:05<00:00, 15.83it/s, loss=1.1, v_num=rbef, BTC_val_a\n","Epoch 14:  99%|▉| 79/80 [00:05<00:00, 15.08it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:05<00:00, 15.08it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Epoch 15:  99%|▉| 79/80 [00:04<00:00, 17.56it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:04<00:00, 17.52it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Epoch 16:  99%|▉| 79/80 [00:04<00:00, 19.13it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:04<00:00, 19.03it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Epoch 17:  99%|▉| 79/80 [00:04<00:00, 17.62it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:04<00:00, 17.56it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Epoch 18:  99%|▉| 79/80 [00:03<00:00, 20.84it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:03<00:00, 20.71it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Epoch 19:  99%|▉| 79/80 [00:04<00:00, 15.94it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:05<00:00, 15.92it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Epoch 20:  99%|▉| 79/80 [00:04<00:00, 19.16it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:04<00:00, 19.09it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Epoch 21:  99%|▉| 79/80 [00:04<00:00, 17.32it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:04<00:00, 17.24it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Epoch 22:  99%|▉| 79/80 [00:04<00:00, 18.37it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:04<00:00, 18.28it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Epoch 23:  99%|▉| 79/80 [00:04<00:00, 17.27it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:04<00:00, 17.17it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Epoch 24:  99%|▉| 79/80 [00:04<00:00, 18.41it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:04<00:00, 18.34it/s, loss=1.1, v_num=rbef, BTC_val_a\u001b[A\n","Epoch 25:  99%|▉| 79/80 [00:04<00:00, 18.54it/s, loss=1.09, v_num=rbef, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:04<00:00, 18.27it/s, loss=1.09, v_num=rbef, BTC_val_\u001b[A\n","Epoch 26:  99%|▉| 79/80 [00:04<00:00, 17.84it/s, loss=1.07, v_num=rbef, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:04<00:00, 17.76it/s, loss=1.07, v_num=rbef, BTC_val_\u001b[A\n","Epoch 27:  99%|▉| 79/80 [00:03<00:00, 20.87it/s, loss=1.02, v_num=rbef, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 1.071\n","Epoch 27: 100%|█| 80/80 [00:03<00:00, 20.71it/s, loss=1.02, v_num=rbef, BTC_val_\n","Epoch 28:  99%|▉| 79/80 [00:03<00:00, 20.06it/s, loss=1.03, v_num=rbef, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 80/80 [00:04<00:00, 19.86it/s, loss=1.03, v_num=rbef, BTC_val_\u001b[A\n","Epoch 29:  99%|▉| 79/80 [00:03<00:00, 20.66it/s, loss=1.02, v_num=rbef, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 80/80 [00:03<00:00, 20.47it/s, loss=1.02, v_num=rbef, BTC_val_\u001b[A\n","Epoch 30:  99%|▉| 79/80 [00:03<00:00, 20.92it/s, loss=1, v_num=rbef, BTC_val_acc\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 80/80 [00:03<00:00, 20.74it/s, loss=1, v_num=rbef, BTC_val_acc\u001b[A\n","Epoch 31:  99%|▉| 79/80 [00:04<00:00, 19.23it/s, loss=0.976, v_num=rbef, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 80/80 [00:04<00:00, 19.11it/s, loss=0.976, v_num=rbef, BTC_val\u001b[A\n","Epoch 32:  99%|▉| 79/80 [00:04<00:00, 17.54it/s, loss=0.965, v_num=rbef, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 80/80 [00:04<00:00, 17.48it/s, loss=0.965, v_num=rbef, BTC_val\u001b[A\n","Epoch 33:  99%|▉| 79/80 [00:03<00:00, 20.26it/s, loss=0.986, v_num=rbef, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 33: 100%|█| 80/80 [00:03<00:00, 20.16it/s, loss=0.986, v_num=rbef, BTC_val\u001b[A\n","Epoch 34:  99%|▉| 79/80 [00:04<00:00, 17.89it/s, loss=0.983, v_num=rbef, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 80/80 [00:04<00:00, 17.73it/s, loss=0.983, v_num=rbef, BTC_val\u001b[A\n","Epoch 35:  99%|▉| 79/80 [00:05<00:00, 14.54it/s, loss=0.987, v_num=rbef, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 35: 100%|█| 80/80 [00:05<00:00, 14.54it/s, loss=0.987, v_num=rbef, BTC_val\u001b[A\n","Epoch 36:  99%|▉| 79/80 [00:04<00:00, 16.50it/s, loss=0.98, v_num=rbef, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 36: 100%|█| 80/80 [00:04<00:00, 16.46it/s, loss=0.98, v_num=rbef, BTC_val_\u001b[A\n","Epoch 37:  99%|▉| 79/80 [00:03<00:00, 21.18it/s, loss=0.973, v_num=rbef, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 37: 100%|█| 80/80 [00:03<00:00, 21.08it/s, loss=0.973, v_num=rbef, BTC_val\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 38:  99%|▉| 79/80 [00:04<00:00, 18.79it/s, loss=1.01, v_num=rbef, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 38: 100%|█| 80/80 [00:04<00:00, 18.69it/s, loss=1.01, v_num=rbef, BTC_val_\u001b[A\n","Epoch 39:  99%|▉| 79/80 [00:03<00:00, 19.91it/s, loss=0.933, v_num=rbef, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n","Epoch 39: 100%|█| 80/80 [00:04<00:00, 18.10it/s, loss=0.933, v_num=rbef, BTC_val\u001b[A\n","Epoch 40:  99%|▉| 79/80 [00:04<00:00, 18.03it/s, loss=0.971, v_num=rbef, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 40: 100%|█| 80/80 [00:04<00:00, 17.94it/s, loss=0.971, v_num=rbef, BTC_val\u001b[A\n","Epoch 41:  99%|▉| 79/80 [00:04<00:00, 18.41it/s, loss=0.928, v_num=rbef, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 41: 100%|█| 80/80 [00:04<00:00, 18.22it/s, loss=0.928, v_num=rbef, BTC_val\u001b[A\n","Epoch 42:  99%|▉| 79/80 [00:04<00:00, 17.85it/s, loss=0.947, v_num=rbef, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 42: 100%|█| 80/80 [00:04<00:00, 17.82it/s, loss=0.947, v_num=rbef, BTC_val\u001b[A\n","Epoch 43:  99%|▉| 79/80 [00:04<00:00, 16.35it/s, loss=0.906, v_num=rbef, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 43: 100%|█| 80/80 [00:04<00:00, 16.29it/s, loss=0.906, v_num=rbef, BTC_val\u001b[A\n","Epoch 44:  99%|▉| 79/80 [00:05<00:00, 15.73it/s, loss=0.922, v_num=rbef, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 44: 100%|█| 80/80 [00:05<00:00, 15.71it/s, loss=0.922, v_num=rbef, BTC_val\u001b[A\n","Epoch 45:  99%|▉| 79/80 [00:04<00:00, 17.13it/s, loss=0.877, v_num=rbef, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 45: 100%|█| 80/80 [00:04<00:00, 17.06it/s, loss=0.877, v_num=rbef, BTC_val\u001b[A\n","Epoch 46:  99%|▉| 79/80 [00:04<00:00, 17.45it/s, loss=0.991, v_num=rbef, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 46: 100%|█| 80/80 [00:04<00:00, 17.39it/s, loss=0.991, v_num=rbef, BTC_val\u001b[A\n","Epoch 47:  99%|▉| 79/80 [00:04<00:00, 17.37it/s, loss=0.914, v_num=rbef, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.071. Signaling Trainer to stop.\n","Epoch 47: 100%|█| 80/80 [00:04<00:00, 17.33it/s, loss=0.914, v_num=rbef, BTC_val\n","Epoch 47: 100%|█| 80/80 [00:04<00:00, 17.31it/s, loss=0.914, v_num=rbef, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 49.09it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.35483869910240173,\n"," 'BTC_test_f1': 0.27419355511665344,\n"," 'ETH_test_acc': 0.4516128897666931,\n"," 'ETH_test_f1': 0.41279301047325134,\n"," 'test_loss': 0.935441792011261}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 74693\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020239-11yjrbef/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020239-11yjrbef/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.4375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.39048\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.47381\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.89546\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 47\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3792\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 225\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621465584\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 171\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.46397\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.43822\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.50515\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.48137\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.90426\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.33333\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.33333\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.22222\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.19048\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.37408\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.35484\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.27419\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.45161\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.41279\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.93544\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▆▆▃▆▆▆▅▄█▅▃▆▃▄▃▅▃▅▆▄▄▁▅▃▂▅▄▃▃▄▂▄▄▅▄▅▇█▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▄▄▃▆▄▇▅▃█▅▃▄▂▄▃▄▃▃▄▃▃▁▅▄▂▅▄▄▂▅▃▄▄▄▄▅▇▇▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▇▅▄▅▄▅▅▄▂▅▃▄▃▅▁▁▄▆▆▄▄▃▂▅▃▅▅▃▆▅▄▅█▄▇▅▇▄▂▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▇▅▄▅▃▅▃▂▂▄▃▂▂▄▁▁▃▄▅▃▃▂▂▄▃▅▅▃▅▆▄▅█▃▇▅▅▃▂▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▆▇██▇████████████▇████▆▇▅▇▃▆▂▅▅▄▆▄▄▁▆▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▃▁▃▃▃▃▂▃▃▃▃▄▃▃▂▄▄▅▆▆▄▃▄▃▅▅▅▆▅▆▆▇▇█████▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▁▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▂▂▂▄▄▅▅▅▆▅▆▆▇▇██████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▂▁▁▂▂▃▃▃▃▃▃▃▃▂▃▂▂▃▃▅▅▂▂▃▄▅▅▅▄▅▆▆▇▇▇▇██▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▂▂▂▂▃▃▂▃▃▃▃▃▂▂▂▂▁▂▂▂▂▁▂▄▄▅▅▅▅▅▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ███████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▅▃▄▃▃▃▃▃▃▂▂▂▂▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▆▆▃█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▄▄▄▃▄▃▄▃▃▄▄▆▄▄▄▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▄▄▄▆▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▅▅▅▃▆▃▅▃▃▆▆█▆▆▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▁▁▄▄▁▁▁▄▄▄█▄███▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▂▂▄▇▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▆▁▂▅▅▁▂▁▆▅▅█▅███▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▂▂▂▂▁▂▁▂▃▅▆▆▄▆▆▅▇▄▇▆▆██\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/11yjrbef\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:06:44.179161: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_loss_weighted_multi_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/77a6g8bz\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020642-77a6g8bz\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 195   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0: 100%|█| 81/81 [00:03<00:00, 22.32it/s, loss=1.11, v_num=g8bz, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.117\n","Epoch 0: 100%|█| 81/81 [00:03<00:00, 22.17it/s, loss=1.11, v_num=g8bz, BTC_val_a\n","Epoch 1: 100%|█| 81/81 [00:04<00:00, 18.71it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.028 >= min_delta = 0.003. New best score: 1.089\n","Epoch 1: 100%|█| 81/81 [00:04<00:00, 18.61it/s, loss=1.1, v_num=g8bz, BTC_val_ac\n","Epoch 2: 100%|█| 81/81 [00:04<00:00, 16.70it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 81/81 [00:04<00:00, 16.63it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:04<00:00, 20.07it/s, loss=1.11, v_num=g8bz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:04<00:00, 19.97it/s, loss=1.11, v_num=g8bz, BTC_val_a\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:03<00:00, 21.06it/s, loss=1.11, v_num=g8bz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:03<00:00, 20.95it/s, loss=1.11, v_num=g8bz, BTC_val_a\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:03<00:00, 21.71it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:03<00:00, 21.60it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:03<00:00, 21.43it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:03<00:00, 21.32it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:03<00:00, 21.60it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:03<00:00, 21.47it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:04<00:00, 19.14it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:04<00:00, 19.06it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:04<00:00, 19.45it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:04<00:00, 19.35it/s, loss=1.1, v_num=g8bz, BTC_val_ac\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:04<00:00, 19.57it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:04<00:00, 19.48it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:04<00:00, 20.09it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:04<00:00, 19.98it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:04<00:00, 18.94it/s, loss=1.11, v_num=g8bz, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:04<00:00, 18.85it/s, loss=1.11, v_num=g8bz, BTC_val_\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:05<00:00, 15.71it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:05<00:00, 15.63it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:04<00:00, 18.11it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:04<00:00, 17.89it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:04<00:00, 17.69it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:04<00:00, 17.60it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:04<00:00, 19.36it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:04<00:00, 19.25it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:04<00:00, 17.08it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:04<00:00, 17.01it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:04<00:00, 18.32it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:04<00:00, 18.23it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:03<00:00, 20.28it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:04<00:00, 20.17it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:04<00:00, 16.85it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:04<00:00, 16.75it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Epoch 21: 100%|█| 81/81 [00:04<00:00, 19.81it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.089. Signaling Trainer to stop.\n","Epoch 21: 100%|█| 81/81 [00:04<00:00, 19.69it/s, loss=1.1, v_num=g8bz, BTC_val_a\n","Epoch 21: 100%|█| 81/81 [00:04<00:00, 19.67it/s, loss=1.1, v_num=g8bz, BTC_val_a\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 40.85it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.25806450843811035,\n"," 'BTC_test_f1': 0.1367289274930954,\n"," 'ETH_test_acc': 0.4193548262119293,\n"," 'ETH_test_f1': 0.19648092985153198,\n"," 'test_loss': 1.1399511098861694}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 75229\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020642-77a6g8bz/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020642-77a6g8bz/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.1875\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.18413\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.1875\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.16239\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.11604\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 21\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1760\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 103\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621465705\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 79\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.35382\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.31259\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.34043\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.30833\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09645\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.22222\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.13333\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.22222\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.18519\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.11259\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.25806\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.13673\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.41935\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19648\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.13995\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▃▁▄▅█▆▂▁▂▃▅▄▃▂▂▄▃▅▃▂▅▂▃▄▇▇▃▂▆▂▆█▅▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▃▁▃▅█▆▃▁▁▃▅▄▃▂▂▃▂▅▃▃▃▂▃▄▇▇▃▂▄▂▅▅▄▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step █▃▅▆▅▅▄▃▅▂▄▅▂▃▂▅▇▅▅▅▆▇▇█▅▇▅▃▄▂▅▁▅▇▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step █▃▄▅▄▅▃▃▄▂▃▅▂▂▂▄▇▄▅▅▅▆▇▄▅▇▄▃▃▂▄▁▅▅▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▁█▅▃▆▃▆▆▆▇▇▅▇▆▇▇▇▆▅▅▄▇▃█▅▄▅▆▆▇▄▅▅▆▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▆▄▇▆▂▃▄▆▅▁█▆▆▅▇▃▄▆▆▂▃▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▅▄█▇▂▄▄▆▆▃█▆▇▅█▁▅▆▄▁▄▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▂▂█▄▄▄▁▅▂▂▆▇▅▃▃▂▃▅▅▅▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▂▂█▅▅▅▂▄▃▃▇▇▄▂▄▁▃▄▅▄▇▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▂▄▄▂▂▂▂▃▂▂▃▁▁▂▂▂▁▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁██▁█▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁██▁█▁▁▁▁▂▁▁▁▁▁▁▁█▂▁▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁██▁█▁▁▁▁▃▁▁▁▁▁▁▁▆▁▁▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁██▁█▁▁▁▁▆▁▁▁▁▁▁▁▇▁▁▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▆▁▄▅▃▆▇▅▆▅▆▇███▇▇▄▇▇▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/77a6g8bz\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:08:40.399661: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_single_lstm__binary_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2io82qdl\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020838-2io82qdl\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 130   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.7 K    Trainable params\n","0         Non-trainable params\n","75.7 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 79/80 [00:01<00:00, 42.72it/s, loss=0.597, v_num=2qdl, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.539\n","Epoch 0: 100%|█| 80/80 [00:01<00:00, 41.00it/s, loss=0.597, v_num=2qdl, BTC_val_\n","Epoch 1:  99%|▉| 79/80 [00:01<00:00, 45.81it/s, loss=0.58, v_num=2qdl, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.522\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 44.69it/s, loss=0.58, v_num=2qdl, BTC_val_a\n","Epoch 2:  99%|▉| 79/80 [00:01<00:00, 44.08it/s, loss=0.575, v_num=2qdl, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 42.75it/s, loss=0.575, v_num=2qdl, BTC_val_\u001b[A\n","Epoch 3:  99%|▉| 79/80 [00:01<00:00, 44.15it/s, loss=0.601, v_num=2qdl, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:01<00:00, 43.12it/s, loss=0.601, v_num=2qdl, BTC_val_\u001b[A\n","Epoch 4:  99%|▉| 79/80 [00:01<00:00, 43.87it/s, loss=0.562, v_num=2qdl, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 42.87it/s, loss=0.562, v_num=2qdl, BTC_val_\u001b[A\n","Epoch 5:  99%|▉| 79/80 [00:01<00:00, 45.80it/s, loss=0.556, v_num=2qdl, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:01<00:00, 44.60it/s, loss=0.556, v_num=2qdl, BTC_val_\u001b[A\n","Epoch 6:  99%|▉| 79/80 [00:01<00:00, 43.33it/s, loss=0.596, v_num=2qdl, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:01<00:00, 42.57it/s, loss=0.596, v_num=2qdl, BTC_val_\u001b[A\n","Epoch 7:  99%|▉| 79/80 [00:01<00:00, 39.92it/s, loss=0.591, v_num=2qdl, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:02<00:00, 39.33it/s, loss=0.591, v_num=2qdl, BTC_val_\u001b[A\n","Epoch 8:  99%|▉| 79/80 [00:02<00:00, 37.77it/s, loss=0.603, v_num=2qdl, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:02<00:00, 37.07it/s, loss=0.603, v_num=2qdl, BTC_val_\u001b[A\n","Epoch 9:  99%|▉| 79/80 [00:02<00:00, 39.48it/s, loss=0.607, v_num=2qdl, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:02<00:00, 38.68it/s, loss=0.607, v_num=2qdl, BTC_val_\u001b[A\n","Epoch 10:  99%|▉| 79/80 [00:02<00:00, 36.79it/s, loss=0.595, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:02<00:00, 36.14it/s, loss=0.595, v_num=2qdl, BTC_val\u001b[A\n","Epoch 11:  99%|▉| 79/80 [00:01<00:00, 46.63it/s, loss=0.588, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:01<00:00, 45.42it/s, loss=0.588, v_num=2qdl, BTC_valMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.515\n","\n","Epoch 12:  99%|▉| 79/80 [00:02<00:00, 35.20it/s, loss=0.559, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:02<00:00, 34.48it/s, loss=0.559, v_num=2qdl, BTC_val\u001b[A\n","Epoch 13:  99%|▉| 79/80 [00:02<00:00, 38.99it/s, loss=0.606, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:02<00:00, 38.25it/s, loss=0.606, v_num=2qdl, BTC_val\u001b[A\n","Epoch 14:  99%|▉| 79/80 [00:01<00:00, 43.34it/s, loss=0.567, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:01<00:00, 42.33it/s, loss=0.567, v_num=2qdl, BTC_val\u001b[A\n","Epoch 15:  99%|▉| 79/80 [00:01<00:00, 46.27it/s, loss=0.545, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:01<00:00, 45.09it/s, loss=0.545, v_num=2qdl, BTC_val\u001b[A\n","Epoch 16:  99%|▉| 79/80 [00:02<00:00, 33.30it/s, loss=0.574, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:02<00:00, 30.43it/s, loss=0.574, v_num=2qdl, BTC_val\u001b[A\n","Epoch 17:  99%|▉| 79/80 [00:02<00:00, 37.23it/s, loss=0.579, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:02<00:00, 36.45it/s, loss=0.579, v_num=2qdl, BTC_val\u001b[A\n","Epoch 18:  99%|▉| 79/80 [00:01<00:00, 41.62it/s, loss=0.549, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:01<00:00, 40.90it/s, loss=0.549, v_num=2qdl, BTC_val\u001b[A\n","Epoch 19:  99%|▉| 79/80 [00:02<00:00, 36.05it/s, loss=0.544, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.510\n","Epoch 19: 100%|█| 80/80 [00:02<00:00, 35.53it/s, loss=0.544, v_num=2qdl, BTC_val\n","Epoch 20:  99%|▉| 79/80 [00:02<00:00, 38.63it/s, loss=0.556, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:02<00:00, 37.65it/s, loss=0.556, v_num=2qdl, BTC_val\u001b[A\n","Epoch 21:  99%|▉| 79/80 [00:01<00:00, 41.36it/s, loss=0.605, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:01<00:00, 40.71it/s, loss=0.605, v_num=2qdl, BTC_val\u001b[A\n","Epoch 22:  99%|▉| 79/80 [00:02<00:00, 35.77it/s, loss=0.55, v_num=2qdl, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:02<00:00, 35.36it/s, loss=0.55, v_num=2qdl, BTC_val_\u001b[A\n","Epoch 23:  99%|▉| 79/80 [00:01<00:00, 42.23it/s, loss=0.559, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:01<00:00, 41.35it/s, loss=0.559, v_num=2qdl, BTC_val\u001b[A\n","Epoch 24:  99%|▉| 79/80 [00:02<00:00, 34.34it/s, loss=0.577, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:02<00:00, 33.89it/s, loss=0.577, v_num=2qdl, BTC_val\u001b[A\n","Epoch 25:  99%|▉| 79/80 [00:01<00:00, 42.73it/s, loss=0.581, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:01<00:00, 41.96it/s, loss=0.581, v_num=2qdl, BTC_val\u001b[A\n","Epoch 26:  99%|▉| 79/80 [00:01<00:00, 42.20it/s, loss=0.587, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:01<00:00, 41.48it/s, loss=0.587, v_num=2qdl, BTC_val\u001b[A\n","Epoch 27:  99%|▉| 79/80 [00:01<00:00, 40.45it/s, loss=0.551, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 80/80 [00:02<00:00, 39.87it/s, loss=0.551, v_num=2qdl, BTC_val\u001b[A\n","Epoch 28:  99%|▉| 79/80 [00:01<00:00, 39.56it/s, loss=0.577, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 80/80 [00:02<00:00, 38.90it/s, loss=0.577, v_num=2qdl, BTC_val\u001b[A\n","Epoch 29:  99%|▉| 79/80 [00:02<00:00, 33.47it/s, loss=0.563, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.502\n","Epoch 29: 100%|█| 80/80 [00:02<00:00, 32.94it/s, loss=0.563, v_num=2qdl, BTC_val\n","Epoch 30:  99%|▉| 79/80 [00:02<00:00, 38.70it/s, loss=0.582, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 80/80 [00:02<00:00, 37.79it/s, loss=0.582, v_num=2qdl, BTC_val\u001b[A\n","Epoch 31:  99%|▉| 79/80 [00:01<00:00, 45.10it/s, loss=0.557, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 80/80 [00:01<00:00, 43.87it/s, loss=0.557, v_num=2qdl, BTC_val\u001b[A\n","Epoch 32:  99%|▉| 79/80 [00:01<00:00, 41.05it/s, loss=0.563, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 80/80 [00:01<00:00, 40.07it/s, loss=0.563, v_num=2qdl, BTC_val\u001b[A\n","Epoch 33:  99%|▉| 79/80 [00:01<00:00, 39.79it/s, loss=0.545, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.497\n","Epoch 33: 100%|█| 80/80 [00:02<00:00, 38.65it/s, loss=0.545, v_num=2qdl, BTC_val\n","Epoch 34:  99%|▉| 79/80 [00:02<00:00, 33.30it/s, loss=0.561, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 80/80 [00:02<00:00, 32.74it/s, loss=0.561, v_num=2qdl, BTC_val\u001b[A\n","Epoch 35:  99%|▉| 79/80 [00:02<00:00, 33.76it/s, loss=0.572, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 35: 100%|█| 80/80 [00:02<00:00, 33.39it/s, loss=0.572, v_num=2qdl, BTC_val\u001b[A\n","Epoch 36:  99%|▉| 79/80 [00:02<00:00, 37.88it/s, loss=0.572, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 36: 100%|█| 80/80 [00:02<00:00, 37.08it/s, loss=0.572, v_num=2qdl, BTC_val\u001b[A\n","Epoch 37:  99%|▉| 79/80 [00:02<00:00, 33.73it/s, loss=0.556, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 37: 100%|█| 80/80 [00:02<00:00, 33.37it/s, loss=0.556, v_num=2qdl, BTC_val\u001b[A\n","Epoch 38:  99%|▉| 79/80 [00:01<00:00, 39.72it/s, loss=0.566, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 38: 100%|█| 80/80 [00:02<00:00, 39.06it/s, loss=0.566, v_num=2qdl, BTC_val\u001b[A\n","Epoch 39:  99%|▉| 79/80 [00:01<00:00, 39.65it/s, loss=0.584, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 39: 100%|█| 80/80 [00:02<00:00, 39.06it/s, loss=0.584, v_num=2qdl, BTC_val\u001b[A\n","Epoch 40:  99%|▉| 79/80 [00:02<00:00, 38.07it/s, loss=0.613, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 40: 100%|█| 80/80 [00:02<00:00, 37.40it/s, loss=0.613, v_num=2qdl, BTC_val\u001b[A\n","Epoch 41:  99%|▉| 79/80 [00:02<00:00, 37.78it/s, loss=0.556, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 41: 100%|█| 80/80 [00:02<00:00, 37.10it/s, loss=0.556, v_num=2qdl, BTC_val\u001b[A\n","Epoch 42:  99%|▉| 79/80 [00:01<00:00, 42.46it/s, loss=0.553, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 42: 100%|█| 80/80 [00:01<00:00, 41.50it/s, loss=0.553, v_num=2qdl, BTC_val\u001b[A\n","Epoch 43:  99%|▉| 79/80 [00:02<00:00, 30.94it/s, loss=0.555, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 43: 100%|█| 80/80 [00:02<00:00, 30.58it/s, loss=0.555, v_num=2qdl, BTC_val\u001b[A\n","Epoch 44:  99%|▉| 79/80 [00:01<00:00, 43.28it/s, loss=0.585, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 44: 100%|█| 80/80 [00:01<00:00, 42.24it/s, loss=0.585, v_num=2qdl, BTC_val\u001b[A\n","Epoch 45:  99%|▉| 79/80 [00:02<00:00, 35.70it/s, loss=0.596, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 45: 100%|█| 80/80 [00:02<00:00, 35.01it/s, loss=0.596, v_num=2qdl, BTC_val\u001b[A\n","Epoch 46:  99%|▉| 79/80 [00:02<00:00, 35.76it/s, loss=0.567, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 46: 100%|█| 80/80 [00:02<00:00, 35.17it/s, loss=0.567, v_num=2qdl, BTC_val\u001b[A\n","Epoch 47:  99%|▉| 79/80 [00:02<00:00, 38.49it/s, loss=0.561, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 47: 100%|█| 80/80 [00:02<00:00, 37.88it/s, loss=0.561, v_num=2qdl, BTC_val\u001b[A\n","Epoch 48:  99%|▉| 79/80 [00:01<00:00, 39.82it/s, loss=0.569, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 48: 100%|█| 80/80 [00:02<00:00, 39.00it/s, loss=0.569, v_num=2qdl, BTC_val\u001b[A\n","Epoch 49:  99%|▉| 79/80 [00:02<00:00, 37.25it/s, loss=0.52, v_num=2qdl, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 49: 100%|█| 80/80 [00:02<00:00, 36.69it/s, loss=0.52, v_num=2qdl, BTC_val_\u001b[A\n","Epoch 50:  99%|▉| 79/80 [00:02<00:00, 39.02it/s, loss=0.547, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 50: 100%|█| 80/80 [00:02<00:00, 38.40it/s, loss=0.547, v_num=2qdl, BTC_val\u001b[A\n","Epoch 51:  99%|▉| 79/80 [00:02<00:00, 35.26it/s, loss=0.551, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 51: 100%|█| 80/80 [00:02<00:00, 34.77it/s, loss=0.551, v_num=2qdl, BTC_val\u001b[A\n","Epoch 52:  99%|▉| 79/80 [00:02<00:00, 35.21it/s, loss=0.589, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 52: 100%|█| 80/80 [00:02<00:00, 34.60it/s, loss=0.589, v_num=2qdl, BTC_val\u001b[A\n","Epoch 53:  99%|▉| 79/80 [00:02<00:00, 35.35it/s, loss=0.591, v_num=2qdl, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.497. Signaling Trainer to stop.\n","Epoch 53: 100%|█| 80/80 [00:02<00:00, 34.72it/s, loss=0.591, v_num=2qdl, BTC_val\n","Epoch 53: 100%|█| 80/80 [00:02<00:00, 34.66it/s, loss=0.591, v_num=2qdl, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 59.40it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.6774193644523621,\n"," 'BTC_test_f1': 0.6673067212104797,\n"," 'ETH_test_acc': 0.7419354915618896,\n"," 'ETH_test_f1': 0.741647481918335,\n"," 'test_loss': 0.5464009642601013}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 75536\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020838-2io82qdl/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_020838-2io82qdl/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.52438\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 53\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4266\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 123\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621465841\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 193\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.73159\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.71597\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.71971\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.70688\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.55512\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.66667\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.66667\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.52501\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.67742\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.66731\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.74194\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.74165\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.5464\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▃▃▅▃▄▆▄▆▄▅▄▃▇▃▂▅▆▄▆▆▆▆█▄▃▅▃▅▃▁▃▃▆▆▂▅▅▆▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▃▃▅▃▄▅▄▆▄▅▄▂▇▃▂▅▆▄▆▆▆▆█▄▃▅▃▅▃▁▃▃▆▆▁▅▅▆▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▃▃▁▅▁▃▆▁▆▆▆█▆▅▃█▁▃█▅▅█▃▆█▆▅▅▅▃▃▆█▅▃▃▅▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▂▄▄▂▅▂▄▆▁▆▆▅█▆▅▃█▂▄█▅▅█▄▇█▆▅▄▂▄▄▇█▅▄▄▅▄▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅█▆▅▄▄▃▃▆▅▄▄▁▅▄▄▄▆▃▃▃▄▁▆▆▆▅▃▅▆▅▆▂▃▅▄▄▄▃▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▅▆▆█▇▇▆▇▇▇▇▇▇▆▇█▇▆▇▇▇▇▇▇▇▇█▇█▇▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▆▆▆▇▇▆▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇██▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▆▆▆▇▆▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇█▇█████▇▇▇█▇▇███▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▆▇▆▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇██▇▇█▇███████▇█▇▇███▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ███████████▁▁▁█▁█▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ███████████▁▁▁█▁█▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▁▅▅▃█▅▃▆▆▅▃▃▅▅▆▅▆▆▅▅▅▅▅▃▆█▆▆▅▅▅▅▆▃▆▃▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▅▅▂█▅▂▆▆▄▂▂▄▅▆▅▆▆▄▄▄▄▄▂▆█▆▆▄▄▄▄▆▂▆▂▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▃▃▃▄▄▄▄▄▃▄▄▃▄▂▃▄▃▃▅▄▄▁▃▇▁▃▅▄▃▅▃▃▃▃▃▄▃█▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_single_lstm__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2io82qdl\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:10:51.617210: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_single_lstm__binary_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2rcpddlf\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021050-2rcpddlf\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 130   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.7 K    Trainable params\n","0         Non-trainable params\n","75.7 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 80/81 [00:01<00:00, 45.40it/s, loss=0.701, v_num=ddlf, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.608\n","Epoch 0: 100%|█| 81/81 [00:01<00:00, 44.32it/s, loss=0.701, v_num=ddlf, BTC_val_\n","Epoch 1:  99%|▉| 80/81 [00:01<00:00, 43.87it/s, loss=0.717, v_num=ddlf, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 81/81 [00:01<00:00, 43.07it/s, loss=0.717, v_num=ddlf, BTC_val_\u001b[A\n","Epoch 2:  99%|▉| 80/81 [00:01<00:00, 44.58it/s, loss=0.693, v_num=ddlf, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 81/81 [00:01<00:00, 43.73it/s, loss=0.693, v_num=ddlf, BTC_val_\u001b[A\n","Epoch 3:  99%|▉| 80/81 [00:01<00:00, 44.17it/s, loss=0.69, v_num=ddlf, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:01<00:00, 43.34it/s, loss=0.69, v_num=ddlf, BTC_val_a\u001b[A\n","Epoch 4:  99%|▉| 80/81 [00:01<00:00, 44.54it/s, loss=0.697, v_num=ddlf, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:01<00:00, 43.67it/s, loss=0.697, v_num=ddlf, BTC_val_\u001b[A\n","Epoch 5:  99%|▉| 80/81 [00:02<00:00, 38.53it/s, loss=0.697, v_num=ddlf, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:02<00:00, 38.00it/s, loss=0.697, v_num=ddlf, BTC_val_\u001b[A\n","Epoch 6:  99%|▉| 80/81 [00:01<00:00, 44.14it/s, loss=0.698, v_num=ddlf, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:01<00:00, 43.35it/s, loss=0.698, v_num=ddlf, BTC_val_\u001b[A\n","Epoch 7:  99%|▉| 80/81 [00:01<00:00, 44.46it/s, loss=0.698, v_num=ddlf, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:01<00:00, 43.66it/s, loss=0.698, v_num=ddlf, BTC_val_\u001b[A\n","Epoch 8:  99%|▉| 80/81 [00:02<00:00, 38.03it/s, loss=0.698, v_num=ddlf, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:02<00:00, 37.45it/s, loss=0.698, v_num=ddlf, BTC_val_\u001b[A\n","Epoch 9:  99%|▉| 80/81 [00:02<00:00, 36.94it/s, loss=0.694, v_num=ddlf, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:02<00:00, 36.43it/s, loss=0.694, v_num=ddlf, BTC_val_\u001b[A\n","Epoch 10:  99%|▉| 80/81 [00:02<00:00, 37.72it/s, loss=0.69, v_num=ddlf, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:02<00:00, 37.19it/s, loss=0.69, v_num=ddlf, BTC_val_\u001b[A\n","Epoch 11:  99%|▉| 80/81 [00:02<00:00, 39.39it/s, loss=0.692, v_num=ddlf, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:02<00:00, 38.76it/s, loss=0.692, v_num=ddlf, BTC_val\u001b[A\n","Epoch 12:  99%|▉| 80/81 [00:01<00:00, 43.88it/s, loss=0.694, v_num=ddlf, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:01<00:00, 43.03it/s, loss=0.694, v_num=ddlf, BTC_val\u001b[A\n","Epoch 13:  99%|▉| 80/81 [00:01<00:00, 43.55it/s, loss=0.694, v_num=ddlf, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:01<00:00, 42.81it/s, loss=0.694, v_num=ddlf, BTC_val\u001b[A\n","Epoch 14:  99%|▉| 80/81 [00:01<00:00, 43.10it/s, loss=0.692, v_num=ddlf, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:01<00:00, 42.27it/s, loss=0.692, v_num=ddlf, BTC_val\u001b[A\n","Epoch 15:  99%|▉| 80/81 [00:02<00:00, 37.69it/s, loss=0.69, v_num=ddlf, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:02<00:00, 37.12it/s, loss=0.69, v_num=ddlf, BTC_val_\u001b[A\n","Epoch 16:  99%|▉| 80/81 [00:01<00:00, 41.45it/s, loss=0.694, v_num=ddlf, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:01<00:00, 40.80it/s, loss=0.694, v_num=ddlf, BTC_val\u001b[A\n","Epoch 17:  99%|▉| 80/81 [00:02<00:00, 39.48it/s, loss=0.692, v_num=ddlf, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:02<00:00, 38.89it/s, loss=0.692, v_num=ddlf, BTC_val\u001b[A\n","Epoch 18:  99%|▉| 80/81 [00:02<00:00, 37.52it/s, loss=0.687, v_num=ddlf, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:02<00:00, 36.85it/s, loss=0.687, v_num=ddlf, BTC_val\u001b[A\n","Epoch 19:  99%|▉| 80/81 [00:02<00:00, 38.53it/s, loss=0.694, v_num=ddlf, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:02<00:00, 37.71it/s, loss=0.694, v_num=ddlf, BTC_val\u001b[A\n","Epoch 20:  99%|▉| 80/81 [00:01<00:00, 41.28it/s, loss=0.692, v_num=ddlf, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.608. Signaling Trainer to stop.\n","Epoch 20: 100%|█| 81/81 [00:02<00:00, 40.39it/s, loss=0.692, v_num=ddlf, BTC_val\n","Epoch 20: 100%|█| 81/81 [00:02<00:00, 40.28it/s, loss=0.692, v_num=ddlf, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 63.17it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.4193548262119293,\n"," 'BTC_test_f1': 0.29533159732818604,\n"," 'ETH_test_acc': 0.6129032373428345,\n"," 'ETH_test_f1': 0.3793548345565796,\n"," 'test_loss': 0.7264933586120605}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 75845\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021050-2rcpddlf/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021050-2rcpddlf/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.27273\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.41818\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.69804\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1680\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 51\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621465901\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.53349\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.37274\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.50355\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.34409\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69267\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.55556\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.35714\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.88889\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.47059\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.67662\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.41935\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.29533\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.37935\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.72649\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▆▆▁▅▃▄▃█▄▃▆▄▇▃▅▃▁▅▆▃▂▆▃▅▃▇▃▇▃▃▄▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▇▇▁▆▃▃▃▅▄▂█▃█▄▃▂▁▃█▂▁▆▂▄▂▄▄▄▂▂▄▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▃▆▄▃▂▆▃▁█▆▆▃▆▆▅▄▄▆▆▂▅▄▄▆▅▇▄▅▃▆▄▁▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▃▆▄▃▂▄▄▁█▆▇▂▇▇▄▃▄▄▅▂▄▄▃▄▃▄▄▃▃▄▃▁▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅▁█▅▄▃▄▂▃▃▃▄▂▄▃▅▅▃▄▅▄▄▅▂▄▁▄▄▅▄▄▆▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▄▂▃▁▂▆▃█▇▄▆▃▇▆▆▅▆▇▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▇█▇▆▇▅▄▄▇▆▅▃▄▂▁▂▂▂▄▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch █▁▃▅▃▁▆▇▃▄▄▆▃▃▆▅▅▄▆▆▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch █▆▆▆▆▄▅▄▆▅▃▃▃▁▁▁▂▂▄▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ▇█▆▄▅▃▂▁▃▁▂▁▂▂▁▁▁▂▁▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▄▃▅▆▅▇▇█▇████▇▇██▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_single_lstm__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2rcpddlf\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:11:51.429283: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_single_lstm__multi_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2ophbjwc\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021149-2ophbjwc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 195   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.8 K    Trainable params\n","0         Non-trainable params\n","75.8 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0: 100%|█| 80/80 [00:02<00:00, 37.22it/s, loss=1.06, v_num=bjwc, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.007\n","Epoch 0: 100%|█| 80/80 [00:02<00:00, 36.77it/s, loss=1.06, v_num=bjwc, BTC_val_a\n","Epoch 1: 100%|█| 80/80 [00:01<00:00, 40.38it/s, loss=1.02, v_num=bjwc, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 80/80 [00:02<00:00, 39.96it/s, loss=1.02, v_num=bjwc, BTC_val_a\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.027 >= min_delta = 0.003. New best score: 0.979\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 42.14it/s, loss=1.02, v_num=bjwc, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 80/80 [00:01<00:00, 41.78it/s, loss=1.02, v_num=bjwc, BTC_val_a\u001b[A\n","Epoch 3:  99%|▉| 79/80 [00:01<00:00, 41.88it/s, loss=0.96, v_num=bjwc, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:01<00:00, 41.17it/s, loss=0.96, v_num=bjwc, BTC_val_a\u001b[A\n","Epoch 4:  99%|▉| 79/80 [00:01<00:00, 41.70it/s, loss=0.947, v_num=bjwc, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:01<00:00, 40.95it/s, loss=0.947, v_num=bjwc, BTC_val_\u001b[A\n","Metric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.971\n","Epoch 5:  99%|▉| 79/80 [00:01<00:00, 41.09it/s, loss=0.978, v_num=bjwc, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:01<00:00, 40.06it/s, loss=0.978, v_num=bjwc, BTC_val_\u001b[A\n","Epoch 6:  99%|▉| 79/80 [00:02<00:00, 35.28it/s, loss=0.96, v_num=bjwc, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:02<00:00, 34.68it/s, loss=0.96, v_num=bjwc, BTC_val_a\u001b[A\n","Epoch 7:  99%|▉| 79/80 [00:01<00:00, 44.70it/s, loss=0.911, v_num=bjwc, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:01<00:00, 43.68it/s, loss=0.911, v_num=bjwc, BTC_val_\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 8:  99%|▉| 79/80 [00:02<00:00, 37.31it/s, loss=0.876, v_num=bjwc, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:02<00:00, 36.61it/s, loss=0.876, v_num=bjwc, BTC_val_\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.016 >= min_delta = 0.003. New best score: 0.954\n","Epoch 9:  99%|▉| 79/80 [00:02<00:00, 29.87it/s, loss=0.973, v_num=bjwc, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:02<00:00, 29.50it/s, loss=0.973, v_num=bjwc, BTC_val_\u001b[A\n","Epoch 10:  99%|▉| 79/80 [00:02<00:00, 37.60it/s, loss=0.93, v_num=bjwc, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:02<00:00, 37.01it/s, loss=0.93, v_num=bjwc, BTC_val_\u001b[A\n","Epoch 11:  99%|▉| 79/80 [00:02<00:00, 36.54it/s, loss=0.955, v_num=bjwc, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:02<00:00, 36.07it/s, loss=0.955, v_num=bjwc, BTC_val\u001b[A\n","Epoch 12:  99%|▉| 79/80 [00:02<00:00, 37.14it/s, loss=0.936, v_num=bjwc, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:02<00:00, 36.62it/s, loss=0.936, v_num=bjwc, BTC_val\u001b[A\n","Epoch 13:  99%|▉| 79/80 [00:01<00:00, 41.31it/s, loss=0.897, v_num=bjwc, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:01<00:00, 40.62it/s, loss=0.897, v_num=bjwc, BTC_val\u001b[A\n","Epoch 14:  99%|▉| 79/80 [00:01<00:00, 42.43it/s, loss=0.927, v_num=bjwc, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:01<00:00, 41.64it/s, loss=0.927, v_num=bjwc, BTC_val\u001b[A\n","Epoch 15:  99%|▉| 79/80 [00:02<00:00, 35.38it/s, loss=0.914, v_num=bjwc, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:02<00:00, 34.92it/s, loss=0.914, v_num=bjwc, BTC_val\u001b[A\n","Epoch 16:  99%|▉| 79/80 [00:02<00:00, 35.96it/s, loss=0.947, v_num=bjwc, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:02<00:00, 35.44it/s, loss=0.947, v_num=bjwc, BTC_val\u001b[A\n","Epoch 17:  99%|▉| 79/80 [00:02<00:00, 34.46it/s, loss=0.939, v_num=bjwc, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:02<00:00, 33.88it/s, loss=0.939, v_num=bjwc, BTC_val\u001b[A\n","Epoch 18:  99%|▉| 79/80 [00:01<00:00, 42.50it/s, loss=0.928, v_num=bjwc, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:01<00:00, 41.49it/s, loss=0.928, v_num=bjwc, BTC_val\u001b[A\n","Epoch 19:  99%|▉| 79/80 [00:02<00:00, 38.49it/s, loss=0.946, v_num=bjwc, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:02<00:00, 37.76it/s, loss=0.946, v_num=bjwc, BTC_val\u001b[A\n","Epoch 20:  99%|▉| 79/80 [00:01<00:00, 40.41it/s, loss=0.941, v_num=bjwc, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:02<00:00, 39.56it/s, loss=0.941, v_num=bjwc, BTC_val\u001b[A\n","Epoch 21:  99%|▉| 79/80 [00:02<00:00, 35.92it/s, loss=0.933, v_num=bjwc, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:02<00:00, 35.21it/s, loss=0.933, v_num=bjwc, BTC_val\u001b[A\n","Epoch 22:  99%|▉| 79/80 [00:02<00:00, 30.70it/s, loss=0.932, v_num=bjwc, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:02<00:00, 30.38it/s, loss=0.932, v_num=bjwc, BTC_val\u001b[A\n","Epoch 23:  99%|▉| 79/80 [00:02<00:00, 35.51it/s, loss=0.913, v_num=bjwc, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:02<00:00, 35.08it/s, loss=0.913, v_num=bjwc, BTC_val\u001b[A\n","Epoch 24:  99%|▉| 79/80 [00:01<00:00, 42.64it/s, loss=0.906, v_num=bjwc, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:01<00:00, 41.86it/s, loss=0.906, v_num=bjwc, BTC_val\u001b[A\n","Epoch 25:  99%|▉| 79/80 [00:01<00:00, 42.28it/s, loss=0.911, v_num=bjwc, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:01<00:00, 41.55it/s, loss=0.911, v_num=bjwc, BTC_val\u001b[A\n","Epoch 26:  99%|▉| 79/80 [00:02<00:00, 36.15it/s, loss=0.882, v_num=bjwc, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:02<00:00, 35.66it/s, loss=0.882, v_num=bjwc, BTC_val\u001b[A\n","Epoch 27:  99%|▉| 79/80 [00:02<00:00, 37.11it/s, loss=0.911, v_num=bjwc, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 80/80 [00:02<00:00, 36.55it/s, loss=0.911, v_num=bjwc, BTC_val\u001b[A\n","Epoch 28:  99%|▉| 79/80 [00:02<00:00, 38.66it/s, loss=0.922, v_num=bjwc, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.954. Signaling Trainer to stop.\n","Epoch 28: 100%|█| 80/80 [00:02<00:00, 37.96it/s, loss=0.922, v_num=bjwc, BTC_val\n","Epoch 28: 100%|█| 80/80 [00:02<00:00, 37.88it/s, loss=0.922, v_num=bjwc, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 63.23it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.5806451439857483,\n"," 'BTC_test_f1': 0.4767591059207916,\n"," 'ETH_test_acc': 0.6129032373428345,\n"," 'ETH_test_f1': 0.480448454618454,\n"," 'test_loss': 0.8642573356628418}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 76018\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021149-2ophbjwc/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021149-2ophbjwc/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.43275\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.8901\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 28\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2291\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 71\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621465980\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 103\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.54553\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.46311\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.53523\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.44893\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.90285\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.22222\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.33333\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.98122\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.58065\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.47676\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.48045\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.86426\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▂▄▃▅▅▄▃▆▅▅▃▄▅▄▅▅▄▃▂▃▄▄█▅▄▆▄▇▅▅▅▄▆▅▄▁▄▅▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▁▁▃▂▃▃▄▄▃▂▂▄▄▂▄▄▁▁▃▄▄█▅▄▂▄▆▅▄▅▄▆▅▄▁▁▄▃▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▂▂▅▅▄▁▃▄▅▄▄▅▄▅▂▃▃▅▃▂▇▂▄▃▅▁▅▅▂▅▄▇▅▃▂█▅▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▂▁▁▃▂▅▁▃▃▂▅▂▃▃▄▁▄▂▅▄▄▅▂▅▃▄▃▄▄▂▆▃▇▄▄▁█▅▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▆▆▅▃▅▇▄▃▄▄▆▄█▃▇▆▇▅▆▆▁▃▄▅▂▅▁▄▅▅▇▂▅▅▆▂▇▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▄▅▅▅▆▆▆▅▇▆▅▆▆▅█▇▇▇▇▇▇█▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▂▁▁▃▄▅▅▆▆▅▇▆▆▇▇▆█▇▇▇▇▇▇█▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▃▃▄▄▅▄▅▆▆▅▆▆▆█▆▆▆▆▆▇▆█▇▇█▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▁▁▂▄▅▄▆▆▆▆▆▆▆█▆▇▇▇▇█▇█▇▇█▇█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▄▄▃▃▂▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▆▆▃▁▃▆▃▆▆▆▆▆▆█▆▆▃▆▃▃▆▁▆▆▆█▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▂▂▂▁▂▅▄▃▂▅▃▅▅█▃▃▂▃▂▂▃▁▃▅▃▅▅▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ██▆▁█▆▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▂▂▁▂█▆▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▃▅█▂▇▆▅▁█▄██▅▆▆▆▅▇█▄▆▆▇▄▃▄▅▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_single_lstm__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2ophbjwc\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:13:10.181227: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_single_lstm__multi_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/5es019y0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021308-5es019y0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 195   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.8 K    Trainable params\n","0         Non-trainable params\n","75.8 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:   0%|                                           | 0/81 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 80/81 [00:01<00:00, 43.25it/s, loss=1.11, v_num=19y0, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.089\n","Epoch 0: 100%|█| 81/81 [00:01<00:00, 41.85it/s, loss=1.11, v_num=19y0, BTC_val_a\n","Epoch 1:  99%|▉| 80/81 [00:01<00:00, 43.59it/s, loss=1.12, v_num=19y0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 81/81 [00:01<00:00, 42.50it/s, loss=1.12, v_num=19y0, BTC_val_a\u001b[A\n","Epoch 2:  99%|▉| 80/81 [00:01<00:00, 43.77it/s, loss=1.1, v_num=19y0, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 81/81 [00:01<00:00, 42.71it/s, loss=1.1, v_num=19y0, BTC_val_ac\u001b[A\n","Epoch 3:  99%|▉| 80/81 [00:01<00:00, 43.27it/s, loss=1.11, v_num=19y0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:01<00:00, 42.25it/s, loss=1.11, v_num=19y0, BTC_val_a\u001b[A\n","Epoch 4:  99%|▉| 80/81 [00:01<00:00, 45.53it/s, loss=1.1, v_num=19y0, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:01<00:00, 44.27it/s, loss=1.1, v_num=19y0, BTC_val_ac\u001b[A\n","Epoch 5:  99%|▉| 80/81 [00:01<00:00, 46.74it/s, loss=1.11, v_num=19y0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:01<00:00, 45.73it/s, loss=1.11, v_num=19y0, BTC_val_a\u001b[A\n","Epoch 6:  99%|▉| 80/81 [00:01<00:00, 44.31it/s, loss=1.1, v_num=19y0, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:01<00:00, 43.48it/s, loss=1.1, v_num=19y0, BTC_val_ac\u001b[A\n","Epoch 7:  99%|▉| 80/81 [00:01<00:00, 43.79it/s, loss=1.1, v_num=19y0, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:01<00:00, 42.96it/s, loss=1.1, v_num=19y0, BTC_val_ac\u001b[A\n","Epoch 8:  99%|▉| 80/81 [00:01<00:00, 41.53it/s, loss=1.09, v_num=19y0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:01<00:00, 40.58it/s, loss=1.09, v_num=19y0, BTC_val_a\u001b[A\n","Epoch 9:  99%|▉| 80/81 [00:01<00:00, 43.60it/s, loss=1.11, v_num=19y0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:01<00:00, 42.82it/s, loss=1.11, v_num=19y0, BTC_val_a\u001b[A\n","Epoch 10:  99%|▉| 80/81 [00:01<00:00, 42.42it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:01<00:00, 41.73it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n","Epoch 11:  99%|▉| 80/81 [00:02<00:00, 38.42it/s, loss=1.09, v_num=19y0, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:02<00:00, 37.90it/s, loss=1.09, v_num=19y0, BTC_val_\u001b[A\n","Epoch 12:  99%|▉| 80/81 [00:01<00:00, 44.33it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:01<00:00, 43.51it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n","Epoch 13:  99%|▉| 80/81 [00:01<00:00, 44.08it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:01<00:00, 43.29it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n","Epoch 14:  99%|▉| 80/81 [00:01<00:00, 43.22it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:01<00:00, 42.46it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n","Epoch 15:  99%|▉| 80/81 [00:01<00:00, 44.48it/s, loss=1.11, v_num=19y0, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:01<00:00, 43.57it/s, loss=1.11, v_num=19y0, BTC_val_\u001b[A\n","Epoch 16:  99%|▉| 80/81 [00:01<00:00, 44.29it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:01<00:00, 43.38it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n","Epoch 17:  99%|▉| 80/81 [00:02<00:00, 37.60it/s, loss=1.09, v_num=19y0, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:02<00:00, 37.07it/s, loss=1.09, v_num=19y0, BTC_val_\u001b[A\n","Epoch 18:  99%|▉| 80/81 [00:01<00:00, 41.54it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:01<00:00, 40.71it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n","Epoch 19:  99%|▉| 80/81 [00:02<00:00, 37.46it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:02<00:00, 36.96it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n","Epoch 20:  99%|▉| 80/81 [00:01<00:00, 41.65it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.089. Signaling Trainer to stop.\n","Epoch 20: 100%|█| 81/81 [00:01<00:00, 40.98it/s, loss=1.1, v_num=19y0, BTC_val_a\n","Epoch 20: 100%|█| 81/81 [00:01<00:00, 40.91it/s, loss=1.1, v_num=19y0, BTC_val_a\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 81.81it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.25806450843811035,\n"," 'BTC_test_f1': 0.1367289274930954,\n"," 'ETH_test_acc': 0.4193548262119293,\n"," 'ETH_test_f1': 0.19648092985153198,\n"," 'test_loss': 1.1192375421524048}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 76228\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021308-5es019y0/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021308-5es019y0/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.30159\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.1875\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.16931\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.11127\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1680\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 50\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621466038\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.35225\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.24898\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.35619\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.25848\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09563\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.2381\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.10094\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.25806\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.13673\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.41935\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19648\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.11924\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▂▅▄▃▅▄▄▇▃▃▆▄▃▄▄▁▅▃█▄▃▄▃▇▄▃▄▄▁▁▃▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▂▆▄▄▇▄▅█▄▅▆▆▂▂▄▁▃▃█▅▃▄▃▄▄▅▄▄▂▂▃▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▅▅▂▅▅▃▄▃▂▃▆▇▅▃▅▅█▄▅▄▄▅▆▇▄▃▅▄▂▃▁▇▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▆▆▂▅▅▃▅▄▃▄▆█▄▂▅▄▄▄▄▄▄▅▆▄▄▂▅▄▂▄▁▇▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▃▃▆▂▃▄▄▂▃▅▁▂▃▄▂█▂▃▁▃▄▃▃▁▃▃▃▄▄▅▄▃▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▃▂▃▁▅▄▂▁▅▃▆▅▇▅█▅▄▅▂▇▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▆▅█▅█▆▇▅▁▄▅▅▅▄▆▄▃▅▂▅▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▄▂▅▇▄▄▃▆▁▂▄▆▃▅▇▆█▃▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▃▃▅▆█▅▆▃▁▂▂▃▄▂▃▄▄▅▂▄▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▄▃▂▂▃▂▁▂▂▁▁▁▁▁▁▁▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc █▁▁▁██▁████████▁█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▇▁▁▁▇█▅▇▇▇█▇▇▇▇▁█▇▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc █▁▁▁██▆████████▆█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▆▁▁▁▆▆▅▆▆▆▆▆▆▆▆█▆▆▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▅▄█▂▂▂▁▂▃▂▂▄▅▄▅▂▂▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_single_lstm__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/5es019y0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:14:16.552940: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm__binary_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3i7gs46x\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021415-3i7gs46x\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 130   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 79/80 [00:03<00:00, 21.36it/s, loss=0.665, v_num=s46x, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.644\n","Epoch 0: 100%|█| 80/80 [00:03<00:00, 21.21it/s, loss=0.665, v_num=s46x, BTC_val_\n","Epoch 1:  99%|▉| 79/80 [00:03<00:00, 21.86it/s, loss=0.584, v_num=s46x, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.109 >= min_delta = 0.003. New best score: 0.535\n","Epoch 1: 100%|█| 80/80 [00:03<00:00, 21.68it/s, loss=0.584, v_num=s46x, BTC_val_\n","Epoch 2:  99%|▉| 79/80 [00:04<00:00, 19.70it/s, loss=0.607, v_num=s46x, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.015 >= min_delta = 0.003. New best score: 0.520\n","Epoch 2: 100%|█| 80/80 [00:04<00:00, 19.59it/s, loss=0.607, v_num=s46x, BTC_val_\n","Epoch 3:  99%|▉| 79/80 [00:03<00:00, 20.76it/s, loss=0.579, v_num=s46x, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:03<00:00, 20.61it/s, loss=0.579, v_num=s46x, BTC_val_\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.035 >= min_delta = 0.003. New best score: 0.484\n","Epoch 4:  99%|▉| 79/80 [00:03<00:00, 20.84it/s, loss=0.623, v_num=s46x, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:03<00:00, 20.67it/s, loss=0.623, v_num=s46x, BTC_val_\u001b[A\n","Epoch 5:  99%|▉| 79/80 [00:03<00:00, 21.69it/s, loss=0.588, v_num=s46x, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 5: 100%|█| 80/80 [00:03<00:00, 21.54it/s, loss=0.588, v_num=s46x, BTC_val_\u001b[A\n","Epoch 6:  99%|▉| 79/80 [00:03<00:00, 20.30it/s, loss=0.567, v_num=s46x, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:03<00:00, 20.20it/s, loss=0.567, v_num=s46x, BTC_val_\u001b[A\n","Epoch 7:  99%|▉| 79/80 [00:04<00:00, 18.77it/s, loss=0.583, v_num=s46x, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:04<00:00, 18.68it/s, loss=0.583, v_num=s46x, BTC_val_\u001b[A\n","Epoch 8:  99%|▉| 79/80 [00:04<00:00, 19.30it/s, loss=0.585, v_num=s46x, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:04<00:00, 19.21it/s, loss=0.585, v_num=s46x, BTC_val_\u001b[A\n","Epoch 9:  99%|▉| 79/80 [00:04<00:00, 16.23it/s, loss=0.577, v_num=s46x, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:04<00:00, 16.23it/s, loss=0.577, v_num=s46x, BTC_val_\u001b[A\n","Epoch 10:  99%|▉| 79/80 [00:03<00:00, 20.56it/s, loss=0.574, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:03<00:00, 20.47it/s, loss=0.574, v_num=s46x, BTC_val\u001b[A\n","Epoch 11:  99%|▉| 79/80 [00:03<00:00, 21.50it/s, loss=0.564, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:03<00:00, 21.36it/s, loss=0.564, v_num=s46x, BTC_val\u001b[A\n","Epoch 12:  99%|▉| 79/80 [00:04<00:00, 16.38it/s, loss=0.594, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:04<00:00, 16.36it/s, loss=0.594, v_num=s46x, BTC_val\u001b[A\n","Epoch 13:  99%|▉| 79/80 [00:05<00:00, 14.56it/s, loss=0.561, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:05<00:00, 14.56it/s, loss=0.561, v_num=s46x, BTC_val\u001b[A\n","Epoch 14:  99%|▉| 79/80 [00:04<00:00, 18.47it/s, loss=0.554, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:04<00:00, 18.43it/s, loss=0.554, v_num=s46x, BTC_val\u001b[A\n","Epoch 15:  99%|▉| 79/80 [00:03<00:00, 21.01it/s, loss=0.589, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:03<00:00, 20.92it/s, loss=0.589, v_num=s46x, BTC_val\u001b[A\n","Epoch 16:  99%|▉| 79/80 [00:03<00:00, 21.27it/s, loss=0.544, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:03<00:00, 21.14it/s, loss=0.544, v_num=s46x, BTC_val\u001b[A\n","Epoch 17:  99%|▉| 79/80 [00:03<00:00, 20.16it/s, loss=0.588, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:03<00:00, 20.10it/s, loss=0.588, v_num=s46x, BTC_val\u001b[A\n","Epoch 18:  99%|▉| 79/80 [00:03<00:00, 20.73it/s, loss=0.573, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:03<00:00, 20.63it/s, loss=0.573, v_num=s46x, BTC_val\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.476\n","Epoch 19:  99%|▉| 79/80 [00:03<00:00, 20.30it/s, loss=0.563, v_num=s46x, BTC_val\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:03<00:00, 20.20it/s, loss=0.563, v_num=s46x, BTC_val\u001b[A\n","Epoch 20:  99%|▉| 79/80 [00:03<00:00, 20.80it/s, loss=0.56, v_num=s46x, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:03<00:00, 20.72it/s, loss=0.56, v_num=s46x, BTC_val_\u001b[A\n","Epoch 21:  99%|▉| 79/80 [00:03<00:00, 21.02it/s, loss=0.571, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:03<00:00, 20.87it/s, loss=0.571, v_num=s46x, BTC_val\u001b[A\n","Epoch 22:  99%|▉| 79/80 [00:03<00:00, 21.26it/s, loss=0.569, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:03<00:00, 21.10it/s, loss=0.569, v_num=s46x, BTC_val\u001b[A\n","Epoch 23:  99%|▉| 79/80 [00:04<00:00, 17.25it/s, loss=0.547, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 80/80 [00:04<00:00, 17.21it/s, loss=0.547, v_num=s46x, BTC_val\u001b[A\n","Epoch 24:  99%|▉| 79/80 [00:04<00:00, 17.67it/s, loss=0.557, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 80/80 [00:04<00:00, 17.62it/s, loss=0.557, v_num=s46x, BTC_val\u001b[A\n","Epoch 25:  99%|▉| 79/80 [00:03<00:00, 20.04it/s, loss=0.583, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 80/80 [00:04<00:00, 19.95it/s, loss=0.583, v_num=s46x, BTC_val\u001b[A\n","Epoch 26:  99%|▉| 79/80 [00:03<00:00, 20.59it/s, loss=0.627, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 80/80 [00:03<00:00, 20.51it/s, loss=0.627, v_num=s46x, BTC_val\u001b[A\n","Epoch 27:  99%|▉| 79/80 [00:04<00:00, 19.37it/s, loss=0.543, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 80/80 [00:04<00:00, 19.29it/s, loss=0.543, v_num=s46x, BTC_val\u001b[A\n","Epoch 28:  99%|▉| 79/80 [00:03<00:00, 20.91it/s, loss=0.579, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 80/80 [00:03<00:00, 20.81it/s, loss=0.579, v_num=s46x, BTC_val\u001b[A\n","Epoch 29:  99%|▉| 79/80 [00:03<00:00, 20.68it/s, loss=0.514, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 80/80 [00:03<00:00, 20.55it/s, loss=0.514, v_num=s46x, BTC_val\u001b[A\n","Epoch 30:  99%|▉| 79/80 [00:03<00:00, 20.49it/s, loss=0.61, v_num=s46x, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 80/80 [00:03<00:00, 20.38it/s, loss=0.61, v_num=s46x, BTC_val_\u001b[A\n","Epoch 31:  99%|▉| 79/80 [00:03<00:00, 20.65it/s, loss=0.546, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 80/80 [00:03<00:00, 20.58it/s, loss=0.546, v_num=s46x, BTC_val\u001b[A\n","Epoch 32:  99%|▉| 79/80 [00:03<00:00, 20.59it/s, loss=0.57, v_num=s46x, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 80/80 [00:03<00:00, 20.49it/s, loss=0.57, v_num=s46x, BTC_val_\u001b[A\n","Epoch 33:  99%|▉| 79/80 [00:03<00:00, 20.59it/s, loss=0.561, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 33: 100%|█| 80/80 [00:03<00:00, 20.50it/s, loss=0.561, v_num=s46x, BTC_val\u001b[A\n","Epoch 34:  99%|▉| 79/80 [00:04<00:00, 19.12it/s, loss=0.537, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 80/80 [00:04<00:00, 19.04it/s, loss=0.537, v_num=s46x, BTC_val\u001b[A\n","Epoch 35:  99%|▉| 79/80 [00:04<00:00, 17.79it/s, loss=0.53, v_num=s46x, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 35: 100%|█| 80/80 [00:04<00:00, 17.69it/s, loss=0.53, v_num=s46x, BTC_val_\u001b[A\n","Epoch 36:  99%|▉| 79/80 [00:04<00:00, 16.80it/s, loss=0.551, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 36: 100%|█| 80/80 [00:04<00:00, 16.74it/s, loss=0.551, v_num=s46x, BTC_val\u001b[A\n","Epoch 37:  99%|▉| 79/80 [00:04<00:00, 17.94it/s, loss=0.518, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 37: 100%|█| 80/80 [00:04<00:00, 17.88it/s, loss=0.518, v_num=s46x, BTC_val\u001b[A\n","Epoch 38:  99%|▉| 79/80 [00:04<00:00, 16.01it/s, loss=0.548, v_num=s46x, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 38: 100%|█| 80/80 [00:05<00:00, 15.97it/s, loss=0.548, v_num=s46x, BTC_valMonitored metric val_loss did not improve in the last 20 records. Best score: 0.476. Signaling Trainer to stop.\n","\n","Epoch 38: 100%|█| 80/80 [00:05<00:00, 15.95it/s, loss=0.548, v_num=s46x, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 40.45it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.6774193644523621,\n"," 'BTC_test_f1': 0.6566065549850464,\n"," 'ETH_test_acc': 0.7096773982048035,\n"," 'ETH_test_f1': 0.7084689140319824,\n"," 'test_loss': 0.5707361102104187}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 76455\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021415-3i7gs46x/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021415-3i7gs46x/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.6875\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.67611\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.6875\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.65368\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.58472\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 38\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3081\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 170\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621466225\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 139\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.73793\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.72296\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.74584\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.7306\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.52697\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.66667\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.64935\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.88889\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.88889\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.50989\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.67742\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.65661\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.70968\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.70847\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.57074\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▅▅▆▅▄▆█▅▃▄▃▆▄▃▄▄▃▂▃▅▄▆▄▃▃▆▃▂▅▆▅▃▅▅▅▅▄▇▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▅▅▇▅▅▆█▅▄▅▃▆▅▃▅▅▄▃▄▅▅▆▅▄▄▆▄▃▅▇▅▄▅▅▅▅▅▇▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▅▅▇▅▅▆▇▇▅▄▃▅▅▅▄█▄▇▄▅▆▆▅▆▅█▅▆▅▆▇▄▅▅▇▃▅▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▅▅▇▅▅▆▇▇▄▄▃▅▅▅▄█▄▇▄▄▆▆▅▆▄█▅▆▅▆▇▄▅▅▆▃▅▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▆▃▂▅▅▂▁▂▆▆▆▃▅▇▅▄▇▄▆▄▄▄▆▅▃▁▃█▆▁▂▆▄▃▂▅▃▂▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▅▆▇▇▆▇▇▇▇▇▇▇▇██▇█▇██▇▇▇▇████▇▇████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▅▆▇▇▆▇▇▇▇▇▇▇▇██▇█▇██▇▇▇▇██▇█▇▇████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▅▆▆▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇█▇▇▇███▇████▇▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▅▆▇▇▇▇▇▇█▇▇▇▇█▇███▇▇█▇▇████▇▇███▇█▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▅█▅██▅████▅▅█▅███▅▅████▅█████▅▅▅▅▅█▁▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▄█▄██▅████▅▅█▅███▄▄████▄█████▄▄▄▄▄█▁▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▃▆▅▆▆▆▁▆▃▃▁▁▁▁▁▁▁▁▆▆▁▃▃▃█▃▅▃▃▃▃▁▃▅▅▁▁▃▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▃▆▅▆▆▆▁▆▃▃▁▁▁▁▁▁▁▁▆▆▁▃▃▃█▃▅▃▃▃▃▁▃▅▅▁▁▃▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▃▃▁▃▂▃▃▄▂▂▄▅▃▆▂▂▃▁▃▃▂▃▁▂▃▄▃▃▃▃▃▃▃▂▇▃▄▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3i7gs46x\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:17:20.369594: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm__binary_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/354h8i92\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021718-354h8i92\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 130   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 80/81 [00:03<00:00, 22.19it/s, loss=0.703, v_num=8i92, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.679\n","Epoch 0: 100%|█| 81/81 [00:03<00:00, 22.00it/s, loss=0.703, v_num=8i92, BTC_val_\n","Epoch 1: 100%|█| 81/81 [00:04<00:00, 17.35it/s, loss=0.699, v_num=8i92, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 81/81 [00:04<00:00, 17.28it/s, loss=0.699, v_num=8i92, BTC_val_\u001b[A\n","Epoch 2: 100%|█| 81/81 [00:04<00:00, 18.28it/s, loss=0.698, v_num=8i92, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 81/81 [00:04<00:00, 18.20it/s, loss=0.698, v_num=8i92, BTC_val_\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:04<00:00, 16.92it/s, loss=0.694, v_num=8i92, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:04<00:00, 16.84it/s, loss=0.694, v_num=8i92, BTC_val_\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:03<00:00, 20.32it/s, loss=0.697, v_num=8i92, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:04<00:00, 20.20it/s, loss=0.697, v_num=8i92, BTC_val_\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:04<00:00, 17.41it/s, loss=0.692, v_num=8i92, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:04<00:00, 17.34it/s, loss=0.692, v_num=8i92, BTC_val_\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:04<00:00, 18.16it/s, loss=0.695, v_num=8i92, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:04<00:00, 18.08it/s, loss=0.695, v_num=8i92, BTC_val_\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:05<00:00, 13.88it/s, loss=0.693, v_num=8i92, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:05<00:00, 13.83it/s, loss=0.693, v_num=8i92, BTC_val_\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 8: 100%|█| 81/81 [00:04<00:00, 17.77it/s, loss=0.696, v_num=8i92, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:04<00:00, 17.68it/s, loss=0.696, v_num=8i92, BTC_val_\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:04<00:00, 16.22it/s, loss=0.694, v_num=8i92, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:05<00:00, 16.16it/s, loss=0.694, v_num=8i92, BTC_val_\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:04<00:00, 16.23it/s, loss=0.691, v_num=8i92, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:05<00:00, 16.15it/s, loss=0.691, v_num=8i92, BTC_val\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:04<00:00, 18.46it/s, loss=0.692, v_num=8i92, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:04<00:00, 18.36it/s, loss=0.692, v_num=8i92, BTC_val\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:04<00:00, 16.64it/s, loss=0.697, v_num=8i92, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:04<00:00, 16.57it/s, loss=0.697, v_num=8i92, BTC_val\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:03<00:00, 20.85it/s, loss=0.695, v_num=8i92, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:03<00:00, 20.74it/s, loss=0.695, v_num=8i92, BTC_val\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:04<00:00, 19.87it/s, loss=0.698, v_num=8i92, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:04<00:00, 19.77it/s, loss=0.698, v_num=8i92, BTC_val\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:04<00:00, 18.57it/s, loss=0.696, v_num=8i92, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:04<00:00, 18.48it/s, loss=0.696, v_num=8i92, BTC_val\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:04<00:00, 18.14it/s, loss=0.69, v_num=8i92, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:04<00:00, 18.05it/s, loss=0.69, v_num=8i92, BTC_val_\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:04<00:00, 17.75it/s, loss=0.693, v_num=8i92, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:04<00:00, 17.65it/s, loss=0.693, v_num=8i92, BTC_val\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:04<00:00, 18.39it/s, loss=0.695, v_num=8i92, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:04<00:00, 18.30it/s, loss=0.695, v_num=8i92, BTC_val\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:04<00:00, 19.20it/s, loss=0.691, v_num=8i92, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:04<00:00, 19.11it/s, loss=0.691, v_num=8i92, BTC_val\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:04<00:00, 16.35it/s, loss=0.682, v_num=8i92, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:04<00:00, 16.27it/s, loss=0.682, v_num=8i92, BTC_val\u001b[A\n","                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.679. Signaling Trainer to stop.\n","Epoch 20: 100%|█| 81/81 [00:04<00:00, 16.21it/s, loss=0.682, v_num=8i92, BTC_val\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 41.77it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.4193548262119293,\n"," 'BTC_test_f1': 0.29533159732818604,\n"," 'ETH_test_acc': 0.6129032373428345,\n"," 'ETH_test_f1': 0.3793548345565796,\n"," 'test_loss': 0.6934098601341248}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 76857\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021718-354h8i92/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021718-354h8i92/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.46667\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.56364\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.68429\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1680\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 105\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621466343\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 75\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.53664\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.45362\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.5264\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.45228\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69115\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.55556\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.35714\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.88889\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.47059\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.67809\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.41935\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.29533\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.37935\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.69341\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▃▆▇▄▄▅▃▁▃▅▄▅▃▄▂▃▂▄▂▆▅▆▅▃▅▄▄▄▄▂▅█▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▃▇█▅▅▆▄▁▂▆▅▆▂▄▂▄▃▄▂▄▆▆▆▃▄▄▄▃▃▂▅▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▆▅▆▅▄▃▆▄▃▆▃▅▆▄▃▆▄▆▃▅▇▃▆▁▅▂▄▄▇▄█▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step █▆█▆▃▄█▃▃█▄▆▄▅▃▇▅█▄▄█▄▇▁▄▂▄▄▅▃▅▄▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▁▃▂▃▃▃▃█▅▃▃▂▂▃▃▂▃▂▄▂▂▃▂▅▃▃▂▃▂▃▂▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▃▁▃▅▂▃▇▅▃▄▆▆▇▃▆▆▇█▄██\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch █▅▅█▆▆▆█▄▅▇▄▄▇▄▄▇▅▁▅▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▆▁▂▂▄▁▆▆▄▇▃█▇▄▄▂▅▆▃▃█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch █▅▆▆▇▅▆▇▅▅▅▅▄▆▃▃▅▃▁▃▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ▆█▅▃▃▃▂▂▃▂▂▂▂▂▂▂▁▁▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc █▁▄▄▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▄█\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 █▁▅▅▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅█\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc █▂▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁█\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 █▃▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁█\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▄██▆▇█▂▆▆▄▅▆▅▆▅▄▆▆▅▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/354h8i92\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:19:24.416353: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm__multi_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1o2nrlyp\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021922-1o2nrlyp\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 195   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0: 100%|█| 80/80 [00:03<00:00, 21.85it/s, loss=1.05, v_num=rlyp, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.986\n","Epoch 0: 100%|█| 80/80 [00:03<00:00, 21.71it/s, loss=1.05, v_num=rlyp, BTC_val_a\n","Epoch 1:  99%|▉| 79/80 [00:03<00:00, 20.65it/s, loss=1.02, v_num=rlyp, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 80/80 [00:03<00:00, 20.51it/s, loss=1.02, v_num=rlyp, BTC_val_a\u001b[A\n","Metric val_loss improved by 0.038 >= min_delta = 0.003. New best score: 0.948\n","Epoch 2:  99%|▉| 79/80 [00:03<00:00, 21.51it/s, loss=1.05, v_num=rlyp, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 80/80 [00:03<00:00, 21.34it/s, loss=1.05, v_num=rlyp, BTC_val_a\u001b[A\n","Epoch 3:  99%|▉| 79/80 [00:03<00:00, 21.99it/s, loss=0.994, v_num=rlyp, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 80/80 [00:03<00:00, 21.86it/s, loss=0.994, v_num=rlyp, BTC_val_\u001b[A\n","Metric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 0.937\n","Epoch 4:  99%|▉| 79/80 [00:04<00:00, 18.25it/s, loss=1, v_num=rlyp, BTC_val_acc=\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 80/80 [00:04<00:00, 18.16it/s, loss=1, v_num=rlyp, BTC_val_acc=\u001b[A\n","Epoch 5:  99%|▉| 79/80 [00:03<00:00, 20.47it/s, loss=1.07, v_num=rlyp, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 80/80 [00:03<00:00, 20.34it/s, loss=1.07, v_num=rlyp, BTC_val_a\u001b[A\n","Epoch 6:  99%|▉| 79/80 [00:03<00:00, 20.77it/s, loss=1.03, v_num=rlyp, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 80/80 [00:03<00:00, 20.64it/s, loss=1.03, v_num=rlyp, BTC_val_a\u001b[A\n","Epoch 7:  99%|▉| 79/80 [00:04<00:00, 17.63it/s, loss=1.03, v_num=rlyp, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 80/80 [00:04<00:00, 17.56it/s, loss=1.03, v_num=rlyp, BTC_val_a\u001b[A\n","Epoch 8:  99%|▉| 79/80 [00:03<00:00, 20.25it/s, loss=1.02, v_num=rlyp, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 80/80 [00:03<00:00, 20.08it/s, loss=1.02, v_num=rlyp, BTC_val_a\u001b[A\n","Epoch 9:  99%|▉| 79/80 [00:04<00:00, 16.64it/s, loss=1.04, v_num=rlyp, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 80/80 [00:04<00:00, 16.62it/s, loss=1.04, v_num=rlyp, BTC_val_a\u001b[A\n","Epoch 10:  99%|▉| 79/80 [00:04<00:00, 19.73it/s, loss=1.02, v_num=rlyp, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 80/80 [00:04<00:00, 19.65it/s, loss=1.02, v_num=rlyp, BTC_val_\u001b[A\n","Epoch 11:  99%|▉| 79/80 [00:04<00:00, 16.51it/s, loss=1.05, v_num=rlyp, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 80/80 [00:04<00:00, 16.44it/s, loss=1.05, v_num=rlyp, BTC_val_\u001b[A\n","Epoch 12:  99%|▉| 79/80 [00:03<00:00, 19.97it/s, loss=1.04, v_num=rlyp, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 80/80 [00:04<00:00, 19.80it/s, loss=1.04, v_num=rlyp, BTC_val_\u001b[A\n","Epoch 13:  99%|▉| 79/80 [00:03<00:00, 19.88it/s, loss=1.03, v_num=rlyp, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 80/80 [00:04<00:00, 19.76it/s, loss=1.03, v_num=rlyp, BTC_val_\u001b[A\n","Epoch 14:  99%|▉| 79/80 [00:04<00:00, 16.79it/s, loss=1.06, v_num=rlyp, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 80/80 [00:04<00:00, 16.77it/s, loss=1.06, v_num=rlyp, BTC_val_\u001b[A\n","Epoch 15:  99%|▉| 79/80 [00:03<00:00, 19.77it/s, loss=1.03, v_num=rlyp, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 80/80 [00:04<00:00, 19.68it/s, loss=1.03, v_num=rlyp, BTC_val_\u001b[A\n","Epoch 16:  99%|▉| 79/80 [00:04<00:00, 19.21it/s, loss=1.03, v_num=rlyp, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 80/80 [00:04<00:00, 19.15it/s, loss=1.03, v_num=rlyp, BTC_val_\u001b[A\n","Epoch 17:  99%|▉| 79/80 [00:03<00:00, 20.21it/s, loss=1.02, v_num=rlyp, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 80/80 [00:03<00:00, 20.11it/s, loss=1.02, v_num=rlyp, BTC_val_\u001b[A\n","Epoch 18:  99%|▉| 79/80 [00:05<00:00, 15.58it/s, loss=0.989, v_num=rlyp, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 80/80 [00:05<00:00, 15.54it/s, loss=0.989, v_num=rlyp, BTC_val\u001b[A\n","Epoch 19:  99%|▉| 79/80 [00:04<00:00, 17.77it/s, loss=0.967, v_num=rlyp, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 80/80 [00:04<00:00, 17.75it/s, loss=0.967, v_num=rlyp, BTC_val\u001b[A\n","Epoch 20:  99%|▉| 79/80 [00:04<00:00, 18.51it/s, loss=0.958, v_num=rlyp, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 80/80 [00:04<00:00, 18.46it/s, loss=0.958, v_num=rlyp, BTC_val\u001b[A\n","Epoch 21:  99%|▉| 79/80 [00:04<00:00, 17.40it/s, loss=0.97, v_num=rlyp, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 80/80 [00:04<00:00, 17.33it/s, loss=0.97, v_num=rlyp, BTC_val_\u001b[A\n","Epoch 22:  99%|▉| 79/80 [00:03<00:00, 20.46it/s, loss=0.963, v_num=rlyp, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 80/80 [00:03<00:00, 20.33it/s, loss=0.963, v_num=rlyp, BTC_val\u001b[A\n","Epoch 23:  99%|▉| 79/80 [00:04<00:00, 18.43it/s, loss=0.952, v_num=rlyp, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.937. Signaling Trainer to stop.\n","Epoch 23: 100%|█| 80/80 [00:04<00:00, 18.35it/s, loss=0.952, v_num=rlyp, BTC_val\n","Epoch 23: 100%|█| 80/80 [00:04<00:00, 18.33it/s, loss=0.952, v_num=rlyp, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 37.37it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.5483871102333069,\n"," 'BTC_test_f1': 0.23566308617591858,\n"," 'ETH_test_acc': 0.4838709533214569,\n"," 'ETH_test_f1': 0.2169238030910492,\n"," 'test_loss': 1.00882887840271}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 77173\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021922-1o2nrlyp/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_021922-1o2nrlyp/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.30736\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.99903\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 23\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1896\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 111\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621466473\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 85\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.4996\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.32438\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.48852\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.32182\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.94819\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.20513\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.03617\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.54839\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.23566\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.48387\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.21692\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.00883\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▅█▅▅▆█▂█▃▆▆▅▇▂▇▅▆▅▅▅▆▄▆▆▅▅▆▆▁▅▅▅▆█▇▇▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▄▅▄▃▆▅▂▅▂▄▄▃▄▂▄▄▄▄▃▄▄▃▄▄▄▄▄▄▁▄▄▃▄▅██▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▅▆▆▃▁▆▃▆▃▇▆▃█▄▅▅▆▅▆▅▅▂▆▃▅▅▆▄▅▄▄▃▆▃▆▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▆▄▅▃▁▄▂▄▃▅▄▂█▃▄▄▄▄▄▄▄▂▄▃▄▄▄▃▄▃▃▂▆▃█▇▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅▃▄▆▆▄▇▂▆▃▇▇▂▆▄▆▅▆▅▆▅█▅▄▅▅▅▅▅▃▄▆▄▁▂▁▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▄▇▆▇▇▆▇▆▇▇▇▇▇▇▇▇▇▇█▅▇█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▄▂▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▃▄▄█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▆▆█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▃▃▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▃▄▄█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▄▂▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ███████████████████▁██▁█\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▃█▁▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc █████████████████████▁▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 █████████████████████▁▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄▂▃▁▂▄▃▃▂▃▃▃▃▃▃▂▂▂▁▆▄█▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1o2nrlyp\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:21:28.408666: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm__multi_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3rnafum3\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022126-3rnafum3\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 195   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:   0%|                                           | 0/81 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0: 100%|█| 81/81 [00:03<00:00, 22.26it/s, loss=1.12, v_num=fum3, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.125\n","Epoch 0: 100%|█| 81/81 [00:03<00:00, 22.12it/s, loss=1.12, v_num=fum3, BTC_val_a\n","Epoch 1: 100%|█| 81/81 [00:04<00:00, 17.87it/s, loss=1.11, v_num=fum3, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.049 >= min_delta = 0.003. New best score: 1.077\n","Epoch 1: 100%|█| 81/81 [00:04<00:00, 17.80it/s, loss=1.11, v_num=fum3, BTC_val_a\n","Epoch 2: 100%|█| 81/81 [00:04<00:00, 17.23it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 81/81 [00:04<00:00, 17.03it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:04<00:00, 19.41it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 81/81 [00:04<00:00, 19.30it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:03<00:00, 21.29it/s, loss=1.11, v_num=fum3, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 81/81 [00:03<00:00, 21.17it/s, loss=1.11, v_num=fum3, BTC_val_a\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:04<00:00, 19.26it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 81/81 [00:04<00:00, 19.15it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:04<00:00, 16.36it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 81/81 [00:04<00:00, 16.29it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:03<00:00, 20.29it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 81/81 [00:04<00:00, 20.18it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:03<00:00, 21.15it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 81/81 [00:03<00:00, 21.05it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:03<00:00, 21.86it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 81/81 [00:03<00:00, 21.72it/s, loss=1.1, v_num=fum3, BTC_val_ac\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:04<00:00, 17.14it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 81/81 [00:04<00:00, 17.06it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:04<00:00, 17.30it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 81/81 [00:04<00:00, 17.19it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:04<00:00, 16.60it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 81/81 [00:04<00:00, 16.53it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:04<00:00, 18.55it/s, loss=1.09, v_num=fum3, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 81/81 [00:04<00:00, 18.47it/s, loss=1.09, v_num=fum3, BTC_val_\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:04<00:00, 18.86it/s, loss=1.11, v_num=fum3, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 81/81 [00:04<00:00, 18.76it/s, loss=1.11, v_num=fum3, BTC_val_\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:04<00:00, 19.31it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 81/81 [00:04<00:00, 19.20it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:05<00:00, 16.01it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 81/81 [00:05<00:00, 15.94it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:05<00:00, 15.99it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 81/81 [00:05<00:00, 15.86it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:04<00:00, 18.90it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 81/81 [00:04<00:00, 18.80it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:04<00:00, 17.94it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 81/81 [00:04<00:00, 17.86it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:04<00:00, 18.84it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 81/81 [00:04<00:00, 18.75it/s, loss=1.1, v_num=fum3, BTC_val_a\u001b[A\n","Epoch 21: 100%|█| 81/81 [00:04<00:00, 19.53it/s, loss=1.09, v_num=fum3, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 81/81 [00:04<00:00, 19.43it/s, loss=1.09, v_num=fum3, BTC_val_\u001b[A\n","Monitored metric val_loss did not improve in the last 20 records. Best score: 1.077. Signaling Trainer to stop.\n","Epoch 21: 100%|█| 81/81 [00:04<00:00, 19.41it/s, loss=1.09, v_num=fum3, BTC_val_\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 47.54it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.25806450843811035,\n"," 'BTC_test_f1': 0.1367289274930954,\n"," 'ETH_test_acc': 0.4193548262119293,\n"," 'ETH_test_f1': 0.19648092985153198,\n"," 'test_loss': 1.105910301208496}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 77464\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022126-3rnafum3/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022126-3rnafum3/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.25\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.125\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.11429\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.1257\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 21\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1760\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 106\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621466592\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 79\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.3751\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.27507\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.35382\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.26463\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09458\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.2381\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.08587\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.25806\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.13673\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.41935\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19648\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.10591\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▅▄▆▄▃▆▅▅▃▄▅▅▅▄▅█▃▅▄▄▄▅▅▁▄▃▅▄▆▄▄▁▅▅▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▄▄▇▄▄▇▄▅▄▃▆▆▅▄▅█▃▅▅▃▄▅▅▁▃▄▅▄▄▅▄▁▃▅▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▄▄▅▄▅▄▄▃▅▄▃▆▃▄▄█▄▄▃▂▅▃▄▁▆▆▄▄▅▄▄▅▃▆▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▄▅▆▅▇▅▅▃▆▄▄█▄▅▃█▃▅▃▂▆▄▄▁▆▇▃▅▄▄▄▄▃▇▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▄▄▂▃▅▄▃▅▃▅▃▂▄▄▄▁▄▃▅▄▄▄▃█▃▃▃▄▃▄▃▅▄▃▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▂▁▄▄▄▂▃▄▅▃▅█▆▃▄▅▆▅▄▄█▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▇▄██▆▇▅▅█▂▇▅▁▃▃▃▄▃▁▁▃▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▆█▆▃▄▅▄▂▅▄▅▃▆▅▆▆▇▅▇▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▅▅█▆▅▇▅▅▅▃▅▃▁▄▃▄▃▄▂▄▂▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▃▄▃▂▃▃▃▃▂▁▂▂▂▁▁▂▂▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁█████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁█████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁█████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁█████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▁▅▅▄▄▃▅▄▅▃▃▃▄▃▄▄▃▃▃▃▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3rnafum3\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:23:27.637485: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/vzwwig1b\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022326-vzwwig1b\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 130   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.7 K    Trainable params\n","0         Non-trainable params\n","75.7 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:   0%|                                           | 0/73 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 72/73 [00:02<00:00, 29.16it/s, loss=0.633, v_num=ig1b, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.625\n","Epoch 0: 100%|█| 73/73 [00:02<00:00, 28.53it/s, loss=0.633, v_num=ig1b, BTC_val_\n","Epoch 1:  99%|▉| 72/73 [00:02<00:00, 31.66it/s, loss=0.599, v_num=ig1b, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 73/73 [00:02<00:00, 31.14it/s, loss=0.599, v_num=ig1b, BTC_val_\u001b[A\n","Epoch 2:  99%|▉| 72/73 [00:02<00:00, 32.19it/s, loss=0.623, v_num=ig1b, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 73/73 [00:02<00:00, 31.37it/s, loss=0.623, v_num=ig1b, BTC_val_\u001b[A\n","Epoch 3:  99%|▉| 72/73 [00:02<00:00, 31.56it/s, loss=0.592, v_num=ig1b, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.036 >= min_delta = 0.003. New best score: 0.589\n","Epoch 3: 100%|█| 73/73 [00:02<00:00, 31.05it/s, loss=0.592, v_num=ig1b, BTC_val_\n","Epoch 4:  99%|▉| 72/73 [00:02<00:00, 27.68it/s, loss=0.549, v_num=ig1b, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 73/73 [00:03<00:00, 24.12it/s, loss=0.549, v_num=ig1b, BTC_val_\u001b[A\n","Epoch 5:  99%|▉| 72/73 [00:02<00:00, 28.12it/s, loss=0.592, v_num=ig1b, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 73/73 [00:02<00:00, 27.58it/s, loss=0.592, v_num=ig1b, BTC_val_\u001b[A\n","Epoch 6:  99%|▉| 72/73 [00:02<00:00, 28.00it/s, loss=0.586, v_num=ig1b, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 73/73 [00:02<00:00, 27.43it/s, loss=0.586, v_num=ig1b, BTC_val_\u001b[A\n","Epoch 7:  99%|▉| 72/73 [00:02<00:00, 26.55it/s, loss=0.584, v_num=ig1b, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 73/73 [00:02<00:00, 26.15it/s, loss=0.584, v_num=ig1b, BTC_val_\u001b[A\n","Epoch 8:  99%|▉| 72/73 [00:02<00:00, 32.17it/s, loss=0.562, v_num=ig1b, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.583\n","Epoch 8: 100%|█| 73/73 [00:02<00:00, 31.60it/s, loss=0.562, v_num=ig1b, BTC_val_\n","Epoch 9:  99%|▉| 72/73 [00:02<00:00, 28.62it/s, loss=0.576, v_num=ig1b, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 73/73 [00:02<00:00, 28.12it/s, loss=0.576, v_num=ig1b, BTC_val_\u001b[A\n","Epoch 10:  99%|▉| 72/73 [00:02<00:00, 26.14it/s, loss=0.616, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 73/73 [00:02<00:00, 25.65it/s, loss=0.616, v_num=ig1b, BTC_val\u001b[A\n","Epoch 11:  99%|▉| 72/73 [00:02<00:00, 30.28it/s, loss=0.569, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 73/73 [00:02<00:00, 29.81it/s, loss=0.569, v_num=ig1b, BTC_val\u001b[A\n","Epoch 12:  99%|▉| 72/73 [00:02<00:00, 24.09it/s, loss=0.556, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 73/73 [00:03<00:00, 23.70it/s, loss=0.556, v_num=ig1b, BTC_val\u001b[A\n","Epoch 13:  99%|▉| 72/73 [00:02<00:00, 26.57it/s, loss=0.582, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 73/73 [00:02<00:00, 26.24it/s, loss=0.582, v_num=ig1b, BTC_val\u001b[A\n","Epoch 14:  99%|▉| 72/73 [00:02<00:00, 27.60it/s, loss=0.568, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 73/73 [00:02<00:00, 27.21it/s, loss=0.568, v_num=ig1b, BTC_val\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.577\n","Epoch 15:  99%|▉| 72/73 [00:02<00:00, 28.08it/s, loss=0.556, v_num=ig1b, BTC_val\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 73/73 [00:02<00:00, 27.48it/s, loss=0.556, v_num=ig1b, BTC_val\u001b[A\n","Epoch 16:  99%|▉| 72/73 [00:02<00:00, 27.05it/s, loss=0.555, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 73/73 [00:02<00:00, 26.67it/s, loss=0.555, v_num=ig1b, BTC_val\u001b[A\n","Epoch 17:  99%|▉| 72/73 [00:02<00:00, 27.22it/s, loss=0.558, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 73/73 [00:02<00:00, 26.85it/s, loss=0.558, v_num=ig1b, BTC_val\u001b[A\n","Epoch 18:  99%|▉| 72/73 [00:02<00:00, 25.07it/s, loss=0.551, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 73/73 [00:02<00:00, 24.69it/s, loss=0.551, v_num=ig1b, BTC_val\u001b[A\n","Epoch 19:  99%|▉| 72/73 [00:02<00:00, 30.66it/s, loss=0.569, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 73/73 [00:02<00:00, 30.09it/s, loss=0.569, v_num=ig1b, BTC_val\u001b[A\n","Epoch 20:  99%|▉| 72/73 [00:02<00:00, 29.09it/s, loss=0.583, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 73/73 [00:02<00:00, 28.62it/s, loss=0.583, v_num=ig1b, BTC_val\u001b[A\n","Epoch 21:  99%|▉| 72/73 [00:03<00:00, 23.11it/s, loss=0.584, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 73/73 [00:03<00:00, 22.88it/s, loss=0.584, v_num=ig1b, BTC_val\u001b[A\n","Epoch 22:  99%|▉| 72/73 [00:02<00:00, 28.68it/s, loss=0.597, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 73/73 [00:02<00:00, 28.25it/s, loss=0.597, v_num=ig1b, BTC_val\u001b[A\n","Epoch 23:  99%|▉| 72/73 [00:02<00:00, 29.44it/s, loss=0.543, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 73/73 [00:02<00:00, 28.81it/s, loss=0.543, v_num=ig1b, BTC_val\u001b[A\n","Epoch 24:  99%|▉| 72/73 [00:02<00:00, 26.76it/s, loss=0.558, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 73/73 [00:02<00:00, 26.30it/s, loss=0.558, v_num=ig1b, BTC_val\u001b[A\n","Epoch 25:  99%|▉| 72/73 [00:02<00:00, 31.59it/s, loss=0.562, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 73/73 [00:02<00:00, 30.98it/s, loss=0.562, v_num=ig1b, BTC_val\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 26:  99%|▉| 72/73 [00:02<00:00, 30.60it/s, loss=0.603, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 73/73 [00:02<00:00, 30.07it/s, loss=0.603, v_num=ig1b, BTC_val\u001b[A\n","Epoch 27:  99%|▉| 72/73 [00:02<00:00, 25.56it/s, loss=0.563, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 73/73 [00:02<00:00, 25.17it/s, loss=0.563, v_num=ig1b, BTC_val\u001b[A\n","Epoch 28:  99%|▉| 72/73 [00:02<00:00, 29.35it/s, loss=0.56, v_num=ig1b, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 73/73 [00:02<00:00, 28.88it/s, loss=0.56, v_num=ig1b, BTC_val_\u001b[A\n","Epoch 29:  99%|▉| 72/73 [00:02<00:00, 24.31it/s, loss=0.546, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 73/73 [00:03<00:00, 23.90it/s, loss=0.546, v_num=ig1b, BTC_val\u001b[A\n","Epoch 30:  99%|▉| 72/73 [00:02<00:00, 28.95it/s, loss=0.526, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 73/73 [00:02<00:00, 27.95it/s, loss=0.526, v_num=ig1b, BTC_val\u001b[A\n","Epoch 31:  99%|▉| 72/73 [00:02<00:00, 25.15it/s, loss=0.576, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 73/73 [00:02<00:00, 24.82it/s, loss=0.576, v_num=ig1b, BTC_val\u001b[A\n","Epoch 32:  99%|▉| 72/73 [00:02<00:00, 28.89it/s, loss=0.55, v_num=ig1b, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 73/73 [00:02<00:00, 28.28it/s, loss=0.55, v_num=ig1b, BTC_val_\u001b[A\n","Epoch 33:  99%|▉| 72/73 [00:02<00:00, 27.41it/s, loss=0.554, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 33: 100%|█| 73/73 [00:02<00:00, 27.07it/s, loss=0.554, v_num=ig1b, BTC_val\u001b[A\n","Epoch 34:  99%|▉| 72/73 [00:02<00:00, 28.11it/s, loss=0.567, v_num=ig1b, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.577. Signaling Trainer to stop.\n","Epoch 34: 100%|█| 73/73 [00:02<00:00, 27.67it/s, loss=0.567, v_num=ig1b, BTC_val\n","Epoch 34: 100%|█| 73/73 [00:02<00:00, 27.62it/s, loss=0.567, v_num=ig1b, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 24.81it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.7142857313156128,\n"," 'BTC_test_f1': 0.7091330289840698,\n"," 'ETH_test_acc': 0.75,\n"," 'ETH_test_f1': 0.7492997050285339,\n"," 'LTC_test_acc': 0.75,\n"," 'LTC_test_f1': 0.7469831109046936,\n"," 'test_loss': 0.5304712057113647}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 77802\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022326-vzwwig1b/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022326-vzwwig1b/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.51515\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.46667\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.5625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.51515\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.76089\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 34\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2520\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 102\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621466708\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 120\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.72585\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.70716\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.72411\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.71111\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.71453\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.69742\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.55891\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.73333\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.46667\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.61905\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.5839\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.71429\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.70913\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.7493\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.74698\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.53047\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▆▅▃▅▆▄▆█▅▃▅▅▆▇▁▃▃▆▆█▇▅▃▅▄▃▅▄▄▇▅▆▃▄▅▆▅▆▇▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▆▅▃▅▇▄▆█▅▃▄▅▆▇▁▃▃▆▆█▇▅▃▅▄▄▅▄▄▇▅▆▃▄▅▇▅▇▇▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▅▇▃▃▃▇▅█▄▆▅▇▅▇▁▃▆▇▅▆▆▅▄▅▅▄█▆██▇▄▄▄▄▆▅█▅▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▄▇▄▄▄▇▅█▄▆▅▇▅▇▁▄▆▇▅▆▆▅▄▅▅▄█▆▆█▆▄▄▄▄▆▅█▅▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step █▅▄▂▅▅▅▅▅▄▄▅▇▆▁▅▂▇▄▅▅▅▅▄▆▄▆▃█▆▅▅▆▁▅▆▄▇▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step █▅▃▂▅▅▅▅▅▄▃▅▇▆▁▅▂▇▄▅▄▅▅▄▆▄▆▃█▆▄▅▆▁▅▆▃▇▆▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▃▂▇▆▄▄▄▃▆▄▅▄▃▃█▇▆▂▄▂▄▄▇▆▄▆▄▇▅▂▃▄▃▅▅▄▅▁▂▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▅▆▅▇▆▇▆▆▇▇▆▆▇▇▇▇██▇█▇▇█▇▇█▇▇▇▆█▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▅▆▆▇▆▇▇▆▇█▆▆▇▇▇▇█▇▇▇▇▇█▇▇█▇▇▇▇▇█▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▅▆▆▆▅▆▆▇▇▇▆▇▇▇▇▇█▇▇▇▇▇██▇▇▇▇▇▇▇▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▆▆▆▇▆▇▇▆▇▇▇▇▇██▇█▇█▇▇▇██▇██▇▇██▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▆▆▇▇▆▇▇▆▇▇▇▆▇▇▇▇█▇▇▇▇▇▇█▇▇▇▇▇▇█▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▆▅▇▇▆▇▇▆▇▇▇▆▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▅▁▁▁▁▅▅▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▅▁▁▁▁▅▅▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▃▃▃▁▁▁▃▃▃▃▃▃▆▃▆██▃█████▃▃▆█▃██▆▃▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▃▃▃▁▁▁▃▃▃▃▃▃▆▃▆██▃█████▃▃▆█▃██▆▃▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▅▅▅▅▅██▅▁▅▁▅▅█▁▅▅▅▅▅▁▅▅▅▅▅▅▅▁▅▅▅▅▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▅▅▅▅▅██▅▁▅▁▅▅█▁▅▅▅▅▅▂▅▅▅▅▅▅▅▁▅▅▅▅▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅█▅▂▅▄▄▃▂▃▄▂▄▅▁▄▄▃▂▄▄▃▄▂▂▁▄▄▁▄▄▄▂▄▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/vzwwig1b\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:25:18.408546: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm_loss_weighted_binary_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/rnqvmitq\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022516-rnqvmitq\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 130   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.7 K    Trainable params\n","0         Non-trainable params\n","75.7 K    Total params\n","0.303     Total estimated model params size (MB)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 73/74 [00:02<00:00, 31.35it/s, loss=0.698, v_num=mitq, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.678\n","Epoch 0: 100%|█| 74/74 [00:02<00:00, 30.75it/s, loss=0.698, v_num=mitq, BTC_val_\n","Epoch 1:  99%|▉| 73/74 [00:02<00:00, 31.99it/s, loss=0.695, v_num=mitq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 74/74 [00:02<00:00, 31.43it/s, loss=0.695, v_num=mitq, BTC_val_\u001b[A\n","Epoch 2:  99%|▉| 73/74 [00:02<00:00, 29.35it/s, loss=0.692, v_num=mitq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 74/74 [00:02<00:00, 28.78it/s, loss=0.692, v_num=mitq, BTC_val_\u001b[A\n","Epoch 3:  99%|▉| 73/74 [00:02<00:00, 25.01it/s, loss=0.697, v_num=mitq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 74/74 [00:02<00:00, 24.72it/s, loss=0.697, v_num=mitq, BTC_val_\u001b[A\n","Epoch 4:  99%|▉| 73/74 [00:02<00:00, 32.01it/s, loss=0.695, v_num=mitq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 74/74 [00:02<00:00, 31.47it/s, loss=0.695, v_num=mitq, BTC_val_\u001b[A\n","Epoch 5:  99%|▉| 73/74 [00:02<00:00, 26.71it/s, loss=0.695, v_num=mitq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 74/74 [00:02<00:00, 26.26it/s, loss=0.695, v_num=mitq, BTC_val_\u001b[A\n","Epoch 6:  99%|▉| 73/74 [00:02<00:00, 24.90it/s, loss=0.695, v_num=mitq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 74/74 [00:03<00:00, 24.61it/s, loss=0.695, v_num=mitq, BTC_val_\u001b[A\n","Epoch 7:  99%|▉| 73/74 [00:02<00:00, 30.01it/s, loss=0.691, v_num=mitq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 74/74 [00:02<00:00, 29.41it/s, loss=0.691, v_num=mitq, BTC_val_\u001b[A\n","Epoch 8:  99%|▉| 73/74 [00:02<00:00, 26.98it/s, loss=0.695, v_num=mitq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 74/74 [00:02<00:00, 26.52it/s, loss=0.695, v_num=mitq, BTC_val_\u001b[A\n","Epoch 9:  99%|▉| 73/74 [00:02<00:00, 30.26it/s, loss=0.696, v_num=mitq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 74/74 [00:02<00:00, 29.72it/s, loss=0.696, v_num=mitq, BTC_val_\u001b[A\n","Epoch 10:  99%|▉| 73/74 [00:02<00:00, 31.17it/s, loss=0.694, v_num=mitq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 74/74 [00:02<00:00, 30.61it/s, loss=0.694, v_num=mitq, BTC_val\u001b[A\n","Epoch 11:  99%|▉| 73/74 [00:02<00:00, 30.48it/s, loss=0.693, v_num=mitq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 74/74 [00:02<00:00, 29.86it/s, loss=0.693, v_num=mitq, BTC_val\u001b[A\n","Epoch 12:  99%|▉| 73/74 [00:02<00:00, 28.67it/s, loss=0.694, v_num=mitq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 74/74 [00:02<00:00, 28.09it/s, loss=0.694, v_num=mitq, BTC_val\u001b[A\n","Epoch 13:  99%|▉| 73/74 [00:02<00:00, 25.41it/s, loss=0.692, v_num=mitq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 74/74 [00:02<00:00, 25.11it/s, loss=0.692, v_num=mitq, BTC_val\u001b[A\n","Epoch 14:  99%|▉| 73/74 [00:02<00:00, 26.92it/s, loss=0.694, v_num=mitq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 74/74 [00:02<00:00, 26.40it/s, loss=0.694, v_num=mitq, BTC_val\u001b[A\n","Epoch 15:  99%|▉| 73/74 [00:02<00:00, 29.20it/s, loss=0.693, v_num=mitq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 74/74 [00:02<00:00, 28.76it/s, loss=0.693, v_num=mitq, BTC_val\u001b[A\n","Epoch 16:  99%|▉| 73/74 [00:02<00:00, 30.85it/s, loss=0.692, v_num=mitq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 74/74 [00:02<00:00, 30.36it/s, loss=0.692, v_num=mitq, BTC_val\u001b[A\n","Epoch 17:  99%|▉| 73/74 [00:02<00:00, 24.79it/s, loss=0.693, v_num=mitq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 74/74 [00:03<00:00, 24.36it/s, loss=0.693, v_num=mitq, BTC_val\u001b[A\n","Epoch 18:  99%|▉| 73/74 [00:02<00:00, 30.23it/s, loss=0.698, v_num=mitq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 74/74 [00:02<00:00, 29.71it/s, loss=0.698, v_num=mitq, BTC_val\u001b[A\n","Epoch 19:  99%|▉| 73/74 [00:02<00:00, 26.90it/s, loss=0.693, v_num=mitq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 74/74 [00:02<00:00, 26.36it/s, loss=0.693, v_num=mitq, BTC_val\u001b[A\n","Epoch 20:  99%|▉| 73/74 [00:02<00:00, 24.64it/s, loss=0.693, v_num=mitq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 74/74 [00:03<00:00, 24.29it/s, loss=0.693, v_num=mitq, BTC_val\u001b[A\n","                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.678. Signaling Trainer to stop.\n","Epoch 20: 100%|█| 74/74 [00:03<00:00, 24.26it/s, loss=0.693, v_num=mitq, BTC_val\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 33.81it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.3928571343421936,\n"," 'BTC_test_f1': 0.2789115607738495,\n"," 'ETH_test_acc': 0.6428571343421936,\n"," 'ETH_test_f1': 0.38938775658607483,\n"," 'LTC_test_acc': 0.4285714328289032,\n"," 'LTC_test_f1': 0.293949156999588,\n"," 'test_loss': 0.7067702412605286}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 78074\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022516-rnqvmitq/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022516-rnqvmitq/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.4375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.30435\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.27273\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.73333\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.69248\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1533\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 65\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621466781\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 72\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.47359\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.43582\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.47619\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.44308\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.52554\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.48676\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69339\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.38462\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.38462\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.38462\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.69181\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.39286\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.27891\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.64286\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.38939\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.42857\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.29395\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.70677\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▅▄▅▅█▅▁▄▅▅▅▆▃▇▅▅▅▄▆▂▄▂▄▁▅▆▅▂▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▆▄▅▄█▅▂▄▅▅▅▆▃▇▅▆▅▂▆▂▃▂▄▁▅▇▅▂▅▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▅▇▅▃▇▇▅▅▇▇▅▇▆▂▅▆▆▆▅▅▅█▇▅▆▆▆▅▁▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▄▇▄▂▆▆▄▄▇▇▅▆▅▂▄▆▆▅▅▄▄█▇▅▅▆▅▄▁▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▃▅▅▅▅▄▅▄▃▅▅▅▅▄▅▅▆▅▆▅▅▅▄▃▅▅▁▇▅█\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▂▅▄▄▃▄▅▃▃▅▅▅▆▄▆▅▆▅▆▅▃▄▃▁▂▅▁▇▅█\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▇▆▅▁█▇▆▆▅▅▄▆▅▅▅▅▄▅▇▆▅▅▆▆▅▆▆▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▇▆▆▆▄▆▄▇▃▂▄▅▅█▆▅▁▇▇▅▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▇▅▅▇▂▆▄█▅▄▅▆▄█▅▅▁▆▅▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▅▇▄▅▁▆▄▃█▆▇▄▂▄▆▇▃▂▄▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▆▆▅▆▁▆▅▅█▅▆▅▂▅▅▇▃▃▃▄▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▆▆▂▅▅▃▆▆▃▅▄▆▂▃▆▄▆▃▁▆█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch █▆▃▆▃▅▇█▆▅▄▇▂▆▆▄▅▂▁█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▇▃▆▃▃▂▂▂▁▁▃▂▂▁▁▂▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ███▁████▁▁███████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ███▁████▁▁███████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ███▁████▁▁███████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ███▁████▁▁███████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▆▆▃▃▆▆▃▆▃▃▃▆▆▆▆▆▆▁▁█▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▄▄▂▂▄▄▂▄▂▂▂▄▄▄▄▄▄▁▂█▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▁▅█▅▅▆▅▇█▆▃▃▄▄▂▁▆▆▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/rnqvmitq\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:26:32.448375: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/pju93dut\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022630-pju93dut\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 195   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.8 K    Trainable params\n","0         Non-trainable params\n","75.8 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 72/73 [00:02<00:00, 30.91it/s, loss=1.11, v_num=3dut, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.092\n","Epoch 0: 100%|█| 73/73 [00:02<00:00, 30.30it/s, loss=1.11, v_num=3dut, BTC_val_a\n","Epoch 1:  99%|▉| 72/73 [00:02<00:00, 31.69it/s, loss=1.1, v_num=3dut, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 73/73 [00:02<00:00, 31.19it/s, loss=1.1, v_num=3dut, BTC_val_ac\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 1.087\n","Epoch 2:  99%|▉| 72/73 [00:02<00:00, 29.70it/s, loss=1.05, v_num=3dut, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.064 >= min_delta = 0.003. New best score: 1.023\n","Epoch 2: 100%|█| 73/73 [00:02<00:00, 29.08it/s, loss=1.05, v_num=3dut, BTC_val_a\n","Epoch 3:  99%|▉| 72/73 [00:02<00:00, 28.91it/s, loss=1.01, v_num=3dut, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 73/73 [00:02<00:00, 28.36it/s, loss=1.01, v_num=3dut, BTC_val_aMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 1.019\n","\n","Epoch 4:  99%|▉| 72/73 [00:02<00:00, 25.86it/s, loss=1, v_num=3dut, BTC_val_acc=\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.023 >= min_delta = 0.003. New best score: 0.997\n","Epoch 4: 100%|█| 73/73 [00:02<00:00, 25.54it/s, loss=1, v_num=3dut, BTC_val_acc=\n","Epoch 5:  99%|▉| 72/73 [00:02<00:00, 29.36it/s, loss=0.993, v_num=3dut, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 73/73 [00:02<00:00, 28.78it/s, loss=0.993, v_num=3dut, BTC_val_\u001b[A\n","Epoch 6:  99%|▉| 72/73 [00:03<00:00, 22.57it/s, loss=1, v_num=3dut, BTC_val_acc=\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 73/73 [00:03<00:00, 22.32it/s, loss=1, v_num=3dut, BTC_val_acc=\u001b[A\n","Epoch 7:  99%|▉| 72/73 [00:02<00:00, 30.63it/s, loss=0.99, v_num=3dut, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 73/73 [00:02<00:00, 30.00it/s, loss=0.99, v_num=3dut, BTC_val_a\u001b[A\n","Epoch 8:  99%|▉| 72/73 [00:03<00:00, 20.92it/s, loss=0.977, v_num=3dut, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 73/73 [00:03<00:00, 20.76it/s, loss=0.977, v_num=3dut, BTC_val_\u001b[A\n","Epoch 9:  99%|▉| 72/73 [00:03<00:00, 21.83it/s, loss=0.988, v_num=3dut, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.984\n","Epoch 9: 100%|█| 73/73 [00:03<00:00, 21.55it/s, loss=0.988, v_num=3dut, BTC_val_\n","Epoch 10:  99%|▉| 72/73 [00:02<00:00, 26.01it/s, loss=0.945, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 73/73 [00:02<00:00, 25.73it/s, loss=0.945, v_num=3dut, BTC_val\u001b[A\n","Epoch 11:  99%|▉| 72/73 [00:02<00:00, 28.82it/s, loss=0.953, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 73/73 [00:02<00:00, 27.78it/s, loss=0.953, v_num=3dut, BTC_val\u001b[A\n","Epoch 12:  99%|▉| 72/73 [00:03<00:00, 21.92it/s, loss=0.977, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 73/73 [00:03<00:00, 21.76it/s, loss=0.977, v_num=3dut, BTC_val\u001b[A\n","Epoch 13:  99%|▉| 72/73 [00:02<00:00, 26.06it/s, loss=0.971, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 73/73 [00:02<00:00, 25.61it/s, loss=0.971, v_num=3dut, BTC_val\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 0.973\n","Epoch 14:  99%|▉| 72/73 [00:02<00:00, 24.43it/s, loss=0.938, v_num=3dut, BTC_val\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 73/73 [00:03<00:00, 24.19it/s, loss=0.938, v_num=3dut, BTC_val\u001b[A\n","Epoch 15:  99%|▉| 72/73 [00:03<00:00, 23.25it/s, loss=0.978, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 73/73 [00:03<00:00, 22.96it/s, loss=0.978, v_num=3dut, BTC_val\u001b[A\n","Epoch 16:  99%|▉| 72/73 [00:02<00:00, 28.30it/s, loss=0.933, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 73/73 [00:02<00:00, 27.70it/s, loss=0.933, v_num=3dut, BTC_val\u001b[A\n","Epoch 17:  99%|▉| 72/73 [00:02<00:00, 27.02it/s, loss=0.955, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 73/73 [00:02<00:00, 26.56it/s, loss=0.955, v_num=3dut, BTC_val\u001b[A\n","Epoch 18:  99%|▉| 72/73 [00:02<00:00, 26.19it/s, loss=0.959, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 73/73 [00:02<00:00, 25.84it/s, loss=0.959, v_num=3dut, BTC_val\u001b[A\n","Epoch 19:  99%|▉| 72/73 [00:02<00:00, 27.66it/s, loss=0.956, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 73/73 [00:02<00:00, 27.31it/s, loss=0.956, v_num=3dut, BTC_val\u001b[A\n","Epoch 20:  99%|▉| 72/73 [00:02<00:00, 30.01it/s, loss=0.963, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 73/73 [00:02<00:00, 29.31it/s, loss=0.963, v_num=3dut, BTC_val\u001b[A\n","Epoch 21:  99%|▉| 72/73 [00:02<00:00, 29.27it/s, loss=0.953, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 73/73 [00:02<00:00, 28.66it/s, loss=0.953, v_num=3dut, BTC_val\u001b[A\n","Epoch 22:  99%|▉| 72/73 [00:03<00:00, 23.64it/s, loss=0.967, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 73/73 [00:03<00:00, 23.38it/s, loss=0.967, v_num=3dut, BTC_val\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 23:  99%|▉| 72/73 [00:03<00:00, 21.83it/s, loss=0.929, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 73/73 [00:03<00:00, 21.64it/s, loss=0.929, v_num=3dut, BTC_val\u001b[A\n","Epoch 24:  99%|▉| 72/73 [00:02<00:00, 24.93it/s, loss=0.98, v_num=3dut, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 73/73 [00:02<00:00, 24.52it/s, loss=0.98, v_num=3dut, BTC_val_\u001b[A\n","Epoch 25:  99%|▉| 72/73 [00:03<00:00, 23.45it/s, loss=0.922, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 73/73 [00:03<00:00, 23.21it/s, loss=0.922, v_num=3dut, BTC_val\u001b[A\n","Epoch 26:  99%|▉| 72/73 [00:02<00:00, 29.12it/s, loss=0.943, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 73/73 [00:02<00:00, 28.61it/s, loss=0.943, v_num=3dut, BTC_val\u001b[A\n","Epoch 27:  99%|▉| 72/73 [00:02<00:00, 27.58it/s, loss=0.963, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 73/73 [00:02<00:00, 27.09it/s, loss=0.963, v_num=3dut, BTC_val\u001b[A\n","Epoch 28:  99%|▉| 72/73 [00:02<00:00, 25.64it/s, loss=0.912, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 73/73 [00:02<00:00, 25.33it/s, loss=0.912, v_num=3dut, BTC_val\u001b[A\n","Epoch 29:  99%|▉| 72/73 [00:02<00:00, 29.88it/s, loss=0.954, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 73/73 [00:02<00:00, 29.17it/s, loss=0.954, v_num=3dut, BTC_val\u001b[A\n","Epoch 30:  99%|▉| 72/73 [00:02<00:00, 24.92it/s, loss=0.918, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 73/73 [00:02<00:00, 24.61it/s, loss=0.918, v_num=3dut, BTC_val\u001b[A\n","Epoch 31:  99%|▉| 72/73 [00:02<00:00, 26.83it/s, loss=0.941, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 73/73 [00:02<00:00, 26.44it/s, loss=0.941, v_num=3dut, BTC_val\u001b[A\n","Epoch 32:  99%|▉| 72/73 [00:02<00:00, 24.17it/s, loss=0.907, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 73/73 [00:03<00:00, 23.84it/s, loss=0.907, v_num=3dut, BTC_val\u001b[A\n","Epoch 33:  99%|▉| 72/73 [00:02<00:00, 29.01it/s, loss=0.909, v_num=3dut, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.973. Signaling Trainer to stop.\n","Epoch 33: 100%|█| 73/73 [00:02<00:00, 28.26it/s, loss=0.909, v_num=3dut, BTC_val\n","Epoch 33: 100%|█| 73/73 [00:02<00:00, 28.21it/s, loss=0.909, v_num=3dut, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 28.92it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.4642857015132904,\n"," 'BTC_test_f1': 0.41789641976356506,\n"," 'ETH_test_acc': 0.5714285969734192,\n"," 'ETH_test_f1': 0.5793651938438416,\n"," 'LTC_test_acc': 0.4642857015132904,\n"," 'LTC_test_f1': 0.4154462218284607,\n"," 'test_loss': 0.9363532662391663}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 78278\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022630-pju93dut/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022630-pju93dut/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.4375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.48571\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.63889\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.33333\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.8142\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 33\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2448\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 106\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621466896\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 116\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.50653\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.47721\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.51262\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.48927\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.50218\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.4773\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.92487\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.51111\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.25\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.24444\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.125\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.13333\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.09139\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.46429\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.4179\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.57143\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.57937\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.46429\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.41545\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.93635\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▂▄▄▄▄▅▂▆▃▁█▄▄▆▄▆▂▅▅▆▄▅▆▄▄▃▃▃▅▅▃▅▅▄▄▆▅▅▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▃▃▄▄▅▂▆▃▁█▃▄▅▄▆▁▅▄▆▄▅▅▄▄▃▂▄▄▅▃▅▅▃▄▆▄▅▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▂▂▁▅▅▅▆▂▃▃▄▅▅█▃▃▃▁▅▅▆▅▅▆▄▄▃▅▄▂▃▅▃▄█▂▄▃▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▁▃▂▄▅▅▆▂▃▃▄▅▅█▃▂▃▁▆▅▆▅▄▆▄▄▃▄▂▁▃▆▃▄█▂▄▃▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▅▆▅▃▅▁▅▅▄▅▅▇▇▅▅▅▄█▃█▅▅▅▇▂▇▃▂▆▅▄▆▆▅▃▇▅▅▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▃▆▄▃▅▁▅▃▄▃▅▆▇▅▃▄▃▇▃█▄▄▄▇▂▇▃▃▅▃▄▆▅▅▄▆▄▅▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ██▇█▅█▅▆▇▇▆▄▄▃▄▅▅▃▄▄▄▄▆▃▅▃▆█▄▅▇▅▂▇▇▁▄▄▅▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▁▄▃▅▄▆▆▆▆▆▆▆▆▆▇▇▇▆▆▆▇▇▇▇▇▇▇▇█▇▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▁▄▃▅▅▆▇▇▆▆▆▇▆▇▇▇▇▆▇▆▇▇▇▇█▇▇▇█▇█▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▂▁▃▄▆▅▆▇▆▆▇▆█▆▇█▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▃▁▄▄▆▆▆▇▆▇▇▆█▆▇▇▇▇▇▇▇▇▇█▇██▇▇▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▁▃▅▅▅▆▆▆▆▇▆▆▇▇▇▆▇▇▆▇▇▇▇▇▇▇▇▆█▇▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▂▁▄▅▅▅▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇███▇▇█▇▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ██▆▅▄▄▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc █▁▆▆▃▃▃▃▆▃▆▆▃▆█▆█▆▃▃▆▆█▆▆█▆▆▆▃▃▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▆▁▆▅▃▃▄▃▅▃▅▅▃▅█▇█▇▃▃▇▇█▅▇█▆▆▆▃▃▆▇▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ██▁▅▁▅▅▁▅▅▅▅▁█▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▁▁▁▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▇▅▁▄▁▄▄▁▅▅▅▅▁█▅▅▅▅▅▄▅▅▅▅▁▄▅▅▅▁▁▁▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▅▃▅▅█▆▃▃▃▆▃▃▅▆▃▃▅▃▆▅▃▃▃▅▃▃▃▃▅▅▆▁▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▃▂▄▄█▆▃▃▃▆▃▃▄▆▃▃▄▃▆▄▃▃▃▄▃▃▃▃▄▄▆▁▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ██▄▄▂▄▃▅▄▂▄▅▄▁▄▅▄▄▂▃▆▄▃▂▄▅▄▄▄▅▄▆▅█\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/pju93dut\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:28:26.452061: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm_loss_weighted_multi_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2mv4skdt\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022824-2mv4skdt\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 195   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.8 K    Trainable params\n","0         Non-trainable params\n","75.8 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 73/74 [00:02<00:00, 31.40it/s, loss=1.1, v_num=skdt, BTC_val_ac\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.075\n","Epoch 0: 100%|█| 74/74 [00:02<00:00, 30.62it/s, loss=1.1, v_num=skdt, BTC_val_ac\n","Epoch 1:  99%|▉| 73/74 [00:02<00:00, 32.13it/s, loss=1.11, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 74/74 [00:02<00:00, 31.58it/s, loss=1.11, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 2:  99%|▉| 73/74 [00:02<00:00, 28.78it/s, loss=1.11, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 74/74 [00:02<00:00, 28.15it/s, loss=1.11, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 3:  99%|▉| 73/74 [00:02<00:00, 29.32it/s, loss=1.11, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 74/74 [00:02<00:00, 28.68it/s, loss=1.11, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 4:  99%|▉| 73/74 [00:02<00:00, 26.63it/s, loss=1.1, v_num=skdt, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 74/74 [00:02<00:00, 26.25it/s, loss=1.1, v_num=skdt, BTC_val_ac\u001b[A\n","Epoch 5:  99%|▉| 73/74 [00:02<00:00, 30.19it/s, loss=1.11, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 74/74 [00:02<00:00, 29.50it/s, loss=1.11, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 6:  99%|▉| 73/74 [00:02<00:00, 24.43it/s, loss=1.1, v_num=skdt, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 74/74 [00:03<00:00, 24.19it/s, loss=1.1, v_num=skdt, BTC_val_ac\u001b[A\n","Epoch 7:  99%|▉| 73/74 [00:02<00:00, 29.96it/s, loss=1.1, v_num=skdt, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 74/74 [00:02<00:00, 29.37it/s, loss=1.1, v_num=skdt, BTC_val_ac\u001b[A\n","Epoch 8:  99%|▉| 73/74 [00:02<00:00, 25.27it/s, loss=1.1, v_num=skdt, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 74/74 [00:02<00:00, 24.86it/s, loss=1.1, v_num=skdt, BTC_val_ac\u001b[A\n","Epoch 9:  99%|▉| 73/74 [00:02<00:00, 27.93it/s, loss=1.1, v_num=skdt, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 74/74 [00:02<00:00, 27.54it/s, loss=1.1, v_num=skdt, BTC_val_ac\u001b[A\n","Epoch 10:  99%|▉| 73/74 [00:02<00:00, 25.78it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 74/74 [00:02<00:00, 25.34it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 11:  99%|▉| 73/74 [00:02<00:00, 30.73it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 74/74 [00:02<00:00, 30.18it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 12:  99%|▉| 73/74 [00:02<00:00, 31.00it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 74/74 [00:02<00:00, 30.48it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 13:  99%|▉| 73/74 [00:02<00:00, 30.06it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 74/74 [00:02<00:00, 29.44it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 14:  99%|▉| 73/74 [00:02<00:00, 26.17it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 74/74 [00:02<00:00, 25.70it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 15:  99%|▉| 73/74 [00:02<00:00, 27.92it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 1.068\n","Epoch 15: 100%|█| 74/74 [00:02<00:00, 27.55it/s, loss=1.1, v_num=skdt, BTC_val_a\n","Epoch 16:  99%|▉| 73/74 [00:02<00:00, 25.41it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 74/74 [00:02<00:00, 25.01it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 17:  99%|▉| 73/74 [00:02<00:00, 26.55it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 74/74 [00:02<00:00, 26.19it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 18:  99%|▉| 73/74 [00:02<00:00, 27.69it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 74/74 [00:02<00:00, 27.16it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 19:  99%|▉| 73/74 [00:02<00:00, 25.00it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 74/74 [00:02<00:00, 24.71it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 20:  99%|▉| 73/74 [00:02<00:00, 30.38it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 74/74 [00:02<00:00, 29.88it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 21:  99%|▉| 73/74 [00:02<00:00, 24.72it/s, loss=1.09, v_num=skdt, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 74/74 [00:03<00:00, 24.35it/s, loss=1.09, v_num=skdt, BTC_val_\u001b[A\n","Epoch 22:  99%|▉| 73/74 [00:02<00:00, 24.73it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 74/74 [00:03<00:00, 24.45it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 23:  99%|▉| 73/74 [00:02<00:00, 25.17it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 74/74 [00:02<00:00, 24.76it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 24:  99%|▉| 73/74 [00:02<00:00, 29.30it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 74/74 [00:02<00:00, 28.85it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 25:  99%|▉| 73/74 [00:02<00:00, 27.89it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 74/74 [00:02<00:00, 27.35it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 26:  99%|▉| 73/74 [00:03<00:00, 22.59it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 74/74 [00:03<00:00, 22.34it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 27:  99%|▉| 73/74 [00:02<00:00, 27.98it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 74/74 [00:02<00:00, 27.60it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 28:  99%|▉| 73/74 [00:03<00:00, 24.04it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 74/74 [00:03<00:00, 23.77it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 29:  99%|▉| 73/74 [00:02<00:00, 29.45it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 74/74 [00:02<00:00, 29.01it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 30:  99%|▉| 73/74 [00:02<00:00, 26.30it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 74/74 [00:02<00:00, 25.81it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 31:  99%|▉| 73/74 [00:02<00:00, 25.59it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 74/74 [00:02<00:00, 25.31it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 32:  99%|▉| 73/74 [00:02<00:00, 25.19it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 74/74 [00:03<00:00, 23.02it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 33:  99%|▉| 73/74 [00:02<00:00, 27.01it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 33: 100%|█| 74/74 [00:02<00:00, 26.63it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 34:  99%|▉| 73/74 [00:02<00:00, 26.55it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 74/74 [00:02<00:00, 26.11it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Epoch 35:  99%|▉| 73/74 [00:03<00:00, 23.49it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 35: 100%|█| 74/74 [00:03<00:00, 23.22it/s, loss=1.1, v_num=skdt, BTC_val_a\u001b[A\n","                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.068. Signaling Trainer to stop.\n","Epoch 35: 100%|█| 74/74 [00:03<00:00, 23.19it/s, loss=1.1, v_num=skdt, BTC_val_a\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 53.68it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.25,\n"," 'BTC_test_f1': 0.1315789520740509,\n"," 'ETH_test_acc': 0.4285714328289032,\n"," 'ETH_test_f1': 0.1991342157125473,\n"," 'LTC_test_acc': 0.3928571343421936,\n"," 'LTC_test_f1': 0.18594105541706085,\n"," 'test_loss': 1.1318554878234863}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 78572\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022824-2mv4skdt/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_022824-2mv4skdt/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.1875\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.15873\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.125\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.09524\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.25\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.21368\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.10967\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 35\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2628\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 110\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621467014\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 124\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.3671\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.31675\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.3368\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.29041\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.329\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.27977\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.0988\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.18182\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.18182\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.25641\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.08214\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.25\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.13158\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.42857\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19913\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.39286\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.18594\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.13186\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▄▆▄▃▅▄▃▅▄▃▄▅▃▆▁▂▅▅▂▄▇▂▆▅▅▆▅▄▄▅▂██▂▆▄▃▃▄▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▃▇▃▃▆▃▃▄▄▃▃▄▃▅▁▂▆▅▁▄▅▂▇▅▄▇▄▃▄▆▂██▁▇▄▃▃▄▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▄▆▄▅▇▇▅▅▆▅▅▅▇▂▅▄▁▄▄▅▅▅▇█▇▆▅▃█▅▅█▅▄▄▅▄▃▇▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▄▆▄▅▇▇▅▅▆▅▅▅▇▃▅▄▁▄▄▅▄▆▇█▇▆▄▃▇▅▅█▅▄▃▅▄▃▇▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▃▇▆▂▅█▃▃▃▅▅▃▅▆▆▄█▄▄▄▃▂▂▅▅▆█▄█▆▇█▁▂▃▅▂▃▇▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▂▇▆▂▅▆▂▃▂▅▄▂▄▆▄▄█▄▄▃▁▂▂▅▄▅▆▄▆▆▇▆▁▂▃▂▂▂▇▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▃▆▆▃▂▆▅▅▇▄▆▅▆▅▆▅▅▅▅▅▄▅▃▁▃▄▆▃▄▅▃▃▅▄▄▄▇▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▄▄▅▅▅▁▂▅▅▅▄▄▄▃▆▇▆▃▆▂▄▇▆█▆▅▅▆█▃▇▅▄▄▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▂▃▆▅▅▁▂▅▆▅▅▅▄▂▇▅▆▂▆▃▃▅▅▆▆▄▄▇▇▁█▄▃▅▆▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▅▃▂▅▄▁█▃▅▅█▆▅▄▁▄▅▄▇▅▇█▅▅█▅▇▄▅▃▅▅▄▆▃▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▃▂▃▅▃▃▇▄▆▄█▇▅▃▂▂▆▃█▅▄▅▃▁▆▄▃▅▂▁▆▄▃▆▃▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▂▄▇▇▅▂▂▄▃▄▂▃▂▃▁▆▇▅▄▅▄▃▅▄▅▅▇█▅▆▅▆▅█▇▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▃▇▆▄▃▃▄▄▃▃▄▃▂▂▄▆▅▅▄▂▂▃▂▄▃▅█▃▄▅▄▃▇▇▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▅▅▅▅██▅▅▅█▅▅▅████▅▅▁▅▅▁▅▅▅▅▅▅▅▅▅▅▅██\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▅▅▅▅██▅▅▅█▅▅▅████▅▅▁▅▅▁▅▅▅▅▅▅▅▅▅▅▅██\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▃▃▃███▃▃▃█▃▃▃████▃▃█▃▃▁▃▃▃▃▃▃▃▃▃▃▃██\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▄▄▄███▄▄▄█▄▄▄████▄▄█▄▄▁▄▄▄▄▄▄▄▄▄▄▄██\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▃▃▂▇▃▅▄▃▃▆▃▅▂▄▅▁▂▅▅▆▆▂▄▅▆▇▆█▆▄▅▆▅▇▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2mv4skdt\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:30:24.297349: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/385fa5zd\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_023022-385fa5zd\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 130   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 72/73 [00:05<00:00, 13.22it/s, loss=0.663, v_num=a5zd, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.781\n","Epoch 0: 100%|█| 73/73 [00:05<00:00, 13.17it/s, loss=0.663, v_num=a5zd, BTC_val_\n","Epoch 1:  99%|▉| 72/73 [00:04<00:00, 14.63it/s, loss=0.638, v_num=a5zd, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.153 >= min_delta = 0.003. New best score: 0.628\n","Epoch 1: 100%|█| 73/73 [00:04<00:00, 14.61it/s, loss=0.638, v_num=a5zd, BTC_val_\n","Epoch 2:  99%|▉| 72/73 [00:05<00:00, 13.81it/s, loss=0.587, v_num=a5zd, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 73/73 [00:05<00:00, 13.73it/s, loss=0.587, v_num=a5zd, BTC_val_\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.033 >= min_delta = 0.003. New best score: 0.595\n","Epoch 3:  99%|▉| 72/73 [00:06<00:00, 11.32it/s, loss=0.588, v_num=a5zd, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 73/73 [00:06<00:00, 11.32it/s, loss=0.588, v_num=a5zd, BTC_val_\u001b[A\n","Epoch 4:  99%|▉| 72/73 [00:05<00:00, 12.42it/s, loss=0.572, v_num=a5zd, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.589\n","Epoch 4: 100%|█| 73/73 [00:05<00:00, 12.42it/s, loss=0.572, v_num=a5zd, BTC_val_\n","Epoch 5:  99%|▉| 72/73 [00:05<00:00, 12.49it/s, loss=0.56, v_num=a5zd, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 73/73 [00:05<00:00, 12.47it/s, loss=0.56, v_num=a5zd, BTC_val_a\u001b[A\n","Epoch 6:  99%|▉| 72/73 [00:04<00:00, 14.51it/s, loss=0.543, v_num=a5zd, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 73/73 [00:05<00:00, 14.44it/s, loss=0.543, v_num=a5zd, BTC_val_\u001b[A\n","Epoch 7:  99%|▉| 72/73 [00:05<00:00, 14.19it/s, loss=0.582, v_num=a5zd, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 73/73 [00:05<00:00, 14.14it/s, loss=0.582, v_num=a5zd, BTC_val_\u001b[A\n","Epoch 8:  99%|▉| 72/73 [00:05<00:00, 14.26it/s, loss=0.599, v_num=a5zd, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 73/73 [00:05<00:00, 14.18it/s, loss=0.599, v_num=a5zd, BTC_val_\u001b[A\n","Epoch 9:  99%|▉| 72/73 [00:04<00:00, 14.46it/s, loss=0.57, v_num=a5zd, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 73/73 [00:05<00:00, 14.42it/s, loss=0.57, v_num=a5zd, BTC_val_a\u001b[A\n","Epoch 10:  99%|▉| 72/73 [00:05<00:00, 12.87it/s, loss=0.564, v_num=a5zd, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 73/73 [00:05<00:00, 12.85it/s, loss=0.564, v_num=a5zd, BTC_val\u001b[A\n","Epoch 11:  99%|▉| 72/73 [00:05<00:00, 12.50it/s, loss=0.561, v_num=a5zd, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 73/73 [00:05<00:00, 12.50it/s, loss=0.561, v_num=a5zd, BTC_val\u001b[A\n","Epoch 12:  99%|▉| 72/73 [00:05<00:00, 12.89it/s, loss=0.57, v_num=a5zd, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 73/73 [00:05<00:00, 12.85it/s, loss=0.57, v_num=a5zd, BTC_val_\u001b[A\n","Epoch 13:  99%|▉| 72/73 [00:05<00:00, 12.91it/s, loss=0.591, v_num=a5zd, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 73/73 [00:05<00:00, 12.86it/s, loss=0.591, v_num=a5zd, BTC_val\u001b[A\n","Epoch 14:  99%|▉| 72/73 [00:06<00:00, 11.86it/s, loss=0.577, v_num=a5zd, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 73/73 [00:06<00:00, 11.85it/s, loss=0.577, v_num=a5zd, BTC_val\u001b[A\n","Epoch 15:  99%|▉| 72/73 [00:05<00:00, 12.39it/s, loss=0.582, v_num=a5zd, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 73/73 [00:05<00:00, 12.35it/s, loss=0.582, v_num=a5zd, BTC_val\u001b[A\n","Epoch 16:  99%|▉| 72/73 [00:05<00:00, 12.21it/s, loss=0.58, v_num=a5zd, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 73/73 [00:05<00:00, 12.18it/s, loss=0.58, v_num=a5zd, BTC_val_\u001b[A\n","Epoch 17:  99%|▉| 72/73 [00:05<00:00, 13.40it/s, loss=0.533, v_num=a5zd, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 73/73 [00:05<00:00, 13.36it/s, loss=0.533, v_num=a5zd, BTC_val\u001b[A\n","Epoch 18:  99%|▉| 72/73 [00:05<00:00, 12.58it/s, loss=0.569, v_num=a5zd, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 73/73 [00:05<00:00, 12.51it/s, loss=0.569, v_num=a5zd, BTC_val\u001b[A\n","Epoch 19:  99%|▉| 72/73 [00:05<00:00, 12.63it/s, loss=0.574, v_num=a5zd, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 73/73 [00:05<00:00, 12.61it/s, loss=0.574, v_num=a5zd, BTC_val\u001b[A\n","Epoch 20:  99%|▉| 72/73 [00:05<00:00, 13.75it/s, loss=0.601, v_num=a5zd, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 73/73 [00:05<00:00, 13.69it/s, loss=0.601, v_num=a5zd, BTC_val\u001b[A\n","Epoch 21:  99%|▉| 72/73 [00:05<00:00, 14.15it/s, loss=0.569, v_num=a5zd, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 73/73 [00:05<00:00, 14.08it/s, loss=0.569, v_num=a5zd, BTC_val\u001b[A\n","Epoch 22:  99%|▉| 72/73 [00:05<00:00, 13.45it/s, loss=0.554, v_num=a5zd, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 73/73 [00:05<00:00, 13.41it/s, loss=0.554, v_num=a5zd, BTC_val\u001b[A\n","Epoch 23:  99%|▉| 72/73 [00:05<00:00, 13.85it/s, loss=0.549, v_num=a5zd, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 73/73 [00:05<00:00, 13.80it/s, loss=0.549, v_num=a5zd, BTC_val\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 24:  99%|▉| 72/73 [00:05<00:00, 14.11it/s, loss=0.556, v_num=a5zd, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 73/73 [00:05<00:00, 14.04it/s, loss=0.556, v_num=a5zd, BTC_val\u001b[A\n","Monitored metric val_loss did not improve in the last 20 records. Best score: 0.589. Signaling Trainer to stop.\n","Epoch 24: 100%|█| 73/73 [00:05<00:00, 14.03it/s, loss=0.556, v_num=a5zd, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 27.94it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.6785714030265808,\n"," 'BTC_test_f1': 0.6746674180030823,\n"," 'ETH_test_acc': 0.75,\n"," 'ETH_test_f1': 0.7492997050285339,\n"," 'LTC_test_acc': 0.7142857313156128,\n"," 'LTC_test_f1': 0.7079364657402039,\n"," 'test_loss': 0.5483826398849487}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 78859\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_023022-385fa5zd/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_023022-385fa5zd/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.76923\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.76923\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.84615\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.84524\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.76923\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.76364\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.38035\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 24\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1800\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 148\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621467170\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 86\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.72498\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.71365\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.73803\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.72703\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.72498\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.70898\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.55434\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.56364\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.61905\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.61905\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.6302\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.67857\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.67467\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.7493\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.71429\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.70794\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.54838\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▂▃▅▆▆▆▅▆▆▆▇▃▆▇▄▅▆▆▃▄▆▃▇█▅▆▆▅▂▁▄▄▅▃▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▃▅▆▆▆▄▆▆▆▇▃▆▇▄▅▆▆▃▄▆▃▇█▅▆▇▅▂▁▄▄▅▁▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▁▃▄█▆▆▆▆▆▆▅▆█▅▅▅▆▆▃▅▄▅▄▄▆▆▇▅▅▄▇▃▅▆▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▁▃▄█▅▇▇▇▆▅▅▆█▅▅▅▇▆▃▅▄▅▄▄▇▇▇▅▅▄▇▃▄▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▄▅▄▆▅▅▆▅▅▅▅▄▇▂▇▅▆▅▂▄▅▅▆▆▇█▇▅▅▅▅▇▆▇▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▄▅▄▆▄▆▆▆▅▄▅▄▇▂▇▅▆▆▂▄▅▆▆▆▇█▇▅▅▅▅▇▆▇▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▅▄▅▃▄▄▃▄▄▃▄▄▃▅▄▃▃▄█▄▅▂▂▃▂▁▂▅▅▅▃▄▃▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▅▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▅▇▇▇▇▇▇▇▇▇▇█▇▇▇▇█▇█▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▅▆▆▇▇▇▇▇▇▇▇███▇▇█▇▇▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▅▆▇▇▇▇▇▇▇▇▇██▇▇▇█▇█▇█▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▆▇▇▇▇▇▇█▇▇▇███▇▇█▇██████\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▆▇██▇▇▇█▇▇▇▇█▇▇▇█▇██████\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▃▆▆█▃▃▃█▃▃▃▃█▃▃▆▃▆▃▃█▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▄▆▆█▄▄▄█▄▄▄▄█▄▄▆▄▆▄▄█▄▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▆▃▃▃█▆█▃█▃▃▃▃██▃▆▃▆▆▃▆▃▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▆▄▄▄█▆█▄█▄▄▄▄██▄▆▄▆▆▄▆▄▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▅▅▅▁▅▅▅▁▅▅▁▅▁█▅▅▅▅█▅▅▅▁▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▅▅▅▁▅▅▅▁▅▅▁▅▁█▅▅▅▅█▅▃▅▁▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▂▁▂▁▂▂▁▂▂▁▂▂▂▄▃▂▂▁▃▃▂▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/385fa5zd\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:33:06.610252: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_loss_weighted_binary_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2ypw6dcq\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_023305-2ypw6dcq\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 130   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 0:   0%|                                           | 0/74 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0: 100%|█| 74/74 [00:04<00:00, 15.34it/s, loss=0.706, v_num=6dcq, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.708\n","Epoch 0: 100%|█| 74/74 [00:04<00:00, 15.24it/s, loss=0.706, v_num=6dcq, BTC_val_\n","Epoch 1: 100%|█| 74/74 [00:05<00:00, 14.30it/s, loss=0.696, v_num=6dcq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 74/74 [00:05<00:00, 14.13it/s, loss=0.696, v_num=6dcq, BTC_val_\u001b[A\n","Epoch 2:  99%|▉| 73/74 [00:06<00:00, 11.99it/s, loss=0.7, v_num=6dcq, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 74/74 [00:06<00:00, 12.00it/s, loss=0.7, v_num=6dcq, BTC_val_ac\u001b[A\n","Epoch 3: 100%|█| 74/74 [00:06<00:00, 11.64it/s, loss=0.695, v_num=6dcq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 74/74 [00:06<00:00, 11.60it/s, loss=0.695, v_num=6dcq, BTC_val_\u001b[A\n","Epoch 4: 100%|█| 74/74 [00:05<00:00, 14.08it/s, loss=0.696, v_num=6dcq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 74/74 [00:05<00:00, 14.02it/s, loss=0.696, v_num=6dcq, BTC_val_\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.704\n","Epoch 5: 100%|█| 74/74 [00:05<00:00, 14.25it/s, loss=0.694, v_num=6dcq, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 74/74 [00:05<00:00, 14.17it/s, loss=0.694, v_num=6dcq, BTC_val_\u001b[A\n","Epoch 6: 100%|█| 74/74 [00:06<00:00, 10.75it/s, loss=0.697, v_num=6dcq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 74/74 [00:06<00:00, 10.71it/s, loss=0.697, v_num=6dcq, BTC_val_\u001b[A\n","Epoch 7: 100%|█| 74/74 [00:07<00:00, 10.29it/s, loss=0.694, v_num=6dcq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 74/74 [00:07<00:00, 10.24it/s, loss=0.694, v_num=6dcq, BTC_val_\u001b[A\n","Epoch 8: 100%|█| 74/74 [00:07<00:00,  9.77it/s, loss=0.693, v_num=6dcq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 74/74 [00:07<00:00,  9.49it/s, loss=0.693, v_num=6dcq, BTC_val_\u001b[A\n","Epoch 9: 100%|█| 74/74 [00:06<00:00, 11.96it/s, loss=0.691, v_num=6dcq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 74/74 [00:06<00:00, 11.91it/s, loss=0.691, v_num=6dcq, BTC_val_\u001b[A\n","Epoch 10: 100%|█| 74/74 [00:05<00:00, 12.74it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 74/74 [00:05<00:00, 12.68it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Epoch 11: 100%|█| 74/74 [00:05<00:00, 12.75it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 74/74 [00:05<00:00, 12.68it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Epoch 12: 100%|█| 74/74 [00:06<00:00, 12.15it/s, loss=0.696, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 74/74 [00:06<00:00, 12.09it/s, loss=0.696, v_num=6dcq, BTC_val\u001b[A\n","Epoch 13: 100%|█| 74/74 [00:06<00:00, 11.95it/s, loss=0.696, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 74/74 [00:06<00:00, 11.89it/s, loss=0.696, v_num=6dcq, BTC_val\u001b[A\n","Epoch 14: 100%|█| 74/74 [00:05<00:00, 12.46it/s, loss=0.696, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 74/74 [00:05<00:00, 12.40it/s, loss=0.696, v_num=6dcq, BTC_val\u001b[A\n","Epoch 15: 100%|█| 74/74 [00:05<00:00, 14.20it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 74/74 [00:05<00:00, 14.13it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Epoch 16: 100%|█| 74/74 [00:05<00:00, 13.98it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 74/74 [00:05<00:00, 13.91it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 0.701\n","Epoch 17: 100%|█| 74/74 [00:05<00:00, 12.36it/s, loss=0.695, v_num=6dcq, BTC_val\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 74/74 [00:06<00:00, 12.31it/s, loss=0.695, v_num=6dcq, BTC_val\u001b[A\n","Epoch 18: 100%|█| 74/74 [00:05<00:00, 12.60it/s, loss=0.695, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 74/74 [00:05<00:00, 12.54it/s, loss=0.695, v_num=6dcq, BTC_val\u001b[A\n","Epoch 19: 100%|█| 74/74 [00:06<00:00, 11.84it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 74/74 [00:06<00:00, 11.80it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Epoch 20: 100%|█| 74/74 [00:06<00:00, 11.90it/s, loss=0.692, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 74/74 [00:06<00:00, 11.85it/s, loss=0.692, v_num=6dcq, BTC_val\u001b[A\n","Epoch 21: 100%|█| 74/74 [00:06<00:00, 11.38it/s, loss=0.692, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 74/74 [00:06<00:00, 11.22it/s, loss=0.692, v_num=6dcq, BTC_val\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 0.697\n","Epoch 22: 100%|█| 74/74 [00:06<00:00, 10.86it/s, loss=0.694, v_num=6dcq, BTC_val\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 74/74 [00:06<00:00, 10.82it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Epoch 23: 100%|█| 74/74 [00:05<00:00, 12.35it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 74/74 [00:06<00:00, 12.30it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Epoch 24: 100%|█| 74/74 [00:06<00:00, 11.21it/s, loss=0.692, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 74/74 [00:06<00:00, 11.17it/s, loss=0.692, v_num=6dcq, BTC_val\u001b[A\n","Epoch 25: 100%|█| 74/74 [00:06<00:00, 11.69it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 74/74 [00:06<00:00, 11.63it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Epoch 26: 100%|█| 74/74 [00:05<00:00, 13.11it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 74/74 [00:05<00:00, 13.04it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Epoch 27: 100%|█| 74/74 [00:06<00:00, 11.29it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 74/74 [00:06<00:00, 11.24it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Epoch 28: 100%|█| 74/74 [00:06<00:00, 10.79it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 74/74 [00:06<00:00, 10.75it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Epoch 29: 100%|█| 74/74 [00:06<00:00, 11.91it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 74/74 [00:06<00:00, 11.86it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Epoch 30: 100%|█| 74/74 [00:06<00:00, 11.99it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 74/74 [00:06<00:00, 11.93it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Epoch 31: 100%|█| 74/74 [00:06<00:00, 11.54it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 74/74 [00:06<00:00, 11.49it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Epoch 32: 100%|█| 74/74 [00:06<00:00, 12.08it/s, loss=0.695, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 74/74 [00:06<00:00, 12.01it/s, loss=0.695, v_num=6dcq, BTC_val\u001b[A\n","Epoch 33: 100%|█| 74/74 [00:05<00:00, 13.94it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 33: 100%|█| 74/74 [00:05<00:00, 13.87it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Epoch 34: 100%|█| 74/74 [00:06<00:00, 11.23it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 74/74 [00:06<00:00, 11.18it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 35: 100%|█| 74/74 [00:06<00:00, 11.93it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 35: 100%|█| 74/74 [00:06<00:00, 11.87it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Epoch 36: 100%|█| 74/74 [00:06<00:00, 10.71it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.693\n","Epoch 36: 100%|█| 74/74 [00:06<00:00, 10.67it/s, loss=0.693, v_num=6dcq, BTC_val\n","Epoch 37: 100%|█| 74/74 [00:06<00:00, 11.56it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.689\n","Epoch 37: 100%|█| 74/74 [00:06<00:00, 11.51it/s, loss=0.694, v_num=6dcq, BTC_val\n","Epoch 38: 100%|█| 74/74 [00:05<00:00, 12.55it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 38: 100%|█| 74/74 [00:05<00:00, 12.50it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Epoch 39: 100%|█| 74/74 [00:06<00:00, 11.33it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 39: 100%|█| 74/74 [00:06<00:00, 11.28it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Epoch 40: 100%|█| 74/74 [00:06<00:00, 10.94it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 40: 100%|█| 74/74 [00:06<00:00, 10.91it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Epoch 41: 100%|█| 74/74 [00:06<00:00, 11.98it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 41: 100%|█| 74/74 [00:06<00:00, 11.94it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Epoch 42: 100%|█| 74/74 [00:05<00:00, 13.49it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 42: 100%|█| 74/74 [00:05<00:00, 13.41it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Epoch 43: 100%|█| 74/74 [00:06<00:00, 11.85it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 43: 100%|█| 74/74 [00:06<00:00, 11.80it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Epoch 44: 100%|█| 74/74 [00:05<00:00, 12.74it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 44: 100%|█| 74/74 [00:05<00:00, 12.68it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Epoch 45: 100%|█| 74/74 [00:06<00:00, 12.28it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 45: 100%|█| 74/74 [00:06<00:00, 12.23it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Epoch 46: 100%|█| 74/74 [00:05<00:00, 13.93it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 46: 100%|█| 74/74 [00:05<00:00, 13.86it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Epoch 47: 100%|█| 74/74 [00:06<00:00, 10.74it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 47: 100%|█| 74/74 [00:06<00:00, 10.70it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Epoch 48: 100%|█| 74/74 [00:05<00:00, 13.96it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 48: 100%|█| 74/74 [00:05<00:00, 13.88it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Epoch 49: 100%|█| 74/74 [00:06<00:00, 11.34it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 49: 100%|█| 74/74 [00:06<00:00, 11.29it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Epoch 50: 100%|█| 74/74 [00:06<00:00, 10.90it/s, loss=0.695, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 50: 100%|█| 74/74 [00:06<00:00, 10.84it/s, loss=0.695, v_num=6dcq, BTC_val\u001b[A\n","Epoch 51: 100%|█| 74/74 [00:06<00:00, 10.99it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 51: 100%|█| 74/74 [00:06<00:00, 10.94it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Epoch 52: 100%|█| 74/74 [00:06<00:00, 10.79it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 52: 100%|█| 74/74 [00:06<00:00, 10.75it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Epoch 53: 100%|█| 74/74 [00:05<00:00, 13.86it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 53: 100%|█| 74/74 [00:05<00:00, 13.79it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Epoch 54: 100%|█| 74/74 [00:05<00:00, 14.15it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 54: 100%|█| 74/74 [00:05<00:00, 14.08it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Epoch 55: 100%|█| 74/74 [00:06<00:00, 11.90it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 55: 100%|█| 74/74 [00:06<00:00, 11.85it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Epoch 56: 100%|█| 74/74 [00:06<00:00, 11.41it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 56: 100%|█| 74/74 [00:06<00:00, 11.36it/s, loss=0.694, v_num=6dcq, BTC_val\u001b[A\n","Epoch 57: 100%|█| 74/74 [00:05<00:00, 13.91it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 57: 100%|█| 74/74 [00:05<00:00, 13.83it/s, loss=0.693, v_num=6dcq, BTC_val\u001b[A\n","                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.689. Signaling Trainer to stop.\n","Epoch 57: 100%|█| 74/74 [00:05<00:00, 13.82it/s, loss=0.693, v_num=6dcq, BTC_val\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 27.34it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.3928571343421936,\n"," 'BTC_test_f1': 0.2789115607738495,\n"," 'ETH_test_acc': 0.6428571343421936,\n"," 'ETH_test_f1': 0.38938775658607483,\n"," 'LTC_test_acc': 0.4285714328289032,\n"," 'LTC_test_f1': 0.293949156999588,\n"," 'test_loss': 0.6936858296394348}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 79215\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_023305-2ypw6dcq/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_023305-2ypw6dcq/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.54656\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.56078\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.25\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.69079\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 57\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4234\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 366\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621467551\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 200\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.47619\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.42176\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.48745\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.42444\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.4961\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.42666\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.6934\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.27273\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.27273\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.27273\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.69462\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.39286\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.27891\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.64286\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.38939\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.42857\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.29395\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.69369\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▇▅▅▄▅▅▆▅▅▆▆▃▇▁▄▄▄▅▆▅▆▄▆▃▅█▇▅▆▄▃▄▆▇█▅▄▄▃▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▇▄▅▄▄▄▆▅▆▆▆▃▇▁▃▄▄▅▆▄▆▃▆▃▅█▇▄▆▄▃▃▅▇█▃▂▄▂▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▇▄▄▃▃█▄▇▇▄▅▅▃▆▄▅█▁▅▅▃▆▆▄█▅▅▂▅▄▅▅▅▅▇▅▃▄▂▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▇▃▂▃▃▇▃▇▇▃▄▄▃▆▄▅█▁▄▅▃▆▅▄█▄▅▂▅▄▅▄▄▅▃▅▃▃▁▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▅▆▅▅▅▆▅▇▆▄▆▆▆▆▆▇▆▅▆▅▇▅▆▆▆▆▇▇▅▇█▅▆▆▁▆▅▆▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▅▅▄▅▅▅▅▇▆▄▆▆▅▆▅▇▆▄▅▄▇▅▅▅▆▆▇▇▄▇█▄▅▆▁▅▄▆▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▂█▇▅▅▄▄▂▁▅▂▄▂▃▄▃▂▅▃▄▃▃▃▄▃▃▃▄▃▃▃▃▃▃▄▃▄▃▄▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▅▇█▄▆▂▆▃▆█▇▄█▄▃▄▂▂▃▆▂▂▂▂▇▄▇▅▅▅▅▅▃▇▄▆▅▅▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▄▅▆▄▇▅▇▅▆▇▆▆█▆▃▆▅▄▅▇▅▁▃▁█▃█▅▅▇▅▆▅█▆▆▅▆▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▄▅█▆▁▅▃▄▅▆▃▃▃▂▂▇▅▄▃▃▄▂▄▃▂▃▂▃▁▂▇▅▃▃▃▄▁▂▁▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▄▄▇▅▅▇▆▆▆▇▄▅▅▅▂█▇▅▅▅▆▂▃▁▅▁▆▄▃▅▇▆▆▆▅▅▃▅▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▃▆▃▁▄▅█▃▄▄▂▅▂▅▅▆▅▆▃▁▅▄▃▆▄▅▅▅▄▄▅▄▂▃▂▄▅▆▃▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▃▅▄▂▅▆█▅▄▅▃▆▄▆▄▇▆▆▅▃▆▂▁▃▅▃▇▆▄▆▆▅▅▆▅▅▅▇▃▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▂▃▃▂▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████▅█▁▁▁▁▁▁█▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▄▄▄█▄▁▁▁▁▁▁▄▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████▅█▁▁▁▁▁▁█▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅█▅▁▁▁▁▁▁▅▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▅██▁█▁▁▁▁▁▁█▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅█▅▅▁▅▁▁▁▁▁▁▅▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄█▃▃▃▃▃▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▂▂▂▁▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2ypw6dcq\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:39:27.052627: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3sm4rqy9\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_023925-3sm4rqy9\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 195   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:   0%|                                           | 0/73 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 72/73 [00:04<00:00, 14.58it/s, loss=1.11, v_num=rqy9, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.098\n","Epoch 0: 100%|█| 73/73 [00:05<00:00, 14.51it/s, loss=1.11, v_num=rqy9, BTC_val_a\n","Epoch 1:  99%|▉| 72/73 [00:05<00:00, 13.87it/s, loss=1.11, v_num=rqy9, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 73/73 [00:05<00:00, 13.85it/s, loss=1.11, v_num=rqy9, BTC_val_a\u001b[A\n","Epoch 2:  99%|▉| 72/73 [00:05<00:00, 13.82it/s, loss=1.11, v_num=rqy9, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 73/73 [00:05<00:00, 13.77it/s, loss=1.11, v_num=rqy9, BTC_val_a\u001b[A\n","Epoch 3:  99%|▉| 72/73 [00:06<00:00, 11.67it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 73/73 [00:06<00:00, 11.67it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n","Epoch 4:  99%|▉| 72/73 [00:06<00:00, 11.99it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 73/73 [00:06<00:00, 11.99it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n","Epoch 5:  99%|▉| 72/73 [00:05<00:00, 12.00it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 73/73 [00:06<00:00, 12.00it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n","Epoch 6:  99%|▉| 72/73 [00:05<00:00, 12.82it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 73/73 [00:05<00:00, 12.80it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n","Epoch 7:  99%|▉| 72/73 [00:06<00:00, 11.72it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 73/73 [00:06<00:00, 11.71it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n","Epoch 8:  99%|▉| 72/73 [00:05<00:00, 12.12it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 73/73 [00:06<00:00, 12.11it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n","Epoch 9:  99%|▉| 72/73 [00:06<00:00, 11.70it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 73/73 [00:06<00:00, 11.69it/s, loss=1.1, v_num=rqy9, BTC_val_ac\u001b[A\n","Epoch 10:  99%|▉| 72/73 [00:05<00:00, 12.83it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 73/73 [00:05<00:00, 12.82it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Epoch 11:  99%|▉| 72/73 [00:05<00:00, 13.69it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 73/73 [00:05<00:00, 13.63it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Epoch 12:  99%|▉| 72/73 [00:05<00:00, 14.13it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 73/73 [00:05<00:00, 14.05it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Epoch 13:  99%|▉| 72/73 [00:06<00:00, 11.31it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 73/73 [00:06<00:00, 11.30it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Epoch 14:  99%|▉| 72/73 [00:05<00:00, 14.10it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 73/73 [00:05<00:00, 14.07it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Epoch 15:  99%|▉| 72/73 [00:05<00:00, 12.06it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 73/73 [00:06<00:00, 12.04it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Epoch 16:  99%|▉| 72/73 [00:05<00:00, 12.06it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 73/73 [00:06<00:00, 12.07it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Epoch 17:  99%|▉| 72/73 [00:06<00:00, 10.94it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 73/73 [00:06<00:00, 10.93it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Epoch 18:  99%|▉| 72/73 [00:06<00:00, 11.24it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 73/73 [00:06<00:00, 11.26it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Epoch 19:  99%|▉| 72/73 [00:06<00:00, 11.22it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 73/73 [00:06<00:00, 11.23it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Epoch 20:  99%|▉| 72/73 [00:05<00:00, 12.65it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.098. Signaling Trainer to stop.\n","Epoch 20: 100%|█| 73/73 [00:05<00:00, 12.63it/s, loss=1.1, v_num=rqy9, BTC_val_a\n","Epoch 20: 100%|█| 73/73 [00:05<00:00, 12.62it/s, loss=1.1, v_num=rqy9, BTC_val_a\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 34.62it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.5357142686843872,\n"," 'BTC_test_f1': 0.2323809713125229,\n"," 'ETH_test_acc': 0.5,\n"," 'ETH_test_f1': 0.2212051898241043,\n"," 'LTC_test_acc': 0.4285714328289032,\n"," 'LTC_test_f1': 0.1999756544828415,\n"," 'test_loss': 1.0990955829620361}\n","--------------------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 80025\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_023925-3sm4rqy9/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_023925-3sm4rqy9/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.18182\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.33333\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.4329\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.09175\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1512\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 133\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621467698\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 72\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.46388\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.27602\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.44386\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.26423\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.46475\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.27176\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09899\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.18182\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.22222\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.25641\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.09812\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.53571\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.23238\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.22121\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.42857\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.19998\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.0991\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▄▅▂▄▅▃▂▁▅▄▃▅▅▃▅▃▂▇▅▅▃▄▃▆▆▂█▄▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▄█▃▅▄▄▃▁▇▄▃▆▄▃▄▃▄▄▅▅▃▃▂▅▃▃█▃▄▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▄▃▅▂▇▄▂▂▅▅▄▂▃▃▃▂▇▇▅▅█▇▁▁▅▂▅▆▆█\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▇▅▆▃▄▅▄▁▇█▃▅▂▄▄▁▇▆▅▅█▇▁▁▃▂▃▄▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▅▂▃▆▃▁▃▅▃▅▄▂▆▇▆▃▆▅▃▆▄▄▃▃▆▄▆█▂▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▆▂▄▄▂▁▂▅▃▄▄▁▃█▂▁▄▂▂▄▃▃▂▂▃▂▅▅▁▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▃▇▄█▂▇▃▅▃▂▂▆▂▂▄▅▁▂▁▃▃▂▇▃▃▆▃▂▄▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▄▄▄▃▁▆▄▄▅▅▆▄▇▇▅█▆█▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch █▅█▅▅▅▄▃▆█▅▃▁▇▃▄▅▃▄▅▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▃▂▄▅▁▇▄▆▄▄▇▅█▆▇█▆▇█▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▅▃██▆▇▅▇▅▆▇▆▄▆▆▅▄▁▅▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▄▄▄▄▁▅▅▅▄▄▅▆▆▅▆▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▆▅█▆▃▃▇▃▃▅▁█▁▂▄▃▅▃▄▅▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▄▃▂▂▃▂▂▁▂▁▂▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc █▁▁██████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 █▁▁██████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc █▁▁██████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 █▁▁██████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▃█▅▄▄▃▂▃▃▁▁▁▅▂▂▂▁▂▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3sm4rqy9\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:41:52.986769: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_loss_weighted_multi_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2o16u86o\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024151-2o16u86o\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 195   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0: 100%|█| 74/74 [00:05<00:00, 12.92it/s, loss=1.13, v_num=u86o, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.084\n","Epoch 0: 100%|█| 74/74 [00:05<00:00, 12.86it/s, loss=1.13, v_num=u86o, BTC_val_a\n","Epoch 1: 100%|█| 74/74 [00:05<00:00, 12.66it/s, loss=1.11, v_num=u86o, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 74/74 [00:05<00:00, 12.61it/s, loss=1.11, v_num=u86o, BTC_val_a\u001b[A\n","Epoch 2: 100%|█| 74/74 [00:05<00:00, 13.45it/s, loss=1.11, v_num=u86o, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 74/74 [00:05<00:00, 13.38it/s, loss=1.11, v_num=u86o, BTC_val_a\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 3: 100%|█| 74/74 [00:06<00:00, 10.93it/s, loss=1.1, v_num=u86o, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 74/74 [00:06<00:00, 10.84it/s, loss=1.1, v_num=u86o, BTC_val_ac\u001b[A\n","Epoch 4: 100%|█| 74/74 [00:07<00:00,  9.56it/s, loss=1.1, v_num=u86o, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 74/74 [00:07<00:00,  9.53it/s, loss=1.1, v_num=u86o, BTC_val_ac\u001b[A\n","Epoch 5: 100%|█| 74/74 [00:05<00:00, 12.64it/s, loss=1.1, v_num=u86o, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 74/74 [00:05<00:00, 12.59it/s, loss=1.1, v_num=u86o, BTC_val_ac\u001b[A\n","Epoch 6: 100%|█| 74/74 [00:06<00:00, 12.30it/s, loss=1.09, v_num=u86o, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 74/74 [00:06<00:00, 12.25it/s, loss=1.09, v_num=u86o, BTC_val_a\u001b[A\n","Epoch 7: 100%|█| 74/74 [00:05<00:00, 12.96it/s, loss=1.1, v_num=u86o, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 74/74 [00:05<00:00, 12.90it/s, loss=1.1, v_num=u86o, BTC_val_ac\u001b[A\n","Epoch 8: 100%|█| 74/74 [00:05<00:00, 13.21it/s, loss=1.1, v_num=u86o, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 74/74 [00:05<00:00, 13.13it/s, loss=1.1, v_num=u86o, BTC_val_ac\u001b[A\n","Epoch 9: 100%|█| 74/74 [00:06<00:00, 12.27it/s, loss=1.09, v_num=u86o, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 74/74 [00:06<00:00, 12.20it/s, loss=1.09, v_num=u86o, BTC_val_a\u001b[A\n","Epoch 10: 100%|█| 74/74 [00:06<00:00, 12.31it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 74/74 [00:06<00:00, 12.24it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n","Epoch 11: 100%|█| 74/74 [00:05<00:00, 12.93it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 74/74 [00:05<00:00, 12.73it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n","Epoch 12: 100%|█| 74/74 [00:05<00:00, 12.51it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 74/74 [00:05<00:00, 12.45it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n","Epoch 13: 100%|█| 74/74 [00:06<00:00, 11.86it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 74/74 [00:06<00:00, 11.80it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n","Epoch 14: 100%|█| 74/74 [00:05<00:00, 12.34it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 74/74 [00:06<00:00, 12.27it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n","Epoch 15: 100%|█| 74/74 [00:06<00:00, 12.24it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 74/74 [00:06<00:00, 12.18it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n","Epoch 16: 100%|█| 74/74 [00:05<00:00, 12.76it/s, loss=1.09, v_num=u86o, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 74/74 [00:05<00:00, 12.70it/s, loss=1.09, v_num=u86o, BTC_val_\u001b[A\n","Epoch 17: 100%|█| 74/74 [00:06<00:00, 11.39it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 74/74 [00:06<00:00, 11.34it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n","Epoch 18: 100%|█| 74/74 [00:05<00:00, 12.72it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 74/74 [00:05<00:00, 12.59it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n","Epoch 19: 100%|█| 74/74 [00:06<00:00, 12.00it/s, loss=1.11, v_num=u86o, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 74/74 [00:06<00:00, 11.95it/s, loss=1.11, v_num=u86o, BTC_val_\u001b[A\n","Epoch 20: 100%|█| 74/74 [00:06<00:00, 12.09it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.084. Signaling Trainer to stop.\n","Epoch 20: 100%|█| 74/74 [00:06<00:00, 12.04it/s, loss=1.1, v_num=u86o, BTC_val_a\n","Epoch 20: 100%|█| 74/74 [00:06<00:00, 12.03it/s, loss=1.1, v_num=u86o, BTC_val_a\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 32.69it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.2142857164144516,\n"," 'BTC_test_f1': 0.11375661939382553,\n"," 'ETH_test_acc': 0.3571428656578064,\n"," 'ETH_test_f1': 0.2409733235836029,\n"," 'LTC_test_acc': 0.25,\n"," 'LTC_test_f1': 0.17094017565250397,\n"," 'test_loss': 1.101166844367981}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 80369\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024151-2o16u86o/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024151-2o16u86o/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.25\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.24444\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.25\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.25098\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.4375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.32955\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.10041\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1533\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 137\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621467848\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 72\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.36623\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.31748\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.34545\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.29993\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.33766\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.29871\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09833\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.18182\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.18182\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.25641\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.09315\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.21429\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.11376\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.35714\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.24097\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.25\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.17094\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.10117\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▂█▆▄▃▄▆▁▄▅▅▃▅▇▅▄▅▄▄▅▂▅█▄▅▆▅▇▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▂▇▅▄▂▃▆▁▄▄▅▃▄█▆▃▅▂▄▄▂▄█▃▆▅▅▇▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▂▂▅▂▄▅▅▂▇▁▄▂▁▅▂▁▄▄▁▄▂▁██▂▇▅▁▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▃▂▅▃▃▆▆▃▇▁▅▃▂▄▃▁▅▃▂▄▃▂██▃▇▆▁▃▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▄▆▅▁▃▄▅▅▄▃▃▂▆▄▃█▄▄▃▅▅▅▅▅▃▄▄▅▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▄▆▄▁▂▃▅▅▄▃▃▂▄▄▃█▄▃▃▅▄▅▃▅▃▃▃▄▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▂▂▆▆▆▃█▄▅▄▆▃▄▄▃▄▃▅▃▄▄▂▁▅▃▂▂▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▃█▁▆▂▅▇▅▄▅▅▂▅▆▃▆▇▅▃▇▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▅▅▁▄▄▆▅▃▆▄▆▂▄▇▅█▇▃▅█▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▃▅▁▄▂▅▅▅▅▅▇▅▂▂▄██▅▇▂▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▄▂▁▃▅▆▆▄▆▄█▅▂▃▅▆▇▃▆▂▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▄▇▂▄▆▅▁▇▃█▇▆▅▅██▄█▄▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▆▃▁▁▇▆▁▅▅▄█▆▂▅▇▆▄▅▅▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▄▃▂▂▂▃▂▁▁▂▂▂▂▁▁▂▁▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▁███████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▆▁███████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▁███████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▁███████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁█▆▄▃▄▂▅▄▂▃▃▃▄▂▂▁▃▃▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2o16u86o\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:44:24.174296: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm__binary_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/187zic8f\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024422-187zic8f\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 130   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.7 K    Trainable params\n","0         Non-trainable params\n","75.7 K    Total params\n","0.303     Total estimated model params size (MB)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0: 100%|█| 73/73 [00:02<00:00, 29.04it/s, loss=0.583, v_num=ic8f, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 0: 100%|█| 73/73 [00:02<00:00, 28.84it/s, loss=0.583, v_num=ic8f, BTC_val_\u001b[A\n","                                                                                \u001b[AMetric val_loss improved. New best score: 0.562\n","Epoch 1:  99%|▉| 72/73 [00:02<00:00, 31.61it/s, loss=0.572, v_num=ic8f, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 73/73 [00:02<00:00, 31.09it/s, loss=0.572, v_num=ic8f, BTC_val_\u001b[A\n","Epoch 2:  99%|▉| 72/73 [00:02<00:00, 31.52it/s, loss=0.563, v_num=ic8f, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.554\n","Epoch 2: 100%|█| 73/73 [00:02<00:00, 30.97it/s, loss=0.563, v_num=ic8f, BTC_val_\n","Epoch 3:  99%|▉| 72/73 [00:02<00:00, 28.93it/s, loss=0.557, v_num=ic8f, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 73/73 [00:02<00:00, 28.36it/s, loss=0.557, v_num=ic8f, BTC_val_\u001b[A\n","Epoch 4:  99%|▉| 72/73 [00:02<00:00, 28.87it/s, loss=0.567, v_num=ic8f, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 73/73 [00:02<00:00, 28.36it/s, loss=0.567, v_num=ic8f, BTC_val_\u001b[A\n","Epoch 5:  99%|▉| 72/73 [00:02<00:00, 26.80it/s, loss=0.594, v_num=ic8f, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 73/73 [00:02<00:00, 26.45it/s, loss=0.594, v_num=ic8f, BTC_val_\u001b[A\n","Epoch 6:  99%|▉| 72/73 [00:03<00:00, 23.06it/s, loss=0.575, v_num=ic8f, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 73/73 [00:03<00:00, 22.78it/s, loss=0.575, v_num=ic8f, BTC_val_\u001b[A\n","Epoch 7:  99%|▉| 72/73 [00:02<00:00, 27.62it/s, loss=0.591, v_num=ic8f, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 73/73 [00:02<00:00, 27.18it/s, loss=0.591, v_num=ic8f, BTC_val_\u001b[A\n","Epoch 8:  99%|▉| 72/73 [00:02<00:00, 30.03it/s, loss=0.546, v_num=ic8f, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 73/73 [00:02<00:00, 29.39it/s, loss=0.546, v_num=ic8f, BTC_val_\u001b[A\n","Epoch 9:  99%|▉| 72/73 [00:03<00:00, 23.03it/s, loss=0.539, v_num=ic8f, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 73/73 [00:03<00:00, 22.80it/s, loss=0.539, v_num=ic8f, BTC_val_\u001b[A\n","Epoch 10:  99%|▉| 72/73 [00:02<00:00, 26.41it/s, loss=0.584, v_num=ic8f, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 73/73 [00:02<00:00, 25.91it/s, loss=0.584, v_num=ic8f, BTC_val\u001b[A\n","Epoch 11:  99%|▉| 72/73 [00:02<00:00, 26.54it/s, loss=0.58, v_num=ic8f, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 73/73 [00:02<00:00, 26.15it/s, loss=0.58, v_num=ic8f, BTC_val_\u001b[A\n","Epoch 12:  99%|▉| 72/73 [00:02<00:00, 30.12it/s, loss=0.55, v_num=ic8f, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 73/73 [00:02<00:00, 29.59it/s, loss=0.55, v_num=ic8f, BTC_val_\u001b[A\n","Epoch 13:  99%|▉| 72/73 [00:02<00:00, 26.24it/s, loss=0.56, v_num=ic8f, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 73/73 [00:02<00:00, 25.77it/s, loss=0.56, v_num=ic8f, BTC_val_\u001b[A\n","Epoch 14:  99%|▉| 72/73 [00:02<00:00, 24.22it/s, loss=0.584, v_num=ic8f, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 73/73 [00:03<00:00, 23.99it/s, loss=0.584, v_num=ic8f, BTC_val\u001b[A\n","Epoch 15:  99%|▉| 72/73 [00:02<00:00, 25.44it/s, loss=0.551, v_num=ic8f, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 73/73 [00:02<00:00, 24.97it/s, loss=0.551, v_num=ic8f, BTC_val\u001b[A\n","Epoch 16:  99%|▉| 72/73 [00:02<00:00, 25.12it/s, loss=0.548, v_num=ic8f, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 73/73 [00:02<00:00, 24.85it/s, loss=0.548, v_num=ic8f, BTC_val\u001b[A\n","Epoch 17:  99%|▉| 72/73 [00:02<00:00, 24.82it/s, loss=0.544, v_num=ic8f, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 73/73 [00:02<00:00, 24.41it/s, loss=0.544, v_num=ic8f, BTC_val\u001b[A\n","Epoch 18:  99%|▉| 72/73 [00:02<00:00, 29.36it/s, loss=0.555, v_num=ic8f, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 73/73 [00:02<00:00, 28.72it/s, loss=0.555, v_num=ic8f, BTC_val\u001b[A\n","Epoch 19:  99%|▉| 72/73 [00:02<00:00, 29.35it/s, loss=0.562, v_num=ic8f, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 73/73 [00:02<00:00, 28.91it/s, loss=0.562, v_num=ic8f, BTC_val\u001b[A\n","Epoch 20:  99%|▉| 72/73 [00:02<00:00, 28.28it/s, loss=0.58, v_num=ic8f, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 73/73 [00:02<00:00, 27.89it/s, loss=0.58, v_num=ic8f, BTC_val_\u001b[A\n","Epoch 21:  99%|▉| 72/73 [00:03<00:00, 22.55it/s, loss=0.554, v_num=ic8f, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 73/73 [00:03<00:00, 22.32it/s, loss=0.554, v_num=ic8f, BTC_val\u001b[A\n","Epoch 22:  99%|▉| 72/73 [00:02<00:00, 28.93it/s, loss=0.567, v_num=ic8f, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.554. Signaling Trainer to stop.\n","Epoch 22: 100%|█| 73/73 [00:02<00:00, 28.51it/s, loss=0.567, v_num=ic8f, BTC_val\n","Epoch 22: 100%|█| 73/73 [00:02<00:00, 28.46it/s, loss=0.567, v_num=ic8f, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 56.96it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.6785714030265808,\n"," 'BTC_test_f1': 0.6750550866127014,\n"," 'ETH_test_acc': 0.6785714030265808,\n"," 'ETH_test_f1': 0.6694621443748474,\n"," 'LTC_test_acc': 0.75,\n"," 'LTC_test_f1': 0.7454982399940491,\n"," 'test_loss': 0.5896449685096741}\n","--------------------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 80732\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024422-187zic8f/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024422-187zic8f/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.6875\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.65368\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.66667\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.56364\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.60141\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 22\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1656\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 72\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621467934\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 79\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.71453\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.69841\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.71976\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.70568\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.70409\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.68924\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.56223\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.56364\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.61905\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.61905\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.58625\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.67857\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.67506\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.67857\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.66946\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.75\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.7455\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.58964\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▄▅▃█▅▄▆▅▁▇▂▂▅▆▅▂▃▂▅▆▅▅▅▅▄▄▅▅▇▅▆▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▃▃▃█▄▃▆▅▁▇▂▂▅▆▅▂▃▂▃▆▅▄▅▅▃▃▅▄▇▄▆▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▅▆▄▆▄▄▅▃▂█▄▂▄▅▅▅▅▄▆▇▅▆▁▅▃▅▅▄▇▄▆▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▆▆▃▆▃▄▆▃▃█▄▃▄▆▅▅▆▄▆▇▆▆▁▅▂▅▆▄▇▄▆▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▆▅▃▇▇▇█▁▅▇▅▅▃▆▇▇▅▃▅▇▃▇▆▇▆▆█▅▇▃█▇▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▅▃▂▇▇▇█▁▅▇▄▄▃▄▇▇▄▃▄▆▃▇▄▇▆▅█▄▇▂█▇▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅▆▇▂▃▅▂▅▆▁▅▆▅▄▄▅▅█▄▃▃▃▅▃▆▅▃▅▁▅▂▂▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▆▆▇▆▇▆▇▆▇▇▇▆▇▇▇▇▆▆██▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▆▆▇▇▇▇▇▆██▇▇▇█▇█▇▇██▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▆▆▆▆▇▆▆▆▇▇▆▇▆▇▇█▇▇█▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▆▇▇▇▇▆▇▆▇▇▇▇▇█▇█▇▇█▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▅▇▆▆▇▆██▇▇▆▇▆▇█▇█▇▇▇▇▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▅▇▇▇▇▇██▇▇▇▇▇██▇██▇▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▅▅█▅▅▁▁▁▅▅▁▁█▁▁█▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▅▅█▅▅▁▁▁▅▅▁▁█▁▁█▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▆▁▃▁▃▁▃▃▃▃▆▆▃▆▆▃█▃▆▆▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▅▁▃▁▃▁▃▃▃▃▆▆▃▆▆▃█▃▆▆▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc █▅▅█▅▅▅█▅▅▅▅▁█▅▁▅▅▅▅▅▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▆▅▃█▅▅▅█▅▅▅▅▁█▅▁▅▅▅▅▅▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▂▆▁▆▅▇▅▆▅▄▆▅▃▇▇▄▆▄▄█▅▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/187zic8f\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:45:49.412436: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm__binary_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3bkiy2jg\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024547-3bkiy2jg\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 130   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.7 K    Trainable params\n","0         Non-trainable params\n","75.7 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 73/74 [00:02<00:00, 30.78it/s, loss=0.709, v_num=y2jg, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.684\n","Epoch 0: 100%|█| 74/74 [00:02<00:00, 30.11it/s, loss=0.709, v_num=y2jg, BTC_val_\n","Epoch 1:  99%|▉| 73/74 [00:02<00:00, 31.10it/s, loss=0.704, v_num=y2jg, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 74/74 [00:02<00:00, 30.41it/s, loss=0.704, v_num=y2jg, BTC_val_\u001b[A\n","Epoch 2:  99%|▉| 73/74 [00:02<00:00, 31.59it/s, loss=0.693, v_num=y2jg, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 74/74 [00:02<00:00, 31.04it/s, loss=0.693, v_num=y2jg, BTC_val_\u001b[A\n","Epoch 3:  99%|▉| 73/74 [00:02<00:00, 32.03it/s, loss=0.698, v_num=y2jg, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 74/74 [00:02<00:00, 31.42it/s, loss=0.698, v_num=y2jg, BTC_val_\u001b[A\n","Epoch 4:  99%|▉| 73/74 [00:02<00:00, 31.99it/s, loss=0.692, v_num=y2jg, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.677\n","Epoch 4: 100%|█| 74/74 [00:02<00:00, 31.34it/s, loss=0.692, v_num=y2jg, BTC_val_\n","Epoch 5:  99%|▉| 73/74 [00:02<00:00, 29.03it/s, loss=0.696, v_num=y2jg, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 74/74 [00:02<00:00, 28.31it/s, loss=0.696, v_num=y2jg, BTC_val_\u001b[A\n","Epoch 6:  99%|▉| 73/74 [00:02<00:00, 29.85it/s, loss=0.693, v_num=y2jg, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 74/74 [00:02<00:00, 29.28it/s, loss=0.693, v_num=y2jg, BTC_val_\u001b[A\n","Epoch 7:  99%|▉| 73/74 [00:02<00:00, 30.97it/s, loss=0.695, v_num=y2jg, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 74/74 [00:02<00:00, 30.37it/s, loss=0.695, v_num=y2jg, BTC_val_\u001b[A\n","Epoch 8:  99%|▉| 73/74 [00:02<00:00, 32.23it/s, loss=0.694, v_num=y2jg, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 74/74 [00:02<00:00, 31.69it/s, loss=0.694, v_num=y2jg, BTC_val_\u001b[A\n","Epoch 9:  99%|▉| 73/74 [00:02<00:00, 30.35it/s, loss=0.695, v_num=y2jg, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 74/74 [00:02<00:00, 29.56it/s, loss=0.695, v_num=y2jg, BTC_val_\u001b[A\n","Epoch 10:  99%|▉| 73/74 [00:02<00:00, 24.95it/s, loss=0.694, v_num=y2jg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 74/74 [00:03<00:00, 24.64it/s, loss=0.694, v_num=y2jg, BTC_val\u001b[A\n","Epoch 11:  99%|▉| 73/74 [00:02<00:00, 25.60it/s, loss=0.692, v_num=y2jg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 74/74 [00:02<00:00, 25.19it/s, loss=0.692, v_num=y2jg, BTC_val\u001b[A\n","Epoch 12:  99%|▉| 73/74 [00:02<00:00, 29.24it/s, loss=0.692, v_num=y2jg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 74/74 [00:02<00:00, 28.51it/s, loss=0.692, v_num=y2jg, BTC_val\u001b[A\n","Epoch 13:  99%|▉| 73/74 [00:02<00:00, 27.74it/s, loss=0.698, v_num=y2jg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 74/74 [00:02<00:00, 27.27it/s, loss=0.698, v_num=y2jg, BTC_val\u001b[A\n","Epoch 14:  99%|▉| 73/74 [00:02<00:00, 26.95it/s, loss=0.692, v_num=y2jg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 74/74 [00:02<00:00, 26.46it/s, loss=0.692, v_num=y2jg, BTC_val\u001b[A\n","Epoch 15:  99%|▉| 73/74 [00:02<00:00, 26.85it/s, loss=0.696, v_num=y2jg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 74/74 [00:02<00:00, 26.52it/s, loss=0.696, v_num=y2jg, BTC_val\u001b[A\n","Epoch 16:  99%|▉| 73/74 [00:02<00:00, 26.79it/s, loss=0.693, v_num=y2jg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 74/74 [00:02<00:00, 26.39it/s, loss=0.693, v_num=y2jg, BTC_val\u001b[A\n","Epoch 17:  99%|▉| 73/74 [00:02<00:00, 26.14it/s, loss=0.694, v_num=y2jg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 74/74 [00:03<00:00, 24.52it/s, loss=0.694, v_num=y2jg, BTC_val\u001b[A\n","Epoch 18:  99%|▉| 73/74 [00:02<00:00, 28.16it/s, loss=0.692, v_num=y2jg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 74/74 [00:02<00:00, 27.71it/s, loss=0.692, v_num=y2jg, BTC_val\u001b[A\n","Epoch 19:  99%|▉| 73/74 [00:03<00:00, 23.13it/s, loss=0.694, v_num=y2jg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 74/74 [00:03<00:00, 22.78it/s, loss=0.694, v_num=y2jg, BTC_val\u001b[A\n","Epoch 20:  99%|▉| 73/74 [00:02<00:00, 30.08it/s, loss=0.695, v_num=y2jg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 74/74 [00:02<00:00, 29.51it/s, loss=0.695, v_num=y2jg, BTC_val\u001b[A\n","Epoch 21:  99%|▉| 73/74 [00:02<00:00, 27.96it/s, loss=0.694, v_num=y2jg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 74/74 [00:02<00:00, 27.40it/s, loss=0.694, v_num=y2jg, BTC_val\u001b[A\n","Epoch 22:  99%|▉| 73/74 [00:02<00:00, 25.37it/s, loss=0.691, v_num=y2jg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 74/74 [00:02<00:00, 25.07it/s, loss=0.691, v_num=y2jg, BTC_val\u001b[A\n","Epoch 23:  99%|▉| 73/74 [00:02<00:00, 29.85it/s, loss=0.691, v_num=y2jg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 74/74 [00:02<00:00, 29.41it/s, loss=0.691, v_num=y2jg, BTC_val\u001b[A\n","Epoch 24:  99%|▉| 73/74 [00:02<00:00, 25.29it/s, loss=0.695, v_num=y2jg, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 74/74 [00:02<00:00, 24.89it/s, loss=0.695, v_num=y2jg, BTC_valMonitored metric val_loss did not improve in the last 20 records. Best score: 0.677. Signaling Trainer to stop.\n","\n","Epoch 24: 100%|█| 74/74 [00:02<00:00, 24.85it/s, loss=0.695, v_num=y2jg, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 44.20it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.3928571343421936,\n"," 'BTC_test_f1': 0.2789115607738495,\n"," 'ETH_test_acc': 0.6428571343421936,\n"," 'ETH_test_f1': 0.38938775658607483,\n"," 'LTC_test_acc': 0.4285714328289032,\n"," 'LTC_test_f1': 0.293949156999588,\n"," 'test_loss': 0.6996669173240662}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 80952\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024547-3bkiy2jg/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024547-3bkiy2jg/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.4375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.30435\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.3125\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.2381\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.27273\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.69268\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 24\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1825\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 76\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621468023\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 86\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.54026\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.36885\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.50216\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.34727\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.49177\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.34737\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69361\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.38462\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.38462\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.38462\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.68654\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.39286\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.27891\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.64286\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.38939\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.42857\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.29395\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.69967\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▅▄▅▂▁█▃▆▄▇▇▄▄▅▆▆▆▆▆█▃▅▃▂▅▅▆▃▆▅▇▄▅▆▃▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▃▄▄▂▁▇▃▇▅██▂▅▄▇▆▅▃▅▇▃▄▂▁▄▄▇▃▆▄█▂▆▅▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▄▄▃▂▅▅▅▆▄▇▂▅▅█▆▅▇▇▇▆▆▁▄▄▄▃▃▅▄▄▃▄▆▅▆▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▅▅▄▂▆▄▆▇▅▆▃▃▆█▇▃▄▄▇▃▇▂▄▅▂▃▄▅▂▂▄▂▅▅▅▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▇▅▄▄▁▇▆▅▇▆▆▄▇▆▆▆█▇▄▇▆▇▅██▅▄▃▄▆▁▅▅▇▆▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▇▄▃▄▁▇▆▅▇▅▆▂▇▆▆▅▆▄▂▄▆▆▃█▆▄▄▃▂▅▁▃▃▄▆▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▁▆▆▆█▃▄▂▃▃▄▅▄▄▄▃▃▃▅▃▄▄▅▄▄▄▅▅▅▄▅▆▄▄▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▅▃▄▂▆▂▅▁▃▁▃▆▇▇▇▆▃▇▅▅▆▃▄▄█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▅▇▇▄▇▆▇▆▃▃▇▂▂▂█▂▅▂▇▆▄▄▄▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▄▃▆▅█▆▂▃▁█▁▄▄▆▁▄▃▄▄▆▃▆▇▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▅▇▇▅██▆▇▃▆▆▁▂▂▆▂▇▂█▆▃▅▅▄▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch █▅▁▄▅▆▅▄▄▆▆▅▃▃▄▂▆▅▃▆▄▃▂▆▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▇█▆▅███▇▅▅█▃▃▁█▂█▂▇▆▅▅▃▅▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc █▁██████▁████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 █▁██████▁████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc █▁██████▁████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 █▁██████▁████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc █▁█▁████▁████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 █▁█▇████▁████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▃█▃▅▁▄▄▄▆▁▅▅▅▅▅▅▅▅▄▂▄▅▅▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3bkiy2jg\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:47:23.634543: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm__multi_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1yic4mss\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024722-1yic4mss\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 195   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.8 K    Trainable params\n","0         Non-trainable params\n","75.8 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0: 100%|█| 73/73 [00:02<00:00, 28.73it/s, loss=1.03, v_num=4mss, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.039\n","Epoch 0: 100%|█| 73/73 [00:02<00:00, 28.46it/s, loss=1.03, v_num=4mss, BTC_val_a\n","Epoch 1:  99%|▉| 72/73 [00:02<00:00, 32.13it/s, loss=1.04, v_num=4mss, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 73/73 [00:02<00:00, 31.38it/s, loss=1.04, v_num=4mss, BTC_val_a\u001b[A\n","Epoch 2:  99%|▉| 72/73 [00:02<00:00, 30.20it/s, loss=1.03, v_num=4mss, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.026 >= min_delta = 0.003. New best score: 1.013\n","Epoch 2: 100%|█| 73/73 [00:02<00:00, 29.60it/s, loss=1.03, v_num=4mss, BTC_val_a\n","Epoch 3:  99%|▉| 72/73 [00:02<00:00, 30.80it/s, loss=0.997, v_num=4mss, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 0.992\n","Epoch 3: 100%|█| 73/73 [00:02<00:00, 30.15it/s, loss=0.997, v_num=4mss, BTC_val_\n","Epoch 4:  99%|▉| 72/73 [00:02<00:00, 28.18it/s, loss=0.955, v_num=4mss, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.019 >= min_delta = 0.003. New best score: 0.973\n","Epoch 4: 100%|█| 73/73 [00:02<00:00, 27.67it/s, loss=0.955, v_num=4mss, BTC_val_\n","Epoch 5:  99%|▉| 72/73 [00:02<00:00, 27.62it/s, loss=0.957, v_num=4mss, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 73/73 [00:02<00:00, 27.20it/s, loss=0.957, v_num=4mss, BTC_val_\u001b[A\n","Epoch 6:  99%|▉| 72/73 [00:02<00:00, 31.66it/s, loss=0.933, v_num=4mss, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 73/73 [00:02<00:00, 31.15it/s, loss=0.933, v_num=4mss, BTC_val_\u001b[A\n","Epoch 7:  99%|▉| 72/73 [00:02<00:00, 32.28it/s, loss=0.917, v_num=4mss, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 73/73 [00:02<00:00, 31.72it/s, loss=0.917, v_num=4mss, BTC_val_\u001b[A\n","Epoch 8:  99%|▉| 72/73 [00:02<00:00, 32.09it/s, loss=0.902, v_num=4mss, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 73/73 [00:02<00:00, 31.41it/s, loss=0.902, v_num=4mss, BTC_val_\u001b[A\n","Epoch 9:  99%|▉| 72/73 [00:02<00:00, 30.96it/s, loss=0.896, v_num=4mss, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.024 >= min_delta = 0.003. New best score: 0.950\n","Epoch 9: 100%|█| 73/73 [00:02<00:00, 30.26it/s, loss=0.896, v_num=4mss, BTC_val_\n","Epoch 10:  99%|▉| 72/73 [00:02<00:00, 29.62it/s, loss=0.915, v_num=4mss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 73/73 [00:02<00:00, 28.92it/s, loss=0.915, v_num=4mss, BTC_val\u001b[A\n","Epoch 11:  99%|▉| 72/73 [00:02<00:00, 27.08it/s, loss=0.938, v_num=4mss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 73/73 [00:02<00:00, 26.71it/s, loss=0.938, v_num=4mss, BTC_val\u001b[A\n","Epoch 12:  99%|▉| 72/73 [00:02<00:00, 27.06it/s, loss=0.908, v_num=4mss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 73/73 [00:02<00:00, 26.49it/s, loss=0.908, v_num=4mss, BTC_val\u001b[A\n","Epoch 13:  99%|▉| 72/73 [00:02<00:00, 31.19it/s, loss=0.924, v_num=4mss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 73/73 [00:02<00:00, 30.46it/s, loss=0.924, v_num=4mss, BTC_val\u001b[A\n","Epoch 14:  99%|▉| 72/73 [00:02<00:00, 29.61it/s, loss=0.921, v_num=4mss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 73/73 [00:02<00:00, 29.19it/s, loss=0.921, v_num=4mss, BTC_val\u001b[A\n","Epoch 15:  99%|▉| 72/73 [00:02<00:00, 26.62it/s, loss=0.918, v_num=4mss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 73/73 [00:02<00:00, 26.19it/s, loss=0.918, v_num=4mss, BTC_val\u001b[A\n","Epoch 16:  99%|▉| 72/73 [00:02<00:00, 28.99it/s, loss=0.927, v_num=4mss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 73/73 [00:02<00:00, 28.40it/s, loss=0.927, v_num=4mss, BTC_val\u001b[A\n","Epoch 17:  99%|▉| 72/73 [00:02<00:00, 24.63it/s, loss=0.935, v_num=4mss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 73/73 [00:02<00:00, 24.38it/s, loss=0.935, v_num=4mss, BTC_val\u001b[A\n","Epoch 18:  99%|▉| 72/73 [00:02<00:00, 26.69it/s, loss=0.948, v_num=4mss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 73/73 [00:02<00:00, 26.19it/s, loss=0.948, v_num=4mss, BTC_val\u001b[A\n","Epoch 19:  99%|▉| 72/73 [00:02<00:00, 24.86it/s, loss=0.909, v_num=4mss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 73/73 [00:02<00:00, 24.59it/s, loss=0.909, v_num=4mss, BTC_val\u001b[A\n","Epoch 20:  99%|▉| 72/73 [00:02<00:00, 26.73it/s, loss=0.907, v_num=4mss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 73/73 [00:02<00:00, 26.29it/s, loss=0.907, v_num=4mss, BTC_val\u001b[A\n","Epoch 21:  99%|▉| 72/73 [00:02<00:00, 30.55it/s, loss=0.909, v_num=4mss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 73/73 [00:02<00:00, 29.93it/s, loss=0.909, v_num=4mss, BTC_val\u001b[A\n","Epoch 22:  99%|▉| 72/73 [00:02<00:00, 25.40it/s, loss=0.957, v_num=4mss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 73/73 [00:02<00:00, 25.13it/s, loss=0.957, v_num=4mss, BTC_val\u001b[A\n","Epoch 23:  99%|▉| 72/73 [00:03<00:00, 23.94it/s, loss=0.901, v_num=4mss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 73/73 [00:03<00:00, 21.87it/s, loss=0.901, v_num=4mss, BTC_val\u001b[A\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 24:  99%|▉| 72/73 [00:02<00:00, 29.14it/s, loss=0.918, v_num=4mss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 73/73 [00:02<00:00, 28.72it/s, loss=0.918, v_num=4mss, BTC_val\u001b[A\n","Epoch 25:  99%|▉| 72/73 [00:02<00:00, 28.98it/s, loss=0.914, v_num=4mss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 73/73 [00:02<00:00, 28.44it/s, loss=0.914, v_num=4mss, BTC_val\u001b[A\n","Epoch 26:  99%|▉| 72/73 [00:02<00:00, 24.11it/s, loss=0.875, v_num=4mss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 73/73 [00:03<00:00, 23.88it/s, loss=0.875, v_num=4mss, BTC_val\u001b[A\n","Epoch 27:  99%|▉| 72/73 [00:02<00:00, 29.91it/s, loss=0.861, v_num=4mss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 73/73 [00:02<00:00, 29.40it/s, loss=0.861, v_num=4mss, BTC_val\u001b[A\n","Epoch 28:  99%|▉| 72/73 [00:02<00:00, 27.89it/s, loss=0.881, v_num=4mss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 73/73 [00:02<00:00, 27.38it/s, loss=0.881, v_num=4mss, BTC_val\u001b[A\n","Epoch 29:  99%|▉| 72/73 [00:02<00:00, 24.71it/s, loss=0.908, v_num=4mss, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.950. Signaling Trainer to stop.\n","Epoch 29: 100%|█| 73/73 [00:02<00:00, 24.42it/s, loss=0.908, v_num=4mss, BTC_val\n","Epoch 29: 100%|█| 73/73 [00:02<00:00, 24.38it/s, loss=0.908, v_num=4mss, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 51.85it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.5357142686843872,\n"," 'BTC_test_f1': 0.3350889980792999,\n"," 'ETH_test_acc': 0.6428571343421936,\n"," 'ETH_test_f1': 0.5395269393920898,\n"," 'LTC_test_acc': 0.5357142686843872,\n"," 'LTC_test_f1': 0.40317463874816895,\n"," 'test_loss': 0.8673278093338013}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 81190\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024722-1yic4mss/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024722-1yic4mss/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.25\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.22944\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.50952\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.50909\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.99466\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 29\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2160\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 88\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621468130\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 103\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.53003\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.39505\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.52567\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.41504\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.51436\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.38816\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.91315\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.37037\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.33333\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.44444\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.01231\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.53571\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.33509\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.64286\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.53953\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.53571\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.40317\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.86733\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▄▆▃▄▃▃▄▃▃▅▅▅█▄▃▂▃▅▆▃▄▅▅▃▄▆▄▂▄▃▆▅▅▃▇▂▂▃▅▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▂▃▂▂▂▂▄▃▂▄▄▅▆▃▃▁▂▃▆▅▄▄▇▄▅▇▅▂▃▂▇▄▄▂█▂▂▃▄▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▄▅▄▅▁▄▃▄▄▄▄▄▆▆▄▄▂▅▄▄▄▅▆▂▆▅▄▃▄▆▆▅▇█▇▂▄▇█▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▂▄▂▂▁▃▂▃▃▄▄▃▄▄▂▃▁▅▃▃▃▅▆▂▆▄▄▃▃▆▆▅▅█▆▂▂▆▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▃▃▃▃▃▄▅▂▃█▃▅▄▆▁▆▃▄▃▂▃▄▄▃▆▄▆▃▃▅▄▄▃▇▅▄▄▃▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▁▁▂▂▂▃▁▂█▃▄▂▇▂▄▃▄▂▂▂▄▆▄▆▃▆▅▂▅▅▄▄▇▅▃▃▃▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▅▄▄▄▄▄▅▂▁▄▄▃▄█▃▃▄▄▄▄▃▁▇▁▁▄▅▄▂▂▄▃▂▁▃▃▃▁▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▃▃▃▄▅▅▄▆▆▆▆▄█▄▅▆▅▆▅▅▆▆▅▆▆▆▅▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▂▁▁▁▃▄▅▅▆▆▇▆▆█▆▇▇▆▇▇▆▇▇▇▇█▇▇▇▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▃▃▄▃▇▆▅▇▇▆▆▇▆▆▆▆██▇▇▇▇██▆█▆█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▃▁▁▂▃▅▆▆▇▇▇▆█▇▇▆▇▇▇█▇▇█████▇█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▃▃▄▅▄▅▆▆▅▆▅▅▆▅▇▄▆▆▅▆▆▆▇▅█▆▆▇▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▂▁▁▁▃▄▅▆▆▅▇▆▇▇▇▇▆▆▇▆▇▇▇█▆█▆▇▇▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▅▃▂▂▂▂▂▂▂▁▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▃▃▃▃▃▆▆▃▁▆▃▆▆▃▃▃▃▃▆▆▆▁▃▃█▃▁▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▂▂▂▂▄▇▆▄▁▅▄▇▆▄▄▄▄▄▆▇▆▁▄▄█▄▁▄▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc █████▅▅███▅▅▅▅▅▅█▅▁▁▅▅▅▅▅▅▅▅▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▁▁█▃▃█▆▆▃▃▃▃▃▃█▃▂▂▃▄▃▃▃▃▃▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ████▆▁▆▆▆▆▆▁▃▃▆▆▃▆▆▁▃▆▃▆▁▆▆▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▁▁▁█▁▆█▆█▆▁▄▃▆█▃▆█▁▃▆▃▆▁▆█▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ██▆▄▃▃▄▄▅▁▅▂▅▅▆▄▇▅▂▃▄▆▇▃▅▆▇▄▄▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1yic4mss\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:49:05.012405: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm__multi_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1rn6aec6\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024903-1rn6aec6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name               | Type        | Params\n","---------------------------------------------------\n","0 | lstm_1             | LSTM        | 67.1 K\n","1 | batch_norm1        | BatchNorm2d | 256   \n","2 | dropout            | Dropout     | 0     \n","3 | linear1            | ModuleList  | 8.3 K \n","4 | activation         | ReLU        | 0     \n","5 | output_layers      | ModuleList  | 195   \n","6 | cross_entropy_loss | ModuleList  | 0     \n","7 | f1_score           | F1          | 0     \n","8 | accuracy_score     | Accuracy    | 0     \n","---------------------------------------------------\n","75.8 K    Trainable params\n","0         Non-trainable params\n","75.8 K    Total params\n","0.303     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 73/74 [00:02<00:00, 31.31it/s, loss=1.12, v_num=aec6, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.092\n","Epoch 0: 100%|█| 74/74 [00:02<00:00, 30.77it/s, loss=1.12, v_num=aec6, BTC_val_a\n","Epoch 1:  99%|▉| 73/74 [00:02<00:00, 32.72it/s, loss=1.11, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 74/74 [00:02<00:00, 31.83it/s, loss=1.11, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 2:  99%|▉| 73/74 [00:02<00:00, 31.21it/s, loss=1.11, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 1.083\n","Epoch 2: 100%|█| 74/74 [00:02<00:00, 30.46it/s, loss=1.11, v_num=aec6, BTC_val_a\n","Epoch 3:  99%|▉| 73/74 [00:02<00:00, 29.84it/s, loss=1.1, v_num=aec6, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 74/74 [00:02<00:00, 29.19it/s, loss=1.1, v_num=aec6, BTC_val_ac\u001b[A\n","Epoch 4:  99%|▉| 73/74 [00:02<00:00, 29.42it/s, loss=1.09, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 74/74 [00:02<00:00, 28.99it/s, loss=1.09, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 5:  99%|▉| 73/74 [00:02<00:00, 32.12it/s, loss=1.1, v_num=aec6, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 74/74 [00:02<00:00, 31.45it/s, loss=1.1, v_num=aec6, BTC_val_ac\u001b[A\n","Epoch 6:  99%|▉| 73/74 [00:02<00:00, 28.63it/s, loss=1.11, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 1.073\n","Epoch 6: 100%|█| 74/74 [00:02<00:00, 28.04it/s, loss=1.11, v_num=aec6, BTC_val_a\n","Epoch 7:  99%|▉| 73/74 [00:02<00:00, 28.92it/s, loss=1.11, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 74/74 [00:02<00:00, 28.49it/s, loss=1.11, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 8:  99%|▉| 73/74 [00:02<00:00, 32.33it/s, loss=1.1, v_num=aec6, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 74/74 [00:02<00:00, 31.75it/s, loss=1.1, v_num=aec6, BTC_val_ac\u001b[A\n","Epoch 9:  99%|▉| 73/74 [00:02<00:00, 31.32it/s, loss=1.1, v_num=aec6, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 74/74 [00:02<00:00, 30.71it/s, loss=1.1, v_num=aec6, BTC_val_ac\u001b[A\n","Epoch 10:  99%|▉| 73/74 [00:02<00:00, 30.87it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 74/74 [00:02<00:00, 30.16it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 11:  99%|▉| 73/74 [00:02<00:00, 30.25it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 74/74 [00:02<00:00, 29.48it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 12:  99%|▉| 73/74 [00:02<00:00, 26.41it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 74/74 [00:02<00:00, 26.00it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 13:  99%|▉| 73/74 [00:02<00:00, 27.22it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 74/74 [00:02<00:00, 26.73it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 14:  99%|▉| 73/74 [00:02<00:00, 25.04it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 74/74 [00:02<00:00, 24.78it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 15:  99%|▉| 73/74 [00:02<00:00, 31.34it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 74/74 [00:02<00:00, 30.67it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 16:  99%|▉| 73/74 [00:02<00:00, 25.58it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 74/74 [00:02<00:00, 25.17it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 17:  99%|▉| 73/74 [00:02<00:00, 27.71it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 74/74 [00:02<00:00, 27.30it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 18:  99%|▉| 73/74 [00:02<00:00, 26.68it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 74/74 [00:02<00:00, 26.18it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 19:  99%|▉| 73/74 [00:02<00:00, 26.93it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 74/74 [00:02<00:00, 26.60it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 20:  99%|▉| 73/74 [00:02<00:00, 27.11it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 74/74 [00:02<00:00, 26.50it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 21:  99%|▉| 73/74 [00:02<00:00, 27.53it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 74/74 [00:02<00:00, 27.02it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 22:  99%|▉| 73/74 [00:03<00:00, 24.30it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 74/74 [00:03<00:00, 24.00it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 23:  99%|▉| 73/74 [00:02<00:00, 24.91it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 1.067\n","Epoch 23: 100%|█| 74/74 [00:03<00:00, 24.53it/s, loss=1.1, v_num=aec6, BTC_val_a\n","Epoch 24:  99%|▉| 73/74 [00:02<00:00, 27.79it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 74/74 [00:02<00:00, 27.33it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 25:  99%|▉| 73/74 [00:03<00:00, 23.53it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 74/74 [00:03<00:00, 22.79it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 26:  99%|▉| 73/74 [00:02<00:00, 28.32it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 74/74 [00:02<00:00, 27.86it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 27:  99%|▉| 73/74 [00:02<00:00, 28.60it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 74/74 [00:02<00:00, 27.76it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 28:  99%|▉| 73/74 [00:02<00:00, 25.26it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 74/74 [00:02<00:00, 24.93it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 29:  99%|▉| 73/74 [00:02<00:00, 24.82it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 74/74 [00:03<00:00, 24.43it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 30:  99%|▉| 73/74 [00:03<00:00, 24.31it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 74/74 [00:03<00:00, 24.04it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 31:  99%|▉| 73/74 [00:02<00:00, 24.52it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 74/74 [00:03<00:00, 24.12it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 32:  99%|▉| 73/74 [00:02<00:00, 24.37it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 74/74 [00:03<00:00, 24.10it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 33:  99%|▉| 73/74 [00:03<00:00, 22.63it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 1.063\n","Epoch 33: 100%|█| 74/74 [00:03<00:00, 22.35it/s, loss=1.1, v_num=aec6, BTC_val_a\n","Epoch 34:  99%|▉| 73/74 [00:02<00:00, 24.71it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 74/74 [00:03<00:00, 24.39it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 1.056\n","Epoch 35:  99%|▉| 73/74 [00:03<00:00, 22.92it/s, loss=1.09, v_num=aec6, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 35: 100%|█| 74/74 [00:03<00:00, 22.67it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n","Epoch 36:  99%|▉| 73/74 [00:02<00:00, 29.21it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 36: 100%|█| 74/74 [00:02<00:00, 28.65it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n","Epoch 37:  99%|▉| 73/74 [00:03<00:00, 22.19it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 37: 100%|█| 74/74 [00:03<00:00, 21.96it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 38:  99%|▉| 73/74 [00:02<00:00, 25.95it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 38: 100%|█| 74/74 [00:02<00:00, 25.58it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 39:  99%|▉| 73/74 [00:02<00:00, 27.05it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 39: 100%|█| 74/74 [00:02<00:00, 26.41it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 40:  99%|▉| 73/74 [00:02<00:00, 28.64it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 40: 100%|█| 74/74 [00:02<00:00, 28.16it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n","Epoch 41:  99%|▉| 73/74 [00:03<00:00, 21.60it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 41: 100%|█| 74/74 [00:03<00:00, 21.34it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 42:  99%|▉| 73/74 [00:02<00:00, 25.57it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 42: 100%|█| 74/74 [00:02<00:00, 25.16it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 43:  99%|▉| 73/74 [00:02<00:00, 27.04it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 1.047\n","Epoch 43: 100%|█| 74/74 [00:02<00:00, 25.87it/s, loss=1.1, v_num=aec6, BTC_val_a\n","Epoch 44:  99%|▉| 73/74 [00:03<00:00, 23.59it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 44: 100%|█| 74/74 [00:03<00:00, 23.28it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 45:  99%|▉| 73/74 [00:03<00:00, 22.76it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 45: 100%|█| 74/74 [00:03<00:00, 22.52it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 46:  99%|▉| 73/74 [00:02<00:00, 26.17it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n","Epoch 46: 100%|█| 74/74 [00:03<00:00, 24.16it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 47:  99%|▉| 73/74 [00:03<00:00, 23.24it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 47: 100%|█| 74/74 [00:03<00:00, 23.01it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 48:  99%|▉| 73/74 [00:02<00:00, 25.24it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 48: 100%|█| 74/74 [00:02<00:00, 24.82it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n","Epoch 49:  99%|▉| 73/74 [00:02<00:00, 25.61it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 49: 100%|█| 74/74 [00:02<00:00, 25.29it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 50:  99%|▉| 73/74 [00:02<00:00, 29.40it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 50: 100%|█| 74/74 [00:02<00:00, 28.82it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 51:  99%|▉| 73/74 [00:03<00:00, 21.35it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 51: 100%|█| 74/74 [00:03<00:00, 21.17it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 52:  99%|▉| 73/74 [00:03<00:00, 22.42it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 52: 100%|█| 74/74 [00:03<00:00, 22.14it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 53:  99%|▉| 73/74 [00:02<00:00, 24.90it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 53: 100%|█| 74/74 [00:03<00:00, 24.58it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n","Epoch 54:  99%|▉| 73/74 [00:03<00:00, 22.03it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 54: 100%|█| 74/74 [00:03<00:00, 21.82it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 55:  99%|▉| 73/74 [00:02<00:00, 26.82it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 55: 100%|█| 74/74 [00:02<00:00, 26.42it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 56:  99%|▉| 73/74 [00:02<00:00, 25.83it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 56: 100%|█| 74/74 [00:02<00:00, 25.29it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 57:  99%|▉| 73/74 [00:02<00:00, 27.63it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 57: 100%|█| 74/74 [00:02<00:00, 27.24it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 58:  99%|▉| 73/74 [00:02<00:00, 25.96it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 58: 100%|█| 74/74 [00:02<00:00, 25.52it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n","Epoch 59:  99%|▉| 73/74 [00:02<00:00, 26.15it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 59: 100%|█| 74/74 [00:02<00:00, 25.65it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 60:  99%|▉| 73/74 [00:03<00:00, 22.98it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 60: 100%|█| 74/74 [00:03<00:00, 22.65it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 61:  99%|▉| 73/74 [00:02<00:00, 27.86it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 61: 100%|█| 74/74 [00:02<00:00, 27.45it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Epoch 62:  99%|▉| 73/74 [00:03<00:00, 21.69it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 62: 100%|█| 74/74 [00:03<00:00, 21.40it/s, loss=1.09, v_num=aec6, BTC_val_\u001b[A\n","Epoch 63:  99%|▉| 73/74 [00:02<00:00, 27.50it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.047. Signaling Trainer to stop.\n","Epoch 63: 100%|█| 74/74 [00:02<00:00, 27.11it/s, loss=1.1, v_num=aec6, BTC_val_a\n","Epoch 63: 100%|█| 74/74 [00:02<00:00, 27.07it/s, loss=1.1, v_num=aec6, BTC_val_a\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n"],"name":"stdout"},{"output_type":"stream","text":["Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 49.83it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.25,\n"," 'BTC_test_f1': 0.1315789520740509,\n"," 'ETH_test_acc': 0.4285714328289032,\n"," 'ETH_test_f1': 0.1991342157125473,\n"," 'LTC_test_acc': 0.3928571343421936,\n"," 'LTC_test_f1': 0.18594105541706085,\n"," 'test_loss': 1.135124683380127}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 81471\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024903-1rn6aec6/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_024903-1rn6aec6/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.4375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.34799\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.4375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.33868\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.3125\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.25641\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.08357\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 63\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4672\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 193\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621468336\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 221\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.37489\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.27562\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.35671\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.27088\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.34892\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.2646\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09686\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.18182\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.18182\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.25641\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.0764\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.25\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.13158\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.42857\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19913\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.39286\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.18594\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.13512\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▃▃▄▁▆▃█▂▃▃▃▁▄▂▆▆▅▅▆▆▃▅▅▅▆▄▅▄▇▆▇▅▆▆█▃▃▃▃▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▃▃▄▁█▃█▂▄▃▂▂▄▂▅▄▅▅▅▅▃▄▅▃▅▄▅▃▅▅▆▄▅▆▇▃▄▃▃▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▅▅▆▃▆█▆▅▆▅▅▂▇▁▅▅██▅▅▅▄▆▅█▃▅▃▄▂▅▇▆▆▆▅▅▅▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▅▄▄▃▆▇▆▅▅▅▃▂▆▁▃▄▇█▅▃▄▃▆▃█▂▄▂▃▁▅▅▅▄▄▄▄▄▄▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▃▄▆▃▃▅▄▇▇▆▆▅▇▅▄▇▆▆▄█▄▄▄█▇▄▂▅▆▆▆▆▅▆▆▁██▃▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▂▄▄▃▃▅▃▇▇▅▅▅▅▄▃▇▅▆▃█▃▃▃▆█▃▂▃▅▅▅▅▄▄▅▁█▆▂▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▄▂█▃▂▁▂▂▃▃▄▂▃▂▃▂▃▃▂▃▃▂▂▃▃▄▃▁▃▂▂▃▃▂▄▂▂▃▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▃▆▃▃▃▅▃▅▅▇▆▄▄█▅▄▄▄▄▇▆▇▅▆▆▇▆▅▇▅▆▅▄▅▅▃█▇▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▄▅▇▄▅▅▇▄█▄▆▅▂▅▅▅▆▄▃▂▃▄▅▁▃▂▂▄▂▂▂▂▄▂▃▁▂▄▄▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▁▃▄▆▇▆▇▅▆▅▄▄▃▆▄▆▅▆▆▆▆▆▆▆▇█▅▆▆▆▆█▅▇▇▅▆▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▃▂▃▄█▇▇▇▆▄▃▄▃▃▃▅▇▄▅▂▁▃▂▁▂▂▄▁▁▁▃▁▇▃▄▃▃▂▃▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▃▄▁▃▅▄▂▃▄█▂▅▅▆▁▆▇▃▃▄▄▇▄▅▅▆▅▅▁▃▄▅▄▆▄▅█▅▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▇█▄▅█▆▆▅██▅▆▅▇▁▇▇▅▅▃▃▅▃▃▄▃▃▃▁▁▃▃▄▅▃▃▇▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▃▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▃▃▅▃▃▃▃▃▃▃█▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▂▂▅▂▂▂▂▂▂▂█▂▃▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc █▁█▁████▁█▁██▁█▁████████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▃▁█▁▃▃▃▃▁▃▁▃█▂▃▁▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc █▁▅▁██▁█▁█▁█▁▁▅▁▁███████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▄▁▆▁▄▄▁█▁▄▁▄▁▁▂▁▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅█▄▅▄▄▄▃▄▄▄▄▄▅▄▄▅▃▃▄▃▃▂▃▄▂▄▁▄▂▂▄▃▅▂▃▂▁▃▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_single_lstm__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1rn6aec6\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:52:33.592465: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm__binary_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3fmo25vq\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_025232-3fmo25vq\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 130   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 0:  99%|▉| 72/73 [00:04<00:00, 14.57it/s, loss=0.683, v_num=25vq, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.691\n","Epoch 0: 100%|█| 73/73 [00:05<00:00, 14.54it/s, loss=0.683, v_num=25vq, BTC_val_\n","Epoch 1:  99%|▉| 72/73 [00:05<00:00, 13.96it/s, loss=0.617, v_num=25vq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.068 >= min_delta = 0.003. New best score: 0.623\n","Epoch 1: 100%|█| 73/73 [00:05<00:00, 13.90it/s, loss=0.617, v_num=25vq, BTC_val_\n","Epoch 2:  99%|▉| 72/73 [00:05<00:00, 14.28it/s, loss=0.563, v_num=25vq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.016 >= min_delta = 0.003. New best score: 0.608\n","Epoch 2: 100%|█| 73/73 [00:05<00:00, 14.23it/s, loss=0.563, v_num=25vq, BTC_val_\n","Epoch 3:  99%|▉| 72/73 [00:06<00:00, 11.62it/s, loss=0.602, v_num=25vq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 73/73 [00:06<00:00, 11.62it/s, loss=0.602, v_num=25vq, BTC_val_\u001b[A\n","Epoch 4:  99%|▉| 72/73 [00:07<00:00,  9.66it/s, loss=0.607, v_num=25vq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 73/73 [00:07<00:00,  9.69it/s, loss=0.607, v_num=25vq, BTC_val_\u001b[A\n","Epoch 5:  99%|▉| 72/73 [00:05<00:00, 12.35it/s, loss=0.601, v_num=25vq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 73/73 [00:05<00:00, 12.34it/s, loss=0.601, v_num=25vq, BTC_val_\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.015 >= min_delta = 0.003. New best score: 0.593\n","Epoch 6:  99%|▉| 72/73 [00:05<00:00, 13.86it/s, loss=0.57, v_num=25vq, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 73/73 [00:05<00:00, 13.80it/s, loss=0.57, v_num=25vq, BTC_val_a\u001b[A\n","Epoch 7:  99%|▉| 72/73 [00:06<00:00, 10.82it/s, loss=0.591, v_num=25vq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 73/73 [00:06<00:00, 10.83it/s, loss=0.591, v_num=25vq, BTC_val_\u001b[A\n","Epoch 8:  99%|▉| 72/73 [00:07<00:00,  9.71it/s, loss=0.577, v_num=25vq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 73/73 [00:07<00:00,  9.72it/s, loss=0.577, v_num=25vq, BTC_val_\u001b[A\n","Epoch 9:  99%|▉| 72/73 [00:05<00:00, 13.56it/s, loss=0.547, v_num=25vq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 73/73 [00:05<00:00, 13.50it/s, loss=0.547, v_num=25vq, BTC_val_\u001b[A\n","Epoch 10:  99%|▉| 72/73 [00:05<00:00, 12.57it/s, loss=0.578, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 73/73 [00:05<00:00, 12.54it/s, loss=0.578, v_num=25vq, BTC_val\u001b[A\n","Epoch 11:  99%|▉| 72/73 [00:06<00:00, 11.47it/s, loss=0.564, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 73/73 [00:06<00:00, 11.46it/s, loss=0.564, v_num=25vq, BTC_val\u001b[A\n","Epoch 12:  99%|▉| 72/73 [00:05<00:00, 12.20it/s, loss=0.563, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 73/73 [00:05<00:00, 12.18it/s, loss=0.563, v_num=25vq, BTC_val\u001b[A\n","Epoch 13:  99%|▉| 72/73 [00:06<00:00, 11.88it/s, loss=0.586, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.586\n","Epoch 13: 100%|█| 73/73 [00:06<00:00, 11.86it/s, loss=0.586, v_num=25vq, BTC_val\n","Epoch 14:  99%|▉| 72/73 [00:06<00:00, 11.60it/s, loss=0.601, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 73/73 [00:06<00:00, 11.58it/s, loss=0.601, v_num=25vq, BTC_val\u001b[A\n","Epoch 15:  99%|▉| 72/73 [00:06<00:00, 11.92it/s, loss=0.589, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 73/73 [00:06<00:00, 11.93it/s, loss=0.589, v_num=25vq, BTC_val\u001b[A\n","Epoch 16:  99%|▉| 72/73 [00:05<00:00, 12.54it/s, loss=0.585, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 73/73 [00:05<00:00, 12.51it/s, loss=0.585, v_num=25vq, BTC_val\u001b[A\n","Epoch 17:  99%|▉| 72/73 [00:06<00:00, 11.94it/s, loss=0.551, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 73/73 [00:06<00:00, 11.94it/s, loss=0.551, v_num=25vq, BTC_val\u001b[A\n","Epoch 18:  99%|▉| 72/73 [00:06<00:00, 11.82it/s, loss=0.554, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 73/73 [00:06<00:00, 11.81it/s, loss=0.554, v_num=25vq, BTC_val\u001b[A\n","Epoch 19:  99%|▉| 72/73 [00:05<00:00, 12.21it/s, loss=0.558, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 73/73 [00:05<00:00, 12.20it/s, loss=0.558, v_num=25vq, BTC_val\u001b[A\n","Epoch 20:  99%|▉| 72/73 [00:05<00:00, 12.90it/s, loss=0.533, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 73/73 [00:05<00:00, 12.86it/s, loss=0.533, v_num=25vq, BTC_val\u001b[A\n","Epoch 21:  99%|▉| 72/73 [00:05<00:00, 12.05it/s, loss=0.553, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 73/73 [00:06<00:00, 12.03it/s, loss=0.553, v_num=25vq, BTC_val\u001b[A\n","Epoch 22:  99%|▉| 72/73 [00:05<00:00, 13.21it/s, loss=0.556, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 73/73 [00:05<00:00, 13.17it/s, loss=0.556, v_num=25vq, BTC_val\u001b[A\n","Epoch 23:  99%|▉| 72/73 [00:05<00:00, 13.18it/s, loss=0.59, v_num=25vq, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 73/73 [00:05<00:00, 13.13it/s, loss=0.59, v_num=25vq, BTC_val_\u001b[A\n","Epoch 24:  99%|▉| 72/73 [00:06<00:00, 11.15it/s, loss=0.549, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 73/73 [00:06<00:00, 11.14it/s, loss=0.549, v_num=25vq, BTC_val\u001b[A\n","Epoch 25:  99%|▉| 72/73 [00:06<00:00, 11.02it/s, loss=0.528, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 73/73 [00:06<00:00, 11.03it/s, loss=0.528, v_num=25vq, BTC_val\u001b[A\n","Epoch 26:  99%|▉| 72/73 [00:05<00:00, 14.30it/s, loss=0.524, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 73/73 [00:05<00:00, 14.23it/s, loss=0.524, v_num=25vq, BTC_val\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.032 >= min_delta = 0.003. New best score: 0.554\n","Epoch 27:  99%|▉| 72/73 [00:05<00:00, 13.36it/s, loss=0.562, v_num=25vq, BTC_val\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 73/73 [00:05<00:00, 13.33it/s, loss=0.562, v_num=25vq, BTC_val\u001b[A\n","Epoch 28:  99%|▉| 72/73 [00:05<00:00, 12.18it/s, loss=0.537, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 73/73 [00:06<00:00, 12.11it/s, loss=0.537, v_num=25vq, BTC_val\u001b[A\n","Epoch 29:  99%|▉| 72/73 [00:06<00:00, 11.43it/s, loss=0.538, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 73/73 [00:06<00:00, 11.41it/s, loss=0.538, v_num=25vq, BTC_val\u001b[A\n","Epoch 30:  99%|▉| 72/73 [00:05<00:00, 12.34it/s, loss=0.511, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 73/73 [00:05<00:00, 12.31it/s, loss=0.511, v_num=25vq, BTC_val\u001b[A\n","Epoch 31:  99%|▉| 72/73 [00:05<00:00, 13.70it/s, loss=0.552, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 73/73 [00:05<00:00, 13.68it/s, loss=0.552, v_num=25vq, BTC_val\u001b[A\n","Epoch 32:  99%|▉| 72/73 [00:05<00:00, 13.80it/s, loss=0.524, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 73/73 [00:05<00:00, 13.74it/s, loss=0.524, v_num=25vq, BTC_val\u001b[A\n","Epoch 33:  99%|▉| 72/73 [00:05<00:00, 14.16it/s, loss=0.502, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 33: 100%|█| 73/73 [00:05<00:00, 14.08it/s, loss=0.502, v_num=25vq, BTC_val\u001b[A\n","Epoch 34:  99%|▉| 72/73 [00:05<00:00, 13.52it/s, loss=0.538, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 73/73 [00:05<00:00, 13.49it/s, loss=0.538, v_num=25vq, BTC_val\u001b[A\n","Epoch 35:  99%|▉| 72/73 [00:06<00:00, 11.90it/s, loss=0.557, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 35: 100%|█| 73/73 [00:06<00:00, 11.90it/s, loss=0.557, v_num=25vq, BTC_val\u001b[A\n","Epoch 36:  99%|▉| 72/73 [00:06<00:00, 11.61it/s, loss=0.554, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 36: 100%|█| 73/73 [00:06<00:00, 11.60it/s, loss=0.554, v_num=25vq, BTC_val\u001b[A\n","Epoch 37:  99%|▉| 72/73 [00:06<00:00, 11.24it/s, loss=0.545, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 37: 100%|█| 73/73 [00:06<00:00, 11.24it/s, loss=0.545, v_num=25vq, BTC_val\u001b[A\n","Epoch 38:  99%|▉| 72/73 [00:05<00:00, 12.33it/s, loss=0.529, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 38: 100%|█| 73/73 [00:05<00:00, 12.31it/s, loss=0.529, v_num=25vq, BTC_val\u001b[A\n","Epoch 39:  99%|▉| 72/73 [00:06<00:00, 10.45it/s, loss=0.553, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 39: 100%|█| 73/73 [00:06<00:00, 10.47it/s, loss=0.553, v_num=25vq, BTC_val\u001b[A\n","Epoch 40:  99%|▉| 72/73 [00:06<00:00, 11.99it/s, loss=0.515, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 40: 100%|█| 73/73 [00:06<00:00, 11.99it/s, loss=0.515, v_num=25vq, BTC_val\u001b[A\n","Epoch 41:  99%|▉| 72/73 [00:05<00:00, 13.87it/s, loss=0.499, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 41: 100%|█| 73/73 [00:05<00:00, 13.79it/s, loss=0.499, v_num=25vq, BTC_val\u001b[A\n","Epoch 42:  99%|▉| 72/73 [00:05<00:00, 13.94it/s, loss=0.505, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 42: 100%|█| 73/73 [00:05<00:00, 13.88it/s, loss=0.505, v_num=25vq, BTC_val\u001b[A\n","Epoch 43:  99%|▉| 72/73 [00:05<00:00, 13.20it/s, loss=0.478, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 43: 100%|█| 73/73 [00:05<00:00, 13.18it/s, loss=0.478, v_num=25vq, BTC_val\u001b[A\n","Epoch 44:  99%|▉| 72/73 [00:06<00:00, 11.25it/s, loss=0.491, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 44: 100%|█| 73/73 [00:06<00:00, 11.24it/s, loss=0.491, v_num=25vq, BTC_val\u001b[A\n","Epoch 45:  99%|▉| 72/73 [00:06<00:00, 11.76it/s, loss=0.532, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 45: 100%|█| 73/73 [00:06<00:00, 11.73it/s, loss=0.532, v_num=25vq, BTC_val\u001b[A\n","Epoch 46:  99%|▉| 72/73 [00:07<00:00, 10.14it/s, loss=0.514, v_num=25vq, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.554. Signaling Trainer to stop.\n","Epoch 46: 100%|█| 73/73 [00:07<00:00, 10.16it/s, loss=0.514, v_num=25vq, BTC_val\n","Epoch 46: 100%|█| 73/73 [00:07<00:00, 10.15it/s, loss=0.514, v_num=25vq, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 33.13it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.6785714030265808,\n"," 'BTC_test_f1': 0.6746674180030823,\n"," 'ETH_test_acc': 0.6785714030265808,\n"," 'ETH_test_f1': 0.6744208335876465,\n"," 'LTC_test_acc': 0.7142857313156128,\n"," 'LTC_test_f1': 0.7079364657402039,\n"," 'test_loss': 0.5609579086303711}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 81967\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_025232-3fmo25vq/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_025232-3fmo25vq/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.56364\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.45894\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.49206\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.70908\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 46\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3384\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 290\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621468642\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 161\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.75283\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.74235\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.74674\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.73339\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.74064\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.7291\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.51025\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.56364\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.46667\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.61905\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.61646\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.67857\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.67467\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.67857\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.67442\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.71429\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.70794\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.56096\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▃▅▅█▅▇▆▅▅█▅▆▇▅▇█▆▆█▄▆▆█▃▄▆▅▇▆█▆▆▅▆▁▅▄▆▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▂▅▅█▅▇▆▅▅█▅▆▇▅▇█▆▆▆▄▆▆█▃▄▆▅▇▆█▆▆▅▅▁▅▄▅▅▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▂▆▂▅▁▆▇▃▆▆▅▃▆▅▃▇▂▅▆▂▂▆▅▂▂▃▂▅▅▇█▆▃▅▁█▃▅▇▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▃▆▃▅▂▆▇▄▆▆▅▄▆▅▄▇▃▅▄▃▃▇▅▃▃▄▃▅▅▇█▆▄▅▂█▄▅▇▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▄▄▆▆▄▆▇▅▅█▅▄█▄▅▆▆▅▄▅▄▆▆▃▄▆▄▆▆▆▆▆▆▁▄▆▅▅▅▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▄▄▇▇▄▆▇▅▅█▅▄█▄▅▆▆▅▄▅▄▆▇▄▄▅▄▇▆▆▆▇▆▁▄▆▅▅▅▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▅▆▃▇▃▂▅▅▂▅▄▂▆▅▃▄▅▃▄▇▃▂█▇▄▆▄▅▁▁▃▄▆█▃▄▃▄█\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇▇▇▇████▇██████\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▅▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇██▇██▇▇████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▅▆▆▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇█▇▇▇█▇██▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇█▇▇▇██▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▃▆▃▃▃▆▃▃█▃█▃█▆▃▆▆▃▆▆▆▃▆▃▆▆▆███▆▆▃▆▆▃▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▄▆▄▄▄▆▄▄█▄█▄█▆▄▆▆▄▆▆▆▄▆▄▆▆▆███▆▆▄▆▆▄▄▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▆▃▆▆▃▃▃▃▃▃▃▃▆▃▆▃▃█▃▃▃▃▃▃▃▃▆▃▃▃▃▃▆▃▃▆▆▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▆▄▆▆▄▄▄▄▄▄▄▄▅▄▆▄▄█▄▄▄▄▄▄▄▄▆▄▄▄▄▄▆▄▄▆▆▆▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▅▅▅▅▅▁▁▅█▅▅▁▅█▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▁▅▅▅▁▅▅▅▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▅▅▅▅▅▁▁▅█▃▅▁▅▆▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▃▁▅▅▅▁▅▅▅▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▃▂▃▃▂▂▃▄▂▄▂▂▂▂▃▂▂▂▂▂▂▃▃▅▁▁▃▂▂▃▃▄▇▃▂▆█▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3fmo25vq\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 02:57:40.574165: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm__binary_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1o3x1f2w\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_025739-1o3x1f2w\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 130   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:   0%|                                           | 0/74 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0: 100%|█| 74/74 [00:05<00:00, 13.39it/s, loss=0.706, v_num=1f2w, BTC_val_\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.703\n","Epoch 0: 100%|█| 74/74 [00:05<00:00, 13.31it/s, loss=0.706, v_num=1f2w, BTC_val_\n","Epoch 1: 100%|█| 74/74 [00:05<00:00, 13.58it/s, loss=0.692, v_num=1f2w, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.034 >= min_delta = 0.003. New best score: 0.669\n","Epoch 1: 100%|█| 74/74 [00:05<00:00, 13.51it/s, loss=0.692, v_num=1f2w, BTC_val_\n","Epoch 2: 100%|█| 74/74 [00:06<00:00, 12.29it/s, loss=0.691, v_num=1f2w, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 74/74 [00:06<00:00, 12.23it/s, loss=0.691, v_num=1f2w, BTC_val_\u001b[A\n","Epoch 3: 100%|█| 74/74 [00:05<00:00, 13.43it/s, loss=0.693, v_num=1f2w, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 74/74 [00:05<00:00, 13.37it/s, loss=0.693, v_num=1f2w, BTC_val_\u001b[A\n","Epoch 4: 100%|█| 74/74 [00:05<00:00, 12.63it/s, loss=0.7, v_num=1f2w, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 74/74 [00:05<00:00, 12.57it/s, loss=0.7, v_num=1f2w, BTC_val_ac\u001b[A\n","Epoch 5: 100%|█| 74/74 [00:05<00:00, 14.29it/s, loss=0.694, v_num=1f2w, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 74/74 [00:05<00:00, 14.20it/s, loss=0.694, v_num=1f2w, BTC_val_\u001b[A\n","Epoch 6: 100%|█| 74/74 [00:05<00:00, 14.62it/s, loss=0.694, v_num=1f2w, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 74/74 [00:05<00:00, 14.50it/s, loss=0.694, v_num=1f2w, BTC_val_\u001b[A\n","Epoch 7: 100%|█| 74/74 [00:05<00:00, 12.62it/s, loss=0.693, v_num=1f2w, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 74/74 [00:05<00:00, 12.55it/s, loss=0.693, v_num=1f2w, BTC_val_\u001b[A\n","Epoch 8: 100%|█| 74/74 [00:06<00:00, 12.28it/s, loss=0.693, v_num=1f2w, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 74/74 [00:06<00:00, 12.22it/s, loss=0.693, v_num=1f2w, BTC_val_\u001b[A\n","Epoch 9: 100%|█| 74/74 [00:06<00:00, 11.39it/s, loss=0.699, v_num=1f2w, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 74/74 [00:06<00:00, 11.34it/s, loss=0.699, v_num=1f2w, BTC_val_\u001b[A\n","Epoch 10: 100%|█| 74/74 [00:06<00:00, 11.85it/s, loss=0.694, v_num=1f2w, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 74/74 [00:06<00:00, 11.79it/s, loss=0.694, v_num=1f2w, BTC_val\u001b[A\n","Epoch 11: 100%|█| 74/74 [00:05<00:00, 13.77it/s, loss=0.692, v_num=1f2w, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 74/74 [00:05<00:00, 13.69it/s, loss=0.692, v_num=1f2w, BTC_val\u001b[A\n","Epoch 12: 100%|█| 74/74 [00:06<00:00, 11.41it/s, loss=0.695, v_num=1f2w, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 74/74 [00:06<00:00, 11.36it/s, loss=0.695, v_num=1f2w, BTC_val\u001b[A\n","Epoch 13: 100%|█| 74/74 [00:06<00:00, 11.55it/s, loss=0.694, v_num=1f2w, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 74/74 [00:06<00:00, 11.51it/s, loss=0.694, v_num=1f2w, BTC_val\u001b[A\n","Epoch 14: 100%|█| 74/74 [00:06<00:00, 12.30it/s, loss=0.693, v_num=1f2w, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 74/74 [00:06<00:00, 12.25it/s, loss=0.693, v_num=1f2w, BTC_val\u001b[A\n","Epoch 15: 100%|█| 74/74 [00:06<00:00, 11.69it/s, loss=0.693, v_num=1f2w, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 74/74 [00:06<00:00, 11.64it/s, loss=0.693, v_num=1f2w, BTC_val\u001b[A\n","Epoch 16: 100%|█| 74/74 [00:06<00:00, 12.31it/s, loss=0.694, v_num=1f2w, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 74/74 [00:06<00:00, 12.26it/s, loss=0.694, v_num=1f2w, BTC_val\u001b[A\n","Epoch 17: 100%|█| 74/74 [00:06<00:00, 11.96it/s, loss=0.696, v_num=1f2w, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 74/74 [00:06<00:00, 11.91it/s, loss=0.696, v_num=1f2w, BTC_val\u001b[A\n","Epoch 18: 100%|█| 74/74 [00:05<00:00, 13.88it/s, loss=0.693, v_num=1f2w, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 74/74 [00:05<00:00, 13.81it/s, loss=0.693, v_num=1f2w, BTC_val\u001b[A\n","Epoch 19: 100%|█| 74/74 [00:06<00:00, 12.29it/s, loss=0.69, v_num=1f2w, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 74/74 [00:06<00:00, 12.24it/s, loss=0.69, v_num=1f2w, BTC_val_\u001b[A\n","Epoch 20: 100%|█| 74/74 [00:06<00:00, 12.31it/s, loss=0.692, v_num=1f2w, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 74/74 [00:06<00:00, 12.26it/s, loss=0.692, v_num=1f2w, BTC_val\u001b[A\n","Epoch 21: 100%|█| 74/74 [00:05<00:00, 13.05it/s, loss=0.695, v_num=1f2w, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 74/74 [00:05<00:00, 12.98it/s, loss=0.695, v_num=1f2w, BTC_val\u001b[A\n","                                                                                \u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.669. Signaling Trainer to stop.\n","Epoch 21: 100%|█| 74/74 [00:05<00:00, 12.97it/s, loss=0.695, v_num=1f2w, BTC_val\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n"],"name":"stdout"},{"output_type":"stream","text":["Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 24.68it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.3928571343421936,\n"," 'BTC_test_f1': 0.2789115607738495,\n"," 'ETH_test_acc': 0.6428571343421936,\n"," 'ETH_test_f1': 0.38938775658607483,\n"," 'LTC_test_acc': 0.4285714328289032,\n"," 'LTC_test_f1': 0.293949156999588,\n"," 'test_loss': 0.7038599848747253}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 82654\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_025739-1o3x1f2w/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_025739-1o3x1f2w/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.38462\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.5625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.45894\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.5625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.36\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.68735\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 21\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1606\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 138\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621468797\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 76\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.53074\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.38029\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.50736\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.35524\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.50043\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.36355\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.69296\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.38462\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.38462\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.38462\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.6806\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.39286\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.27891\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.64286\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.38939\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.42857\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.29395\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.70386\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▄█▅▅▅▁▄█▁▂▅▄▆▅▅▅▇▅▆▅▅▅▆▇▅▅▄▅▅▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▄█▅▅▅▁▂█▁▂▅▂▃▅▅▄▅▃▅▅▃▄▅▃▄▄▂▅▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▃▄▄▄▅▄▄▅▅▅▄▅▅▅▃▆██▆▄▆▇▄▅▇▁▅▅▆▆▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▃▄▄▃▆▃▄▄▅▅▄▅▅▃▃▆█▆▆▂▇▄▂▄▆▁▃▅▆▄▃▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▆▂▃▅▅▄▁▂▁▁▃▆▄▅▂▆▅▄▆█▁▄▂▅▅▃▆▃▁▆▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▆▂▃▅▅▄▁▂▁▁▂▆▄▃▂▆▃▂▃█▁▂▂▄▃▂▅▃▁▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▆▁▂▃▃▄▃▂▄▄▂▃▂▄▂▁▁▁▂▂▂▄▂▁▃▂▃▃▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▃█▅▄▂▃▅▅█▆▄▆▇▆█▆█▄▅▆▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▅▆█▇▅▆▄▄▆▃▆▄▃▃▆▅▄▃▅▄▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▃▆▁▆▄▃▄▃▄██▆▆▇▃▅▄▆▇█▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▇█▆█▆▇▅▆▇▃█▅▄▄▆▅▄▃▇▆▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch █▃▄█▁▁▂▅█▅▆▄▂▆▄▂▃▃▁▃▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch █▆▇█▅▆▄▆█▂▇▄▃▃▅▃▃▂▅▄▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▃▂▃▃▂▁▁▁▂▁▁▁▁▁▂▁▂▁▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁█████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁█████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁█████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁█████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁█████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁█████████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▁▃▃▃▃▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1o3x1f2w\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-05-20 03:00:17.004036: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm__multi_classification_trend_removed\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/14vn2tag\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_030015-14vn2tag\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 195   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:   0%|                                           | 0/73 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 72/73 [00:04<00:00, 14.72it/s, loss=1.05, v_num=2tag, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.047\n","Epoch 0: 100%|█| 73/73 [00:04<00:00, 14.64it/s, loss=1.05, v_num=2tag, BTC_val_a\n","Epoch 1:  99%|▉| 72/73 [00:05<00:00, 13.68it/s, loss=1.04, v_num=2tag, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 73/73 [00:05<00:00, 13.67it/s, loss=1.04, v_num=2tag, BTC_val_a\u001b[A\n","Epoch 2:  99%|▉| 72/73 [00:05<00:00, 13.73it/s, loss=1.03, v_num=2tag, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 1.036\n","Epoch 2: 100%|█| 73/73 [00:05<00:00, 13.70it/s, loss=1.03, v_num=2tag, BTC_val_a\n","Epoch 3:  99%|▉| 72/73 [00:06<00:00, 11.77it/s, loss=1.04, v_num=2tag, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 73/73 [00:06<00:00, 11.78it/s, loss=1.04, v_num=2tag, BTC_val_a\u001b[A\n","Epoch 4:  99%|▉| 72/73 [00:05<00:00, 12.68it/s, loss=1.02, v_num=2tag, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 73/73 [00:05<00:00, 12.66it/s, loss=1.02, v_num=2tag, BTC_val_a\u001b[A\n","Epoch 5:  99%|▉| 72/73 [00:04<00:00, 14.52it/s, loss=1.04, v_num=2tag, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 73/73 [00:05<00:00, 14.43it/s, loss=1.04, v_num=2tag, BTC_val_a\u001b[A\n","Epoch 6:  99%|▉| 72/73 [00:05<00:00, 14.23it/s, loss=1.03, v_num=2tag, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 73/73 [00:05<00:00, 14.20it/s, loss=1.03, v_num=2tag, BTC_val_a\u001b[A\n","Epoch 7:  99%|▉| 72/73 [00:04<00:00, 14.48it/s, loss=1.01, v_num=2tag, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 73/73 [00:05<00:00, 14.39it/s, loss=1.01, v_num=2tag, BTC_val_a\u001b[A\n","Epoch 8:  99%|▉| 72/73 [00:05<00:00, 14.20it/s, loss=1.01, v_num=2tag, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 73/73 [00:05<00:00, 14.17it/s, loss=1.01, v_num=2tag, BTC_val_a\u001b[A\n","Epoch 9:  99%|▉| 72/73 [00:05<00:00, 12.66it/s, loss=1.06, v_num=2tag, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 73/73 [00:05<00:00, 12.66it/s, loss=1.06, v_num=2tag, BTC_val_a\u001b[A\n","Epoch 10:  99%|▉| 72/73 [00:05<00:00, 13.70it/s, loss=1.04, v_num=2tag, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 73/73 [00:05<00:00, 13.64it/s, loss=1.04, v_num=2tag, BTC_val_\u001b[A\n","Epoch 11:  99%|▉| 72/73 [00:05<00:00, 14.38it/s, loss=1.02, v_num=2tag, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 73/73 [00:05<00:00, 14.34it/s, loss=1.02, v_num=2tag, BTC_val_\u001b[A\n","Epoch 12:  99%|▉| 72/73 [00:05<00:00, 14.02it/s, loss=1.01, v_num=2tag, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 73/73 [00:05<00:00, 13.96it/s, loss=1.01, v_num=2tag, BTC_val_\u001b[A\n","Epoch 13:  99%|▉| 72/73 [00:05<00:00, 14.16it/s, loss=1.03, v_num=2tag, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 1.031\n","Epoch 13: 100%|█| 73/73 [00:05<00:00, 14.08it/s, loss=1.03, v_num=2tag, BTC_val_\n","Epoch 14:  99%|▉| 72/73 [00:05<00:00, 12.79it/s, loss=0.978, v_num=2tag, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 1.018\n","Epoch 14: 100%|█| 73/73 [00:05<00:00, 12.77it/s, loss=0.978, v_num=2tag, BTC_val\n","Epoch 15:  99%|▉| 72/73 [00:05<00:00, 12.40it/s, loss=0.972, v_num=2tag, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 73/73 [00:05<00:00, 12.39it/s, loss=0.972, v_num=2tag, BTC_val\u001b[A\n","                                                                                \u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 1.015\n","Epoch 16:  99%|▉| 72/73 [00:05<00:00, 13.12it/s, loss=0.974, v_num=2tag, BTC_val\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.042 >= min_delta = 0.003. New best score: 0.972\n","Epoch 16: 100%|█| 73/73 [00:05<00:00, 13.09it/s, loss=0.974, v_num=2tag, BTC_val\n","Epoch 17:  99%|▉| 72/73 [00:06<00:00, 11.99it/s, loss=0.956, v_num=2tag, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 73/73 [00:06<00:00, 11.99it/s, loss=0.956, v_num=2tag, BTC_val\u001b[A\n","Epoch 18:  99%|▉| 72/73 [00:05<00:00, 12.50it/s, loss=0.941, v_num=2tag, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 73/73 [00:05<00:00, 12.49it/s, loss=0.941, v_num=2tag, BTC_val\u001b[A\n","Epoch 19:  99%|▉| 72/73 [00:06<00:00, 11.66it/s, loss=0.96, v_num=2tag, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 73/73 [00:06<00:00, 11.66it/s, loss=0.96, v_num=2tag, BTC_val_\u001b[A\n","Epoch 20:  99%|▉| 72/73 [00:06<00:00, 11.70it/s, loss=0.94, v_num=2tag, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 20: 100%|█| 73/73 [00:06<00:00, 11.68it/s, loss=0.94, v_num=2tag, BTC_val_\u001b[A\n","Epoch 21:  99%|▉| 72/73 [00:06<00:00, 11.98it/s, loss=0.955, v_num=2tag, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 21: 100%|█| 73/73 [00:06<00:00, 11.96it/s, loss=0.955, v_num=2tag, BTC_val\u001b[A\n","Epoch 22:  99%|▉| 72/73 [00:05<00:00, 12.18it/s, loss=0.928, v_num=2tag, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 22: 100%|█| 73/73 [00:06<00:00, 12.16it/s, loss=0.928, v_num=2tag, BTC_val\u001b[A\n","Epoch 23:  99%|▉| 72/73 [00:06<00:00, 10.88it/s, loss=0.911, v_num=2tag, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 23: 100%|█| 73/73 [00:06<00:00, 10.87it/s, loss=0.911, v_num=2tag, BTC_val\u001b[A\n","Epoch 24:  99%|▉| 72/73 [00:06<00:00, 10.82it/s, loss=0.956, v_num=2tag, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 24: 100%|█| 73/73 [00:06<00:00, 10.82it/s, loss=0.956, v_num=2tag, BTC_val\u001b[A\n","Epoch 25:  99%|▉| 72/73 [00:06<00:00, 11.91it/s, loss=0.89, v_num=2tag, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 25: 100%|█| 73/73 [00:06<00:00, 11.90it/s, loss=0.89, v_num=2tag, BTC_val_\u001b[A\n","Epoch 26:  99%|▉| 72/73 [00:05<00:00, 13.11it/s, loss=0.905, v_num=2tag, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 26: 100%|█| 73/73 [00:05<00:00, 13.10it/s, loss=0.905, v_num=2tag, BTC_val\u001b[A\n","Epoch 27:  99%|▉| 72/73 [00:05<00:00, 14.05it/s, loss=0.929, v_num=2tag, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 27: 100%|█| 73/73 [00:05<00:00, 13.99it/s, loss=0.929, v_num=2tag, BTC_val\u001b[A\n","Epoch 28:  99%|▉| 72/73 [00:05<00:00, 13.99it/s, loss=0.915, v_num=2tag, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 28: 100%|█| 73/73 [00:05<00:00, 13.95it/s, loss=0.915, v_num=2tag, BTC_val\u001b[A\n","Epoch 29:  99%|▉| 72/73 [00:05<00:00, 13.98it/s, loss=0.945, v_num=2tag, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 29: 100%|█| 73/73 [00:05<00:00, 13.90it/s, loss=0.945, v_num=2tag, BTC_val\u001b[A\n","Epoch 30:  99%|▉| 72/73 [00:06<00:00, 11.56it/s, loss=0.911, v_num=2tag, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 30: 100%|█| 73/73 [00:06<00:00, 11.53it/s, loss=0.911, v_num=2tag, BTC_val\u001b[A\n","Epoch 31:  99%|▉| 72/73 [00:06<00:00, 11.67it/s, loss=0.916, v_num=2tag, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 31: 100%|█| 73/73 [00:06<00:00, 11.68it/s, loss=0.916, v_num=2tag, BTC_val\u001b[A\n","Epoch 32:  99%|▉| 72/73 [00:06<00:00, 11.51it/s, loss=0.932, v_num=2tag, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 32: 100%|█| 73/73 [00:06<00:00, 11.52it/s, loss=0.932, v_num=2tag, BTC_val\u001b[A\n","Epoch 33:  99%|▉| 72/73 [00:05<00:00, 12.03it/s, loss=0.906, v_num=2tag, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 33: 100%|█| 73/73 [00:06<00:00, 11.98it/s, loss=0.906, v_num=2tag, BTC_val\u001b[A\n","Epoch 34:  99%|▉| 72/73 [00:06<00:00, 11.63it/s, loss=0.909, v_num=2tag, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 34: 100%|█| 73/73 [00:06<00:00, 11.62it/s, loss=0.909, v_num=2tag, BTC_val\u001b[A\n","Epoch 35:  99%|▉| 72/73 [00:05<00:00, 12.35it/s, loss=0.928, v_num=2tag, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 35: 100%|█| 73/73 [00:05<00:00, 12.32it/s, loss=0.928, v_num=2tag, BTC_val\u001b[A\n","Epoch 36:  99%|▉| 72/73 [00:06<00:00, 11.32it/s, loss=0.903, v_num=2tag, BTC_val\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.972. Signaling Trainer to stop.\n","Epoch 36: 100%|█| 73/73 [00:06<00:00, 11.31it/s, loss=0.903, v_num=2tag, BTC_val\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 36: 100%|█| 73/73 [00:06<00:00, 11.31it/s, loss=0.903, v_num=2tag, BTC_val\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 28.05it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.5357142686843872,\n"," 'BTC_test_f1': 0.2323809713125229,\n"," 'ETH_test_acc': 0.5,\n"," 'ETH_test_f1': 0.2212051898241043,\n"," 'LTC_test_acc': 0.4285714328289032,\n"," 'LTC_test_f1': 0.1999756544828415,\n"," 'test_loss': 0.9154524207115173}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 83017\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_030015-14vn2tag/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_030015-14vn2tag/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.5625\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.54456\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.1875\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.14701\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.3125\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.21442\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.88663\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 36\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2664\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 222\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621469037\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 127\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.54308\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.45122\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.5309\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.43051\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.54569\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.43189\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.90727\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.37037\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.33333\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.38889\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.04834\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.53571\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.23238\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.22121\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.42857\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.19998\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.91545\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▅▇▇▅▄▇▇▅▇▅█▄▇▄▆▄▃▅▅▆▁▆▆▄▃▄▄▆▅▆▇▅▅▅▇▇▆▅▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▃▄▄▃▄▄▄▃▆▃▄▃▄▃▃▃▂▄▃▅▁▆▅▃▂▃▄▆▆▅▇▇▄▆▆█▇▅▄█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▅▇▅▄▃▆▄▄▅█▇▃▇▃▃▃▂▄▅▅▄▆▃▆▅▃▆▄▃▆▆▃▃▆▄▆▄▄▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▃▄▃▃▂▄▃▃▃▅▄▂▄▂▂▂▁▄▃▅▃▄▂▇▅▂▇▅▄▄▇▃▄█▆█▆▆▄▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▆▇▄▅▄▃█▆▃▆▅▂▆▃▄▂▁▇▃▃▁▅▂▅▂▆▆▅▅▆▆▂▂▅▂█▂▅█▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▂▃▂▂▃▂▃▂▂▂▂▁▂▂▂▁▁▃▂▂▁▃▁▄▁▃▄▂▅█▃▂▂▄▂▇▂▅█▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▆▆▆▇▇▅▇▅▅▄█▄▇▆█▇▄▅▆▆▅▆▅▅▅▅▃▅▃▄▇▇▄▅▁▇▆▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▂▁▃▃▃▃▃▃▃▃▃▃▃▃▂▂▁▂▂▁▃▃▅▆▄▆▆▄▄▇▆▇█▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▃▂▃▄▄▅▆▅▆▇▆▇▆▇██▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▃▂▃▄▃▂▆▄▄▆▇▅▇▆██▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▄▄▄▅▅▅▆▇▇▇▆▇██▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▃▄▂▃▃▃▃▃▃▃▃▃▃▃▃▂▃▃▂▃▄▃▄▅▅▆▇▃▇▅▆▆▆▇▆█\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▃▂▄▄▄▅▅▆▇▆▇▆▆▇██▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▇▇▇▇▇▇▇▇▇▇▇▇▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ████████████████████▁█▁▁▁█▁█▁▁▁▁▁▁▁██\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▅▂▁▅▂▅▂█▅█▅▅▅▅▅▅▅█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ███████████████▅██▅▅▅▅▅▅▅▅▁▁▁▁█▅▅▁▁▁▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▃▃▂▂▂▂▂▄▂▅▄▃▃▁█▅▄▃▁▃▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▆█▆▆▅▃▅▃▃▅▁▃▃▃▆▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▅▂▂▅▁▅█▅▇▃▃▄▃▃▄▁▃▃▃▅▄▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▅▄▅▄▄▅▄▄▅▅▄▄▄▃▃▁▃▃▃▄▂▃▄▅▃▅▄▆▆█▇▆▇▇▆▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/14vn2tag\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n","2021-05-20 03:04:18.191291: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm__multi_classification_\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/37qvnx2r\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_030416-37qvnx2r\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name               | Type        | Params\n","----------------------------------------------------\n","0  | lstm_1             | LSTM        | 67.1 K\n","1  | batch_norm1        | BatchNorm2d | 256   \n","2  | lstm_2             | LSTM        | 132 K \n","3  | batch_norm2        | BatchNorm2d | 256   \n","4  | lstm_3             | LSTM        | 132 K \n","5  | batch_norm3        | BatchNorm2d | 256   \n","6  | dropout            | Dropout     | 0     \n","7  | linear1            | ModuleList  | 8.3 K \n","8  | activation         | ReLU        | 0     \n","9  | output_layers      | ModuleList  | 195   \n","10 | cross_entropy_loss | ModuleList  | 0     \n","11 | f1_score           | F1          | 0     \n","12 | accuracy_score     | Accuracy    | 0     \n","----------------------------------------------------\n","340 K     Trainable params\n","0         Non-trainable params\n","340 K     Total params\n","1.362     Total estimated model params size (MB)\n","Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n"],"name":"stdout"},{"output_type":"stream","text":["Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:  99%|▉| 73/74 [00:04<00:00, 15.04it/s, loss=1.12, v_num=nx2r, BTC_val_a\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 0: 100%|█| 74/74 [00:04<00:00, 15.00it/s, loss=1.12, v_num=nx2r, BTC_val_a\u001b[A\n","                                                                                \u001b[AMetric val_loss improved. New best score: 1.066\n","Epoch 1: 100%|█| 74/74 [00:05<00:00, 14.65it/s, loss=1.1, v_num=nx2r, BTC_val_ac\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 1: 100%|█| 74/74 [00:05<00:00, 14.57it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n","Epoch 2: 100%|█| 74/74 [00:05<00:00, 14.74it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 2: 100%|█| 74/74 [00:05<00:00, 14.66it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n","Epoch 3: 100%|█| 74/74 [00:05<00:00, 14.23it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 3: 100%|█| 74/74 [00:05<00:00, 14.14it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n","Epoch 4:  99%|▉| 73/74 [00:06<00:00, 11.18it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 4: 100%|█| 74/74 [00:06<00:00, 11.19it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n","Epoch 5: 100%|█| 74/74 [00:05<00:00, 13.13it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 5: 100%|█| 74/74 [00:05<00:00, 13.07it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n","Epoch 6: 100%|█| 74/74 [00:06<00:00, 12.19it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 6: 100%|█| 74/74 [00:06<00:00, 12.13it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n","Epoch 7: 100%|█| 74/74 [00:06<00:00, 12.27it/s, loss=1.11, v_num=nx2r, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 7: 100%|█| 74/74 [00:06<00:00, 12.22it/s, loss=1.11, v_num=nx2r, BTC_val_a\u001b[A\n","Epoch 8: 100%|█| 74/74 [00:05<00:00, 12.34it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 8: 100%|█| 74/74 [00:06<00:00, 12.29it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n","Epoch 9: 100%|█| 74/74 [00:05<00:00, 13.51it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 9: 100%|█| 74/74 [00:05<00:00, 13.44it/s, loss=1.1, v_num=nx2r, BTC_val_ac\u001b[A\n","Epoch 10: 100%|█| 74/74 [00:05<00:00, 14.13it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 10: 100%|█| 74/74 [00:05<00:00, 14.06it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n","Epoch 11:  99%|▉| 73/74 [00:05<00:00, 14.14it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 11: 100%|█| 74/74 [00:05<00:00, 14.11it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n","Epoch 12: 100%|█| 74/74 [00:05<00:00, 12.49it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 12: 100%|█| 74/74 [00:05<00:00, 12.44it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n","Epoch 13: 100%|█| 74/74 [00:05<00:00, 13.22it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 13: 100%|█| 74/74 [00:05<00:00, 13.16it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n","Epoch 14: 100%|█| 74/74 [00:05<00:00, 13.33it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 14: 100%|█| 74/74 [00:05<00:00, 13.26it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n","Epoch 15: 100%|█| 74/74 [00:06<00:00, 12.23it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 15: 100%|█| 74/74 [00:06<00:00, 12.17it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n","Epoch 16: 100%|█| 74/74 [00:05<00:00, 12.89it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 16: 100%|█| 74/74 [00:05<00:00, 12.82it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n","Epoch 17: 100%|█| 74/74 [00:06<00:00, 11.86it/s, loss=1.09, v_num=nx2r, BTC_val_\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 17: 100%|█| 74/74 [00:06<00:00, 11.80it/s, loss=1.09, v_num=nx2r, BTC_val_\u001b[A\n","Epoch 18: 100%|█| 74/74 [00:06<00:00, 11.54it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 18: 100%|█| 74/74 [00:06<00:00, 11.49it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n","Epoch 19: 100%|█| 74/74 [00:06<00:00, 11.41it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 19: 100%|█| 74/74 [00:06<00:00, 11.37it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n","Epoch 20: 100%|█| 74/74 [00:05<00:00, 12.36it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.066. Signaling Trainer to stop.\n","Epoch 20: 100%|█| 74/74 [00:06<00:00, 12.30it/s, loss=1.1, v_num=nx2r, BTC_val_a\n","Epoch 20: 100%|█| 74/74 [00:06<00:00, 12.29it/s, loss=1.1, v_num=nx2r, BTC_val_a\u001b[A\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 27.34it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'BTC_test_acc': 0.25,\n"," 'BTC_test_f1': 0.1315789520740509,\n"," 'ETH_test_acc': 0.4285714328289032,\n"," 'ETH_test_f1': 0.1991342157125473,\n"," 'LTC_test_acc': 0.3928571343421936,\n"," 'LTC_test_f1': 0.18594105541706085,\n"," 'test_loss': 1.1109517812728882}\n","--------------------------------------------------------------------------------\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 83545\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_030416-37qvnx2r/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_030416-37qvnx2r/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.25877\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.30501\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.3125\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.18519\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.09871\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1533\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 130\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621469186\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 72\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.3619\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.2683\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.34199\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.26888\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.32727\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.2613\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.09938\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.18182\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.375\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.18182\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.25641\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.08925\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.25\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.13158\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.42857\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19913\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.39286\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.18594\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.11095\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▆█▃▃▂▆▂▅▅▆▆█▆▃▃▅▂▇▁▅▂▆▆▃▅▅▆▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▂▂▇▄▂▂▂▂▃▄▄▅█▆▃▃▃▃▅▁▂▂▄▅▂▄▄▄▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▅▆▃▂▅▂▅▆▅▅▁▅▅▆▃▅▁▅▂▁▇▅▇▇▅▃▆█▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▆▅▃▃▆▂▄▆▄▅▁▃▅▆▄▅▁▄▂▂▇▄▆█▄▃▅▇▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▄▅▁▇▅▁▅▂▄▁▅▅▁▅▇▂▄▆▄▅▄▁█▅▃▃▇▆▇▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▄▄▁▇▆▁▂▂▄▁▅▅▁▅█▂▃▇▄▅▂▁▆▅▂▃▆▅▅▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▃▅▃▅█▂▆▃▆▅▅▆▂▃▅▅▃▅▄▃▅▁▁▄▄▄▂▂▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▃▁▆▇▆▅▆▅▇▅▇▅▅▆▇▆▆▇▇█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▃▃█▄▃▃▄▂▆▁▄▃▂▂▁▂▄▁▅▄▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▃▁▁▂▁▄▂▁▅█▆▃▄▄▅▃▇▄▆▅▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▇▇▇▂▂▆▅▄█▇▇▆▆▄▅▃█▁▅▄▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▄▅▅▁▂▁▄▅▇█▄▆▇▇▅▄▇▄▅▆▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▇██▂▂▂▄▄▇▅▄▅▆▆▂▂▆▁▄▄▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▃▄▃▃▂▂▂▂▁▂▂▂▂▂▁▁▁▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ███████▁▁████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ███████▁▁████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc █████▁█▅▅████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 █████▁█▆█████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▃▅▆▆█▇▇▇▅▃▄▅▅▄▆▅▅▅▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/37qvnx2r\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cRBfsKcsXAnT","executionInfo":{"status":"ok","timestamp":1625550990890,"user_tz":-180,"elapsed":13114,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}},"outputId":"e309be83-d059-4624-e6d3-d431b63149a0"},"source":["!git status"],"execution_count":null,"outputs":[{"output_type":"stream","text":["On branch master\n","Your branch is up to date with 'origin/master'.\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git checkout -- <file>...\" to discard changes in working directory)\n","\n","\t\u001b[31mmodified:   09_ak_experimenting_lstm-2.ipynb\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":570},"id":"t02aohU2Y3hs","executionInfo":{"status":"error","timestamp":1625555601962,"user_tz":-180,"elapsed":382,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}},"outputId":"168bca78-d104-4ebc-eddc-5d319109a47a"},"source":["CURRENCY_LST = config[\"currency_list\"]\n","N_CLASSES = 3\n","LSTM_HIDDEN_SIZES = config[\"lstm_hidden_sizes\"]\n","BIDIRECTIONAL = config[\"bidirectional\"]\n","REMOVE_TREND =config[\"remove_trend\"]\n","LOSS_WEIGHT_CALCULATE = config[\"loss_weight_calculate\"]\n","\n","TRAIN_PERCENTAGE, VAL_PERCENTAGE, TEST_PERCENTAGE = config[\"dataset_percentages\"] \n","WINDOW_SIZE = config[\"window_size\"]\n","FREQUENCY = config[\"frenquency\"]\n","NEUTRAL_QUANTILE = config[\"neutral_quantile\"] if N_CLASSES > 2 else 0 \n","BATCH_SIZE= config[\"batch_size\"]\n","#####\n","\n","X, y, features, dfs = get_data(CURRENCY_LST,\n","                            N_CLASSES,\n","                             FREQUENCY, \n","                             WINDOW_SIZE,\n","                             neutral_quantile = NEUTRAL_QUANTILE,\n","                             log_price=True,\n","                             remove_trend=REMOVE_TREND,\n","                             include_indicators = True,\n","                             include_imfs = False\n","                            )"],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'high'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-55-7b4f78285cc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                              \u001b[0mremove_trend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREMOVE_TREND\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                              \u001b[0minclude_indicators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                              \u001b[0minclude_imfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                             )\n","\u001b[0;32m/content/drive/MyDrive/aysenurk/multi_task_price_change_prediction/notebooks/DataPreparation.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(currency_lst, n_classes, frequency, window_size, neutral_quantile, beg_date, end_date, log_price, remove_trend, include_indicators, include_imfs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minclude_indicators\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0;32mfrom\u001b[0m \u001b[0mta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd_all_ta_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_all_ta_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"open\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"high\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"low\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"close\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"volume\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfillna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ta/wrapper.py\u001b[0m in \u001b[0;36madd_all_ta_features\u001b[0;34m(df, open, high, low, close, volume, fillna, colprefix)\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0mvolume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvolume\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0mfillna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0mcolprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolprefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m     )\n\u001b[1;32m    556\u001b[0m     df = add_volatility_ta(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ta/wrapper.py\u001b[0m in \u001b[0;36madd_volume_ta\u001b[0;34m(df, high, low, close, volume, fillna, colprefix)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# Accumulation Distribution Index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     df[f\"{colprefix}volume_adi\"] = AccDistIndexIndicator(\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvolume\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfillna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     ).acc_dist_index()\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'high'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"id":"YF3mcbPQbIU7","executionInfo":{"status":"ok","timestamp":1625551655210,"user_tz":-180,"elapsed":367,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}},"outputId":"afe9ecd5-cc34-4619-dbc9-95e435d44a9c"},"source":["pd.Series(y[0]).hist()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f1498493bd0>"]},"metadata":{"tags":[]},"execution_count":25},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9_KCaFl2bKKP","executionInfo":{"status":"ok","timestamp":1625555245664,"user_tz":-180,"elapsed":352,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}},"outputId":"ccb40060-d3cc-4057-b806-ecfc3e5dc8cf"},"source":["!git status"],"execution_count":null,"outputs":[{"output_type":"stream","text":["On branch master\n","Your branch is up to date with 'origin/master'.\n","\n","Changes not staged for commit:\n","  (use \"git add/rm <file>...\" to update what will be committed)\n","  (use \"git checkout -- <file>...\" to discard changes in working directory)\n","\n","\t\u001b[31mmodified:   02_fc_data_preparation.ipynb\u001b[m\n","\t\u001b[31mmodified:   09_ak_experimenting_lstm-2.ipynb\u001b[m\n","\t\u001b[31mdeleted:    ../pipelines/multi_task_price_change_prediction/DataPreparation.py\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2bHaxHR4oRfg"},"source":["!git config --global user.email \"aysenurkulunk0@gmail.com\"\n","!git config --global user.name \"Aysenur\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3t8TdbJ1oEO_","executionInfo":{"status":"ok","timestamp":1625555235054,"user_tz":-180,"elapsed":1099,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}},"outputId":"d7acec26-23ae-4e1f-8820-e5615ad857f9"},"source":["!git add ../pipelines/multi_task_price_change_prediction/experiment_lstm.py\n","!git add ../pipelines/multi_task_price_change_prediction/experiment_mha.py\n","!git commit -m \"experimentation extended for both indicators and imfs\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["[master bc27e8c] experimentation extended for both indicators and imfs\n"," 2 files changed, 23 insertions(+), 7 deletions(-)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ukKgpRHcosCF","executionInfo":{"status":"ok","timestamp":1625555240438,"user_tz":-180,"elapsed":2691,"user":{"displayName":"Ayşenur Külünk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64","userId":"06684409182189324537"}},"outputId":"6448631a-9559-49bf-c4d9-045dad6cf751"},"source":["!git push"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Counting objects: 6, done.\n","Delta compression using up to 2 threads.\n","Compressing objects:  16% (1/6)   \rCompressing objects:  33% (2/6)   \rCompressing objects:  50% (3/6)   \rCompressing objects:  66% (4/6)   \rCompressing objects:  83% (5/6)   \rCompressing objects: 100% (6/6)   \rCompressing objects: 100% (6/6), done.\n","Writing objects:  16% (1/6)   \rWriting objects:  33% (2/6)   \rWriting objects:  50% (3/6)   \rWriting objects:  66% (4/6)   \rWriting objects:  83% (5/6)   \rWriting objects: 100% (6/6)   \rWriting objects: 100% (6/6), 882 bytes | 882.00 KiB/s, done.\n","Total 6 (delta 5), reused 0 (delta 0)\n","remote: Resolving deltas:   0% (0/5)\u001b[K\rremote: Resolving deltas:  20% (1/5)\u001b[K\rremote: Resolving deltas:  40% (2/5)\u001b[K\rremote: Resolving deltas:  60% (3/5)\u001b[K\rremote: Resolving deltas:  80% (4/5)\u001b[K\rremote: Resolving deltas: 100% (5/5)\u001b[K\rremote: Resolving deltas: 100% (5/5), completed with 5 local objects.\u001b[K\n","To https://github.com/aysenurk/multi_task_price_change_prediction.git\n","   a48fe07..bc27e8c  master -> master\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zpuupeTSozNp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kCKoO_0xQq6K"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N6-D9ekYfB-5"},"source":[""],"execution_count":null,"outputs":[]}]}