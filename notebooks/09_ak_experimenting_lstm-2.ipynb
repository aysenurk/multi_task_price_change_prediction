{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sOIssxJtXxll"
   },
   "outputs": [],
   "source": [
    "#aysenur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1625569760353,
     "user": {
      "displayName": "Ayşenur Külünk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64",
      "userId": "06684409182189324537"
     },
     "user_tz": -180
    },
    "id": "5b86SSBfWENB"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from DataPreparation import get_data\n",
    "from TimeSeriesLearningUtils import TimeSeriesDataset, CosineWarmupScheduler, get_data\n",
    "from LSTMModel import LSTM_based_classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1625569886538,
     "user": {
      "displayName": "Ayşenur Külünk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64",
      "userId": "06684409182189324537"
     },
     "user_tz": -180
    },
    "id": "MDSC6zPkWENQ"
   },
   "outputs": [],
   "source": [
    "def name_model(config):\n",
    "    name =[]\n",
    "    if len(config[\"currency_list\"])  > 1:\n",
    "        name.append(\"multi_task_\" + \"_\".join(config[\"currency_list\"]))\n",
    "    else:\n",
    "        name.append(config[\"currency_list\"][0])\n",
    "        \n",
    "    if config[\"indicators\"] or config[\"imfs\"] or config [\"ohlv\"]:\n",
    "        name.append(\"multi_variate\")\n",
    "    \n",
    "    lstm = \"stack_lstm\" if len(config[\"lstm_hidden_sizes\"]) > 1 else \"lstm\"\n",
    "    name.append(lstm)\n",
    "    \n",
    "    name.append(config[\"pred_frequency\"])\n",
    "    classification = \"multi_clf\" if config[\"n_classes\"] > 2 else \"binary_clf\"\n",
    "    name.append(classification)\n",
    "    \n",
    "    return \"_\".join(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1625569888235,
     "user": {
      "displayName": "Ayşenur Külünk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64",
      "userId": "06684409182189324537"
     },
     "user_tz": -180
    },
    "id": "jXok5ufCWENS"
   },
   "outputs": [],
   "source": [
    "WANDBPROJECT = \"deneme\"\n",
    "\n",
    "config = {\"window_size\": 50, \n",
    "          \"dataset_percentages\": [0.97, 0.007, 0.023],\n",
    "          \"data_frequency\": \"6h\", \n",
    "          \"pred_frequency\": \"6h\",\n",
    "          \"ma_period\": 10,\n",
    "          \"neutral_quantile\": 0.33,\n",
    "          \"batch_size\": 16,\n",
    "          \"bidirectional\": True, \n",
    "          \"n_classes\": 2,\n",
    "          \"currency_list\": ['BTC', 'ETH', 'LTC'],\n",
    "          \"remove_trend\": True,\n",
    "          \"lstm_hidden_sizes\": [128, 128, 128],\n",
    "          \"loss_weight_calculate\": True, \n",
    "          \"indicators\": True, \n",
    "          \"imfs\": False,\n",
    "          \"ohlv\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multi_task_BTC_ETH_LTC_multi_variate_stack_lstm_6h_binary_clf'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_model(config)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Binance klasöründen alınca olmuyor. \n",
    "def get_data(currency_lst,\n",
    "             data_frequency,\n",
    "             pred_frequency, \n",
    "             n_classes,\n",
    "             window_size,\n",
    "             neutral_quantile = 0.33,\n",
    "             beg_date = pd.Timestamp(2013,1,1),\n",
    "             end_date = pd.Timestamp.now(),\n",
    "             log_price = True,\n",
    "             remove_trend = False,\n",
    "             ma_period = 0,\n",
    "             include_indicators = False,\n",
    "             include_imfs = False,\n",
    "             open_high_low_volume = False):\n",
    "        \n",
    "        X, y, dfs = {}, {}, {}     \n",
    "        \n",
    "        for cur in currency_lst:\n",
    "            df = pd.read_csv(f\"../data/0_raw/{str.lower(cur)}_usdt_{data_frequency}.csv\", index_col=0)\n",
    "            df.index = pd.to_datetime(df.index, unit='s')\n",
    "            df.sort_index(inplace=True)\n",
    "            df.drop([\"Date\"], axis=1, inplace=True)\n",
    "            df.rename(str.lower, axis=1, inplace=True) \n",
    "            \n",
    "            if include_indicators:\n",
    "                from ta import add_all_ta_features\n",
    "                indicators_df = add_all_ta_features(df, open=\"open\", high=\"high\", low=\"low\", close=\"close\", volume=\"volume\", fillna=True)\n",
    "                df[indicators_df.columns] = indicators_df\n",
    "            \n",
    "            if include_imfs:\n",
    "                from PyEMD import EEMD\n",
    "                eemd = EEMD()\n",
    "                imfs = eemd(df[\"close\"].values)\n",
    "                imf_features = [\"imf_\"+str(i) for i in range(imfs.shape[0])]\n",
    "                df = pd.concat((df, pd.DataFrame(imfs.T, columns=imf_features, index=df.index)), axis=1)\n",
    "            \n",
    "            if log_price:\n",
    "                df[[\"close\", \"open\", \"high\", \"low\"]] = df[[\"close\", \"open\", \"high\", \"low\"]].apply(np.log, axis=1)\n",
    "                   \n",
    "            if n_classes == 3:\n",
    "                df['pct_diff'] = df['close'].pct_change()\n",
    "                neutral_quantiles = df['pct_diff'].abs().quantile(neutral_quantile)\n",
    "                \n",
    "                conditions = [(df['pct_diff'] < 0) & (df['pct_diff'].abs() > neutral_quantiles),\n",
    "                              (df['pct_diff'] > 0) & (df['pct_diff'].abs() > neutral_quantiles)]\n",
    "\n",
    "                classes = [0,1] # 2 is the default class if none of conditions is met, i.e. price change in the neutral range\n",
    "            \n",
    "                change_dir = np.select(conditions, classes, default=2)\n",
    "            \n",
    "            else:\n",
    "                df['diff'] = df['close'].diff()\n",
    "                change_dir = df['diff'].apply(lambda x: 0 if x <= 0 else 1)\n",
    "            \n",
    "            df.insert(loc=0, column=\"change_dir\", value=change_dir)   \n",
    "            df.dropna(inplace=True)  \n",
    "            \n",
    "            if remove_trend:\n",
    "                from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "                components = seasonal_decompose(df[\"close\"], model=\"additive\", period = ma_period, two_sided=False)\n",
    "                df[\"close\"] -= components.trend\n",
    "                df.dropna(inplace=True)\n",
    "                \n",
    "            if not open_high_low_volume:\n",
    "                df.drop([\"open\", \"high\", \"low\", \"volume\"], axis=1, inplace=True)\n",
    "\n",
    "            dfs[cur] = df\n",
    "        \n",
    "        min_dates = [df.index.min() for cur, df in dfs.items()]\n",
    "        max_dates = [df.index.max() for cur, df in dfs.items()]\n",
    "        beg_date = max([max(min_dates), beg_date])\n",
    "        end_date = min([min(max_dates), end_date])\n",
    "        common_range = pd.date_range(beg_date, end_date, freq=pred_frequency)\n",
    "        \n",
    "        diff_col = 'pct_diff' if n_classes == 3 else 'diff'\n",
    "        X = np.array([dfs[cur].loc[common_range].drop([\"change_dir\", diff_col], axis=1).values for cur in currency_lst])\n",
    "        y = np.array([dfs[cur].loc[common_range, \"change_dir\"].values for cur in currency_lst])\n",
    "        features = df.columns.tolist()\n",
    "        \n",
    "        return X, y, features, dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1965,
     "status": "ok",
     "timestamp": 1625570085490,
     "user": {
      "displayName": "Ayşenur Külünk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64",
      "userId": "06684409182189324537"
     },
     "user_tz": -180
    },
    "id": "JBHyTlBJgwIp",
    "outputId": "6b95c62b-4c72-4774-e2c8-2232138a7997"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = name_model(config)\n",
    "\n",
    "CURRENCY_LST = config[\"currency_list\"]\n",
    "N_CLASSES = config[\"n_classes\"]\n",
    "LSTM_HIDDEN_SIZES = config[\"lstm_hidden_sizes\"]\n",
    "BIDIRECTIONAL = config[\"bidirectional\"]\n",
    "REMOVE_TREND =config[\"remove_trend\"]\n",
    "LOSS_WEIGHT_CALCULATE = config[\"loss_weight_calculate\"]\n",
    "\n",
    "TRAIN_PERCENTAGE, VAL_PERCENTAGE, TEST_PERCENTAGE = config[\"dataset_percentages\"] \n",
    "WINDOW_SIZE = config[\"window_size\"]\n",
    "DATA_FREQUENCY = config[\"data_frequency\"]\n",
    "PRED_FREQUENCY = config[\"pred_frequency\"]\n",
    "MA_PERIOD = config[\"ma_period\"]\n",
    "NEUTRAL_QUANTILE = config[\"neutral_quantile\"] if N_CLASSES > 2 else 0 \n",
    "BATCH_SIZE= config[\"batch_size\"]\n",
    "INDICATORS = config[\"indicators\"]\n",
    "IMFS = config[\"imfs\"]\n",
    "OHLV = config[\"ohlv\"]\n",
    "#####\n",
    "\n",
    "X, y, features, dfs = get_data(currency_lst = CURRENCY_LST,\n",
    "                               data_frequency =DATA_FREQUENCY,\n",
    "                               pred_frequency = PRED_FREQUENCY,\n",
    "                               n_classes = N_CLASSES,\n",
    "                               window_size=WINDOW_SIZE,\n",
    "                               neutral_quantile = NEUTRAL_QUANTILE,\n",
    "                               log_price=True,\n",
    "                               remove_trend=REMOVE_TREND,\n",
    "                               ma_period=MA_PERIOD,\n",
    "                               include_indicators = INDICATORS,\n",
    "                               include_imfs = IMFS, \n",
    "                               open_high_low_volume = OHLV, \n",
    "                               drop_missing=True)\n",
    "\n",
    "\n",
    "INPUT_FEATURE_SIZE = X.shape[-1]\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = [TimeSeriesDataset(CURRENCY_LST, \n",
    "                                                          X, \n",
    "                                                          y, \n",
    "                                                          dtype, \n",
    "                                                          TRAIN_PERCENTAGE, \n",
    "                                                          VAL_PERCENTAGE, \n",
    "                                                          TEST_PERCENTAGE, \n",
    "                                                          WINDOW_SIZE) for dtype in ['train', 'val', 'test']]\n",
    "\n",
    "config[\"dataset_sizes\"] = [len(train_dataset), len(val_dataset), len(test_dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5340, 88)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ebf0421749e0432bade49a54fa66eca8",
      "782949ba5ff44cb9a9a37b3aa4456b32",
      "528684a138fe44abb63c7c84da2a9b39",
      "cf9b4e2a5b284e94ae3c4048bb0b6f43",
      "fee172130956407cbd4cef7772b5a60e",
      "b99e03bd189a400bb3cd7e8543269891",
      "911ccbab9ac4425680111cc78146b6e0",
      "34608d61d2584ccda71aade54f21769a",
      "232e3f40a26c4aa4b8934fd820687530",
      "84f9b7a06cc74d78a1e6021713da12c0",
      "f1c1bb057a894fef9bc5b10420a9048f",
      "f185f66dd39c434ab7b164437c7e7fb8",
      "877ccdc4a7b348f79ea3c39d605af725",
      "5da980c0bac14ebe9c7ac855d48204d3",
      "2ffa58aa61814c9186924de42e6ce464",
      "7fca719d592749779e7700c0ef732a38",
      "ee4acb2bb6454af99a8597ffb4b3ad52",
      "9aa4829013f0413a9c0c2b3943c7b2c6",
      "3d8d4cd8a7d64997a6267449f06a7a45",
      "23bccc402fa24a1a83460204e010f918",
      "ffae011089d1468ea74b9b17acdec20f",
      "2e2488dd08e646e48f0fdebbb431be1f",
      "45d0d84f45fa4ca08b24c490586319de",
      "ac138621be054e0ebe78ca44ebf9f0a5",
      "1be29621ae064cc2b4e0cedd4df48a90",
      "6644970fe5f0412197edf44f588c1b28",
      "669185de465a43ababdc5b34577e8700",
      "2b71ea6f4e634dd8ac919ae7a60192c4",
      "c4453ab8b2504d7db690f1c92ae8f967",
      "1429d3b406c84a8da158a31c37e27093",
      "f089fefd584240cfb263909b0e4ff38e",
      "21daa732d2914ec6b0343684cae7d671",
      "05b1609fd7b6435894a7118faa671bd7",
      "ca40adc1088b456caca137c68c304b87",
      "3ad5b57d31c24c9d8c5b8beb951a4df1",
      "aab735b043674e279438d35b0c3829e4",
      "df967d7bfa654ce9aa7bc1b93d9a48c7",
      "f612ca8d591f457381f3397523fc7bb3",
      "fa5aae008f6d41e2a9a7428b7a615c3b",
      "cabcfe06cca64b4291a17f2591fac81b",
      "31c96cf4f6cf4625b2c7f42a9d9e4456",
      "1b4c71595f2345c592d3879ea8dc3fd5",
      "7b364d9e7d934c4bbce500ea282e55a0",
      "fb73a9e3ebaf466183729edd4fcb2ea1",
      "4564330337014a6d9e238095de3902ba",
      "ef1656930f764faca37a51440d1321cc",
      "ad62407911e7478fb5f33515c9e64dcd",
      "ec7da0f02286452486398a75cb131430",
      "6b049434616c4b95924c4e9df8193f83",
      "38b42ebc7be545a2b668d961440ae5d3",
      "79070d53d7164ce5b3ee6b8d0001f178",
      "34ddec300ac640909eefab973ebb11fa",
      "0b4ae2f7e3f641e1826c84eeb56a0d23",
      "92d397dce25c4977bb454f50dcf4f119",
      "1b272f78d2784f51be805adba62ec27b",
      "880d5a5ac1a84fa1a9665171d63159b3",
      "9db3a957f14b4cbb8647fde8b674ed8e",
      "8444c9620a964f629d0b687f2f476277",
      "1afc725ba84f499db1a2c4f557ae43e9",
      "688c166e368e4391b99f88702cb2c751",
      "9b2b9db9d2d24f078fc0551ac0e2afac",
      "c8b14122d94447c180536812afec72fc",
      "5d53a958248242dc8bc4aece83e4348d",
      "c5e35dae04eb43c9be1b206ff98e5a13",
      "001002aa7ae54d1f9eea85ec7c2d2461",
      "cffb821306cc40578d57c4ca3abb7541",
      "030d6ea6da3b424a820dd988e2146a9c",
      "65af5c35d8f742508f26d1cffc205628",
      "298f0f714c4a461ea403d74794058d7d",
      "1ce76e0ea3694dc9b62989a4a8919ea9",
      "85c8d92483ca4033b3b47de761be3e7d",
      "9afb61e97ee84541af2cb9e14658dd9e",
      "15d5239c57004349961ec6245e722016",
      "863ff2d4a3ec4f18a9e4b274ad9ab09d",
      "94d26342f8264f45964fab6ca14f52f2",
      "86c2b5672d90460ca07dc230b2be58b2",
      "d1e59c2672014bd8a536d0a223371195",
      "4f2c45f34d5a4a60aec3e11f17339bcd",
      "789da7bf5aeb4206a3c5c8b1aabe2637",
      "2d39e2890b1f4506a6edd4a3b2c2d12c",
      "b4568a28ca3c41aaa27b365079ad1bec",
      "937b9ab4d33448269ba3ff1b64923b0d",
      "6ff89bf75a444ca1aba38fdcac7538ec",
      "2b4479a31b1147149f1f7ef6bb771d1e",
      "98ffafe96f134f4180cee6c75306bd0a",
      "34c39fd383404d35b2cf0737f3b66981",
      "c6f0c32efccd4368b4cdbf29e5eee033",
      "4e9b61655db94a04bdd4b6532e3ba7c5",
      "ee1ba2d34fab42b5aac4fceb7159ed3d",
      "ded409ddc4814342b6a9dfb2bea2d97f",
      "b56b6868a11c4a19b0fb8c28b8e925ba",
      "a8d1591e402d48b8b9614466879ed770",
      "d75b360658f8471e94028db9b7c220f9",
      "432b70435d2b47afb587e7cbf257754a",
      "af64f25c469d4d52a57eff3420bcbfbe",
      "13568ce2679846e883ddc002283913d9",
      "dc75341527d848ce83a822613b081c91",
      "5acaf2b196554817b5dcb42a2e861e46",
      "234f859aa2744f28a9697787b80485f2",
      "3f9caad4e9b842c08bd91e4555b6969c",
      "b44f04486bf143619b3fe171aa687f18",
      "2da8dade2d1a42e8b100e0288fb5188d",
      "a4d9ce425e55436ca95ed398ac5eb356",
      "85ab5ec85e4945949dc4e5a763099611",
      "6c450f114fa74e8e8a8d00d58c35c2ba",
      "2328c71b3fd74611b8d1b85e645c0508",
      "e5059c546bbd4529943fd6cbce34486b",
      "dca1b6ab2cf34a09914ae43b0bf5df53",
      "6a9d16cc8be64bf983af3ce976a892d3",
      "85dd78cb7a334dc5ac7a4c91d14bac53",
      "0381cec4bcd34e61a456437f92b69a82",
      "8895fc082ac5443184e4a600eba89b7d",
      "97e6a48a97964de6bd3b0e00b28f9a54",
      "83e710060a3644849abaa259d8f4f2e6",
      "eb8f44b5cf7a410aabbcab2fa12a47a4",
      "5f39212d17e74155855ec3d86b474810",
      "3f57f1391b6e418785fbcf32221ee893",
      "e6a2a6c6678243dcb9f57715eca1586b",
      "7c56e5a600fb4f709e4fbbc640b19a65",
      "5430acd7f77444d58eb9db16097aee0a",
      "c9519776aaa54bc6a717ec251748bc0e",
      "e6b49f2082f348fa843a25085c0d80e8",
      "d14fd2cdefb94e81914fe4de68ce12cb",
      "ff7dd27010bc44feb5ff2f1eaa77a52e",
      "fba760627f07473cbc527bf0bc811e7d",
      "be540232d5c64e87aae655338dc050ad",
      "ee3ad82f1523475a898afdfdbe88ac05",
      "facf644d5e9842f5b8fe5e7598eaa7cb",
      "e7bb8595244b4407b4215b7d52ae3839",
      "b2f19ead2d744899b6d4cc450768be48",
      "81fd519c0cfd4a2f8307a918e37645d3",
      "f1b87bbda1f94e32b827751fd7ef8dbc",
      "c8a500ff80ad40e494aa67bb81824d47",
      "e60302c23be24b328b148202f132179b",
      "567b80eb991c4af396635a68d2a719f1",
      "f2be62d0639b4cdb8488c50afb034119",
      "b6404efd23814cafbc6e1331683e7727",
      "57e8113a683641f69633d987556e26ff",
      "b2a7e67f3bdc4f7782ed8374ac799376",
      "4ba12041852b4200a23a5536a4339b93",
      "d37a9c1baa894d729817d01d18fec45e",
      "c3ff1c933b34436ab5ed83645664c7ae",
      "c922125e6f6c47398ea956bbfee20531",
      "70075f5fd4c34ebba9497ec97eb8b806",
      "4847aed5b8c347398a13c30e10c0410b",
      "435523d75063464382a528bc063a421c",
      "72b038cdda4f4a9e915cd524759ec8a5",
      "2c22af65a86945af921ad7056a54cc4e",
      "f287119ddc4b408eb98273405c74026c",
      "61031c90cd484bd58a4db6fd3e90e746",
      "e93337ba1da74acbb3fa347d9d49e9ea",
      "5bca4a2c9ac746778bd129ab5e0bcbf5",
      "7699bf3f282847ee87bf9595ffb57854",
      "3ea6355a982e446aa5298069e5f18308",
      "b1721b764e55419aa655f1734dc3f03c",
      "78cd948fb8514a46a577ce8b52091df7",
      "712a1758ba8143f3b65523ac70cb3e91",
      "939c8981feb7434d8777694b08a2670a",
      "d29691ca7c5b4d29b0810d24cc8837ac",
      "add1a0907fd14da9bb6b2757d44e75ae",
      "590e385531db4402a17779e8238b575c",
      "56e9e8878a394cd8a639f95c76565722",
      "1e616b4d2fbe42d4942b50f50ff3a19e",
      "d84faa4176b342f6890e552a10ca58bc",
      "581c8413af05406b96af1975dc014eea",
      "e6c3c2e74f8849cdac743d5399977ec2",
      "004b1c07e44d4d40810abaee5a99ad3f",
      "33ce28d3c500425b867369920a55e987",
      "ccff8c1550624a28bf89b5a4a07304ad",
      "004204cf73ec41b1a4523a23744ad9d0",
      "5eb5dbf1dfd243729c4873b9ee4ff40e",
      "52db72907a60408b9e1db1ed91e908b4",
      "ac9b08e03a954f089d8efaaffe38bc81",
      "173ba810845448a7aa1ce5bbc5faf973",
      "79048080a924456580e7193c912e270e",
      "97fbd8eeb4ef42f5a8cca961c015e74b",
      "9a8d479b302c46afaa7f3c0eff07c9f3",
      "101a0888d70b48dab438635e1ad2cea2",
      "bcc76021f1f64b0f98a601aeeb87274f",
      "d9ce85bf66144c649b32d492b2d3b753",
      "073459ac986d4a6b9bbd0331864c2b9c",
      "5f7c64cb893e40408670c6b2c8f064e8",
      "dadef3f82f654698830226a05311267b",
      "5b30843f46a14c6aa6114e13dc43ad06",
      "a4d597b8f44249c3a71b041f4d789e6d",
      "bd064493e1bf4538baa1bcde3001f2db",
      "9c8fa7cbc7814736a9258cd47b7426b3",
      "11282acb50e34b0894baa996c9267157",
      "1cc6501157cf4cb28e8a9cfad3317a1c",
      "31edfbdb82af40e8b67ad9b6585a562a",
      "0898797b70fa4de09b6c876080fd0390",
      "42c6237380fc4494b1caf12812754794",
      "8be94868429b4bcd96e4d4dce85278e1",
      "4d788c5265504d58b4de2cfab3e20b03",
      "c5e68d2f7e8548fa811d18dec0989d34",
      "0eb5cf4f737549c688a77d4b4e5d95d5",
      "52b624236c7846dfbf56cf05585f2f30",
      "0a77ff0cce9a4dd9805de3934eda4e40",
      "f650e7cf31e640df8c4c0bb0b24401d8",
      "14fd68e87bfa4a06ae7ec08e179a3e8b",
      "5c7b1d54e5d849669399989a4f3bb638",
      "149ea47b64eb4375ace1beb9871b6b98",
      "6d1c4884c9d1462abf9a9a98151be7a8",
      "d7dba68c81284dd3a9d33a5326df1bcd",
      "1ce03d4f3de6400689145312ed4a5d86",
      "de670cc3f13b471090155fddd9e9a798",
      "a44c8953d967468bb48673652b144bfe",
      "6ac7c1fc95264bd7bae2ef5c6c828249",
      "0838e59beb634c9d9f51d37873cf6170",
      "a91d3b3c1f66413e80af6f91cdfa8333",
      "a9b0b16eb0ed4afd811c2205ba32b38f",
      "01cc5f07613a4557ac7f2adf1baf1b8d",
      "c32af4c22cf84d43a04042f93f07458f",
      "e7a968e61fd94cacba24309b55ece42f",
      "ad8dbb0dc017486cbc042a92f4738370",
      "1dc435eef2354b5390ab041dbde32292"
     ]
    },
    "executionInfo": {
     "elapsed": 92510,
     "status": "ok",
     "timestamp": 1625570183387,
     "user": {
      "displayName": "Ayşenur Külünk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgtyDH7ay5KBopfW0RMUKef0nAVLCcxQb8lvsA-=s64",
      "userId": "06684409182189324537"
     },
     "user_tz": -180
    },
    "id": "CcE6NLzyWENT",
    "outputId": "1d4efeb3-ab52-4f96-ebcc-5d6124733e93"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.33<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">multi_task_BTC_ETH_LTC_multi_variate_stack_lstm_6h_binary_clf</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/multi_task_price_prediction/deneme\" target=\"_blank\">https://wandb.ai/multi_task_price_prediction/deneme</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/multi_task_price_prediction/deneme/runs/ixa3079g\" target=\"_blank\">https://wandb.ai/multi_task_price_prediction/deneme/runs/ixa3079g</a><br/>\n",
       "                Run data is saved locally in <code>/home/aysenurk/Projects/aysenurk/multi_task_price_change_prediction/notebooks/wandb/run-20210816_130651-ixa3079g</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'last_layer_fsz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0e8b34e0a861>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#     #logger = TensorBoardLogger(\"../output/models/lstm_model_logs\", name=\"lstm_multi_task\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m model = LSTM_based_classification_model(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m      \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'last_layer_fsz'"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"deneme\",\n",
    "           config=config,\n",
    "           entity='multi_task_price_prediction',\n",
    "           name = MODEL_NAME)\n",
    "\n",
    "logger = WandbLogger()\n",
    "#     #logger = TensorBoardLogger(\"../output/models/lstm_model_logs\", name=\"lstm_multi_task\")\n",
    "\n",
    "model = LSTM_based_classification_model(\n",
    "    train_dataset = train_dataset,\n",
    "     val_dataset = val_dataset,\n",
    "     test_dataset = test_dataset,\n",
    "     calculate_loss_weights = LOSS_WEIGHT_CALCULATE,\n",
    "     currencies = CURRENCY_LST,\n",
    "     num_classes = N_CLASSES,\n",
    "     window_size = WINDOW_SIZE,\n",
    "     input_size = INPUT_FEATURE_SIZE,\n",
    "     batch_size=BATCH_SIZE,\n",
    "     lstm_hidden_sizes = LSTM_HIDDEN_SIZES,\n",
    "     bidirectional = BIDIRECTIONAL)\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='val_loss',\n",
    "   min_delta=0.003,\n",
    "   patience=20,\n",
    "   verbose=True,\n",
    "   mode='min'\n",
    ")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='../output/',\n",
    "    filename=MODEL_NAME +'-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(gpus=-1, \n",
    "                     max_epochs= 80,\n",
    "                     logger = logger, \n",
    "                     callbacks=[early_stop_callback, checkpoint_callback])\n",
    "trainer.fit(model)\n",
    "\n",
    "trainer.test(ckpt_path = checkpoint_callback.best_model_path)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9BiNc_hcWENW"
   },
   "outputs": [],
   "source": [
    "#denemeleri tamamlandı experiment_lstm.py dosyasına yazıldı işlemler\n",
    "def experiment(script):\n",
    "    !python ../pipelines/multi_task_price_change_prediction/experiment_lstm.py $script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TSJlHMoWWENX"
   },
   "outputs": [],
   "source": [
    "CONFIG = {#fix for this project\n",
    "          \"window_size\": 50, \n",
    "          \"dataset_percentages\": [0.97, 0.007, 0.023],\n",
    "          \"frenquency\": \"D\", \n",
    "          \"neutral_quantile\": 0.33,\n",
    "          \"batch_size\": 16, \n",
    "    \"imfs\": False, \n",
    "    \"ohlv\": False, \n",
    "    \"remove_trend\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xJoL5IXsWENX"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "param_grid = {\"n_classes\": [2,3],\n",
    "          \"currency_list\": [['BTC'], ['ETH'], ['LTC'], ['BTC', 'ETH'],  ['BTC', 'ETH', 'LTC']],\n",
    "          \n",
    "          \"lstm_hidden_sizes\": [[128, 128, 128]],\n",
    "          \"loss_weight_calculate\": [True, False],\n",
    "          \"indicators\": [True, False],\n",
    "             \"bidirectional\":[True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SrJ_AVsyWENX",
    "outputId": "b17f073e-3d19-4f13-e13b-06cea3d980e4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:37:19.420059: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/2fwbi3d3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_093718-2fwbi3d3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 219 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 258   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.177     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 32.92it/s, loss=0.709, v_num=i3d3, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.704\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 33.09it/s, loss=0.709, v_num=i3d3, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 33.92it/s, loss=0.692, v_num=i3d3, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 34.04it/s, loss=0.692, v_num=i3d3, BTC_val_\u001b[A\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 33.27it/s, loss=0.691, v_num=i3d3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 32.85it/s, loss=0.691, v_num=i3d3, BTC_val_\u001b[A\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 31.56it/s, loss=0.677, v_num=i3d3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 0.689\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 31.64it/s, loss=0.677, v_num=i3d3, BTC_val_\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 34.25it/s, loss=0.656, v_num=i3d3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 34.33it/s, loss=0.656, v_num=i3d3, BTC_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 31.97it/s, loss=0.619, v_num=i3d3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 32.03it/s, loss=0.619, v_num=i3d3, BTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 31.70it/s, loss=0.598, v_num=i3d3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.672\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 31.70it/s, loss=0.598, v_num=i3d3, BTC_val_\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 32.07it/s, loss=0.566, v_num=i3d3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 32.44it/s, loss=0.566, v_num=i3d3, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 31.81it/s, loss=0.555, v_num=i3d3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 32.07it/s, loss=0.555, v_num=i3d3, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 31.36it/s, loss=0.52, v_num=i3d3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 31.58it/s, loss=0.52, v_num=i3d3, BTC_val_a\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 32.63it/s, loss=0.492, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 31.95it/s, loss=0.492, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 32.76it/s, loss=0.496, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 33.09it/s, loss=0.496, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 33.21it/s, loss=0.476, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 33.45it/s, loss=0.476, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 33.15it/s, loss=0.46, v_num=i3d3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 33.25it/s, loss=0.46, v_num=i3d3, BTC_val_\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 31.76it/s, loss=0.462, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.656\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 32.01it/s, loss=0.462, v_num=i3d3, BTC_val\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 31.24it/s, loss=0.446, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 31.59it/s, loss=0.446, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 32.00it/s, loss=0.433, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 31.45it/s, loss=0.433, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 33.21it/s, loss=0.424, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 32.95it/s, loss=0.424, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 28.53it/s, loss=0.428, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 28.81it/s, loss=0.428, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 33.93it/s, loss=0.427, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 34.10it/s, loss=0.427, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 31.47it/s, loss=0.415, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 31.15it/s, loss=0.415, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 32.88it/s, loss=0.421, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 32.68it/s, loss=0.421, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 31.34it/s, loss=0.416, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 31.50it/s, loss=0.416, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 31.65it/s, loss=0.439, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 31.88it/s, loss=0.439, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 31.21it/s, loss=0.412, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 31.49it/s, loss=0.412, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 32.47it/s, loss=0.416, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 32.70it/s, loss=0.416, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 31.90it/s, loss=0.39, v_num=i3d3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 32.04it/s, loss=0.39, v_num=i3d3, BTC_val_\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 31.91it/s, loss=0.398, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 32.31it/s, loss=0.398, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 31.96it/s, loss=0.379, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 32.31it/s, loss=0.379, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 31.98it/s, loss=0.374, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 32.09it/s, loss=0.374, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 32.68it/s, loss=0.386, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 32.98it/s, loss=0.386, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 33.10it/s, loss=0.382, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 33.04it/s, loss=0.382, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 32.75it/s, loss=0.369, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 33.03it/s, loss=0.369, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 31.05it/s, loss=0.363, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 31.33it/s, loss=0.363, v_num=i3d3, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 30.62it/s, loss=0.348, v_num=i3d3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.656. Signaling Trainer to stop.\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 30.78it/s, loss=0.348, v_num=i3d3, BTC_val\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 30.62it/s, loss=0.348, v_num=i3d3, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 61.45it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6969696879386902,\n",
      " 'BTC_test_f1': 0.6898496150970459,\n",
      " 'test_loss': 0.6414384245872498}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 89794\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_093718-2fwbi3d3/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_093718-2fwbi3d3/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.83214\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.82982\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.34905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 34\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 700\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 31\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625899069\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 84\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.69231\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.675\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.68367\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.82927\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.82763\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.28414\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.69697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.68985\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.64144\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▂▃▃▄▅▆▅▆▆▇▇▇▇▇▇██▇█▇▇▇▇▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▂▃▃▄▅▆▅▆▆▇▇▇▇▇▇█▇▇█▇▇▇▇▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ███▇▇▆▆▅▅▄▄▄▃▃▃▃▃▂▃▃▂▂▂▃▂▂▂▂▂▁▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▃▃▁▆▃▁▁▆█▃▃▁▁▃▁▃▃▃▃▃▁▆▃▃▆▃▃▃▃▆▃█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▅▄▁▇▄▁▁▇█▅▄▄▁▅▄▅▅▅▄▄▁▅▅▅▇▅▄▄▅▇▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▁▁▁▁▂▁▁▂▃▁▂▁▂▁▄▂▁▁▃▁▄▃▆▁▂▅▃▄█▆▄▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▆▆▃█▅▇▅▇▆▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▆▆▃█▅▇▅▇▆▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▅▅█▂▅▃▄▄▄▃▄▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/2fwbi3d3\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:38:19.200294: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/7nxh0sjq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_093817-7nxh0sjq\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 219 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 387   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.178     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 35.67it/s, loss=1.11, v_num=0sjq, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.113\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 35.89it/s, loss=1.11, v_num=0sjq, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 34.91it/s, loss=1.08, v_num=0sjq, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 35.21it/s, loss=1.08, v_num=0sjq, BTC_val_a\u001b[A\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 34.22it/s, loss=1.06, v_num=0sjq, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 33.60it/s, loss=1.06, v_num=0sjq, BTC_val_a\u001b[A\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 33.23it/s, loss=1.05, v_num=0sjq, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 33.55it/s, loss=1.05, v_num=0sjq, BTC_val_a\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 33.66it/s, loss=1.03, v_num=0sjq, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 33.93it/s, loss=1.03, v_num=0sjq, BTC_val_a\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 32.43it/s, loss=0.987, v_num=0sjq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 32.48it/s, loss=0.987, v_num=0sjq, BTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 32.75it/s, loss=0.914, v_num=0sjq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 32.42it/s, loss=0.914, v_num=0sjq, BTC_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 33.62it/s, loss=0.889, v_num=0sjq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 34.01it/s, loss=0.889, v_num=0sjq, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 34.42it/s, loss=0.842, v_num=0sjq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.048 >= min_delta = 0.003. New best score: 1.066\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 34.57it/s, loss=0.842, v_num=0sjq, BTC_val_\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 34.19it/s, loss=0.827, v_num=0sjq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 33.92it/s, loss=0.827, v_num=0sjq, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 33.25it/s, loss=0.815, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 33.57it/s, loss=0.815, v_num=0sjq, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 33.91it/s, loss=0.806, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 33.82it/s, loss=0.806, v_num=0sjq, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 33.66it/s, loss=0.763, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.065 >= min_delta = 0.003. New best score: 1.001\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 33.96it/s, loss=0.763, v_num=0sjq, BTC_val\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 33.10it/s, loss=0.72, v_num=0sjq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 33.33it/s, loss=0.72, v_num=0sjq, BTC_val_\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 32.83it/s, loss=0.735, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.053 >= min_delta = 0.003. New best score: 0.948\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 32.68it/s, loss=0.735, v_num=0sjq, BTC_val\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 33.16it/s, loss=0.709, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 33.43it/s, loss=0.709, v_num=0sjq, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 32.45it/s, loss=0.726, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.942\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 32.77it/s, loss=0.726, v_num=0sjq, BTC_val\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 34.39it/s, loss=0.71, v_num=0sjq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 34.63it/s, loss=0.71, v_num=0sjq, BTC_val_\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 30.08it/s, loss=0.747, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 30.46it/s, loss=0.747, v_num=0sjq, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 33.15it/s, loss=0.707, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 33.38it/s, loss=0.707, v_num=0sjq, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 32.88it/s, loss=0.688, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 33.02it/s, loss=0.688, v_num=0sjq, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 33.92it/s, loss=0.683, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 33.96it/s, loss=0.683, v_num=0sjq, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 33.17it/s, loss=0.668, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 32.58it/s, loss=0.668, v_num=0sjq, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 32.48it/s, loss=0.653, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 32.74it/s, loss=0.653, v_num=0sjq, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 34.16it/s, loss=0.63, v_num=0sjq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 34.52it/s, loss=0.63, v_num=0sjq, BTC_val_\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 32.84it/s, loss=0.644, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 33.09it/s, loss=0.644, v_num=0sjq, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 31.47it/s, loss=0.638, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 31.79it/s, loss=0.638, v_num=0sjq, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 33.67it/s, loss=0.623, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 33.95it/s, loss=0.623, v_num=0sjq, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 32.58it/s, loss=0.602, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 32.93it/s, loss=0.602, v_num=0sjq, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 34.32it/s, loss=0.625, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 34.52it/s, loss=0.625, v_num=0sjq, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 33.40it/s, loss=0.595, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 33.78it/s, loss=0.595, v_num=0sjq, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 32.16it/s, loss=0.582, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 31.96it/s, loss=0.582, v_num=0sjq, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 32.40it/s, loss=0.582, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 32.67it/s, loss=0.582, v_num=0sjq, BTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 33.14it/s, loss=0.592, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 33.48it/s, loss=0.592, v_num=0sjq, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 32.92it/s, loss=0.56, v_num=0sjq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 32.80it/s, loss=0.56, v_num=0sjq, BTC_val_\u001b[A\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 33.24it/s, loss=0.555, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 33.38it/s, loss=0.555, v_num=0sjq, BTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 32.79it/s, loss=0.516, v_num=0sjq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.942. Signaling Trainer to stop.\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 33.06it/s, loss=0.516, v_num=0sjq, BTC_val\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 32.94it/s, loss=0.516, v_num=0sjq, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 65.31it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.39393940567970276,\n",
      " 'BTC_test_f1': 0.3155929148197174,\n",
      " 'test_loss': 1.1090818643569946}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 90005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_093817-7nxh0sjq/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_093817-7nxh0sjq/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.78918\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.78584\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.51668\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 36\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 740\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625899129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 88\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.46154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.38095\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.91281\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.68293\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.68323\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.81196\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.39394\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.31559\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.10908\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▂▂▃▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▂▃▃▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ██▇▇▇▇▆▅▅▅▅▄▄▃▄▃▃▃▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁█▁▁▁▆▄▄▄▆▃▁▄▆▁▄▃▄▁▁▃▆▃▆▄▃▃▄▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁▇▁▁▁▆▅▅▅▆▄▁▅▆▁▆▄▅▃▁▄▆▄█▅▄▄▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▂▂▂▂▂▃▄▄▂▂▂▅▁▂▁▁▁▄▂▂▁▅▂▃▄▃▅▄▂█▁▂▂▆▂▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▃▄▅▄▄▆▅▄▆▆▇█▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▃▄▄▄▄▅▅▄▆▆▇█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ██▆▆▅▆▄▅▅▄▄▂▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/7nxh0sjq\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:39:18.722710: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/l1mrmage\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_093917-l1mrmage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 219 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 258   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.177     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 35.24it/s, loss=0.699, v_num=mage, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.701\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 35.53it/s, loss=0.699, v_num=mage, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 34.97it/s, loss=0.692, v_num=mage, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 35.13it/s, loss=0.692, v_num=mage, BTC_val_\u001b[A\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 33.13it/s, loss=0.688, v_num=mage, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 33.35it/s, loss=0.688, v_num=mage, BTC_val_\u001b[A\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 33.05it/s, loss=0.677, v_num=mage, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 33.29it/s, loss=0.677, v_num=mage, BTC_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 35.31it/s, loss=0.673, v_num=mage, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 35.42it/s, loss=0.673, v_num=mage, BTC_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 33.84it/s, loss=0.634, v_num=mage, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 33.68it/s, loss=0.634, v_num=mage, BTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 34.58it/s, loss=0.603, v_num=mage, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 34.17it/s, loss=0.603, v_num=mage, BTC_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 34.31it/s, loss=0.569, v_num=mage, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 34.48it/s, loss=0.569, v_num=mage, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 33.86it/s, loss=0.558, v_num=mage, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.030 >= min_delta = 0.003. New best score: 0.671\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 34.05it/s, loss=0.558, v_num=mage, BTC_val_\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 33.34it/s, loss=0.533, v_num=mage, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 33.35it/s, loss=0.533, v_num=mage, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 33.52it/s, loss=0.5, v_num=mage, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 33.62it/s, loss=0.5, v_num=mage, BTC_val_a\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 32.66it/s, loss=0.493, v_num=mage, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 32.96it/s, loss=0.493, v_num=mage, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 33.97it/s, loss=0.485, v_num=mage, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 33.96it/s, loss=0.485, v_num=mage, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 34.06it/s, loss=0.475, v_num=mage, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 34.35it/s, loss=0.475, v_num=mage, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 32.85it/s, loss=0.455, v_num=mage, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 32.85it/s, loss=0.455, v_num=mage, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 34.08it/s, loss=0.467, v_num=mage, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 34.13it/s, loss=0.467, v_num=mage, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 32.28it/s, loss=0.455, v_num=mage, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 32.56it/s, loss=0.455, v_num=mage, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 33.76it/s, loss=0.44, v_num=mage, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 33.77it/s, loss=0.44, v_num=mage, BTC_val_\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 30.19it/s, loss=0.454, v_num=mage, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 30.47it/s, loss=0.454, v_num=mage, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 34.13it/s, loss=0.406, v_num=mage, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 33.32it/s, loss=0.406, v_num=mage, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 32.92it/s, loss=0.441, v_num=mage, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 33.27it/s, loss=0.441, v_num=mage, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 34.00it/s, loss=0.415, v_num=mage, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 34.23it/s, loss=0.415, v_num=mage, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 32.58it/s, loss=0.439, v_num=mage, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 32.80it/s, loss=0.439, v_num=mage, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 34.33it/s, loss=0.418, v_num=mage, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 34.33it/s, loss=0.418, v_num=mage, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 32.51it/s, loss=0.39, v_num=mage, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 32.27it/s, loss=0.39, v_num=mage, BTC_val_\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 32.74it/s, loss=0.395, v_num=mage, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 32.99it/s, loss=0.395, v_num=mage, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 34.23it/s, loss=0.378, v_num=mage, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 34.54it/s, loss=0.378, v_num=mage, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 33.63it/s, loss=0.411, v_num=mage, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 33.96it/s, loss=0.411, v_num=mage, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 32.17it/s, loss=0.369, v_num=mage, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.671. Signaling Trainer to stop.\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 32.46it/s, loss=0.369, v_num=mage, BTC_val\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 32.31it/s, loss=0.369, v_num=mage, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 87.56it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.4848484992980957,\n",
      " 'BTC_test_f1': 0.37178051471710205,\n",
      " 'test_loss': 0.7272545099258423}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 90213\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_093917-l1mrmage/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_093917-l1mrmage/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.83453\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.83098\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.36286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 28\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 580\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625899183\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 69\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.61538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.57516\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.82474\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.78104\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.45978\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.48485\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.37178\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.72725\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▁▁▂▂▄▄▅▅▆▆▆▇▇▇▇▇▇▇█▇▇▇▇█▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▁▁▂▂▃▄▅▅▆▆▆▇▇▇▇▇▇▇█▇▇▇▇█▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ████▇▇▆▅▅▅▄▄▃▃▃▃▃▃▃▂▃▂▃▂▂▂▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▅▅▁▅▅▁▅█▅▁▁▁▅▁▁▅▁▅▅██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁▇▆▁▆▂▁▅█▂▁▁▁▅▁▁▇▁▆▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▁▁▁▂▂▄▃▁▁▂▂▂▂▃▂▂▄▄█▂▅▅▁▇▃▂▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▄▆▄█▅▅█▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▄▆▅█▆▅███▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▇▃▆▁▄▄▁▃▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/l1mrmage\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:40:13.454167: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/4pxs9qsc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094012-4pxs9qsc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 219 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 387   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.178     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 33.47it/s, loss=1.11, v_num=9qsc, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.101\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 33.64it/s, loss=1.11, v_num=9qsc, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 34.77it/s, loss=1.07, v_num=9qsc, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 35.13it/s, loss=1.07, v_num=9qsc, BTC_val_a\u001b[A\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 35.03it/s, loss=1.05, v_num=9qsc, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 34.33it/s, loss=1.05, v_num=9qsc, BTC_val_a\u001b[A\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 32.41it/s, loss=1.05, v_num=9qsc, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 32.74it/s, loss=1.05, v_num=9qsc, BTC_val_a\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 32.87it/s, loss=1.03, v_num=9qsc, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 32.68it/s, loss=1.03, v_num=9qsc, BTC_val_a\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 32.29it/s, loss=0.994, v_num=9qsc, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 32.35it/s, loss=0.994, v_num=9qsc, BTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 34.78it/s, loss=0.95, v_num=9qsc, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 34.26it/s, loss=0.95, v_num=9qsc, BTC_val_a\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 33.71it/s, loss=0.882, v_num=9qsc, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 33.90it/s, loss=0.882, v_num=9qsc, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 32.72it/s, loss=0.839, v_num=9qsc, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.045 >= min_delta = 0.003. New best score: 1.056\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 32.91it/s, loss=0.839, v_num=9qsc, BTC_val_\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 32.19it/s, loss=0.809, v_num=9qsc, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.040 >= min_delta = 0.003. New best score: 1.016\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 32.31it/s, loss=0.809, v_num=9qsc, BTC_val_\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 33.19it/s, loss=0.779, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 33.13it/s, loss=0.779, v_num=9qsc, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 33.71it/s, loss=0.786, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 33.87it/s, loss=0.786, v_num=9qsc, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 32.13it/s, loss=0.754, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 32.31it/s, loss=0.754, v_num=9qsc, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 32.35it/s, loss=0.743, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 32.62it/s, loss=0.743, v_num=9qsc, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 32.23it/s, loss=0.715, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.085 >= min_delta = 0.003. New best score: 0.931\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 32.36it/s, loss=0.715, v_num=9qsc, BTC_val\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 32.81it/s, loss=0.712, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 33.08it/s, loss=0.712, v_num=9qsc, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 32.58it/s, loss=0.7, v_num=9qsc, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 32.57it/s, loss=0.7, v_num=9qsc, BTC_val_a\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 32.16it/s, loss=0.68, v_num=9qsc, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 31.82it/s, loss=0.68, v_num=9qsc, BTC_val_\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 29.35it/s, loss=0.688, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 29.42it/s, loss=0.688, v_num=9qsc, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 32.60it/s, loss=0.688, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 32.91it/s, loss=0.688, v_num=9qsc, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 33.48it/s, loss=0.649, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 33.70it/s, loss=0.649, v_num=9qsc, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 32.01it/s, loss=0.652, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 31.97it/s, loss=0.652, v_num=9qsc, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 31.58it/s, loss=0.637, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 31.52it/s, loss=0.637, v_num=9qsc, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 32.59it/s, loss=0.636, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 32.87it/s, loss=0.636, v_num=9qsc, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 32.27it/s, loss=0.632, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 32.65it/s, loss=0.632, v_num=9qsc, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 32.12it/s, loss=0.655, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 32.29it/s, loss=0.655, v_num=9qsc, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 32.07it/s, loss=0.63, v_num=9qsc, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 32.33it/s, loss=0.63, v_num=9qsc, BTC_val_\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 32.26it/s, loss=0.591, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 32.20it/s, loss=0.591, v_num=9qsc, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 33.97it/s, loss=0.608, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 34.14it/s, loss=0.608, v_num=9qsc, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 33.72it/s, loss=0.571, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 33.70it/s, loss=0.571, v_num=9qsc, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 32.53it/s, loss=0.581, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 32.37it/s, loss=0.581, v_num=9qsc, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 34.10it/s, loss=0.569, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 34.21it/s, loss=0.569, v_num=9qsc, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 32.11it/s, loss=0.569, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 32.39it/s, loss=0.569, v_num=9qsc, BTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 33.10it/s, loss=0.551, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 33.28it/s, loss=0.551, v_num=9qsc, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 33.01it/s, loss=0.563, v_num=9qsc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.931. Signaling Trainer to stop.\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 33.10it/s, loss=0.563, v_num=9qsc, BTC_val\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 32.96it/s, loss=0.563, v_num=9qsc, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 86.73it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.3636363744735718,\n",
      " 'BTC_test_f1': 0.2788461744785309,\n",
      " 'test_loss': 1.2211397886276245}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 90431\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094012-4pxs9qsc/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094012-4pxs9qsc/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.74861\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.74098\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.56557\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 34\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 700\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625899242\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 84\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.53846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.52381\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.3533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.82927\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.83464\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.4245\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.36364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.27885\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.22114\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▃▃▃▄▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▂▂▃▄▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ██▇▇▇▇▆▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▄▄▁▁▁▁▆▁▃▃▄▄▃▄▄█▄▁▄▄█▄▃▆▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁▅▄▁▁▁▁▅▁▃▃▅▄▃▄▅█▄▃▅▅█▅▃▇▇▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▂▃▂▃▃▃▅▃▂▂▂▂▇█▁▃▂▃▃▂▇▂▂▂▁▃▃▂▂▅▇▂▄▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▃▅▆▅█▆▆▇▃▇▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▃▅▆▆█▆▆▇▃▇▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▇▅▅▅▂▄▄▃▇▃▃▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/4pxs9qsc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:41:11.571372: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/bdnodjuq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094110-bdnodjuq\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 134 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 258   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "959 K     Trainable params\n",
      "0         Non-trainable params\n",
      "959 K     Total params\n",
      "3.837     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 37.13it/s, loss=0.687, v_num=djuq, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.687\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 37.20it/s, loss=0.687, v_num=djuq, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 34.81it/s, loss=0.596, v_num=djuq, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.073 >= min_delta = 0.003. New best score: 0.614\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 34.95it/s, loss=0.596, v_num=djuq, BTC_val_\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 33.95it/s, loss=0.574, v_num=djuq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 34.17it/s, loss=0.574, v_num=djuq, BTC_val_\u001b[A\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 32.91it/s, loss=0.565, v_num=djuq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 33.19it/s, loss=0.565, v_num=djuq, BTC_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 34.07it/s, loss=0.524, v_num=djuq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 34.28it/s, loss=0.524, v_num=djuq, BTC_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 34.23it/s, loss=0.509, v_num=djuq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 34.37it/s, loss=0.509, v_num=djuq, BTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 33.94it/s, loss=0.518, v_num=djuq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 34.11it/s, loss=0.518, v_num=djuq, BTC_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 34.27it/s, loss=0.49, v_num=djuq, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.027 >= min_delta = 0.003. New best score: 0.587\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 34.61it/s, loss=0.49, v_num=djuq, BTC_val_a\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 34.41it/s, loss=0.504, v_num=djuq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 34.38it/s, loss=0.504, v_num=djuq, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 35.60it/s, loss=0.488, v_num=djuq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 35.47it/s, loss=0.488, v_num=djuq, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 34.42it/s, loss=0.496, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 34.77it/s, loss=0.496, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 33.99it/s, loss=0.486, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 33.50it/s, loss=0.486, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 33.46it/s, loss=0.476, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 33.65it/s, loss=0.476, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 33.88it/s, loss=0.477, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 34.05it/s, loss=0.477, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 33.09it/s, loss=0.471, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 33.36it/s, loss=0.471, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 34.97it/s, loss=0.474, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 35.18it/s, loss=0.474, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 36.69it/s, loss=0.48, v_num=djuq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 35.99it/s, loss=0.48, v_num=djuq, BTC_val_\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 34.28it/s, loss=0.471, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 34.58it/s, loss=0.471, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 34.87it/s, loss=0.487, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 35.11it/s, loss=0.487, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 35.04it/s, loss=0.474, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 35.12it/s, loss=0.474, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 35.28it/s, loss=0.466, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 35.61it/s, loss=0.466, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 30.43it/s, loss=0.47, v_num=djuq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 30.74it/s, loss=0.47, v_num=djuq, BTC_val_\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 34.75it/s, loss=0.467, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 34.98it/s, loss=0.467, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 34.67it/s, loss=0.478, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 34.96it/s, loss=0.478, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 35.32it/s, loss=0.454, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 35.50it/s, loss=0.454, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 33.94it/s, loss=0.475, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 34.03it/s, loss=0.475, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 35.05it/s, loss=0.47, v_num=djuq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 34.29it/s, loss=0.47, v_num=djuq, BTC_val_\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 34.32it/s, loss=0.464, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.583\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 34.46it/s, loss=0.464, v_num=djuq, BTC_val\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 34.75it/s, loss=0.461, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 35.14it/s, loss=0.461, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 33.82it/s, loss=0.452, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 33.91it/s, loss=0.452, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 34.14it/s, loss=0.465, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 34.46it/s, loss=0.465, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 34.56it/s, loss=0.451, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 34.48it/s, loss=0.451, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 31.84it/s, loss=0.461, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 31.97it/s, loss=0.461, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 35.49it/s, loss=0.449, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 35.84it/s, loss=0.449, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 34.24it/s, loss=0.453, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 34.53it/s, loss=0.453, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 34.95it/s, loss=0.441, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 35.03it/s, loss=0.441, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 33.09it/s, loss=0.446, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 33.40it/s, loss=0.446, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 35.07it/s, loss=0.455, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 34.80it/s, loss=0.455, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 20/21 [00:00<00:00, 35.51it/s, loss=0.449, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 35.81it/s, loss=0.449, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 20/21 [00:00<00:00, 35.83it/s, loss=0.448, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 36.14it/s, loss=0.448, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 34.50it/s, loss=0.434, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 34.68it/s, loss=0.434, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 20/21 [00:00<00:00, 34.12it/s, loss=0.44, v_num=djuq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 34.46it/s, loss=0.44, v_num=djuq, BTC_val_\u001b[A\n",
      "Epoch 42:  95%|▉| 20/21 [00:00<00:00, 34.49it/s, loss=0.461, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 34.63it/s, loss=0.461, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 20/21 [00:00<00:00, 34.59it/s, loss=0.443, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 34.56it/s, loss=0.443, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 44:  95%|▉| 20/21 [00:00<00:00, 34.13it/s, loss=0.441, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 34.09it/s, loss=0.441, v_num=djuq, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45:  95%|▉| 20/21 [00:00<00:00, 32.65it/s, loss=0.438, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 32.91it/s, loss=0.438, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 20/21 [00:00<00:00, 34.51it/s, loss=0.433, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 34.38it/s, loss=0.433, v_num=djuq, BTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 20/21 [00:00<00:00, 34.29it/s, loss=0.435, v_num=djuq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.583. Signaling Trainer to stop.\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 34.59it/s, loss=0.435, v_num=djuq, BTC_val\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 34.37it/s, loss=0.435, v_num=djuq, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 63.52it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6969696879386902,\n",
      " 'BTC_test_f1': 0.6726190447807312,\n",
      " 'test_loss': 0.5953559875488281}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 90631\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094110-bdnodjuq/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094110-bdnodjuq/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.78918\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.78749\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.43195\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 47\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 960\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 37\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625899307\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 115\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.61538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.60606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.61567\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.4481\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.69697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.67262\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.59536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▅▅▅▇▆▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇██▇▇█▇██▇▇███▇██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇██▇▇█▇██▇▇███▇██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▅▅▃▃▃▃▃▃▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▁▁▅▁▅▅▁▅▁█▅▅▅▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▂▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▂▂▅▂▅▅▁▅▁█▅▅▅▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▃▄▄▄▄▁▂▅▃▃▂▃▂▂▂▄▃▄▄▃▄▂▁▃▂▃▆▃▅▆▁▃▃▄▄▄▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▃▃▃▇▃▆█▃▃▅▂█▆▃▃▆▁▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▃▃▃▆▃▆█▃▃▅▂▇▆▃▂▆▁▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆█▅▂▅▃▁▆▅▄▆▃▂▅▄▂▅▇▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/bdnodjuq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:42:14.807870: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/m1o1xmea\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094213-m1o1xmea\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 134 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 387   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "959 K     Trainable params\n",
      "0         Non-trainable params\n",
      "959 K     Total params\n",
      "3.838     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 34.63it/s, loss=1.1, v_num=xmea, BTC_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.094\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 34.46it/s, loss=1.1, v_num=xmea, BTC_val_ac\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 32.56it/s, loss=0.995, v_num=xmea, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.173 >= min_delta = 0.003. New best score: 0.920\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 32.97it/s, loss=0.995, v_num=xmea, BTC_val_\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 33.74it/s, loss=0.937, v_num=xmea, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.081 >= min_delta = 0.003. New best score: 0.839\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 33.58it/s, loss=0.937, v_num=xmea, BTC_val_\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 31.17it/s, loss=0.887, v_num=xmea, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 31.50it/s, loss=0.887, v_num=xmea, BTC_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 32.94it/s, loss=0.865, v_num=xmea, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.058 >= min_delta = 0.003. New best score: 0.781\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 32.95it/s, loss=0.865, v_num=xmea, BTC_val_\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 33.60it/s, loss=0.826, v_num=xmea, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 33.74it/s, loss=0.826, v_num=xmea, BTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 34.39it/s, loss=0.806, v_num=xmea, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.776\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 34.51it/s, loss=0.806, v_num=xmea, BTC_val_\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 32.56it/s, loss=0.825, v_num=xmea, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 32.86it/s, loss=0.825, v_num=xmea, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 32.38it/s, loss=0.802, v_num=xmea, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 32.50it/s, loss=0.802, v_num=xmea, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 32.00it/s, loss=0.775, v_num=xmea, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 32.26it/s, loss=0.775, v_num=xmea, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 33.43it/s, loss=0.786, v_num=xmea, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 33.62it/s, loss=0.786, v_num=xmea, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 32.24it/s, loss=0.757, v_num=xmea, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 32.51it/s, loss=0.757, v_num=xmea, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 32.57it/s, loss=0.768, v_num=xmea, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 32.51it/s, loss=0.768, v_num=xmea, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 34.39it/s, loss=0.743, v_num=xmea, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 34.54it/s, loss=0.743, v_num=xmea, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 33.35it/s, loss=0.734, v_num=xmea, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 33.56it/s, loss=0.734, v_num=xmea, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 33.15it/s, loss=0.737, v_num=xmea, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 33.45it/s, loss=0.737, v_num=xmea, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 32.73it/s, loss=0.76, v_num=xmea, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 33.00it/s, loss=0.76, v_num=xmea, BTC_val_\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 34.08it/s, loss=0.751, v_num=xmea, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 34.12it/s, loss=0.751, v_num=xmea, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 33.07it/s, loss=0.734, v_num=xmea, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 33.36it/s, loss=0.734, v_num=xmea, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 33.19it/s, loss=0.736, v_num=xmea, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 33.27it/s, loss=0.736, v_num=xmea, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 34.46it/s, loss=0.715, v_num=xmea, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 34.28it/s, loss=0.715, v_num=xmea, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 28.88it/s, loss=0.736, v_num=xmea, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 29.00it/s, loss=0.736, v_num=xmea, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 33.51it/s, loss=0.746, v_num=xmea, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 33.53it/s, loss=0.746, v_num=xmea, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 33.36it/s, loss=0.736, v_num=xmea, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 33.65it/s, loss=0.736, v_num=xmea, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 33.46it/s, loss=0.718, v_num=xmea, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 33.80it/s, loss=0.718, v_num=xmea, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 33.70it/s, loss=0.722, v_num=xmea, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 33.75it/s, loss=0.722, v_num=xmea, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 32.69it/s, loss=0.729, v_num=xmea, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.776. Signaling Trainer to stop.\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 33.11it/s, loss=0.729, v_num=xmea, BTC_val\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 32.99it/s, loss=0.729, v_num=xmea, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 91.02it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.7272727489471436,\n",
      " 'BTC_test_f1': 0.7286325097084045,\n",
      " 'test_loss': 0.7747744917869568}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 90821\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094213-m1o1xmea/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094213-m1o1xmea/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.6778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.66273\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.7265\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 540\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625899358\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.53846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.52706\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.95184\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.73171\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.71157\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.63121\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.72727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.72863\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.77477\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▄▅▆▆▆▇▇▇▇▇▇███▇▇████▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▄▅▆▆▇▇▇▇▇▇▇████▇█████▇███▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▅▄▄▃▃▃▃▂▂▂▂▁▁▁▂▂▁▁▁▁▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▃▃▅▅█▅▅▆▆▁▅▁▅▃▅▆▅▃▅▃█▁▅▅▆▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▂▂▅▅█▅▅▆▆▁▃▁▃▂▃▆▃▂▅▂█▁▃▃▆▃▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▄▂▂▁▃▁▁▂▆▃▇▃▅▄▂▄▄▄▄▂▅▂▄▅▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▁▇▅█▆▃▅▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▂▁▇▅█▆▄▅▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▇▄▅▃▃▅▆▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/m1o1xmea\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:43:06.272481: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/b1uibibc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094305-b1uibibc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 134 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 258   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "959 K     Trainable params\n",
      "0         Non-trainable params\n",
      "959 K     Total params\n",
      "3.837     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 37.19it/s, loss=0.686, v_num=bibc, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.687\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 37.27it/s, loss=0.686, v_num=bibc, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 37.05it/s, loss=0.608, v_num=bibc, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.089 >= min_delta = 0.003. New best score: 0.597\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 37.18it/s, loss=0.608, v_num=bibc, BTC_val_\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 32.59it/s, loss=0.56, v_num=bibc, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.018 >= min_delta = 0.003. New best score: 0.579\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 32.26it/s, loss=0.56, v_num=bibc, BTC_val_a\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 33.31it/s, loss=0.564, v_num=bibc, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 33.56it/s, loss=0.564, v_num=bibc, BTC_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 35.16it/s, loss=0.533, v_num=bibc, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 34.88it/s, loss=0.533, v_num=bibc, BTC_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 33.89it/s, loss=0.513, v_num=bibc, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 33.97it/s, loss=0.513, v_num=bibc, BTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 33.30it/s, loss=0.51, v_num=bibc, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 33.37it/s, loss=0.51, v_num=bibc, BTC_val_a\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 32.34it/s, loss=0.5, v_num=bibc, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 32.57it/s, loss=0.5, v_num=bibc, BTC_val_ac\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 33.01it/s, loss=0.5, v_num=bibc, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 33.29it/s, loss=0.5, v_num=bibc, BTC_val_ac\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 33.74it/s, loss=0.491, v_num=bibc, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 33.91it/s, loss=0.491, v_num=bibc, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 33.93it/s, loss=0.481, v_num=bibc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 33.94it/s, loss=0.481, v_num=bibc, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 34.78it/s, loss=0.491, v_num=bibc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 34.96it/s, loss=0.491, v_num=bibc, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 34.16it/s, loss=0.494, v_num=bibc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 34.26it/s, loss=0.494, v_num=bibc, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 32.55it/s, loss=0.487, v_num=bibc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 32.82it/s, loss=0.487, v_num=bibc, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 33.78it/s, loss=0.47, v_num=bibc, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 33.81it/s, loss=0.47, v_num=bibc, BTC_val_\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 33.70it/s, loss=0.483, v_num=bibc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 33.86it/s, loss=0.483, v_num=bibc, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 33.54it/s, loss=0.483, v_num=bibc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 33.79it/s, loss=0.483, v_num=bibc, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 31.85it/s, loss=0.471, v_num=bibc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 32.26it/s, loss=0.471, v_num=bibc, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 35.54it/s, loss=0.468, v_num=bibc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 35.48it/s, loss=0.468, v_num=bibc, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 32.81it/s, loss=0.467, v_num=bibc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 32.96it/s, loss=0.467, v_num=bibc, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 33.88it/s, loss=0.459, v_num=bibc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 34.08it/s, loss=0.459, v_num=bibc, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 30.44it/s, loss=0.467, v_num=bibc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 30.59it/s, loss=0.467, v_num=bibc, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 33.02it/s, loss=0.461, v_num=bibc, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.579. Signaling Trainer to stop.\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 33.29it/s, loss=0.461, v_num=bibc, BTC_val\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 33.17it/s, loss=0.461, v_num=bibc, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 95.93it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6666666865348816,\n",
      " 'BTC_test_f1': 0.6458536386489868,\n",
      " 'test_loss': 0.5902164578437805}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 90993\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094305-b1uibibc/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094305-b1uibibc/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.78361\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.77808\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.46189\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 22\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 460\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 22\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625899407\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 55\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.69231\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.69048\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.6131\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.70312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.70305\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.603\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.64585\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.59022\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▅▆▆▆▇▇▇▇▇██▇█████▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▅▆▆▆▇▇▇▇▇██▇██████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▄▄▃▃▃▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁█▅▅█████▅█████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁█▆▆█████▆█████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▂▁▃▄▂▂▄▂▄▃▃▅▂▄▃▄▂▃▄▅▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▃▂▃▁▅▇▂█▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▄▃▃▁▅▇▃█▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▄▆▄▆▂▁█▁▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/b1uibibc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:43:54.533739: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/vz74njar\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094353-vz74njar\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 134 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 387   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "959 K     Trainable params\n",
      "0         Non-trainable params\n",
      "959 K     Total params\n",
      "3.838     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 35.95it/s, loss=1.1, v_num=njar, BTC_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.104\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 35.97it/s, loss=1.1, v_num=njar, BTC_val_ac\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 35.51it/s, loss=1.03, v_num=njar, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.147 >= min_delta = 0.003. New best score: 0.958\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 35.61it/s, loss=1.03, v_num=njar, BTC_val_a\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 32.47it/s, loss=0.914, v_num=njar, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.137 >= min_delta = 0.003. New best score: 0.820\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 32.63it/s, loss=0.914, v_num=njar, BTC_val_\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 34.09it/s, loss=0.889, v_num=njar, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.039 >= min_delta = 0.003. New best score: 0.781\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 34.10it/s, loss=0.889, v_num=njar, BTC_val_\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 33.22it/s, loss=0.848, v_num=njar, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 33.44it/s, loss=0.848, v_num=njar, BTC_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 33.37it/s, loss=0.85, v_num=njar, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 33.67it/s, loss=0.85, v_num=njar, BTC_val_a\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 32.90it/s, loss=0.794, v_num=njar, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 33.20it/s, loss=0.794, v_num=njar, BTC_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 33.73it/s, loss=0.783, v_num=njar, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 33.83it/s, loss=0.783, v_num=njar, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 32.70it/s, loss=0.809, v_num=njar, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 32.63it/s, loss=0.809, v_num=njar, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 32.02it/s, loss=0.769, v_num=njar, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 32.25it/s, loss=0.769, v_num=njar, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 34.56it/s, loss=0.777, v_num=njar, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 34.62it/s, loss=0.777, v_num=njar, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 33.01it/s, loss=0.764, v_num=njar, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 33.10it/s, loss=0.764, v_num=njar, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 35.04it/s, loss=0.779, v_num=njar, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 34.90it/s, loss=0.779, v_num=njar, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 33.21it/s, loss=0.802, v_num=njar, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 33.26it/s, loss=0.802, v_num=njar, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 32.93it/s, loss=0.758, v_num=njar, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 32.42it/s, loss=0.758, v_num=njar, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 32.83it/s, loss=0.742, v_num=njar, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 33.07it/s, loss=0.742, v_num=njar, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 33.82it/s, loss=0.735, v_num=njar, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 34.02it/s, loss=0.735, v_num=njar, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 32.95it/s, loss=0.755, v_num=njar, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 33.29it/s, loss=0.755, v_num=njar, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 33.92it/s, loss=0.749, v_num=njar, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 33.74it/s, loss=0.749, v_num=njar, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 35.21it/s, loss=0.765, v_num=njar, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 35.26it/s, loss=0.765, v_num=njar, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 34.01it/s, loss=0.75, v_num=njar, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 33.97it/s, loss=0.75, v_num=njar, BTC_val_\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 29.87it/s, loss=0.764, v_num=njar, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 30.25it/s, loss=0.764, v_num=njar, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 33.64it/s, loss=0.736, v_num=njar, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 33.38it/s, loss=0.736, v_num=njar, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 33.30it/s, loss=0.742, v_num=njar, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.781. Signaling Trainer to stop.\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 33.54it/s, loss=0.742, v_num=njar, BTC_val\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 33.40it/s, loss=0.742, v_num=njar, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 86.94it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6666666865348816,\n",
      " 'BTC_test_f1': 0.6626983880996704,\n",
      " 'test_loss': 0.8248701095581055}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 91164\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094353-vz74njar/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094353-vz74njar/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.67462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.66486\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.74303\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 23\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 480\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 23\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625899456\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 57\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.53846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.46296\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.97132\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.67188\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.65758\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.72299\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.6627\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.82487\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▃▆▆▇▆▇▇▇▇▇█▇▇██████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▃▆▆▇▆▇▇▇▇▇█▇▇██████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▃▅▃█▁▆▃▅▃▅▆▅▁▅▅▅▁▅▅▅▃▃▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▂▃▃█▂▇▃▅▃▅▆▃▁▃▄▄▁▄▄▃▂▂▆▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▅▂▁▆▂▅▅▃▃▃▄▇▄▃▄█▄▃▄▅▄▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▃▃▄▂▁█▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▂▄▄▃▁█▄▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ██▅▆▇▁▅▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/vz74njar\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:44:48.421028: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/1bl7x9jn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094447-1bl7x9jn\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 219 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 258   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.177     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 35.22it/s, loss=0.703, v_num=x9jn, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.690\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 34.65it/s, loss=0.703, v_num=x9jn, ETH_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 35.01it/s, loss=0.697, v_num=x9jn, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 35.25it/s, loss=0.697, v_num=x9jn, ETH_val_\u001b[A\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 32.56it/s, loss=0.689, v_num=x9jn, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 32.68it/s, loss=0.689, v_num=x9jn, ETH_val_\u001b[A\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 32.49it/s, loss=0.686, v_num=x9jn, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 32.88it/s, loss=0.686, v_num=x9jn, ETH_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 33.31it/s, loss=0.675, v_num=x9jn, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 33.19it/s, loss=0.675, v_num=x9jn, ETH_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 34.20it/s, loss=0.661, v_num=x9jn, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 33.85it/s, loss=0.661, v_num=x9jn, ETH_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 33.16it/s, loss=0.61, v_num=x9jn, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 33.49it/s, loss=0.61, v_num=x9jn, ETH_val_a\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 32.66it/s, loss=0.575, v_num=x9jn, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 32.79it/s, loss=0.575, v_num=x9jn, ETH_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 32.98it/s, loss=0.547, v_num=x9jn, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.045 >= min_delta = 0.003. New best score: 0.645\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 33.02it/s, loss=0.547, v_num=x9jn, ETH_val_\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 31.35it/s, loss=0.531, v_num=x9jn, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 31.76it/s, loss=0.531, v_num=x9jn, ETH_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 32.26it/s, loss=0.484, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.130 >= min_delta = 0.003. New best score: 0.515\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 32.58it/s, loss=0.484, v_num=x9jn, ETH_val\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 33.27it/s, loss=0.483, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 0.494\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 33.54it/s, loss=0.483, v_num=x9jn, ETH_val\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 32.26it/s, loss=0.451, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.158 >= min_delta = 0.003. New best score: 0.336\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 31.84it/s, loss=0.451, v_num=x9jn, ETH_val\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 30.72it/s, loss=0.462, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 30.97it/s, loss=0.462, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 31.28it/s, loss=0.442, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 31.33it/s, loss=0.442, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 31.80it/s, loss=0.476, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 31.89it/s, loss=0.476, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 32.32it/s, loss=0.423, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 32.71it/s, loss=0.423, v_num=x9jn, ETH_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 31.09it/s, loss=0.423, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 31.20it/s, loss=0.423, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 27.75it/s, loss=0.427, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 28.00it/s, loss=0.427, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 32.75it/s, loss=0.422, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 32.73it/s, loss=0.422, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 33.13it/s, loss=0.394, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 33.47it/s, loss=0.394, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 33.05it/s, loss=0.384, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 33.38it/s, loss=0.384, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 31.93it/s, loss=0.389, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 32.15it/s, loss=0.389, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 32.95it/s, loss=0.367, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 33.22it/s, loss=0.367, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 32.44it/s, loss=0.368, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 32.70it/s, loss=0.368, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 35.18it/s, loss=0.37, v_num=x9jn, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 35.43it/s, loss=0.37, v_num=x9jn, ETH_val_\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 32.77it/s, loss=0.368, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 32.86it/s, loss=0.368, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 32.56it/s, loss=0.372, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 32.73it/s, loss=0.372, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 32.33it/s, loss=0.344, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 0.325\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 32.67it/s, loss=0.344, v_num=x9jn, ETH_val\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 34.25it/s, loss=0.35, v_num=x9jn, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 34.41it/s, loss=0.35, v_num=x9jn, ETH_val_\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 32.52it/s, loss=0.346, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 32.59it/s, loss=0.346, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 32.51it/s, loss=0.354, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 32.86it/s, loss=0.354, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 32.82it/s, loss=0.327, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 33.15it/s, loss=0.327, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 31.93it/s, loss=0.323, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 32.21it/s, loss=0.323, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 29.95it/s, loss=0.33, v_num=x9jn, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 30.14it/s, loss=0.33, v_num=x9jn, ETH_val_\u001b[A\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 31.57it/s, loss=0.31, v_num=x9jn, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 31.80it/s, loss=0.31, v_num=x9jn, ETH_val_\u001b[A\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 32.77it/s, loss=0.284, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 33.10it/s, loss=0.284, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 31.83it/s, loss=0.31, v_num=x9jn, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 31.80it/s, loss=0.31, v_num=x9jn, ETH_val_\u001b[A\n",
      "Epoch 38:  95%|▉| 20/21 [00:00<00:00, 31.74it/s, loss=0.3, v_num=x9jn, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 32.08it/s, loss=0.3, v_num=x9jn, ETH_val_a\u001b[A\n",
      "Epoch 39:  95%|▉| 20/21 [00:00<00:00, 32.87it/s, loss=0.343, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 33.05it/s, loss=0.343, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 30.42it/s, loss=0.327, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 30.66it/s, loss=0.327, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 41:  95%|▉| 20/21 [00:00<00:00, 31.40it/s, loss=0.3, v_num=x9jn, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 31.53it/s, loss=0.3, v_num=x9jn, ETH_val_a\u001b[A\n",
      "Epoch 42:  95%|▉| 20/21 [00:00<00:00, 33.49it/s, loss=0.273, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 33.79it/s, loss=0.273, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 43:  95%|▉| 20/21 [00:00<00:00, 33.14it/s, loss=0.273, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 33.23it/s, loss=0.273, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 44:  95%|▉| 20/21 [00:00<00:00, 33.62it/s, loss=0.265, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 33.85it/s, loss=0.265, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 45:  95%|▉| 20/21 [00:00<00:00, 33.94it/s, loss=0.293, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 34.27it/s, loss=0.293, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 46:  95%|▉| 20/21 [00:00<00:00, 33.31it/s, loss=0.261, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 33.65it/s, loss=0.261, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 47:  95%|▉| 20/21 [00:00<00:00, 32.58it/s, loss=0.283, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 32.61it/s, loss=0.283, v_num=x9jn, ETH_val\u001b[A\n",
      "Epoch 48:  95%|▉| 20/21 [00:00<00:00, 33.53it/s, loss=0.264, v_num=x9jn, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.325. Signaling Trainer to stop.\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 33.71it/s, loss=0.264, v_num=x9jn, ETH_val\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 33.58it/s, loss=0.264, v_num=x9jn, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 61.63it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.6666666865348816,\n",
      " 'ETH_test_f1': 0.6458536386489868,\n",
      " 'test_loss': 0.612669050693512}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 91509\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094447-1bl7x9jn/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094447-1bl7x9jn/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.88783\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.88665\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.26644\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 48\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 39\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625899526\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 117\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.84615\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.84524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.33206\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.89062\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.8893\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.33662\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.64585\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.61267\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▁▂▂▂▄▄▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▂▂▃▄▅▅▆▆▆▆▆▇▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █████▇▆▆▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▄▄▁▃▁▃▄▄▃▅█▅▇▆▇▃▅▅▆▅▄▆▅▇▇▇▇▆▆▇▆▆▆▆▇▇▅▅▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▂▂▁▂▂▂▂▂▂▄█▅▇▆▇▂▅▅▆▅▃▆▅▇▇▇▇▆▆▇▆▆▆▆▇▇▄▅▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▅▅▅▅▆▅▅▅▃▁▃▂▃▃▅▃▄▃▂▅▁▂▁▃▂▂▂▂▂▃▅▃▃▂▂█▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▃▁▃▅▆▆▇▆▅▅▆▇█▆▇▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▃▁▃▅▆▆▇▅▅▅▆▇█▆▇▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ██▇▇▄▄▃▄▅▅▄▂▁▁▁▃▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/1bl7x9jn\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:45:57.364303: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/j72urtl6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094556-j72urtl6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 219 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 387   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.178     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 32.60it/s, loss=1.12, v_num=rtl6, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.128\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 32.63it/s, loss=1.12, v_num=rtl6, ETH_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 33.08it/s, loss=1.1, v_num=rtl6, ETH_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 33.10it/s, loss=1.1, v_num=rtl6, ETH_val_ac\u001b[A\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 30.81it/s, loss=1.09, v_num=rtl6, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 30.80it/s, loss=1.09, v_num=rtl6, ETH_val_a\u001b[A\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 30.28it/s, loss=1.08, v_num=rtl6, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 30.50it/s, loss=1.08, v_num=rtl6, ETH_val_a\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 31.74it/s, loss=1.07, v_num=rtl6, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 31.95it/s, loss=1.07, v_num=rtl6, ETH_val_a\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 32.19it/s, loss=1.07, v_num=rtl6, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 32.46it/s, loss=1.07, v_num=rtl6, ETH_val_a\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 32.95it/s, loss=1.01, v_num=rtl6, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 33.13it/s, loss=1.01, v_num=rtl6, ETH_val_a\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 32.38it/s, loss=0.97, v_num=rtl6, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 32.08it/s, loss=0.97, v_num=rtl6, ETH_val_a\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 32.73it/s, loss=0.933, v_num=rtl6, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 1.121\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 33.00it/s, loss=0.933, v_num=rtl6, ETH_val_\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 31.15it/s, loss=0.875, v_num=rtl6, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 30.80it/s, loss=0.875, v_num=rtl6, ETH_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 33.92it/s, loss=0.839, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 33.75it/s, loss=0.839, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 31.27it/s, loss=0.812, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.070 >= min_delta = 0.003. New best score: 1.051\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 31.53it/s, loss=0.812, v_num=rtl6, ETH_val\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 32.47it/s, loss=0.795, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.173 >= min_delta = 0.003. New best score: 0.878\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 32.58it/s, loss=0.795, v_num=rtl6, ETH_val\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 31.80it/s, loss=0.801, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.036 >= min_delta = 0.003. New best score: 0.842\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 31.92it/s, loss=0.801, v_num=rtl6, ETH_val\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 32.80it/s, loss=0.801, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 32.90it/s, loss=0.801, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 33.62it/s, loss=0.755, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 33.65it/s, loss=0.755, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 33.17it/s, loss=0.738, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 32.98it/s, loss=0.738, v_num=rtl6, ETH_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 31.02it/s, loss=0.771, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 0.830\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 31.28it/s, loss=0.771, v_num=rtl6, ETH_val\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 29.54it/s, loss=0.756, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 29.62it/s, loss=0.756, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 31.79it/s, loss=0.731, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 31.88it/s, loss=0.731, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 32.97it/s, loss=0.75, v_num=rtl6, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 33.14it/s, loss=0.75, v_num=rtl6, ETH_val_\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 32.40it/s, loss=0.717, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 32.47it/s, loss=0.717, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 33.27it/s, loss=0.708, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.091 >= min_delta = 0.003. New best score: 0.739\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 33.19it/s, loss=0.708, v_num=rtl6, ETH_val\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 32.13it/s, loss=0.665, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 32.36it/s, loss=0.665, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 32.04it/s, loss=0.654, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 32.37it/s, loss=0.654, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 33.49it/s, loss=0.687, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 33.50it/s, loss=0.687, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 32.48it/s, loss=0.679, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.722\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 32.75it/s, loss=0.679, v_num=rtl6, ETH_val\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 32.84it/s, loss=0.632, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 32.87it/s, loss=0.632, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 33.14it/s, loss=0.63, v_num=rtl6, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.018 >= min_delta = 0.003. New best score: 0.704\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 33.43it/s, loss=0.63, v_num=rtl6, ETH_val_\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 33.05it/s, loss=0.634, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 33.07it/s, loss=0.634, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 32.70it/s, loss=0.62, v_num=rtl6, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 32.92it/s, loss=0.62, v_num=rtl6, ETH_val_\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 31.92it/s, loss=0.635, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 32.21it/s, loss=0.635, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 32.49it/s, loss=0.609, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 32.76it/s, loss=0.609, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 33.09it/s, loss=0.608, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 32.91it/s, loss=0.608, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 31.59it/s, loss=0.608, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 31.84it/s, loss=0.608, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 33.75it/s, loss=0.574, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 34.01it/s, loss=0.574, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 34.12it/s, loss=0.581, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 33.93it/s, loss=0.581, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 30.87it/s, loss=0.594, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 31.01it/s, loss=0.594, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 38:  95%|▉| 20/21 [00:00<00:00, 32.99it/s, loss=0.543, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 33.21it/s, loss=0.543, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 39:  95%|▉| 20/21 [00:00<00:00, 32.08it/s, loss=0.557, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 32.25it/s, loss=0.557, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 31.79it/s, loss=0.534, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 31.89it/s, loss=0.534, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 41:  95%|▉| 20/21 [00:00<00:00, 31.30it/s, loss=0.532, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 31.38it/s, loss=0.532, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 42:  95%|▉| 20/21 [00:00<00:00, 32.41it/s, loss=0.516, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 32.60it/s, loss=0.516, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 43:  95%|▉| 20/21 [00:00<00:00, 33.03it/s, loss=0.497, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 32.87it/s, loss=0.497, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 44:  95%|▉| 20/21 [00:00<00:00, 32.09it/s, loss=0.475, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 31.87it/s, loss=0.475, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 45:  95%|▉| 20/21 [00:00<00:00, 33.77it/s, loss=0.48, v_num=rtl6, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 34.05it/s, loss=0.48, v_num=rtl6, ETH_val_\u001b[A\n",
      "Epoch 46:  95%|▉| 20/21 [00:00<00:00, 33.39it/s, loss=0.475, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 32.78it/s, loss=0.475, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 47:  95%|▉| 20/21 [00:00<00:00, 32.47it/s, loss=0.481, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 31.99it/s, loss=0.481, v_num=rtl6, ETH_val\u001b[A\n",
      "Epoch 48:  95%|▉| 20/21 [00:00<00:00, 32.84it/s, loss=0.496, v_num=rtl6, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.704. Signaling Trainer to stop.\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 33.29it/s, loss=0.496, v_num=rtl6, ETH_val\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 33.18it/s, loss=0.496, v_num=rtl6, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 62.13it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.6969696879386902,\n",
      " 'ETH_test_f1': 0.7003175020217896,\n",
      " 'test_loss': 0.935767650604248}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 91726\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094556-j72urtl6/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094556-j72urtl6/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.77486\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.76782\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.4954\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 48\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 980\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625899596\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 117\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.46154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.40909\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.1976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.76269\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.46148\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.69697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.70032\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.93577\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▁▂▂▃▄▄▄▅▆▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▂▂▂▃▄▄▅▆▆▆▅▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █████▇▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▁▁▁▁▃▃▃▁▄▆█▄▅▅▅▅▅▇▄▅▆▅▆▅▅▅▄▆▄▄▅▅▅█▅▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▁▁▁▃▂▂▁▄▆█▄▅▅▅▅▅▇▄▅▆▅▆▅▅▅▄▆▄▄▅▅▅█▅▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▆▇▆▆▆▅▅▆▄▃▂▄▂▂▃▄▃▁▃▅▂▂▁▄▃▃▆▂▄█▄▂▅▁▃▃▄▃▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▂▁▃▂▅▅▇▄▄▅█▃▇▅▆▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▂▁▃▂▅▅▆▄▄▅█▃▇▅▆█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ██▆▆▅▅▄▅▆▄▂▆▃▄▄▃▃▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/j72urtl6\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:47:09.192352: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/94b6kysk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094707-94b6kysk\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 219 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 258   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.177     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "                                                                                /home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 33.48it/s, loss=0.717, v_num=kysk, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.690\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 33.66it/s, loss=0.717, v_num=kysk, ETH_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 31.74it/s, loss=0.695, v_num=kysk, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 31.80it/s, loss=0.695, v_num=kysk, ETH_val_\u001b[A\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 32.47it/s, loss=0.69, v_num=kysk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 32.54it/s, loss=0.69, v_num=kysk, ETH_val_a\u001b[A\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 32.80it/s, loss=0.682, v_num=kysk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 0.678\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 33.11it/s, loss=0.682, v_num=kysk, ETH_val_\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 31.33it/s, loss=0.661, v_num=kysk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 31.44it/s, loss=0.661, v_num=kysk, ETH_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 30.93it/s, loss=0.635, v_num=kysk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 31.05it/s, loss=0.635, v_num=kysk, ETH_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 32.00it/s, loss=0.61, v_num=kysk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 31.89it/s, loss=0.61, v_num=kysk, ETH_val_a\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 33.94it/s, loss=0.57, v_num=kysk, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 33.90it/s, loss=0.57, v_num=kysk, ETH_val_a\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 32.36it/s, loss=0.526, v_num=kysk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.042 >= min_delta = 0.003. New best score: 0.636\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 32.59it/s, loss=0.526, v_num=kysk, ETH_val_\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 32.24it/s, loss=0.493, v_num=kysk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.069 >= min_delta = 0.003. New best score: 0.566\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 32.10it/s, loss=0.493, v_num=kysk, ETH_val_\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 32.51it/s, loss=0.492, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 32.64it/s, loss=0.492, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 32.96it/s, loss=0.47, v_num=kysk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 32.97it/s, loss=0.47, v_num=kysk, ETH_val_\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 32.07it/s, loss=0.448, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 32.46it/s, loss=0.448, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 33.09it/s, loss=0.448, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.562\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 33.27it/s, loss=0.448, v_num=kysk, ETH_val\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 30.62it/s, loss=0.431, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.037 >= min_delta = 0.003. New best score: 0.525\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 30.39it/s, loss=0.431, v_num=kysk, ETH_val\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 32.30it/s, loss=0.425, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 32.37it/s, loss=0.425, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 31.27it/s, loss=0.433, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.085 >= min_delta = 0.003. New best score: 0.440\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 31.60it/s, loss=0.433, v_num=kysk, ETH_val\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 31.28it/s, loss=0.404, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 30.89it/s, loss=0.404, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 28.59it/s, loss=0.405, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 28.93it/s, loss=0.405, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 33.09it/s, loss=0.413, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 33.37it/s, loss=0.413, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 31.48it/s, loss=0.416, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 31.44it/s, loss=0.416, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 33.81it/s, loss=0.387, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 33.94it/s, loss=0.387, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 31.79it/s, loss=0.38, v_num=kysk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 32.04it/s, loss=0.38, v_num=kysk, ETH_val_\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 33.92it/s, loss=0.386, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 0.419\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 34.04it/s, loss=0.386, v_num=kysk, ETH_val\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 31.88it/s, loss=0.367, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 31.81it/s, loss=0.367, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 32.02it/s, loss=0.367, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.038 >= min_delta = 0.003. New best score: 0.381\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 32.25it/s, loss=0.367, v_num=kysk, ETH_val\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 32.37it/s, loss=0.356, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 32.27it/s, loss=0.356, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 32.02it/s, loss=0.346, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 32.30it/s, loss=0.346, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 32.39it/s, loss=0.321, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 32.61it/s, loss=0.321, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 32.12it/s, loss=0.333, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 32.38it/s, loss=0.333, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 33.22it/s, loss=0.357, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 33.26it/s, loss=0.357, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 31.83it/s, loss=0.324, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.373\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 31.85it/s, loss=0.324, v_num=kysk, ETH_val\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 31.90it/s, loss=0.334, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 32.17it/s, loss=0.334, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 32.67it/s, loss=0.326, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 32.89it/s, loss=0.326, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 32.37it/s, loss=0.329, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.047 >= min_delta = 0.003. New best score: 0.326\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 32.58it/s, loss=0.329, v_num=kysk, ETH_val\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 31.50it/s, loss=0.314, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 31.53it/s, loss=0.314, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 32.25it/s, loss=0.326, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 32.13it/s, loss=0.326, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 29.01it/s, loss=0.333, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 28.72it/s, loss=0.333, v_num=kysk, ETH_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38:  95%|▉| 20/21 [00:00<00:00, 31.55it/s, loss=0.291, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 31.48it/s, loss=0.291, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 39:  95%|▉| 20/21 [00:00<00:00, 31.88it/s, loss=0.332, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 32.09it/s, loss=0.332, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 32.42it/s, loss=0.301, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 32.70it/s, loss=0.301, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 41:  95%|▉| 20/21 [00:00<00:00, 32.54it/s, loss=0.278, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 32.70it/s, loss=0.278, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 42:  95%|▉| 20/21 [00:00<00:00, 31.11it/s, loss=0.297, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 31.48it/s, loss=0.297, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 43:  95%|▉| 20/21 [00:00<00:00, 31.50it/s, loss=0.276, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 31.42it/s, loss=0.276, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 44:  95%|▉| 20/21 [00:00<00:00, 31.57it/s, loss=0.267, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 31.79it/s, loss=0.267, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 45:  95%|▉| 20/21 [00:00<00:00, 32.21it/s, loss=0.263, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 32.54it/s, loss=0.263, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 46:  95%|▉| 20/21 [00:00<00:00, 31.61it/s, loss=0.283, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 31.88it/s, loss=0.283, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 47:  95%|▉| 20/21 [00:00<00:00, 33.01it/s, loss=0.251, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 32.83it/s, loss=0.251, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 48:  95%|▉| 20/21 [00:00<00:00, 33.74it/s, loss=0.236, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 33.88it/s, loss=0.236, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 49:  95%|▉| 20/21 [00:00<00:00, 34.47it/s, loss=0.249, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 21/21 [00:00<00:00, 33.86it/s, loss=0.249, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 50:  95%|▉| 20/21 [00:00<00:00, 30.84it/s, loss=0.255, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 21/21 [00:00<00:00, 30.88it/s, loss=0.255, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 51:  95%|▉| 20/21 [00:00<00:00, 32.46it/s, loss=0.265, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 21/21 [00:00<00:00, 32.30it/s, loss=0.265, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 52:  95%|▉| 20/21 [00:00<00:00, 31.61it/s, loss=0.259, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 21/21 [00:00<00:00, 31.74it/s, loss=0.259, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 53:  95%|▉| 20/21 [00:00<00:00, 30.62it/s, loss=0.261, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 21/21 [00:00<00:00, 30.56it/s, loss=0.261, v_num=kysk, ETH_val\u001b[A\n",
      "Epoch 54:  95%|▉| 20/21 [00:00<00:00, 32.85it/s, loss=0.232, v_num=kysk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.326. Signaling Trainer to stop.\n",
      "Epoch 54: 100%|█| 21/21 [00:00<00:00, 32.94it/s, loss=0.232, v_num=kysk, ETH_val\n",
      "Epoch 54: 100%|█| 21/21 [00:00<00:00, 32.55it/s, loss=0.232, v_num=kysk, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 78.16it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.6666666865348816,\n",
      " 'ETH_test_f1': 0.6552706956863403,\n",
      " 'test_loss': 0.6191502809524536}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 92163\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094707-94b6kysk/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094707-94b6kysk/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.89419\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.89114\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.23154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 54\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 45\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625899672\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 132\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.61538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.57516\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.57458\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.92683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.92683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.2297\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.65527\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.61915\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▂▂▃▃▄▅▆▆▆▆▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▂▃▄▄▅▆▆▆▆▇▆▇▆▇▇▇▇▇▇▇▇▇█▇▇▇▇▇█▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ███▇▇▆▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▂▂▂▁▁▂▃▆▃▂▃▆▆▇▇▇▃▆▃▅▅▇▆▆▇█▅▅▆▃█▃▃▆▅▆▇▆▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▁▁▁▁▄▆▃▃▃▆▆▇▇▇▄▆▄▅▅▇▆▆▇█▅▅▆▄█▄▄▆▅▆▇▆▆▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ██████▇▅▆▅▅▆▃▅▄▆▄▃▄▃▄▂▅▂▃▁▃▃▄▄▂▄▃▂▄▂▃▁▃▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▃▃▅▆▅▇▆▅██▇▇▅▆▅▇▇▇▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▃▃▅▆▅▇▆▅██▇▇▅▆▅▇▇▇▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▇▇▄▄▃▃▂▄▁▁▃▃▇▃▆▂▂▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/94b6kysk\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:48:23.024766: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/2mcjyguv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094821-2mcjyguv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 219 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 387   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.178     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 34.03it/s, loss=1.13, v_num=yguv, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.143\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 34.10it/s, loss=1.13, v_num=yguv, ETH_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 34.51it/s, loss=1.1, v_num=yguv, ETH_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 34.72it/s, loss=1.1, v_num=yguv, ETH_val_ac\u001b[A\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 32.77it/s, loss=1.09, v_num=yguv, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 33.00it/s, loss=1.09, v_num=yguv, ETH_val_a\u001b[A\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 31.54it/s, loss=1.08, v_num=yguv, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 31.77it/s, loss=1.08, v_num=yguv, ETH_val_a\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 33.01it/s, loss=1.08, v_num=yguv, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 33.08it/s, loss=1.08, v_num=yguv, ETH_val_a\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 32.80it/s, loss=1.06, v_num=yguv, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 32.85it/s, loss=1.06, v_num=yguv, ETH_val_a\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 32.44it/s, loss=1.03, v_num=yguv, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 32.71it/s, loss=1.03, v_num=yguv, ETH_val_a\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 34.67it/s, loss=0.962, v_num=yguv, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 34.41it/s, loss=0.962, v_num=yguv, ETH_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 32.97it/s, loss=0.899, v_num=yguv, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 33.03it/s, loss=0.899, v_num=yguv, ETH_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 32.49it/s, loss=0.873, v_num=yguv, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.077 >= min_delta = 0.003. New best score: 1.065\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 32.61it/s, loss=0.873, v_num=yguv, ETH_val_\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 32.33it/s, loss=0.84, v_num=yguv, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 32.40it/s, loss=0.84, v_num=yguv, ETH_val_\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 32.78it/s, loss=0.8, v_num=yguv, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.046 >= min_delta = 0.003. New best score: 1.019\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 32.73it/s, loss=0.8, v_num=yguv, ETH_val_a\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 32.57it/s, loss=0.8, v_num=yguv, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 32.40it/s, loss=0.8, v_num=yguv, ETH_val_a\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 31.81it/s, loss=0.795, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 31.93it/s, loss=0.795, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 33.44it/s, loss=0.769, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 33.54it/s, loss=0.769, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 33.17it/s, loss=0.775, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.103 >= min_delta = 0.003. New best score: 0.917\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 33.46it/s, loss=0.775, v_num=yguv, ETH_val\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 34.63it/s, loss=0.733, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 34.82it/s, loss=0.733, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 32.39it/s, loss=0.729, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 32.49it/s, loss=0.729, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 29.73it/s, loss=0.705, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 30.02it/s, loss=0.705, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 32.47it/s, loss=0.73, v_num=yguv, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 32.58it/s, loss=0.73, v_num=yguv, ETH_val_\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 33.91it/s, loss=0.726, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 33.71it/s, loss=0.726, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 30.03it/s, loss=0.727, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 30.20it/s, loss=0.727, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 31.71it/s, loss=0.682, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 31.81it/s, loss=0.682, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 34.94it/s, loss=0.691, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 34.57it/s, loss=0.691, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 34.89it/s, loss=0.69, v_num=yguv, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 34.18it/s, loss=0.69, v_num=yguv, ETH_val_\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 32.34it/s, loss=0.656, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.096 >= min_delta = 0.003. New best score: 0.821\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 32.01it/s, loss=0.656, v_num=yguv, ETH_val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 31.06it/s, loss=0.624, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 31.40it/s, loss=0.624, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 33.46it/s, loss=0.631, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 33.76it/s, loss=0.631, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 33.45it/s, loss=0.651, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 32.87it/s, loss=0.651, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 32.08it/s, loss=0.612, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 32.08it/s, loss=0.612, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 33.11it/s, loss=0.623, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 32.98it/s, loss=0.623, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 32.52it/s, loss=0.619, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 32.75it/s, loss=0.619, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 33.20it/s, loss=0.563, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 33.24it/s, loss=0.563, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 33.36it/s, loss=0.554, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 33.36it/s, loss=0.554, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 33.90it/s, loss=0.585, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 33.43it/s, loss=0.585, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 32.57it/s, loss=0.548, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 32.30it/s, loss=0.548, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 33.29it/s, loss=0.542, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 33.57it/s, loss=0.542, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 31.67it/s, loss=0.546, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 31.86it/s, loss=0.546, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 38:  95%|▉| 20/21 [00:00<00:00, 32.85it/s, loss=0.501, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.065 >= min_delta = 0.003. New best score: 0.756\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 33.13it/s, loss=0.501, v_num=yguv, ETH_val\n",
      "Epoch 39:  95%|▉| 20/21 [00:00<00:00, 32.70it/s, loss=0.514, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 32.31it/s, loss=0.514, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 33.17it/s, loss=0.551, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 32.99it/s, loss=0.551, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 41:  95%|▉| 20/21 [00:00<00:00, 32.66it/s, loss=0.477, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 32.92it/s, loss=0.477, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 42:  95%|▉| 20/21 [00:00<00:00, 32.48it/s, loss=0.503, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 32.94it/s, loss=0.503, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 43:  95%|▉| 20/21 [00:00<00:00, 32.86it/s, loss=0.524, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 33.07it/s, loss=0.524, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 44:  95%|▉| 20/21 [00:00<00:00, 34.15it/s, loss=0.485, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 34.12it/s, loss=0.485, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 45:  95%|▉| 20/21 [00:00<00:00, 32.20it/s, loss=0.456, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 32.59it/s, loss=0.456, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 46:  95%|▉| 20/21 [00:00<00:00, 31.98it/s, loss=0.468, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 31.70it/s, loss=0.468, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 47:  95%|▉| 20/21 [00:00<00:00, 35.30it/s, loss=0.469, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 35.39it/s, loss=0.469, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 48:  95%|▉| 20/21 [00:00<00:00, 32.51it/s, loss=0.433, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 32.44it/s, loss=0.433, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 49:  95%|▉| 20/21 [00:00<00:00, 33.37it/s, loss=0.441, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 21/21 [00:00<00:00, 33.47it/s, loss=0.441, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 50:  95%|▉| 20/21 [00:00<00:00, 33.42it/s, loss=0.418, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 21/21 [00:00<00:00, 33.22it/s, loss=0.418, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 51:  95%|▉| 20/21 [00:00<00:00, 32.33it/s, loss=0.427, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 21/21 [00:00<00:00, 32.52it/s, loss=0.427, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 52:  95%|▉| 20/21 [00:00<00:00, 32.06it/s, loss=0.4, v_num=yguv, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 21/21 [00:00<00:00, 32.32it/s, loss=0.4, v_num=yguv, ETH_val_a\u001b[A\n",
      "Epoch 53:  95%|▉| 20/21 [00:00<00:00, 34.75it/s, loss=0.408, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 21/21 [00:00<00:00, 34.94it/s, loss=0.408, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 54:  95%|▉| 20/21 [00:00<00:00, 31.98it/s, loss=0.381, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 21/21 [00:00<00:00, 32.26it/s, loss=0.381, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 55:  95%|▉| 20/21 [00:00<00:00, 34.06it/s, loss=0.381, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 21/21 [00:00<00:00, 33.93it/s, loss=0.381, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 56:  95%|▉| 20/21 [00:00<00:00, 32.52it/s, loss=0.392, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 21/21 [00:00<00:00, 32.74it/s, loss=0.392, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 57:  95%|▉| 20/21 [00:00<00:00, 33.26it/s, loss=0.365, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 21/21 [00:00<00:00, 33.51it/s, loss=0.365, v_num=yguv, ETH_val\u001b[A\n",
      "Epoch 58:  95%|▉| 20/21 [00:00<00:00, 33.05it/s, loss=0.376, v_num=yguv, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.756. Signaling Trainer to stop.\n",
      "Epoch 58: 100%|█| 21/21 [00:00<00:00, 33.12it/s, loss=0.376, v_num=yguv, ETH_val\n",
      "Epoch 58: 100%|█| 21/21 [00:00<00:00, 32.99it/s, loss=0.376, v_num=yguv, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 92.52it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.6969696879386902,\n",
      " 'ETH_test_f1': 0.6798046231269836,\n",
      " 'test_loss': 0.8332961797714233}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 92576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094821-2mcjyguv/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094821-2mcjyguv/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.82816\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.82438\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.37578\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 58\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1180\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 46\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625899747\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 141\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.46154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.40909\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.25147\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.84375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.84402\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.35419\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.69697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.6798\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.8333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▂▂▂▃▄▅▅▅▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▂▃▃▄▅▅▅▆▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ████▇▆▆▅▅▅▅▄▄▄▄▄▄▄▃▄▃▃▃▃▃▃▂▃▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▂▄▂▂▁▂▂▂▄▆▆▃▇▆▆▃▆▇█▆▇█▆▇▇▇█▆▇█▇██▇█▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▂▁▁▁▁▁▁▅▅▅▃▆▅▅▃▅▇█▅▆▇▅▆▆▇█▅▆▇▇▇▇▆▇▆▆▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄▄▄▄▄▄▄▅▃▄▅▅▂▃▄█▃▁▂▄▂▁▂▄▂▂▁▂▂▁▃▂▁▃▃▃▂▃▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▂▂▃▅▅▆▄▇▅▅▅█▆▇▆▆▆▆▆▇▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▂▂▃▅▅▆▄▇▄▅▅█▇▇▆▆▆▇▆█▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ██▇▆▅▆▄▅▃▃▅▆▁▂▂▃▄▂▃▄▁▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/2mcjyguv\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:49:37.238231: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/1j8peumk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094936-1j8peumk\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 134 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 258   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "959 K     Trainable params\n",
      "0         Non-trainable params\n",
      "959 K     Total params\n",
      "3.837     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 37.75it/s, loss=0.685, v_num=eumk, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.687\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 37.71it/s, loss=0.685, v_num=eumk, ETH_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 35.70it/s, loss=0.606, v_num=eumk, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.212 >= min_delta = 0.003. New best score: 0.475\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 35.88it/s, loss=0.606, v_num=eumk, ETH_val_\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 32.31it/s, loss=0.562, v_num=eumk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.131 >= min_delta = 0.003. New best score: 0.344\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 32.34it/s, loss=0.562, v_num=eumk, ETH_val_\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 31.10it/s, loss=0.531, v_num=eumk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 31.38it/s, loss=0.531, v_num=eumk, ETH_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 34.36it/s, loss=0.531, v_num=eumk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.061 >= min_delta = 0.003. New best score: 0.283\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 34.47it/s, loss=0.531, v_num=eumk, ETH_val_\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 32.30it/s, loss=0.511, v_num=eumk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 32.64it/s, loss=0.511, v_num=eumk, ETH_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 34.32it/s, loss=0.515, v_num=eumk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.279\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 34.29it/s, loss=0.515, v_num=eumk, ETH_val_\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 34.33it/s, loss=0.514, v_num=eumk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 34.59it/s, loss=0.514, v_num=eumk, ETH_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 32.48it/s, loss=0.506, v_num=eumk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 32.55it/s, loss=0.506, v_num=eumk, ETH_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 36.65it/s, loss=0.486, v_num=eumk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.042 >= min_delta = 0.003. New best score: 0.237\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 36.66it/s, loss=0.486, v_num=eumk, ETH_val_\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 34.85it/s, loss=0.48, v_num=eumk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 34.02it/s, loss=0.48, v_num=eumk, ETH_val_\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 34.36it/s, loss=0.482, v_num=eumk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 34.62it/s, loss=0.482, v_num=eumk, ETH_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 33.61it/s, loss=0.486, v_num=eumk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 33.76it/s, loss=0.486, v_num=eumk, ETH_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 33.53it/s, loss=0.474, v_num=eumk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 33.47it/s, loss=0.474, v_num=eumk, ETH_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 33.12it/s, loss=0.483, v_num=eumk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 33.25it/s, loss=0.483, v_num=eumk, ETH_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 34.07it/s, loss=0.472, v_num=eumk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 34.34it/s, loss=0.472, v_num=eumk, ETH_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 34.31it/s, loss=0.471, v_num=eumk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 34.57it/s, loss=0.471, v_num=eumk, ETH_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 33.51it/s, loss=0.464, v_num=eumk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 33.48it/s, loss=0.464, v_num=eumk, ETH_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 34.55it/s, loss=0.479, v_num=eumk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 34.34it/s, loss=0.479, v_num=eumk, ETH_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 32.69it/s, loss=0.492, v_num=eumk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 32.83it/s, loss=0.492, v_num=eumk, ETH_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 34.42it/s, loss=0.469, v_num=eumk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 34.70it/s, loss=0.469, v_num=eumk, ETH_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 29.94it/s, loss=0.466, v_num=eumk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 30.27it/s, loss=0.466, v_num=eumk, ETH_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 34.07it/s, loss=0.471, v_num=eumk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 34.08it/s, loss=0.471, v_num=eumk, ETH_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 35.09it/s, loss=0.462, v_num=eumk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 34.57it/s, loss=0.462, v_num=eumk, ETH_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 33.62it/s, loss=0.459, v_num=eumk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 33.85it/s, loss=0.459, v_num=eumk, ETH_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 33.45it/s, loss=0.453, v_num=eumk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 33.56it/s, loss=0.453, v_num=eumk, ETH_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 33.16it/s, loss=0.47, v_num=eumk, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 33.45it/s, loss=0.47, v_num=eumk, ETH_val_\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 33.32it/s, loss=0.454, v_num=eumk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 32.72it/s, loss=0.454, v_num=eumk, ETH_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 33.98it/s, loss=0.463, v_num=eumk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 34.14it/s, loss=0.463, v_num=eumk, ETH_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 32.86it/s, loss=0.457, v_num=eumk, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.237. Signaling Trainer to stop.\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 32.53it/s, loss=0.457, v_num=eumk, ETH_val\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 32.34it/s, loss=0.457, v_num=eumk, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 83.38it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.6969696879386902,\n",
      " 'ETH_test_f1': 0.6898496150970459,\n",
      " 'test_loss': 0.52360600233078}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 92911\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094936-1j8peumk/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_094936-1j8peumk/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.78282\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.77846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.45705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 600\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625899802\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.92308\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.92308\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.31567\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.82927\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.82553\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.45026\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.69697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.68985\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.52361\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▄▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██▇█▇████▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇████▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▂▁▁▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▆▇▇███████████▇███▇████▇██▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▆▇▇███████████▇███▇████▇██▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▅▃▃▂▂▂▂▂▁▂▂▁▂▁▁▁▃▂▂▂▁▂▂▂▂▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▃▂▆▂▇▇▄▅▁█▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▃▂▆▂▇▇▄▅▁█▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▃▆▄▄▂▄▄▄█▁▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/1j8peumk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:50:30.435393: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/38vq0koy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095029-38vq0koy\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 134 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 387   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "959 K     Trainable params\n",
      "0         Non-trainable params\n",
      "959 K     Total params\n",
      "3.838     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 35.58it/s, loss=1.1, v_num=0koy, ETH_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.072\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 35.65it/s, loss=1.1, v_num=0koy, ETH_val_ac\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 34.47it/s, loss=0.995, v_num=0koy, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.152 >= min_delta = 0.003. New best score: 0.919\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 34.62it/s, loss=0.995, v_num=0koy, ETH_val_\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 32.68it/s, loss=0.942, v_num=0koy, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 32.97it/s, loss=0.942, v_num=0koy, ETH_val_\u001b[A\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 34.72it/s, loss=0.924, v_num=0koy, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 35.03it/s, loss=0.924, v_num=0koy, ETH_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 34.07it/s, loss=0.886, v_num=0koy, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.075 >= min_delta = 0.003. New best score: 0.844\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 34.18it/s, loss=0.886, v_num=0koy, ETH_val_\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 33.45it/s, loss=0.863, v_num=0koy, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.826\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 33.43it/s, loss=0.863, v_num=0koy, ETH_val_\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 34.22it/s, loss=0.845, v_num=0koy, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 34.07it/s, loss=0.845, v_num=0koy, ETH_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 34.79it/s, loss=0.822, v_num=0koy, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.112 >= min_delta = 0.003. New best score: 0.714\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 34.82it/s, loss=0.822, v_num=0koy, ETH_val_\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 34.78it/s, loss=0.815, v_num=0koy, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 34.82it/s, loss=0.815, v_num=0koy, ETH_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 34.95it/s, loss=0.851, v_num=0koy, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 34.70it/s, loss=0.851, v_num=0koy, ETH_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 34.58it/s, loss=0.827, v_num=0koy, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 34.84it/s, loss=0.827, v_num=0koy, ETH_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 34.75it/s, loss=0.805, v_num=0koy, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 34.96it/s, loss=0.805, v_num=0koy, ETH_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 33.78it/s, loss=0.807, v_num=0koy, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 34.09it/s, loss=0.807, v_num=0koy, ETH_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 35.10it/s, loss=0.795, v_num=0koy, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 35.30it/s, loss=0.795, v_num=0koy, ETH_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 34.79it/s, loss=0.798, v_num=0koy, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 34.91it/s, loss=0.798, v_num=0koy, ETH_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 35.95it/s, loss=0.794, v_num=0koy, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 35.68it/s, loss=0.794, v_num=0koy, ETH_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 32.58it/s, loss=0.793, v_num=0koy, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 32.85it/s, loss=0.793, v_num=0koy, ETH_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 35.72it/s, loss=0.772, v_num=0koy, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 35.78it/s, loss=0.772, v_num=0koy, ETH_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 35.16it/s, loss=0.778, v_num=0koy, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 35.23it/s, loss=0.778, v_num=0koy, ETH_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 34.16it/s, loss=0.799, v_num=0koy, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 33.73it/s, loss=0.799, v_num=0koy, ETH_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 34.25it/s, loss=0.777, v_num=0koy, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 34.37it/s, loss=0.777, v_num=0koy, ETH_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 27.92it/s, loss=0.78, v_num=0koy, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 28.34it/s, loss=0.78, v_num=0koy, ETH_val_\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 32.84it/s, loss=0.777, v_num=0koy, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 33.09it/s, loss=0.777, v_num=0koy, ETH_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 33.15it/s, loss=0.777, v_num=0koy, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 33.53it/s, loss=0.777, v_num=0koy, ETH_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 31.67it/s, loss=0.755, v_num=0koy, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 31.95it/s, loss=0.755, v_num=0koy, ETH_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 33.99it/s, loss=0.776, v_num=0koy, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 34.26it/s, loss=0.776, v_num=0koy, ETH_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 32.17it/s, loss=0.779, v_num=0koy, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 32.13it/s, loss=0.779, v_num=0koy, ETH_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 32.74it/s, loss=0.756, v_num=0koy, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.714. Signaling Trainer to stop.\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 32.90it/s, loss=0.756, v_num=0koy, ETH_val\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 32.78it/s, loss=0.756, v_num=0koy, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 77.14it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.7272727489471436,\n",
      " 'ETH_test_f1': 0.7244019508361816,\n",
      " 'test_loss': 0.7022320032119751}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 93218\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095029-38vq0koy/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095029-38vq0koy/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.65473\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.64728\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.75655\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 27\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 560\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625899854\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 67\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.53846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.52778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.7715\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.65625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.65866\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.6373\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.72727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.7244\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.70223\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▄▅▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇█▇██████▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▄▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██▇███▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▅▄▄▃▃▂▂▃▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▅▁▃▃▅▃▆▆▅▅▆▅▅▆▆▆▅▃▅▅▆▆█▅▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▃▄▅▆▄▇▇▅▅▇▆▆▇▇▇▆▄▅▆▇▇█▅▆▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▅▆▅▄▃▄▁▃▃▂▂▂▃▃▂▂▃▂▂▃▂▂▃▂▃▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▄▇▅▇▃▇▄▅▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▂▇▆▇▂▇▃▃▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇█▃▇▃▇▅▇█▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/38vq0koy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:51:23.330597: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/1saf66jj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095121-1saf66jj\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 134 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 258   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "959 K     Trainable params\n",
      "0         Non-trainable params\n",
      "959 K     Total params\n",
      "3.837     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 32.85it/s, loss=0.678, v_num=66jj, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.685\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 33.16it/s, loss=0.678, v_num=66jj, ETH_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 34.54it/s, loss=0.613, v_num=66jj, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.218 >= min_delta = 0.003. New best score: 0.468\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 34.68it/s, loss=0.613, v_num=66jj, ETH_val_\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 33.04it/s, loss=0.551, v_num=66jj, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.151 >= min_delta = 0.003. New best score: 0.316\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 33.22it/s, loss=0.551, v_num=66jj, ETH_val_\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 34.00it/s, loss=0.549, v_num=66jj, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 34.06it/s, loss=0.549, v_num=66jj, ETH_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 34.16it/s, loss=0.528, v_num=66jj, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.025 >= min_delta = 0.003. New best score: 0.291\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 33.97it/s, loss=0.528, v_num=66jj, ETH_val_\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 33.32it/s, loss=0.512, v_num=66jj, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 33.63it/s, loss=0.512, v_num=66jj, ETH_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 33.09it/s, loss=0.501, v_num=66jj, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.016 >= min_delta = 0.003. New best score: 0.275\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 33.36it/s, loss=0.501, v_num=66jj, ETH_val_\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 32.99it/s, loss=0.511, v_num=66jj, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 33.05it/s, loss=0.511, v_num=66jj, ETH_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 32.39it/s, loss=0.501, v_num=66jj, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 32.74it/s, loss=0.501, v_num=66jj, ETH_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 33.77it/s, loss=0.484, v_num=66jj, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 33.84it/s, loss=0.484, v_num=66jj, ETH_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 33.17it/s, loss=0.521, v_num=66jj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 33.11it/s, loss=0.521, v_num=66jj, ETH_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 34.09it/s, loss=0.505, v_num=66jj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 34.03it/s, loss=0.505, v_num=66jj, ETH_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 32.98it/s, loss=0.488, v_num=66jj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 33.26it/s, loss=0.488, v_num=66jj, ETH_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 33.18it/s, loss=0.485, v_num=66jj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 33.32it/s, loss=0.485, v_num=66jj, ETH_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 32.44it/s, loss=0.48, v_num=66jj, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 32.70it/s, loss=0.48, v_num=66jj, ETH_val_\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 35.17it/s, loss=0.467, v_num=66jj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 35.22it/s, loss=0.467, v_num=66jj, ETH_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 33.31it/s, loss=0.478, v_num=66jj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 33.44it/s, loss=0.478, v_num=66jj, ETH_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 31.36it/s, loss=0.462, v_num=66jj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 31.73it/s, loss=0.462, v_num=66jj, ETH_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 34.49it/s, loss=0.479, v_num=66jj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 34.68it/s, loss=0.479, v_num=66jj, ETH_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 32.56it/s, loss=0.48, v_num=66jj, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 32.72it/s, loss=0.48, v_num=66jj, ETH_val_\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 33.44it/s, loss=0.461, v_num=66jj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 33.58it/s, loss=0.461, v_num=66jj, ETH_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 29.32it/s, loss=0.472, v_num=66jj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 29.07it/s, loss=0.472, v_num=66jj, ETH_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 32.46it/s, loss=0.466, v_num=66jj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 32.73it/s, loss=0.466, v_num=66jj, ETH_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 34.12it/s, loss=0.483, v_num=66jj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 34.32it/s, loss=0.483, v_num=66jj, ETH_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 33.76it/s, loss=0.507, v_num=66jj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 33.90it/s, loss=0.507, v_num=66jj, ETH_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 31.47it/s, loss=0.47, v_num=66jj, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 31.42it/s, loss=0.47, v_num=66jj, ETH_val_\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 33.20it/s, loss=0.455, v_num=66jj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.275. Signaling Trainer to stop.\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 33.39it/s, loss=0.455, v_num=66jj, ETH_val\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 33.25it/s, loss=0.455, v_num=66jj, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 95.59it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.6969696879386902,\n",
      " 'ETH_test_f1': 0.682692289352417,\n",
      " 'test_loss': 0.4818102717399597}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 93426\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095121-1saf66jj/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095121-1saf66jj/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.7685\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.76359\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.45562\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 540\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625899907\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.92308\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.92121\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.31727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.73171\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.72584\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.53744\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.69697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.68269\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.48181\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▄▅▆▆▇▆▇▇▇▇▇█▇▇█▇█▇█████▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▄▅▆▆▇▆▇▇▇▇▇█▇▇█▇█▇▇████▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▄▄▃▃▂▃▂▂▃▃▂▂▂▁▂▁▂▂▁▁▁▂▃▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▅▇▆▇▇▇█▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▅▇▆▇▇▇█▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▄▂▃▁▂▁▂▂▁▃▂▂▂▂▂▂▂▂▂▁▂▂▂▄▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▆▃▆▆▄█▄▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▆▃▆▆▄█▄▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▂▃▄▂▅▁▂▃▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/1saf66jj\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:52:14.756124: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/3863dfqi\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095213-3863dfqi\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 134 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 387   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "959 K     Trainable params\n",
      "0         Non-trainable params\n",
      "959 K     Total params\n",
      "3.838     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 34.69it/s, loss=1.1, v_num=dfqi, ETH_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.090\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 34.66it/s, loss=1.1, v_num=dfqi, ETH_val_ac\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 34.83it/s, loss=1.01, v_num=dfqi, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.148 >= min_delta = 0.003. New best score: 0.942\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 34.96it/s, loss=1.01, v_num=dfqi, ETH_val_a\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 33.83it/s, loss=0.921, v_num=dfqi, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.069 >= min_delta = 0.003. New best score: 0.874\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 33.98it/s, loss=0.921, v_num=dfqi, ETH_val_\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 33.80it/s, loss=0.912, v_num=dfqi, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.044 >= min_delta = 0.003. New best score: 0.830\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 34.04it/s, loss=0.912, v_num=dfqi, ETH_val_\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 34.02it/s, loss=0.897, v_num=dfqi, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.044 >= min_delta = 0.003. New best score: 0.786\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 33.93it/s, loss=0.897, v_num=dfqi, ETH_val_\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 33.25it/s, loss=0.898, v_num=dfqi, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 33.05it/s, loss=0.898, v_num=dfqi, ETH_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 34.22it/s, loss=0.858, v_num=dfqi, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 34.47it/s, loss=0.858, v_num=dfqi, ETH_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 33.34it/s, loss=0.873, v_num=dfqi, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 33.59it/s, loss=0.873, v_num=dfqi, ETH_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 33.39it/s, loss=0.871, v_num=dfqi, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 33.78it/s, loss=0.871, v_num=dfqi, ETH_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 32.66it/s, loss=0.849, v_num=dfqi, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 32.86it/s, loss=0.849, v_num=dfqi, ETH_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 31.49it/s, loss=0.832, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.779\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 31.75it/s, loss=0.832, v_num=dfqi, ETH_val\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 33.14it/s, loss=0.842, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 33.52it/s, loss=0.842, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 32.76it/s, loss=0.817, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 32.95it/s, loss=0.817, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 31.76it/s, loss=0.803, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 31.86it/s, loss=0.803, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 33.72it/s, loss=0.817, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 34.02it/s, loss=0.817, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 33.81it/s, loss=0.784, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 33.96it/s, loss=0.784, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 32.86it/s, loss=0.782, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 32.75it/s, loss=0.782, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 31.94it/s, loss=0.779, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.028 >= min_delta = 0.003. New best score: 0.750\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 32.26it/s, loss=0.779, v_num=dfqi, ETH_val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 34.99it/s, loss=0.768, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 35.26it/s, loss=0.768, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 33.61it/s, loss=0.823, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 33.77it/s, loss=0.823, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 32.86it/s, loss=0.784, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 33.03it/s, loss=0.784, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 30.29it/s, loss=0.766, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.020 >= min_delta = 0.003. New best score: 0.730\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 30.49it/s, loss=0.766, v_num=dfqi, ETH_val\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 33.40it/s, loss=0.788, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 33.37it/s, loss=0.788, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 33.13it/s, loss=0.769, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 32.99it/s, loss=0.769, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 32.75it/s, loss=0.775, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 33.06it/s, loss=0.775, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 31.94it/s, loss=0.756, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 32.21it/s, loss=0.756, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 32.71it/s, loss=0.762, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 32.94it/s, loss=0.762, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 32.12it/s, loss=0.752, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 32.42it/s, loss=0.752, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 32.47it/s, loss=0.733, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 32.45it/s, loss=0.733, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 32.93it/s, loss=0.742, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 33.17it/s, loss=0.742, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 33.23it/s, loss=0.771, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 33.54it/s, loss=0.771, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 32.14it/s, loss=0.757, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 32.17it/s, loss=0.757, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 35.25it/s, loss=0.768, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 34.50it/s, loss=0.768, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 32.59it/s, loss=0.742, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 32.48it/s, loss=0.742, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 33.46it/s, loss=0.72, v_num=dfqi, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 33.31it/s, loss=0.72, v_num=dfqi, ETH_val_\u001b[A\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 33.16it/s, loss=0.718, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 33.50it/s, loss=0.718, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 31.74it/s, loss=0.735, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 31.56it/s, loss=0.735, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 30.98it/s, loss=0.743, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 31.35it/s, loss=0.743, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 38:  95%|▉| 20/21 [00:00<00:00, 33.53it/s, loss=0.726, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.038 >= min_delta = 0.003. New best score: 0.692\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 33.80it/s, loss=0.726, v_num=dfqi, ETH_val\n",
      "Epoch 39:  95%|▉| 20/21 [00:00<00:00, 34.40it/s, loss=0.717, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 34.37it/s, loss=0.717, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 32.77it/s, loss=0.706, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 33.07it/s, loss=0.706, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 41:  95%|▉| 20/21 [00:00<00:00, 32.63it/s, loss=0.69, v_num=dfqi, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 32.96it/s, loss=0.69, v_num=dfqi, ETH_val_\u001b[A\n",
      "Epoch 42:  95%|▉| 20/21 [00:00<00:00, 33.58it/s, loss=0.706, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 33.61it/s, loss=0.706, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 43:  95%|▉| 20/21 [00:00<00:00, 34.44it/s, loss=0.707, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 34.76it/s, loss=0.707, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 44:  95%|▉| 20/21 [00:00<00:00, 33.61it/s, loss=0.708, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 32.99it/s, loss=0.708, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 45:  95%|▉| 20/21 [00:00<00:00, 33.36it/s, loss=0.712, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 33.64it/s, loss=0.712, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 46:  95%|▉| 20/21 [00:00<00:00, 33.39it/s, loss=0.704, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 33.65it/s, loss=0.704, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 47:  95%|▉| 20/21 [00:00<00:00, 32.07it/s, loss=0.691, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 32.16it/s, loss=0.691, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 48:  95%|▉| 20/21 [00:00<00:00, 32.12it/s, loss=0.725, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 32.44it/s, loss=0.725, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 49:  95%|▉| 20/21 [00:00<00:00, 32.33it/s, loss=0.702, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 21/21 [00:00<00:00, 32.17it/s, loss=0.702, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 50:  95%|▉| 20/21 [00:00<00:00, 33.80it/s, loss=0.703, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 21/21 [00:00<00:00, 33.73it/s, loss=0.703, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 51:  95%|▉| 20/21 [00:00<00:00, 32.88it/s, loss=0.676, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 21/21 [00:00<00:00, 32.94it/s, loss=0.676, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 52:  95%|▉| 20/21 [00:00<00:00, 32.35it/s, loss=0.677, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 21/21 [00:00<00:00, 32.40it/s, loss=0.677, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 53:  95%|▉| 20/21 [00:00<00:00, 33.22it/s, loss=0.681, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 21/21 [00:00<00:00, 33.38it/s, loss=0.681, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 54:  95%|▉| 20/21 [00:00<00:00, 33.39it/s, loss=0.675, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 21/21 [00:00<00:00, 33.45it/s, loss=0.675, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 55:  95%|▉| 20/21 [00:00<00:00, 32.71it/s, loss=0.665, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 21/21 [00:00<00:00, 32.78it/s, loss=0.665, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 56:  95%|▉| 20/21 [00:00<00:00, 32.66it/s, loss=0.676, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 21/21 [00:00<00:00, 32.81it/s, loss=0.676, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 57:  95%|▉| 20/21 [00:00<00:00, 32.64it/s, loss=0.655, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 21/21 [00:00<00:00, 33.03it/s, loss=0.655, v_num=dfqi, ETH_val\u001b[A\n",
      "Epoch 58:  95%|▉| 20/21 [00:00<00:00, 32.43it/s, loss=0.655, v_num=dfqi, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.692. Signaling Trainer to stop.\n",
      "Epoch 58: 100%|█| 21/21 [00:00<00:00, 32.76it/s, loss=0.655, v_num=dfqi, ETH_val\n",
      "Epoch 58: 100%|█| 21/21 [00:00<00:00, 32.63it/s, loss=0.655, v_num=dfqi, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 72.61it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.6363636255264282,\n",
      " 'ETH_test_f1': 0.5802867412567139,\n",
      " 'test_loss': 0.6839034557342529}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 93618\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095213-3863dfqi/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095213-3863dfqi/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.69212\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.67687\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.65754\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 58\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1180\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 46\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625899979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 141\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.46154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.4127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.82124\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.75599\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.61421\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.63636\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.58029\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.6839\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▄▅▆▅▆▆▆▆▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇█▇▇█▇▇████▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▄▅▆▅▆▆▇▆▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▅▅▅▄▄▄▄▃▃▃▃▄▃▃▃▃▃▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▃▃▆▅▅▃█▃▅▃▃▅▃▅▆▅▅▅▅▅▅▅▅▅▅▆▆▃▅▅▅▆▃▅▅▃▃▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▂▂▆▄▄▂█▂▄▂▂▄▂▄▆▄▄▄▄▄▄▄▄▄▄▆▆▁▄▄▄▆▂▄▄▁▂▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▅▄▃▄▄▄▃▃▃▃▃▂▃▄▄▄▂▃▂▂▂▂▂▂▂▁▂▃▄▃▂▂▅▂▃▃▅▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▅▂▄▄▅▂▂▅▂▆▁▄▆▅█▆▆▅▆█▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▄▁▄▄▄▂▂▄▂▆▁▃▇▅█▆▅▆▆█▃▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▇█▇▆█▆▆▅▆▄▅▄▃▃▂▂▅▂▅▁▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/3863dfqi\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:53:27.848798: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/38h9lg3e\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095326-38h9lg3e\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 219 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 258   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.177     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:00<00:00, 33.73it/s, loss=0.7, v_num=lg3e, LTC_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.701\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 33.73it/s, loss=0.7, v_num=lg3e, LTC_val_ac\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 34.79it/s, loss=0.696, v_num=lg3e, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.104 >= min_delta = 0.003. New best score: 0.598\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 34.80it/s, loss=0.696, v_num=lg3e, LTC_val_\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 32.42it/s, loss=0.689, v_num=lg3e, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 19/19 [00:00<00:00, 32.68it/s, loss=0.689, v_num=lg3e, LTC_val_\u001b[A\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 32.31it/s, loss=0.687, v_num=lg3e, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.027 >= min_delta = 0.003. New best score: 0.571\n",
      "Epoch 3: 100%|█| 19/19 [00:00<00:00, 32.21it/s, loss=0.687, v_num=lg3e, LTC_val_\n",
      "Epoch 4:  95%|▉| 18/19 [00:00<00:00, 31.71it/s, loss=0.672, v_num=lg3e, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.029 >= min_delta = 0.003. New best score: 0.542\n",
      "Epoch 4: 100%|█| 19/19 [00:00<00:00, 31.93it/s, loss=0.672, v_num=lg3e, LTC_val_\n",
      "Epoch 5:  95%|▉| 18/19 [00:00<00:00, 30.11it/s, loss=0.632, v_num=lg3e, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 19/19 [00:00<00:00, 30.33it/s, loss=0.632, v_num=lg3e, LTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 29.71it/s, loss=0.602, v_num=lg3e, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.078 >= min_delta = 0.003. New best score: 0.464\n",
      "Epoch 6: 100%|█| 19/19 [00:00<00:00, 30.14it/s, loss=0.602, v_num=lg3e, LTC_val_\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 31.17it/s, loss=0.576, v_num=lg3e, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 19/19 [00:00<00:00, 31.24it/s, loss=0.576, v_num=lg3e, LTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 31.97it/s, loss=0.53, v_num=lg3e, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.132 >= min_delta = 0.003. New best score: 0.332\n",
      "Epoch 8: 100%|█| 19/19 [00:00<00:00, 32.16it/s, loss=0.53, v_num=lg3e, LTC_val_a\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 31.15it/s, loss=0.505, v_num=lg3e, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:00<00:00, 31.29it/s, loss=0.505, v_num=lg3e, LTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 32.17it/s, loss=0.5, v_num=lg3e, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:00<00:00, 32.45it/s, loss=0.5, v_num=lg3e, LTC_val_a\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 32.03it/s, loss=0.489, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.127 >= min_delta = 0.003. New best score: 0.205\n",
      "Epoch 11: 100%|█| 19/19 [00:00<00:00, 32.03it/s, loss=0.489, v_num=lg3e, LTC_val\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 30.82it/s, loss=0.456, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:00<00:00, 30.76it/s, loss=0.456, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 29.76it/s, loss=0.448, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:00<00:00, 30.06it/s, loss=0.448, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 31.60it/s, loss=0.431, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:00<00:00, 31.92it/s, loss=0.431, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 30.47it/s, loss=0.452, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:00<00:00, 30.79it/s, loss=0.452, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 31.13it/s, loss=0.44, v_num=lg3e, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:00<00:00, 31.20it/s, loss=0.44, v_num=lg3e, LTC_val_\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 31.78it/s, loss=0.456, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:00<00:00, 32.10it/s, loss=0.456, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 31.87it/s, loss=0.423, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:00<00:00, 31.87it/s, loss=0.423, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 27.35it/s, loss=0.43, v_num=lg3e, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:00<00:00, 27.46it/s, loss=0.43, v_num=lg3e, LTC_val_\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 31.87it/s, loss=0.408, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:00<00:00, 31.81it/s, loss=0.408, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 32.60it/s, loss=0.418, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:00<00:00, 32.05it/s, loss=0.418, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:00<00:00, 32.07it/s, loss=0.39, v_num=lg3e, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:00<00:00, 32.35it/s, loss=0.39, v_num=lg3e, LTC_val_\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 31.94it/s, loss=0.391, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:00<00:00, 32.32it/s, loss=0.391, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 31.16it/s, loss=0.363, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:00<00:00, 31.22it/s, loss=0.363, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 32.23it/s, loss=0.36, v_num=lg3e, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:00<00:00, 32.11it/s, loss=0.36, v_num=lg3e, LTC_val_\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:00<00:00, 30.90it/s, loss=0.401, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:00<00:00, 31.31it/s, loss=0.401, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 31.30it/s, loss=0.364, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:00<00:00, 31.55it/s, loss=0.364, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 31.75it/s, loss=0.367, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.084 >= min_delta = 0.003. New best score: 0.121\n",
      "Epoch 28: 100%|█| 19/19 [00:00<00:00, 31.76it/s, loss=0.367, v_num=lg3e, LTC_val\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 32.27it/s, loss=0.362, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:00<00:00, 32.08it/s, loss=0.362, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 31.74it/s, loss=0.355, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:00<00:00, 32.18it/s, loss=0.355, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 31.19it/s, loss=0.343, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:00<00:00, 31.47it/s, loss=0.343, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:00<00:00, 30.14it/s, loss=0.36, v_num=lg3e, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:00<00:00, 30.05it/s, loss=0.36, v_num=lg3e, LTC_val_\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33:  95%|▉| 18/19 [00:00<00:00, 31.86it/s, loss=0.33, v_num=lg3e, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:00<00:00, 32.13it/s, loss=0.33, v_num=lg3e, LTC_val_\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:00<00:00, 33.37it/s, loss=0.334, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:00<00:00, 33.51it/s, loss=0.334, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:00<00:00, 32.04it/s, loss=0.332, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:00<00:00, 32.32it/s, loss=0.332, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:00<00:00, 31.58it/s, loss=0.306, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 19/19 [00:00<00:00, 31.87it/s, loss=0.306, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 18/19 [00:00<00:00, 31.18it/s, loss=0.327, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:00<00:00, 31.60it/s, loss=0.327, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:00<00:00, 30.50it/s, loss=0.338, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 19/19 [00:00<00:00, 30.72it/s, loss=0.338, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 18/19 [00:00<00:00, 32.74it/s, loss=0.311, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 19/19 [00:00<00:00, 32.57it/s, loss=0.311, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 18/19 [00:00<00:00, 31.51it/s, loss=0.296, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 19/19 [00:00<00:00, 31.87it/s, loss=0.296, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 18/19 [00:00<00:00, 30.63it/s, loss=0.281, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 19/19 [00:00<00:00, 30.76it/s, loss=0.281, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 18/19 [00:00<00:00, 32.67it/s, loss=0.283, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 19/19 [00:00<00:00, 32.90it/s, loss=0.283, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 18/19 [00:00<00:00, 31.42it/s, loss=0.298, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 19/19 [00:00<00:00, 31.28it/s, loss=0.298, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 44:  95%|▉| 18/19 [00:00<00:00, 32.75it/s, loss=0.275, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 19/19 [00:00<00:00, 32.98it/s, loss=0.275, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 45:  95%|▉| 18/19 [00:00<00:00, 32.27it/s, loss=0.284, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 19/19 [00:00<00:00, 32.55it/s, loss=0.284, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 18/19 [00:00<00:00, 31.80it/s, loss=0.274, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 19/19 [00:00<00:00, 32.12it/s, loss=0.274, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 18/19 [00:00<00:00, 32.32it/s, loss=0.265, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 19/19 [00:00<00:00, 32.72it/s, loss=0.265, v_num=lg3e, LTC_val\u001b[A\n",
      "Epoch 48:  95%|▉| 18/19 [00:00<00:00, 32.71it/s, loss=0.283, v_num=lg3e, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.121. Signaling Trainer to stop.\n",
      "Epoch 48: 100%|█| 19/19 [00:00<00:00, 32.71it/s, loss=0.283, v_num=lg3e, LTC_val\n",
      "Epoch 48: 100%|█| 19/19 [00:00<00:00, 32.57it/s, loss=0.283, v_num=lg3e, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 66.91it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.6000000238418579,\n",
      " 'LTC_test_f1': 0.5693780183792114,\n",
      " 'test_loss': 0.842387855052948}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 93847\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095326-38h9lg3e/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095326-38h9lg3e/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.86736\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.86625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.28551\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 48\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 882\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 38\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625900044\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 115\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.91667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.87368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.24036\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.87389\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.24093\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.56938\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.84239\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▁▂▂▃▄▅▅▆▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▁▂▂▃▄▅▅▆▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ████▇▆▆▅▅▄▄▄▄▄▄▃▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▆▆▆▆█▇█▆▇▆▃█▇▆█▅▅████▆█▅█▇▅███▄██▆▅▇▆█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▃▃▃▃█▆█▅▆▆▃█▆▅█▅▅████▅█▅█▇▅███▄██▆▅▇▅█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▆▆▆▆▅▅▃▅▂▃█▃▃▅▃▅▅▂▂▃▂▄▁▅▂▃▄▂▂▁▅▁▁▃▄▃▄▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▃▇▆▆▆▅▇██▇▆█▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▃▇▆▆▆▅▇██▇▆█▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▇▅▅▄▄▅▄▃▁▂▃▂▃▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/38h9lg3e\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:54:33.839307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/24urf075\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095432-24urf075\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 219 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 387   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.178     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:00<00:00, 33.05it/s, loss=1.12, v_num=f075, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.080\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 33.44it/s, loss=1.12, v_num=f075, LTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 35.03it/s, loss=1.1, v_num=f075, LTC_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 35.56it/s, loss=1.1, v_num=f075, LTC_val_ac\u001b[A\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 33.18it/s, loss=1.09, v_num=f075, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 1.077\n",
      "Epoch 2: 100%|█| 19/19 [00:00<00:00, 33.49it/s, loss=1.09, v_num=f075, LTC_val_a\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 30.82it/s, loss=1.09, v_num=f075, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 19/19 [00:00<00:00, 31.19it/s, loss=1.09, v_num=f075, LTC_val_a\u001b[A\n",
      "Epoch 4:  95%|▉| 18/19 [00:00<00:00, 32.72it/s, loss=1.07, v_num=f075, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 19/19 [00:00<00:00, 32.95it/s, loss=1.07, v_num=f075, LTC_val_a\u001b[A\n",
      "Epoch 5:  95%|▉| 18/19 [00:00<00:00, 32.36it/s, loss=1.04, v_num=f075, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.067 >= min_delta = 0.003. New best score: 1.009\n",
      "Epoch 5: 100%|█| 19/19 [00:00<00:00, 32.50it/s, loss=1.04, v_num=f075, LTC_val_a\n",
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 30.51it/s, loss=0.994, v_num=f075, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 19/19 [00:00<00:00, 30.83it/s, loss=0.994, v_num=f075, LTC_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 31.94it/s, loss=0.924, v_num=f075, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 19/19 [00:00<00:00, 32.27it/s, loss=0.924, v_num=f075, LTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 30.50it/s, loss=0.878, v_num=f075, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 19/19 [00:00<00:00, 30.47it/s, loss=0.878, v_num=f075, LTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 32.07it/s, loss=0.867, v_num=f075, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:00<00:00, 32.33it/s, loss=0.867, v_num=f075, LTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 32.83it/s, loss=0.844, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.105 >= min_delta = 0.003. New best score: 0.905\n",
      "Epoch 10: 100%|█| 19/19 [00:00<00:00, 33.17it/s, loss=0.844, v_num=f075, LTC_val\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 31.82it/s, loss=0.814, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.146 >= min_delta = 0.003. New best score: 0.758\n",
      "Epoch 11: 100%|█| 19/19 [00:00<00:00, 32.06it/s, loss=0.814, v_num=f075, LTC_val\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 34.27it/s, loss=0.784, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:00<00:00, 34.35it/s, loss=0.784, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 31.80it/s, loss=0.756, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:00<00:00, 31.82it/s, loss=0.756, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 32.75it/s, loss=0.739, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:00<00:00, 32.69it/s, loss=0.739, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 32.27it/s, loss=0.754, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:00<00:00, 32.52it/s, loss=0.754, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 32.98it/s, loss=0.742, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:00<00:00, 33.29it/s, loss=0.742, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 32.40it/s, loss=0.751, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.020 >= min_delta = 0.003. New best score: 0.738\n",
      "Epoch 17: 100%|█| 19/19 [00:00<00:00, 32.70it/s, loss=0.751, v_num=f075, LTC_val\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 28.84it/s, loss=0.739, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:00<00:00, 28.97it/s, loss=0.739, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 33.89it/s, loss=0.714, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:00<00:00, 34.17it/s, loss=0.714, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 32.22it/s, loss=0.754, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.070 >= min_delta = 0.003. New best score: 0.668\n",
      "Epoch 20: 100%|█| 19/19 [00:00<00:00, 31.70it/s, loss=0.754, v_num=f075, LTC_val\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 31.65it/s, loss=0.682, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:00<00:00, 32.00it/s, loss=0.682, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:00<00:00, 30.58it/s, loss=0.658, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:00<00:00, 30.75it/s, loss=0.658, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 31.63it/s, loss=0.648, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:00<00:00, 31.86it/s, loss=0.648, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 31.07it/s, loss=0.633, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.659\n",
      "Epoch 24: 100%|█| 19/19 [00:00<00:00, 31.27it/s, loss=0.633, v_num=f075, LTC_val\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 33.37it/s, loss=0.614, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:00<00:00, 33.63it/s, loss=0.614, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:00<00:00, 32.34it/s, loss=0.633, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.064 >= min_delta = 0.003. New best score: 0.595\n",
      "Epoch 26: 100%|█| 19/19 [00:00<00:00, 32.63it/s, loss=0.633, v_num=f075, LTC_val\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 33.31it/s, loss=0.679, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:00<00:00, 33.31it/s, loss=0.679, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 32.48it/s, loss=0.636, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:00<00:00, 32.82it/s, loss=0.636, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 31.85it/s, loss=0.602, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:00<00:00, 32.06it/s, loss=0.602, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 31.63it/s, loss=0.602, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.109 >= min_delta = 0.003. New best score: 0.486\n",
      "Epoch 30: 100%|█| 19/19 [00:00<00:00, 31.98it/s, loss=0.602, v_num=f075, LTC_val\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 32.43it/s, loss=0.601, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:00<00:00, 32.37it/s, loss=0.601, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:00<00:00, 31.77it/s, loss=0.574, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:00<00:00, 31.65it/s, loss=0.574, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 18/19 [00:00<00:00, 33.52it/s, loss=0.574, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:00<00:00, 33.50it/s, loss=0.574, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:00<00:00, 31.93it/s, loss=0.568, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:00<00:00, 32.20it/s, loss=0.568, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:00<00:00, 32.43it/s, loss=0.598, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:00<00:00, 32.50it/s, loss=0.598, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:00<00:00, 32.56it/s, loss=0.549, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 19/19 [00:00<00:00, 32.71it/s, loss=0.549, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 18/19 [00:00<00:00, 32.39it/s, loss=0.546, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:00<00:00, 32.48it/s, loss=0.546, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:00<00:00, 31.83it/s, loss=0.536, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 19/19 [00:00<00:00, 31.88it/s, loss=0.536, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 18/19 [00:00<00:00, 32.04it/s, loss=0.536, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 19/19 [00:00<00:00, 32.26it/s, loss=0.536, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 18/19 [00:00<00:00, 30.96it/s, loss=0.52, v_num=f075, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.038 >= min_delta = 0.003. New best score: 0.448\n",
      "Epoch 40: 100%|█| 19/19 [00:00<00:00, 31.08it/s, loss=0.52, v_num=f075, LTC_val_\n",
      "Epoch 41:  95%|▉| 18/19 [00:00<00:00, 32.28it/s, loss=0.569, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 19/19 [00:00<00:00, 32.70it/s, loss=0.569, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 18/19 [00:00<00:00, 32.29it/s, loss=0.551, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 19/19 [00:00<00:00, 32.38it/s, loss=0.551, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 18/19 [00:00<00:00, 33.84it/s, loss=0.509, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 19/19 [00:00<00:00, 34.10it/s, loss=0.509, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 44:  95%|▉| 18/19 [00:00<00:00, 30.84it/s, loss=0.503, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.016 >= min_delta = 0.003. New best score: 0.432\n",
      "Epoch 44: 100%|█| 19/19 [00:00<00:00, 31.00it/s, loss=0.503, v_num=f075, LTC_val\n",
      "Epoch 45:  95%|▉| 18/19 [00:00<00:00, 31.91it/s, loss=0.476, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 19/19 [00:00<00:00, 32.20it/s, loss=0.476, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 18/19 [00:00<00:00, 32.88it/s, loss=0.538, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 19/19 [00:00<00:00, 33.26it/s, loss=0.538, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 18/19 [00:00<00:00, 31.86it/s, loss=0.543, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 19/19 [00:00<00:00, 32.18it/s, loss=0.543, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 48:  95%|▉| 18/19 [00:00<00:00, 32.80it/s, loss=0.486, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 19/19 [00:00<00:00, 32.91it/s, loss=0.486, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 49:  95%|▉| 18/19 [00:00<00:00, 31.00it/s, loss=0.483, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 19/19 [00:00<00:00, 30.91it/s, loss=0.483, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 50:  95%|▉| 18/19 [00:00<00:00, 32.64it/s, loss=0.451, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 19/19 [00:00<00:00, 32.72it/s, loss=0.451, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 51:  95%|▉| 18/19 [00:00<00:00, 32.05it/s, loss=0.43, v_num=f075, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 19/19 [00:00<00:00, 31.70it/s, loss=0.43, v_num=f075, LTC_val_\u001b[A\n",
      "Epoch 52:  95%|▉| 18/19 [00:00<00:00, 32.54it/s, loss=0.429, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 19/19 [00:00<00:00, 32.79it/s, loss=0.429, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 53:  95%|▉| 18/19 [00:00<00:00, 33.09it/s, loss=0.424, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 19/19 [00:00<00:00, 33.28it/s, loss=0.424, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 54:  95%|▉| 18/19 [00:00<00:00, 32.60it/s, loss=0.418, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 19/19 [00:00<00:00, 32.98it/s, loss=0.418, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 55:  95%|▉| 18/19 [00:00<00:00, 32.68it/s, loss=0.428, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 19/19 [00:00<00:00, 33.03it/s, loss=0.428, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 56:  95%|▉| 18/19 [00:00<00:00, 33.28it/s, loss=0.404, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.069 >= min_delta = 0.003. New best score: 0.363\n",
      "Epoch 56: 100%|█| 19/19 [00:00<00:00, 32.84it/s, loss=0.404, v_num=f075, LTC_val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57:  95%|▉| 18/19 [00:00<00:00, 31.80it/s, loss=0.689, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 19/19 [00:00<00:00, 32.15it/s, loss=0.689, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 58:  95%|▉| 18/19 [00:00<00:00, 32.42it/s, loss=0.574, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 19/19 [00:00<00:00, 32.68it/s, loss=0.574, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 59:  95%|▉| 18/19 [00:00<00:00, 32.30it/s, loss=0.504, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|█| 19/19 [00:00<00:00, 32.51it/s, loss=0.504, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 60:  95%|▉| 18/19 [00:00<00:00, 31.31it/s, loss=0.442, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 60: 100%|█| 19/19 [00:00<00:00, 31.66it/s, loss=0.442, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 61:  95%|▉| 18/19 [00:00<00:00, 32.50it/s, loss=0.448, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|█| 19/19 [00:00<00:00, 32.82it/s, loss=0.448, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 62:  95%|▉| 18/19 [00:00<00:00, 32.89it/s, loss=0.43, v_num=f075, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 62: 100%|█| 19/19 [00:00<00:00, 32.56it/s, loss=0.43, v_num=f075, LTC_val_\u001b[A\n",
      "Epoch 63:  95%|▉| 18/19 [00:00<00:00, 31.43it/s, loss=0.434, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 63: 100%|█| 19/19 [00:00<00:00, 31.71it/s, loss=0.434, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 64:  95%|▉| 18/19 [00:00<00:00, 32.79it/s, loss=0.422, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 64: 100%|█| 19/19 [00:00<00:00, 32.76it/s, loss=0.422, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 65:  95%|▉| 18/19 [00:00<00:00, 34.57it/s, loss=0.385, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 65: 100%|█| 19/19 [00:00<00:00, 34.93it/s, loss=0.385, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 66:  95%|▉| 18/19 [00:00<00:00, 31.67it/s, loss=0.394, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 66: 100%|█| 19/19 [00:00<00:00, 31.54it/s, loss=0.394, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 67:  95%|▉| 18/19 [00:00<00:00, 32.37it/s, loss=0.354, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 67: 100%|█| 19/19 [00:00<00:00, 32.57it/s, loss=0.354, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 68:  95%|▉| 18/19 [00:00<00:00, 33.21it/s, loss=0.349, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 68: 100%|█| 19/19 [00:00<00:00, 33.69it/s, loss=0.349, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 69:  95%|▉| 18/19 [00:00<00:00, 33.02it/s, loss=0.333, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 69: 100%|█| 19/19 [00:00<00:00, 32.76it/s, loss=0.333, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 70:  95%|▉| 18/19 [00:00<00:00, 31.89it/s, loss=0.362, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 70: 100%|█| 19/19 [00:00<00:00, 32.10it/s, loss=0.362, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 71:  95%|▉| 18/19 [00:00<00:00, 33.10it/s, loss=0.335, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 71: 100%|█| 19/19 [00:00<00:00, 33.35it/s, loss=0.335, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 72:  95%|▉| 18/19 [00:00<00:00, 33.93it/s, loss=0.35, v_num=f075, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 72: 100%|█| 19/19 [00:00<00:00, 33.81it/s, loss=0.35, v_num=f075, LTC_val_\u001b[A\n",
      "Epoch 73:  95%|▉| 18/19 [00:00<00:00, 33.26it/s, loss=0.354, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 73: 100%|█| 19/19 [00:00<00:00, 33.29it/s, loss=0.354, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 74:  95%|▉| 18/19 [00:00<00:00, 33.58it/s, loss=0.337, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 74: 100%|█| 19/19 [00:00<00:00, 33.60it/s, loss=0.337, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 75:  95%|▉| 18/19 [00:00<00:00, 31.96it/s, loss=0.314, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 75: 100%|█| 19/19 [00:00<00:00, 31.81it/s, loss=0.314, v_num=f075, LTC_val\u001b[A\n",
      "Epoch 76:  95%|▉| 18/19 [00:00<00:00, 33.59it/s, loss=0.303, v_num=f075, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.363. Signaling Trainer to stop.\n",
      "Epoch 76: 100%|█| 19/19 [00:00<00:00, 33.61it/s, loss=0.303, v_num=f075, LTC_val\n",
      "Epoch 76: 100%|█| 19/19 [00:00<00:00, 33.01it/s, loss=0.303, v_num=f075, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 67.55it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.6333333253860474,\n",
      " 'LTC_test_f1': 0.609565258026123,\n",
      " 'test_loss': 1.2425730228424072}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 94100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095432-24urf075/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095432-24urf075/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.86213\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.85972\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.29404\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 76\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1386\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625900125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 181\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.49603\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.77112\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.86207\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.86411\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.30677\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.63333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.60957\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.24257\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▁▂▃▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇▆▇▇▇▇█▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▁▂▂▃▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇▆▇▇▇▇█▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ███▇▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▃▂▂▂▂▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▃▂▃▃▄▁▄▁▂▅▃▃▅▃▄▅▅▄▅▆▆▆█▇▆▃▃▃██▅▆▅▆▆▇▇▆▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▂▄▁▅▂▄▂▂▆▄▄▆▄▄▅▅▅▆▆▆▆█▇▇▄▄▄██▅▅▆▆▆▇▇▆▆▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▅▅▅▅█▃▆▅▃▄▃▃▄▃▃▂▃▄▂▂▁▂▁▄▄▄▇▂▁▃▂▃▂▂▂▁▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▂▂▄▄▅▆▅▆▅▅▆▇▆▆▅▅▇▆▆▆▆▆▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▂▂▄▄▅▆▅▆▅▅▆▇▅▆▅▅▇▆▆▆▆▆▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▇▇▆▆▅▃▄▃▅▅▃▃▄▃▅▄▃▃▃▄▃▄▃▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/24urf075\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:55:56.649025: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/2xpzt4mn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095555-2xpzt4mn\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 219 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 258   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.177     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:00<00:00, 33.63it/s, loss=0.705, v_num=t4mn, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.712\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 33.70it/s, loss=0.705, v_num=t4mn, LTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 34.35it/s, loss=0.691, v_num=t4mn, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.069 >= min_delta = 0.003. New best score: 0.642\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 34.76it/s, loss=0.691, v_num=t4mn, LTC_val_\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 32.46it/s, loss=0.686, v_num=t4mn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 19/19 [00:00<00:00, 32.69it/s, loss=0.686, v_num=t4mn, LTC_val_\u001b[A\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 32.42it/s, loss=0.678, v_num=t4mn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.089 >= min_delta = 0.003. New best score: 0.553\n",
      "Epoch 3: 100%|█| 19/19 [00:00<00:00, 32.80it/s, loss=0.678, v_num=t4mn, LTC_val_\n",
      "Epoch 4:  95%|▉| 18/19 [00:00<00:00, 33.30it/s, loss=0.662, v_num=t4mn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 19/19 [00:00<00:00, 33.42it/s, loss=0.662, v_num=t4mn, LTC_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 18/19 [00:00<00:00, 31.51it/s, loss=0.647, v_num=t4mn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.024 >= min_delta = 0.003. New best score: 0.529\n",
      "Epoch 5: 100%|█| 19/19 [00:00<00:00, 31.73it/s, loss=0.647, v_num=t4mn, LTC_val_\n",
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 33.16it/s, loss=0.597, v_num=t4mn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.066 >= min_delta = 0.003. New best score: 0.464\n",
      "Epoch 6: 100%|█| 19/19 [00:00<00:00, 33.57it/s, loss=0.597, v_num=t4mn, LTC_val_\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 31.70it/s, loss=0.578, v_num=t4mn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.088 >= min_delta = 0.003. New best score: 0.376\n",
      "Epoch 7: 100%|█| 19/19 [00:00<00:00, 31.77it/s, loss=0.578, v_num=t4mn, LTC_val_\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 31.40it/s, loss=0.578, v_num=t4mn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 0.361\n",
      "Epoch 8: 100%|█| 19/19 [00:00<00:00, 30.82it/s, loss=0.578, v_num=t4mn, LTC_val_\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 32.10it/s, loss=0.518, v_num=t4mn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.352\n",
      "Epoch 9: 100%|█| 19/19 [00:00<00:00, 32.55it/s, loss=0.518, v_num=t4mn, LTC_val_\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 32.42it/s, loss=0.504, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.097 >= min_delta = 0.003. New best score: 0.255\n",
      "Epoch 10: 100%|█| 19/19 [00:00<00:00, 32.58it/s, loss=0.504, v_num=t4mn, LTC_val\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 33.40it/s, loss=0.486, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:00<00:00, 33.44it/s, loss=0.486, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 33.05it/s, loss=0.476, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:00<00:00, 33.39it/s, loss=0.476, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 33.16it/s, loss=0.457, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.050 >= min_delta = 0.003. New best score: 0.204\n",
      "Epoch 13: 100%|█| 19/19 [00:00<00:00, 33.36it/s, loss=0.457, v_num=t4mn, LTC_val\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 33.13it/s, loss=0.438, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:00<00:00, 32.63it/s, loss=0.438, v_num=t4mn, LTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 32.92it/s, loss=0.446, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:00<00:00, 32.51it/s, loss=0.446, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 31.66it/s, loss=0.434, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:00<00:00, 31.76it/s, loss=0.434, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 31.54it/s, loss=0.442, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:00<00:00, 31.98it/s, loss=0.442, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 32.20it/s, loss=0.413, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:00<00:00, 32.56it/s, loss=0.413, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 29.51it/s, loss=0.391, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:00<00:00, 29.56it/s, loss=0.391, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 34.17it/s, loss=0.4, v_num=t4mn, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:00<00:00, 34.44it/s, loss=0.4, v_num=t4mn, LTC_val_a\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 33.56it/s, loss=0.391, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:00<00:00, 33.80it/s, loss=0.391, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:00<00:00, 31.90it/s, loss=0.393, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:00<00:00, 32.16it/s, loss=0.393, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 32.39it/s, loss=0.383, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:00<00:00, 32.52it/s, loss=0.383, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 34.70it/s, loss=0.37, v_num=t4mn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.116 >= min_delta = 0.003. New best score: 0.088\n",
      "Epoch 24: 100%|█| 19/19 [00:00<00:00, 34.91it/s, loss=0.37, v_num=t4mn, LTC_val_\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 31.85it/s, loss=0.373, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:00<00:00, 32.03it/s, loss=0.373, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:00<00:00, 34.42it/s, loss=0.346, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:00<00:00, 34.86it/s, loss=0.346, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 33.15it/s, loss=0.345, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:00<00:00, 33.14it/s, loss=0.345, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 33.08it/s, loss=0.354, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:00<00:00, 33.12it/s, loss=0.354, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 33.18it/s, loss=0.348, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:00<00:00, 33.55it/s, loss=0.348, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 33.63it/s, loss=0.366, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:00<00:00, 34.13it/s, loss=0.366, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 33.06it/s, loss=0.364, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:00<00:00, 32.87it/s, loss=0.364, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:00<00:00, 32.29it/s, loss=0.349, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:00<00:00, 32.63it/s, loss=0.349, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 18/19 [00:00<00:00, 32.47it/s, loss=0.34, v_num=t4mn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:00<00:00, 32.84it/s, loss=0.34, v_num=t4mn, LTC_val_\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:00<00:00, 30.12it/s, loss=0.346, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:00<00:00, 30.48it/s, loss=0.346, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:00<00:00, 32.52it/s, loss=0.312, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:00<00:00, 32.89it/s, loss=0.312, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:00<00:00, 33.91it/s, loss=0.317, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 19/19 [00:00<00:00, 34.18it/s, loss=0.317, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 18/19 [00:00<00:00, 32.90it/s, loss=0.309, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:00<00:00, 33.14it/s, loss=0.309, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:00<00:00, 34.43it/s, loss=0.32, v_num=t4mn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 19/19 [00:00<00:00, 34.83it/s, loss=0.32, v_num=t4mn, LTC_val_\u001b[A\n",
      "Epoch 39:  95%|▉| 18/19 [00:00<00:00, 32.13it/s, loss=0.31, v_num=t4mn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 19/19 [00:00<00:00, 32.39it/s, loss=0.31, v_num=t4mn, LTC_val_\u001b[A\n",
      "Epoch 40:  95%|▉| 18/19 [00:00<00:00, 31.89it/s, loss=0.282, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 19/19 [00:00<00:00, 32.32it/s, loss=0.282, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 18/19 [00:00<00:00, 33.49it/s, loss=0.283, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 19/19 [00:00<00:00, 33.70it/s, loss=0.283, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 18/19 [00:00<00:00, 31.97it/s, loss=0.284, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 19/19 [00:00<00:00, 32.19it/s, loss=0.284, v_num=t4mn, LTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 18/19 [00:00<00:00, 32.93it/s, loss=0.27, v_num=t4mn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 19/19 [00:00<00:00, 33.19it/s, loss=0.27, v_num=t4mn, LTC_val_\u001b[A\n",
      "Epoch 44:  95%|▉| 18/19 [00:00<00:00, 31.74it/s, loss=0.265, v_num=t4mn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.088. Signaling Trainer to stop.\n",
      "Epoch 44: 100%|█| 19/19 [00:00<00:00, 31.90it/s, loss=0.265, v_num=t4mn, LTC_val\n",
      "Epoch 44: 100%|█| 19/19 [00:00<00:00, 31.77it/s, loss=0.265, v_num=t4mn, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 96.18it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.6000000238418579,\n",
      " 'LTC_test_f1': 0.5693780183792114,\n",
      " 'test_loss': 1.022457242012024}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 94341\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095555-2xpzt4mn/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095555-2xpzt4mn/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.86824\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.86693\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.25881\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 44\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 810\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 34\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625900189\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 106\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.58333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.56521\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.79688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.79683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.34667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.56938\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.02246\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▂▂▃▃▄▄▅▆▆▆▇▆▇▆▇▇▇▇▇▇▇▇▇█▇▇▇▇▇█▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▂▂▃▃▄▄▅▆▆▆▇▆▇▆▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ████▇▇▆▆▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▃▂▃▂▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▇▇▇▇▇▇▇▇▇█▆█▄▄▃█▄▅▇▆▄█▅▄▄▂▄▆█▄▅▄▄▇█▆▇▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▄▄▄▄▄▄▄▄▆█▆█▄▄▃█▄▅▆▆▄█▅▄▄▃▄▆█▄▅▄▄▆█▆▇▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▄▄▄▄▄▃▃▃▂▂▃▂▅▄▇▂▄▃▂▃▅▁▃▆▅█▆▃▁▄▃▄▄▂▂▃▂▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▄▆▅▆▇█▇██▇▇███▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▄▆▅▆▇█▇██▇▇██▇▇\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▆▆▅▄▄▄▄▂▁▂▂▁▁▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/2xpzt4mn\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:56:59.312783: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/2uem6hjc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095658-2uem6hjc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 219 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 387   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.178     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:00<00:00, 33.49it/s, loss=1.14, v_num=6hjc, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.140\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 33.79it/s, loss=1.14, v_num=6hjc, LTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 33.51it/s, loss=1.11, v_num=6hjc, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.061 >= min_delta = 0.003. New best score: 1.080\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 33.86it/s, loss=1.11, v_num=6hjc, LTC_val_a\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 32.43it/s, loss=1.1, v_num=6hjc, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.032 >= min_delta = 0.003. New best score: 1.048\n",
      "Epoch 2: 100%|█| 19/19 [00:00<00:00, 32.51it/s, loss=1.1, v_num=6hjc, LTC_val_ac\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 30.95it/s, loss=1.09, v_num=6hjc, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 19/19 [00:00<00:00, 31.19it/s, loss=1.09, v_num=6hjc, LTC_val_a\u001b[A\n",
      "Epoch 4:  95%|▉| 18/19 [00:00<00:00, 31.10it/s, loss=1.08, v_num=6hjc, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 19/19 [00:00<00:00, 31.30it/s, loss=1.08, v_num=6hjc, LTC_val_a\u001b[A\n",
      "Epoch 5:  95%|▉| 18/19 [00:00<00:00, 31.37it/s, loss=1.07, v_num=6hjc, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 1.039\n",
      "Epoch 5: 100%|█| 19/19 [00:00<00:00, 31.72it/s, loss=1.07, v_num=6hjc, LTC_val_a\n",
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 31.87it/s, loss=1.02, v_num=6hjc, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 19/19 [00:00<00:00, 31.93it/s, loss=1.02, v_num=6hjc, LTC_val_a\u001b[A\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 32.14it/s, loss=0.953, v_num=6hjc, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.241 >= min_delta = 0.003. New best score: 0.797\n",
      "Epoch 7: 100%|█| 19/19 [00:00<00:00, 32.45it/s, loss=0.953, v_num=6hjc, LTC_val_\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 31.98it/s, loss=0.93, v_num=6hjc, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.075 >= min_delta = 0.003. New best score: 0.722\n",
      "Epoch 8: 100%|█| 19/19 [00:00<00:00, 31.84it/s, loss=0.93, v_num=6hjc, LTC_val_a\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 32.47it/s, loss=0.888, v_num=6hjc, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.025 >= min_delta = 0.003. New best score: 0.696\n",
      "Epoch 9: 100%|█| 19/19 [00:00<00:00, 32.59it/s, loss=0.888, v_num=6hjc, LTC_val_\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 32.61it/s, loss=0.851, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:00<00:00, 32.58it/s, loss=0.851, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 32.03it/s, loss=0.825, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:00<00:00, 32.17it/s, loss=0.825, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 33.09it/s, loss=0.817, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.113 >= min_delta = 0.003. New best score: 0.583\n",
      "Epoch 12: 100%|█| 19/19 [00:00<00:00, 33.31it/s, loss=0.817, v_num=6hjc, LTC_val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 32.48it/s, loss=0.806, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:00<00:00, 32.71it/s, loss=0.806, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 30.44it/s, loss=0.82, v_num=6hjc, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:00<00:00, 30.79it/s, loss=0.82, v_num=6hjc, LTC_val_\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 32.28it/s, loss=0.787, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:00<00:00, 32.70it/s, loss=0.787, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 33.81it/s, loss=0.754, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:00<00:00, 33.90it/s, loss=0.754, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 32.77it/s, loss=0.736, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:00<00:00, 33.14it/s, loss=0.736, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 32.32it/s, loss=0.735, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.045 >= min_delta = 0.003. New best score: 0.538\n",
      "Epoch 18: 100%|█| 19/19 [00:00<00:00, 32.38it/s, loss=0.735, v_num=6hjc, LTC_val\n",
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 29.95it/s, loss=0.752, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:00<00:00, 29.98it/s, loss=0.752, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 32.11it/s, loss=0.712, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:00<00:00, 32.51it/s, loss=0.712, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 33.17it/s, loss=0.694, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:00<00:00, 33.25it/s, loss=0.694, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:00<00:00, 31.68it/s, loss=0.687, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:00<00:00, 31.50it/s, loss=0.687, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 33.20it/s, loss=0.683, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:00<00:00, 33.40it/s, loss=0.683, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 33.13it/s, loss=0.688, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:00<00:00, 33.40it/s, loss=0.688, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 32.93it/s, loss=0.693, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:00<00:00, 33.32it/s, loss=0.693, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:00<00:00, 34.04it/s, loss=0.677, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 0.527\n",
      "Epoch 26: 100%|█| 19/19 [00:00<00:00, 34.15it/s, loss=0.677, v_num=6hjc, LTC_val\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 31.77it/s, loss=0.645, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:00<00:00, 31.94it/s, loss=0.645, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 30.61it/s, loss=0.65, v_num=6hjc, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:00<00:00, 31.15it/s, loss=0.65, v_num=6hjc, LTC_val_\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 31.09it/s, loss=0.65, v_num=6hjc, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:00<00:00, 31.37it/s, loss=0.65, v_num=6hjc, LTC_val_\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 31.79it/s, loss=0.666, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:00<00:00, 32.03it/s, loss=0.666, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 31.03it/s, loss=0.651, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:00<00:00, 31.42it/s, loss=0.651, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:00<00:00, 32.62it/s, loss=0.651, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:00<00:00, 32.98it/s, loss=0.651, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 18/19 [00:00<00:00, 33.12it/s, loss=0.61, v_num=6hjc, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:00<00:00, 33.53it/s, loss=0.61, v_num=6hjc, LTC_val_\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:00<00:00, 31.20it/s, loss=0.627, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:00<00:00, 31.31it/s, loss=0.627, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:00<00:00, 32.14it/s, loss=0.614, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:00<00:00, 32.41it/s, loss=0.614, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:00<00:00, 31.62it/s, loss=0.582, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 19/19 [00:00<00:00, 31.78it/s, loss=0.582, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 18/19 [00:00<00:00, 31.67it/s, loss=0.548, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.086 >= min_delta = 0.003. New best score: 0.440\n",
      "Epoch 37: 100%|█| 19/19 [00:00<00:00, 31.78it/s, loss=0.548, v_num=6hjc, LTC_val\n",
      "Epoch 38:  95%|▉| 18/19 [00:00<00:00, 32.95it/s, loss=0.546, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 19/19 [00:00<00:00, 33.17it/s, loss=0.546, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 18/19 [00:00<00:00, 32.76it/s, loss=0.547, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 19/19 [00:00<00:00, 33.06it/s, loss=0.547, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 18/19 [00:00<00:00, 32.12it/s, loss=0.517, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 19/19 [00:00<00:00, 32.19it/s, loss=0.517, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 18/19 [00:00<00:00, 32.79it/s, loss=0.538, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 19/19 [00:00<00:00, 33.12it/s, loss=0.538, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 18/19 [00:00<00:00, 32.08it/s, loss=0.526, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 19/19 [00:00<00:00, 32.43it/s, loss=0.526, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 18/19 [00:00<00:00, 32.07it/s, loss=0.517, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 19/19 [00:00<00:00, 32.44it/s, loss=0.517, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 44:  95%|▉| 18/19 [00:00<00:00, 32.21it/s, loss=0.504, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 19/19 [00:00<00:00, 32.64it/s, loss=0.504, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 45:  95%|▉| 18/19 [00:00<00:00, 31.85it/s, loss=0.473, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 19/19 [00:00<00:00, 31.98it/s, loss=0.473, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 18/19 [00:00<00:00, 33.03it/s, loss=0.456, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 19/19 [00:00<00:00, 32.96it/s, loss=0.456, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 18/19 [00:00<00:00, 33.41it/s, loss=0.481, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.022 >= min_delta = 0.003. New best score: 0.418\n",
      "Epoch 47: 100%|█| 19/19 [00:00<00:00, 33.71it/s, loss=0.481, v_num=6hjc, LTC_val\n",
      "Epoch 48:  95%|▉| 18/19 [00:00<00:00, 33.76it/s, loss=0.467, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 19/19 [00:00<00:00, 34.03it/s, loss=0.467, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 49:  95%|▉| 18/19 [00:00<00:00, 32.71it/s, loss=0.483, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 19/19 [00:00<00:00, 33.01it/s, loss=0.483, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 50:  95%|▉| 18/19 [00:00<00:00, 32.11it/s, loss=0.491, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|█| 19/19 [00:00<00:00, 32.40it/s, loss=0.491, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 51:  95%|▉| 18/19 [00:00<00:00, 30.31it/s, loss=0.445, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 19/19 [00:00<00:00, 30.74it/s, loss=0.445, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 52:  95%|▉| 18/19 [00:00<00:00, 32.87it/s, loss=0.419, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 19/19 [00:00<00:00, 32.90it/s, loss=0.419, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 53:  95%|▉| 18/19 [00:00<00:00, 30.63it/s, loss=0.433, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 19/19 [00:00<00:00, 30.94it/s, loss=0.433, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 54:  95%|▉| 18/19 [00:00<00:00, 31.15it/s, loss=0.411, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 19/19 [00:00<00:00, 31.60it/s, loss=0.411, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 55:  95%|▉| 18/19 [00:00<00:00, 32.92it/s, loss=0.405, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 19/19 [00:00<00:00, 33.29it/s, loss=0.405, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 56:  95%|▉| 18/19 [00:00<00:00, 33.21it/s, loss=0.392, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 19/19 [00:00<00:00, 33.22it/s, loss=0.392, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 57:  95%|▉| 18/19 [00:00<00:00, 30.86it/s, loss=0.377, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 19/19 [00:00<00:00, 31.14it/s, loss=0.377, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 58:  95%|▉| 18/19 [00:00<00:00, 31.93it/s, loss=0.371, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 19/19 [00:00<00:00, 31.79it/s, loss=0.371, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 59:  95%|▉| 18/19 [00:00<00:00, 33.53it/s, loss=0.372, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|█| 19/19 [00:00<00:00, 33.84it/s, loss=0.372, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 60:  95%|▉| 18/19 [00:00<00:00, 32.66it/s, loss=0.38, v_num=6hjc, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 60: 100%|█| 19/19 [00:00<00:00, 32.83it/s, loss=0.38, v_num=6hjc, LTC_val_\u001b[A\n",
      "Epoch 61:  95%|▉| 18/19 [00:00<00:00, 32.02it/s, loss=0.378, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|█| 19/19 [00:00<00:00, 32.50it/s, loss=0.378, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 62:  95%|▉| 18/19 [00:00<00:00, 32.01it/s, loss=0.372, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 62: 100%|█| 19/19 [00:00<00:00, 32.35it/s, loss=0.372, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 63:  95%|▉| 18/19 [00:00<00:00, 32.06it/s, loss=0.382, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 63: 100%|█| 19/19 [00:00<00:00, 32.04it/s, loss=0.382, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 64:  95%|▉| 18/19 [00:00<00:00, 30.92it/s, loss=0.357, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 64: 100%|█| 19/19 [00:00<00:00, 31.07it/s, loss=0.357, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 65:  95%|▉| 18/19 [00:00<00:00, 33.29it/s, loss=0.361, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 65: 100%|█| 19/19 [00:00<00:00, 33.63it/s, loss=0.361, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 66:  95%|▉| 18/19 [00:00<00:00, 33.36it/s, loss=0.345, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 66: 100%|█| 19/19 [00:00<00:00, 33.67it/s, loss=0.345, v_num=6hjc, LTC_val\u001b[A\n",
      "Epoch 67:  95%|▉| 18/19 [00:00<00:00, 31.69it/s, loss=0.335, v_num=6hjc, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.418. Signaling Trainer to stop.\n",
      "Epoch 67: 100%|█| 19/19 [00:00<00:00, 32.10it/s, loss=0.335, v_num=6hjc, LTC_val\n",
      "Epoch 67: 100%|█| 19/19 [00:00<00:00, 31.97it/s, loss=0.335, v_num=6hjc, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 76.65it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.5,\n",
      " 'LTC_test_f1': 0.505698025226593,\n",
      " 'test_loss': 0.952131986618042}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 94529\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095658-2uem6hjc/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095658-2uem6hjc/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.84991\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.84808\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.33773\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 67\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 48\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625900266\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.58333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.58889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.23779\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.81322\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.49947\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.5057\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.95213\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▂▂▂▃▄▅▅▅▅▆▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▃▃▃▄▅▅▆▅▆▆▆▆▆▆▆▆▇▆▆▆▇▇▇▇▇▇▇▇▇▇█▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ██▇▇▇▆▅▅▅▅▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▄▃▅▁▆▃▆▆▃▃▇▄▅▃▃▄▃▃▆▃▅█▄▄▃▆▅▄▃▆▆▆▇▅▆▆▆▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▄▃▄▁▆▄▅▆▄▃▇▄▅▄▄▄▄▃▆▄▅█▅▅▄▇▅▄▄▆▆▇▇▅▇▇▆▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▄▅▄▅▃▄▂▂▃█▂▃▂▃▃▃▃▇▂▃▂▁▄▃▄▂▄▆▆▂▂▂▂▇▂▂▃▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▃▃▅▅▅▆▅▆▇▆▅▇▇▆▇█▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▃▃▄▄▅▆▅▆▇▆▅▇▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▇▇▆▆▄▅▅▄▄▃▄▃▄▄▃▃▄▃▃▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/2uem6hjc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:58:16.629668: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/9d7av4v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095815-9d7av4v3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 134 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 258   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "959 K     Trainable params\n",
      "0         Non-trainable params\n",
      "959 K     Total params\n",
      "3.837     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 37.12it/s, loss=0.702, v_num=v4v3, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.689\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 36.42it/s, loss=0.702, v_num=v4v3, LTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 36.38it/s, loss=0.629, v_num=v4v3, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.194 >= min_delta = 0.003. New best score: 0.495\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 36.41it/s, loss=0.629, v_num=v4v3, LTC_val_\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 35.01it/s, loss=0.564, v_num=v4v3, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.221 >= min_delta = 0.003. New best score: 0.274\n",
      "Epoch 2: 100%|█| 19/19 [00:00<00:00, 35.22it/s, loss=0.564, v_num=v4v3, LTC_val_\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 35.35it/s, loss=0.557, v_num=v4v3, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 0.264\n",
      "Epoch 3: 100%|█| 19/19 [00:00<00:00, 35.60it/s, loss=0.557, v_num=v4v3, LTC_val_\n",
      "Epoch 4:  95%|▉| 18/19 [00:00<00:00, 35.53it/s, loss=0.517, v_num=v4v3, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 19/19 [00:00<00:00, 35.70it/s, loss=0.517, v_num=v4v3, LTC_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 18/19 [00:00<00:00, 34.79it/s, loss=0.507, v_num=v4v3, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.016 >= min_delta = 0.003. New best score: 0.248\n",
      "Epoch 5: 100%|█| 19/19 [00:00<00:00, 34.84it/s, loss=0.507, v_num=v4v3, LTC_val_\n",
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 32.03it/s, loss=0.515, v_num=v4v3, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 19/19 [00:00<00:00, 32.41it/s, loss=0.515, v_num=v4v3, LTC_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 32.93it/s, loss=0.522, v_num=v4v3, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 19/19 [00:00<00:00, 32.91it/s, loss=0.522, v_num=v4v3, LTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 33.19it/s, loss=0.497, v_num=v4v3, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 19/19 [00:00<00:00, 33.41it/s, loss=0.497, v_num=v4v3, LTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 34.60it/s, loss=0.495, v_num=v4v3, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:00<00:00, 34.72it/s, loss=0.495, v_num=v4v3, LTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 34.78it/s, loss=0.47, v_num=v4v3, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:00<00:00, 34.87it/s, loss=0.47, v_num=v4v3, LTC_val_\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 33.93it/s, loss=0.502, v_num=v4v3, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:00<00:00, 34.27it/s, loss=0.502, v_num=v4v3, LTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 33.33it/s, loss=0.499, v_num=v4v3, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:00<00:00, 33.33it/s, loss=0.499, v_num=v4v3, LTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 32.57it/s, loss=0.486, v_num=v4v3, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:00<00:00, 32.99it/s, loss=0.486, v_num=v4v3, LTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 35.40it/s, loss=0.485, v_num=v4v3, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:00<00:00, 35.41it/s, loss=0.485, v_num=v4v3, LTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 32.34it/s, loss=0.462, v_num=v4v3, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:00<00:00, 32.49it/s, loss=0.462, v_num=v4v3, LTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 34.70it/s, loss=0.468, v_num=v4v3, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:00<00:00, 34.96it/s, loss=0.468, v_num=v4v3, LTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 33.56it/s, loss=0.493, v_num=v4v3, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:00<00:00, 33.69it/s, loss=0.493, v_num=v4v3, LTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 33.67it/s, loss=0.479, v_num=v4v3, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:00<00:00, 33.78it/s, loss=0.479, v_num=v4v3, LTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 33.12it/s, loss=0.477, v_num=v4v3, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:00<00:00, 33.40it/s, loss=0.477, v_num=v4v3, LTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 34.45it/s, loss=0.468, v_num=v4v3, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:00<00:00, 34.71it/s, loss=0.468, v_num=v4v3, LTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 34.35it/s, loss=0.452, v_num=v4v3, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:00<00:00, 34.62it/s, loss=0.452, v_num=v4v3, LTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:00<00:00, 29.80it/s, loss=0.472, v_num=v4v3, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:00<00:00, 30.26it/s, loss=0.472, v_num=v4v3, LTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 35.12it/s, loss=0.459, v_num=v4v3, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:00<00:00, 35.41it/s, loss=0.459, v_num=v4v3, LTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 33.88it/s, loss=0.46, v_num=v4v3, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:00<00:00, 33.96it/s, loss=0.46, v_num=v4v3, LTC_val_\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 33.52it/s, loss=0.461, v_num=v4v3, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.248. Signaling Trainer to stop.\n",
      "Epoch 25: 100%|█| 19/19 [00:00<00:00, 33.84it/s, loss=0.461, v_num=v4v3, LTC_val\n",
      "Epoch 25: 100%|█| 19/19 [00:00<00:00, 33.63it/s, loss=0.461, v_num=v4v3, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 38.81it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.7666666507720947,\n",
      " 'LTC_test_f1': 0.7643097639083862,\n",
      " 'test_loss': 0.5028652548789978}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 94776\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095815-9d7av4v3/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095815-9d7av4v3/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.78098\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.77491\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.46568\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 468\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 22\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625900317\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 61\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.91667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.87368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.37503\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.84483\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.8337\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.39364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.76667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.76431\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.50287\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▄▆▆▇▇▇▇▇▇█▇▇█████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▅▇▇▇▇▇▇█▇█▇▇█████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▄▄▃▂▃▃▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▇▇▇▇▇▇▁▇▇▂▇▁▅██▇▇▇▇▇▇▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▆▆▆▆▆▆▁▆▆▂▆▁▅██▆▆▆▆▆▆▆▆█▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▅▁▁▂▁▅▃▃▆▂▅▄▂▂▃▂▂▂▃▄▂▁▁▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▁▇▁▇▄▇▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▁▇▁█▄█▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▇▃▆▅█▄▅▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/9d7av4v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 09:59:06.659852: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/xe4k8fwn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095905-xe4k8fwn\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 134 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 387   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "959 K     Trainable params\n",
      "0         Non-trainable params\n",
      "959 K     Total params\n",
      "3.838     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:00<00:00, 34.26it/s, loss=1.1, v_num=8fwn, LTC_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.075\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 34.16it/s, loss=1.1, v_num=8fwn, LTC_val_ac\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 35.94it/s, loss=1.02, v_num=8fwn, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.140 >= min_delta = 0.003. New best score: 0.935\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 36.24it/s, loss=1.02, v_num=8fwn, LTC_val_a\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 32.55it/s, loss=0.947, v_num=8fwn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.081 >= min_delta = 0.003. New best score: 0.854\n",
      "Epoch 2: 100%|█| 19/19 [00:00<00:00, 32.97it/s, loss=0.947, v_num=8fwn, LTC_val_\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 35.12it/s, loss=0.927, v_num=8fwn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.047 >= min_delta = 0.003. New best score: 0.807\n",
      "Epoch 3: 100%|█| 19/19 [00:00<00:00, 35.06it/s, loss=0.927, v_num=8fwn, LTC_val_\n",
      "Epoch 4:  95%|▉| 18/19 [00:00<00:00, 32.26it/s, loss=0.888, v_num=8fwn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.064 >= min_delta = 0.003. New best score: 0.743\n",
      "Epoch 4: 100%|█| 19/19 [00:00<00:00, 31.99it/s, loss=0.888, v_num=8fwn, LTC_val_\n",
      "Epoch 5:  95%|▉| 18/19 [00:00<00:00, 32.20it/s, loss=0.878, v_num=8fwn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.739\n",
      "Epoch 5: 100%|█| 19/19 [00:00<00:00, 31.82it/s, loss=0.878, v_num=8fwn, LTC_val_\n",
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 33.53it/s, loss=0.858, v_num=8fwn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.029 >= min_delta = 0.003. New best score: 0.710\n",
      "Epoch 6: 100%|█| 19/19 [00:00<00:00, 33.81it/s, loss=0.858, v_num=8fwn, LTC_val_\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 32.35it/s, loss=0.84, v_num=8fwn, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.061 >= min_delta = 0.003. New best score: 0.649\n",
      "Epoch 7: 100%|█| 19/19 [00:00<00:00, 32.80it/s, loss=0.84, v_num=8fwn, LTC_val_a\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 33.03it/s, loss=0.807, v_num=8fwn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.640\n",
      "Epoch 8: 100%|█| 19/19 [00:00<00:00, 33.33it/s, loss=0.807, v_num=8fwn, LTC_val_\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 32.77it/s, loss=0.809, v_num=8fwn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:00<00:00, 33.19it/s, loss=0.809, v_num=8fwn, LTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 33.32it/s, loss=0.825, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:00<00:00, 33.52it/s, loss=0.825, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 35.09it/s, loss=0.814, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:00<00:00, 35.37it/s, loss=0.814, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 33.88it/s, loss=0.82, v_num=8fwn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:00<00:00, 33.90it/s, loss=0.82, v_num=8fwn, LTC_val_\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 35.22it/s, loss=0.814, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:00<00:00, 35.49it/s, loss=0.814, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 34.43it/s, loss=0.819, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:00<00:00, 34.72it/s, loss=0.819, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 32.96it/s, loss=0.803, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:00<00:00, 33.38it/s, loss=0.803, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 32.63it/s, loss=0.797, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.026 >= min_delta = 0.003. New best score: 0.614\n",
      "Epoch 16: 100%|█| 19/19 [00:00<00:00, 32.26it/s, loss=0.797, v_num=8fwn, LTC_val\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 32.76it/s, loss=0.804, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:00<00:00, 32.13it/s, loss=0.804, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 31.62it/s, loss=0.81, v_num=8fwn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:00<00:00, 31.75it/s, loss=0.81, v_num=8fwn, LTC_val_\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 34.96it/s, loss=0.809, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:00<00:00, 35.26it/s, loss=0.809, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 34.33it/s, loss=0.801, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:00<00:00, 34.63it/s, loss=0.801, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 33.68it/s, loss=0.792, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:00<00:00, 34.03it/s, loss=0.792, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:00<00:00, 31.19it/s, loss=0.786, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:00<00:00, 31.67it/s, loss=0.786, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 35.26it/s, loss=0.786, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:00<00:00, 35.22it/s, loss=0.786, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 34.66it/s, loss=0.784, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:00<00:00, 34.93it/s, loss=0.784, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 34.55it/s, loss=0.765, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:00<00:00, 34.70it/s, loss=0.765, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:00<00:00, 34.64it/s, loss=0.771, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:00<00:00, 34.60it/s, loss=0.771, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 33.22it/s, loss=0.766, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:00<00:00, 33.44it/s, loss=0.766, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 33.90it/s, loss=0.768, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:00<00:00, 33.97it/s, loss=0.768, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 32.89it/s, loss=0.779, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:00<00:00, 33.27it/s, loss=0.779, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 33.15it/s, loss=0.757, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:00<00:00, 33.23it/s, loss=0.757, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 33.25it/s, loss=0.752, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:00<00:00, 33.60it/s, loss=0.752, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:00<00:00, 33.48it/s, loss=0.767, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:00<00:00, 33.57it/s, loss=0.767, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 18/19 [00:00<00:00, 33.41it/s, loss=0.767, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:00<00:00, 33.79it/s, loss=0.767, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:00<00:00, 32.93it/s, loss=0.79, v_num=8fwn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:00<00:00, 33.10it/s, loss=0.79, v_num=8fwn, LTC_val_\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35:  95%|▉| 18/19 [00:00<00:00, 34.73it/s, loss=0.75, v_num=8fwn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.608\n",
      "Epoch 35: 100%|█| 19/19 [00:00<00:00, 34.71it/s, loss=0.75, v_num=8fwn, LTC_val_\n",
      "Epoch 36:  95%|▉| 18/19 [00:00<00:00, 32.83it/s, loss=0.745, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 19/19 [00:00<00:00, 33.11it/s, loss=0.745, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 18/19 [00:00<00:00, 33.76it/s, loss=0.752, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:00<00:00, 34.07it/s, loss=0.752, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:00<00:00, 34.97it/s, loss=0.74, v_num=8fwn, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.043 >= min_delta = 0.003. New best score: 0.565\n",
      "Epoch 38: 100%|█| 19/19 [00:00<00:00, 35.18it/s, loss=0.74, v_num=8fwn, LTC_val_\n",
      "Epoch 39:  95%|▉| 18/19 [00:00<00:00, 33.46it/s, loss=0.731, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 19/19 [00:00<00:00, 33.34it/s, loss=0.731, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 18/19 [00:00<00:00, 33.76it/s, loss=0.738, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 19/19 [00:00<00:00, 33.93it/s, loss=0.738, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 18/19 [00:00<00:00, 33.67it/s, loss=0.748, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 19/19 [00:00<00:00, 33.81it/s, loss=0.748, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 18/19 [00:00<00:00, 33.57it/s, loss=0.733, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 19/19 [00:00<00:00, 33.50it/s, loss=0.733, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 18/19 [00:00<00:00, 32.83it/s, loss=0.739, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 19/19 [00:00<00:00, 33.06it/s, loss=0.739, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 44:  95%|▉| 18/19 [00:00<00:00, 31.69it/s, loss=0.735, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 19/19 [00:00<00:00, 31.75it/s, loss=0.735, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 45:  95%|▉| 18/19 [00:00<00:00, 33.28it/s, loss=0.733, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 19/19 [00:00<00:00, 33.69it/s, loss=0.733, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 18/19 [00:00<00:00, 33.68it/s, loss=0.717, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 19/19 [00:00<00:00, 33.69it/s, loss=0.717, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 18/19 [00:00<00:00, 31.07it/s, loss=0.714, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 19/19 [00:00<00:00, 31.25it/s, loss=0.714, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 48:  95%|▉| 18/19 [00:00<00:00, 33.42it/s, loss=0.724, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 19/19 [00:00<00:00, 33.63it/s, loss=0.724, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 49:  95%|▉| 18/19 [00:00<00:00, 33.62it/s, loss=0.711, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 19/19 [00:00<00:00, 33.46it/s, loss=0.711, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 50:  95%|▉| 18/19 [00:00<00:00, 33.36it/s, loss=0.705, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 19/19 [00:00<00:00, 33.65it/s, loss=0.705, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 51:  95%|▉| 18/19 [00:00<00:00, 33.68it/s, loss=0.714, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 19/19 [00:00<00:00, 33.30it/s, loss=0.714, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 52:  95%|▉| 18/19 [00:00<00:00, 34.52it/s, loss=0.701, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 19/19 [00:00<00:00, 34.65it/s, loss=0.701, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 53:  95%|▉| 18/19 [00:00<00:00, 32.25it/s, loss=0.708, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 19/19 [00:00<00:00, 32.37it/s, loss=0.708, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 54:  95%|▉| 18/19 [00:00<00:00, 32.93it/s, loss=0.704, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 19/19 [00:00<00:00, 33.27it/s, loss=0.704, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 55:  95%|▉| 18/19 [00:00<00:00, 33.98it/s, loss=0.709, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 19/19 [00:00<00:00, 34.31it/s, loss=0.709, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 56:  95%|▉| 18/19 [00:00<00:00, 33.32it/s, loss=0.713, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 19/19 [00:00<00:00, 33.39it/s, loss=0.713, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 57:  95%|▉| 18/19 [00:00<00:00, 33.59it/s, loss=0.685, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 19/19 [00:00<00:00, 33.69it/s, loss=0.685, v_num=8fwn, LTC_val\u001b[A\n",
      "Epoch 58:  95%|▉| 18/19 [00:00<00:00, 31.56it/s, loss=0.688, v_num=8fwn, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.565. Signaling Trainer to stop.\n",
      "Epoch 58: 100%|█| 19/19 [00:00<00:00, 31.98it/s, loss=0.688, v_num=8fwn, LTC_val\n",
      "Epoch 58: 100%|█| 19/19 [00:00<00:00, 31.83it/s, loss=0.688, v_num=8fwn, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 48.38it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.6000000238418579,\n",
      " 'LTC_test_f1': 0.4571428596973419,\n",
      " 'test_loss': 0.8079367280006409}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 94959\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095905-xe4k8fwn/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_095905-xe4k8fwn/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.67539\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.67022\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.6815\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 58\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1062\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625900387\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 139\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.68254\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.56538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.65625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.65255\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.63152\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.45714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.80794\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▄▅▆▅▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▄▄▆▅▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▅▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▂▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▁▅▅▃▃▆▅▃▅▃▆▃▅▅█▃█▆█▆▆▅▆▆██▅▅█▆▆██▃▃▃▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▄▆▆▆▅▇▅▅▇▅▇▅▇▆█▅█▇█▇▇▆▇▇██▆▆█▇▇██▆▆▆▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▆▅▃▃▂▂▃▂▃▂▂▂▃▂▃▂▂▃▃▂▃▃▃▂▃▁▃▄▂▄▃▃▂▃▄▄▅▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▄▄█▄▂▆▁▄▅▁▅▅▄▆▃▅▆▆▄▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▄▄█▄▂▆▁▄▅▁▅▅▄▆▄▅▅▆▄▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▆▃▆▆▄█▃▄▆▆█▄▂▅▃▄▃▅▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/xe4k8fwn\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:00:15.360668: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/249m4suh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100014-249m4suh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 134 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 258   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "959 K     Trainable params\n",
      "0         Non-trainable params\n",
      "959 K     Total params\n",
      "3.837     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:00<00:00, 37.65it/s, loss=0.691, v_num=4suh, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.665\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 37.86it/s, loss=0.691, v_num=4suh, LTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 35.71it/s, loss=0.631, v_num=4suh, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.201 >= min_delta = 0.003. New best score: 0.464\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 35.73it/s, loss=0.631, v_num=4suh, LTC_val_\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 34.83it/s, loss=0.553, v_num=4suh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.169 >= min_delta = 0.003. New best score: 0.295\n",
      "Epoch 2: 100%|█| 19/19 [00:00<00:00, 35.06it/s, loss=0.553, v_num=4suh, LTC_val_\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 33.36it/s, loss=0.55, v_num=4suh, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 19/19 [00:00<00:00, 32.99it/s, loss=0.55, v_num=4suh, LTC_val_a\u001b[A\n",
      "Epoch 4:  95%|▉| 18/19 [00:00<00:00, 33.27it/s, loss=0.535, v_num=4suh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 19/19 [00:00<00:00, 33.28it/s, loss=0.535, v_num=4suh, LTC_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 18/19 [00:00<00:00, 33.30it/s, loss=0.515, v_num=4suh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 19/19 [00:00<00:00, 33.58it/s, loss=0.515, v_num=4suh, LTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 31.96it/s, loss=0.492, v_num=4suh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.282\n",
      "Epoch 6: 100%|█| 19/19 [00:00<00:00, 32.24it/s, loss=0.492, v_num=4suh, LTC_val_\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 33.45it/s, loss=0.497, v_num=4suh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.056 >= min_delta = 0.003. New best score: 0.226\n",
      "Epoch 7: 100%|█| 19/19 [00:00<00:00, 33.76it/s, loss=0.497, v_num=4suh, LTC_val_\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 33.69it/s, loss=0.498, v_num=4suh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 19/19 [00:00<00:00, 33.40it/s, loss=0.498, v_num=4suh, LTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 33.69it/s, loss=0.483, v_num=4suh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:00<00:00, 33.83it/s, loss=0.483, v_num=4suh, LTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 34.08it/s, loss=0.485, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:00<00:00, 34.19it/s, loss=0.485, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 32.31it/s, loss=0.495, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:00<00:00, 32.55it/s, loss=0.495, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 33.71it/s, loss=0.486, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:00<00:00, 33.86it/s, loss=0.486, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 34.57it/s, loss=0.483, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:00<00:00, 34.88it/s, loss=0.483, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 30.83it/s, loss=0.483, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:00<00:00, 31.16it/s, loss=0.483, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 34.86it/s, loss=0.482, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:00<00:00, 34.98it/s, loss=0.482, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 33.93it/s, loss=0.47, v_num=4suh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:00<00:00, 34.13it/s, loss=0.47, v_num=4suh, LTC_val_\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 33.18it/s, loss=0.458, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:00<00:00, 33.54it/s, loss=0.458, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 32.30it/s, loss=0.473, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.025 >= min_delta = 0.003. New best score: 0.201\n",
      "Epoch 18: 100%|█| 19/19 [00:00<00:00, 32.71it/s, loss=0.473, v_num=4suh, LTC_val\n",
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 32.41it/s, loss=0.478, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:00<00:00, 32.67it/s, loss=0.478, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 34.00it/s, loss=0.458, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:00<00:00, 33.99it/s, loss=0.458, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 29.10it/s, loss=0.459, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:00<00:00, 29.51it/s, loss=0.459, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:00<00:00, 33.36it/s, loss=0.452, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:00<00:00, 33.59it/s, loss=0.452, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 35.92it/s, loss=0.464, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:00<00:00, 36.16it/s, loss=0.464, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 33.16it/s, loss=0.46, v_num=4suh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:00<00:00, 33.44it/s, loss=0.46, v_num=4suh, LTC_val_\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 32.62it/s, loss=0.464, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:00<00:00, 32.72it/s, loss=0.464, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:00<00:00, 33.45it/s, loss=0.47, v_num=4suh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:00<00:00, 33.49it/s, loss=0.47, v_num=4suh, LTC_val_\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 34.64it/s, loss=0.464, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:00<00:00, 34.78it/s, loss=0.464, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 32.45it/s, loss=0.457, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:00<00:00, 32.90it/s, loss=0.457, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 32.35it/s, loss=0.455, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:00<00:00, 32.53it/s, loss=0.455, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 34.91it/s, loss=0.456, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:00<00:00, 35.27it/s, loss=0.456, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 34.59it/s, loss=0.443, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:00<00:00, 34.93it/s, loss=0.443, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:00<00:00, 32.26it/s, loss=0.451, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:00<00:00, 32.60it/s, loss=0.451, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 18/19 [00:00<00:00, 34.27it/s, loss=0.444, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:00<00:00, 34.55it/s, loss=0.444, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:00<00:00, 33.59it/s, loss=0.445, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:00<00:00, 33.84it/s, loss=0.445, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:00<00:00, 32.21it/s, loss=0.447, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:00<00:00, 32.21it/s, loss=0.447, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:00<00:00, 31.78it/s, loss=0.451, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 19/19 [00:00<00:00, 32.03it/s, loss=0.451, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 18/19 [00:00<00:00, 34.87it/s, loss=0.461, v_num=4suh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:00<00:00, 35.00it/s, loss=0.461, v_num=4suh, LTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:00<00:00, 34.46it/s, loss=0.46, v_num=4suh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.201. Signaling Trainer to stop.\n",
      "Epoch 38: 100%|█| 19/19 [00:00<00:00, 34.58it/s, loss=0.46, v_num=4suh, LTC_val_\n",
      "Epoch 38: 100%|█| 19/19 [00:00<00:00, 34.40it/s, loss=0.46, v_num=4suh, LTC_val_\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 83.44it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.800000011920929,\n",
      " 'LTC_test_f1': 0.7991071939468384,\n",
      " 'test_loss': 0.4441697299480438}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 95230\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100014-249m4suh/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100014-249m4suh/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.78098\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.77536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.46028\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 38\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 702\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625900444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.22548\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.74603\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.46419\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.79911\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.44417\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▄▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇██▇▇█▇██▇█████████▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▄▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇▇██▇██▇█████████▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▄▄▄▃▂▃▃▂▂▃▂▂▂▂▂▁▂▂▂▂▁▂▂▂▂▂▂▁▁▁▁▁▁▁▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▃█▆▁▆▆▆█▆▆▆▆██▆█▆▃█▆▆██▆▆▁▆█▆▆▃▆█▆▆▆▃██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁█▆▄▆▆▆█▆▆▆▆██▆█▆▅█▆▆██▆▆▄▆█▆▆▅▆█▆▆▆▅██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▅▂▅▃▃▂▁▄▃▃▃▂▂▃▃▄▄▁▃▃▂▂▄▂▅▂▃▃▂▄▃▂▃▃▃▄▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▂▄▁▁▄▄▄▅▃▅▄▆█▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▂▅▁▂▄▄▅▅▄▅▄▆█▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▄██▇▅▅▃▇▅▄▃▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/249m4suh\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:01:15.014678: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/2swpy668\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100113-2swpy668\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 134 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 387   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "959 K     Trainable params\n",
      "0         Non-trainable params\n",
      "959 K     Total params\n",
      "3.838     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:00<00:00, 36.32it/s, loss=1.11, v_num=y668, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.101\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 36.15it/s, loss=1.11, v_num=y668, LTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 36.60it/s, loss=1.04, v_num=y668, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.068 >= min_delta = 0.003. New best score: 1.033\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 36.25it/s, loss=1.04, v_num=y668, LTC_val_a\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 34.02it/s, loss=0.954, v_num=y668, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.204 >= min_delta = 0.003. New best score: 0.829\n",
      "Epoch 2: 100%|█| 19/19 [00:00<00:00, 34.19it/s, loss=0.954, v_num=y668, LTC_val_\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 34.67it/s, loss=0.905, v_num=y668, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 19/19 [00:00<00:00, 34.59it/s, loss=0.905, v_num=y668, LTC_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 18/19 [00:00<00:00, 32.97it/s, loss=0.877, v_num=y668, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.812\n",
      "Epoch 4: 100%|█| 19/19 [00:00<00:00, 33.16it/s, loss=0.877, v_num=y668, LTC_val_\n",
      "Epoch 5:  95%|▉| 18/19 [00:00<00:00, 34.56it/s, loss=0.876, v_num=y668, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.069 >= min_delta = 0.003. New best score: 0.743\n",
      "Epoch 5: 100%|█| 19/19 [00:00<00:00, 34.88it/s, loss=0.876, v_num=y668, LTC_val_\n",
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 35.21it/s, loss=0.855, v_num=y668, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.061 >= min_delta = 0.003. New best score: 0.681\n",
      "Epoch 6: 100%|█| 19/19 [00:00<00:00, 35.08it/s, loss=0.855, v_num=y668, LTC_val_\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 34.13it/s, loss=0.856, v_num=y668, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 19/19 [00:00<00:00, 34.09it/s, loss=0.856, v_num=y668, LTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 33.74it/s, loss=0.844, v_num=y668, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 19/19 [00:00<00:00, 33.97it/s, loss=0.844, v_num=y668, LTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 34.51it/s, loss=0.836, v_num=y668, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 0.670\n",
      "Epoch 9: 100%|█| 19/19 [00:00<00:00, 34.64it/s, loss=0.836, v_num=y668, LTC_val_\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 34.02it/s, loss=0.822, v_num=y668, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.040 >= min_delta = 0.003. New best score: 0.629\n",
      "Epoch 10: 100%|█| 19/19 [00:00<00:00, 34.21it/s, loss=0.822, v_num=y668, LTC_val\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 33.72it/s, loss=0.807, v_num=y668, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:00<00:00, 33.80it/s, loss=0.807, v_num=y668, LTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 34.69it/s, loss=0.797, v_num=y668, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:00<00:00, 34.96it/s, loss=0.797, v_num=y668, LTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 34.74it/s, loss=0.796, v_num=y668, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:00<00:00, 34.80it/s, loss=0.796, v_num=y668, LTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 33.65it/s, loss=0.79, v_num=y668, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:00<00:00, 33.82it/s, loss=0.79, v_num=y668, LTC_val_\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 36.70it/s, loss=0.787, v_num=y668, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:00<00:00, 36.93it/s, loss=0.787, v_num=y668, LTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 35.66it/s, loss=0.853, v_num=y668, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:00<00:00, 35.99it/s, loss=0.853, v_num=y668, LTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 34.37it/s, loss=0.832, v_num=y668, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:00<00:00, 34.37it/s, loss=0.832, v_num=y668, LTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 33.27it/s, loss=0.798, v_num=y668, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:00<00:00, 33.31it/s, loss=0.798, v_num=y668, LTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 31.56it/s, loss=0.787, v_num=y668, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:00<00:00, 31.72it/s, loss=0.787, v_num=y668, LTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 34.44it/s, loss=0.803, v_num=y668, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:00<00:00, 34.81it/s, loss=0.803, v_num=y668, LTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 35.47it/s, loss=0.769, v_num=y668, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:00<00:00, 35.48it/s, loss=0.769, v_num=y668, LTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:00<00:00, 29.69it/s, loss=0.815, v_num=y668, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:00<00:00, 30.12it/s, loss=0.815, v_num=y668, LTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 34.15it/s, loss=0.791, v_num=y668, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:00<00:00, 33.57it/s, loss=0.791, v_num=y668, LTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 37.04it/s, loss=0.79, v_num=y668, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:00<00:00, 36.16it/s, loss=0.79, v_num=y668, LTC_val_\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 35.55it/s, loss=0.781, v_num=y668, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:00<00:00, 35.68it/s, loss=0.781, v_num=y668, LTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:00<00:00, 33.72it/s, loss=0.764, v_num=y668, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:00<00:00, 34.03it/s, loss=0.764, v_num=y668, LTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 33.67it/s, loss=0.782, v_num=y668, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:00<00:00, 33.98it/s, loss=0.782, v_num=y668, LTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 33.25it/s, loss=0.803, v_num=y668, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:00<00:00, 33.41it/s, loss=0.803, v_num=y668, LTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 34.28it/s, loss=0.768, v_num=y668, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:00<00:00, 33.97it/s, loss=0.768, v_num=y668, LTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 34.89it/s, loss=0.761, v_num=y668, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.629. Signaling Trainer to stop.\n",
      "Epoch 30: 100%|█| 19/19 [00:00<00:00, 35.26it/s, loss=0.761, v_num=y668, LTC_val\n",
      "Epoch 30: 100%|█| 19/19 [00:00<00:00, 33.90it/s, loss=0.761, v_num=y668, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 67.71it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.6000000238418579,\n",
      " 'LTC_test_f1': 0.5174603462219238,\n",
      " 'test_loss': 0.8021482825279236}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 95433\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100113-2swpy668/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100113-2swpy668/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.64136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.63092\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.77101\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 558\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625900499\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 73\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.72222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.62671\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.64062\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.64642\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.82938\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.51746\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.80215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▄▅▆▆▇▆▇▇▇▇▇█▇▇█▇▇████▇█████▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▄▅▆▆▇▇▇▇▇▇▇█▇▇█▇▇██▇█▇█████▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▅▄▃▃▃▃▃▂▂▂▂▂▂▂▃▂▂▁▁▁▂▁▁▁▁▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▄▄▃▄▆▄▆▄▄▆▄▇▇▇▄▄▄▆█▇▄▆▇▇▇▇▆▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▅▅▅▅▇▆▇▅▆▇▆▇▇▇▅▆▆▇█▇▅▇▇▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▇▄▄▄▃▂▃▂▂▁▂▂▁▂▂▃▃▂▂▂▄▂▂▂▂▃▁▃▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▇▃▅▇▃▃██▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▇▄▅▆▃▃█▇▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▄█▃▃█▄▁▃▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/2swpy668\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:02:07.614003: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/2eaqs6ti\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100206-2eaqs6ti\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 219 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 258   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.177     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 21.57it/s, loss=0.708, v_num=s6ti, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.714\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 21.18it/s, loss=0.708, v_num=s6ti, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 21.86it/s, loss=0.693, v_num=s6ti, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.030 >= min_delta = 0.003. New best score: 0.684\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 21.41it/s, loss=0.693, v_num=s6ti, BTC_val_\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 21.11it/s, loss=0.684, v_num=s6ti, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:01<00:00, 20.73it/s, loss=0.684, v_num=s6ti, BTC_val_\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 21.34it/s, loss=0.674, v_num=s6ti, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:01<00:00, 20.92it/s, loss=0.674, v_num=s6ti, BTC_val_\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 21.24it/s, loss=0.63, v_num=s6ti, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 0.673\n",
      "Epoch 4: 100%|█| 21/21 [00:01<00:00, 20.82it/s, loss=0.63, v_num=s6ti, BTC_val_a\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 21.81it/s, loss=0.575, v_num=s6ti, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 21.36it/s, loss=0.575, v_num=s6ti, BTC_val_\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 21.11it/s, loss=0.526, v_num=s6ti, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.047 >= min_delta = 0.003. New best score: 0.627\n",
      "Epoch 6: 100%|█| 21/21 [00:01<00:00, 20.72it/s, loss=0.526, v_num=s6ti, BTC_val_\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 21.74it/s, loss=0.526, v_num=s6ti, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 21.30it/s, loss=0.526, v_num=s6ti, BTC_val_\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:01<00:00, 20.55it/s, loss=0.509, v_num=s6ti, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:01<00:00, 20.18it/s, loss=0.509, v_num=s6ti, BTC_val_\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:01<00:00, 20.11it/s, loss=0.487, v_num=s6ti, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.059 >= min_delta = 0.003. New best score: 0.568\n",
      "Epoch 9: 100%|█| 21/21 [00:01<00:00, 19.65it/s, loss=0.487, v_num=s6ti, BTC_val_\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 20.90it/s, loss=0.462, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 21.10it/s, loss=0.462, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 21.25it/s, loss=0.451, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:01<00:00, 20.76it/s, loss=0.451, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 21.60it/s, loss=0.435, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.037 >= min_delta = 0.003. New best score: 0.530\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 21.18it/s, loss=0.435, v_num=s6ti, BTC_val\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 21.17it/s, loss=0.422, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:01<00:00, 20.69it/s, loss=0.422, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 21.61it/s, loss=0.442, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 21.18it/s, loss=0.442, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 21.31it/s, loss=0.445, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:01<00:00, 20.88it/s, loss=0.445, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 21.38it/s, loss=0.434, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:01<00:00, 20.84it/s, loss=0.434, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 22.09it/s, loss=0.419, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 21.58it/s, loss=0.419, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 21.84it/s, loss=0.397, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 21.36it/s, loss=0.397, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 21.86it/s, loss=0.399, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 21.36it/s, loss=0.399, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 22.14it/s, loss=0.395, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 21.60it/s, loss=0.395, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 21.66it/s, loss=0.391, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.513\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 21.08it/s, loss=0.391, v_num=s6ti, BTC_val\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 21.67it/s, loss=0.38, v_num=s6ti, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 21.25it/s, loss=0.38, v_num=s6ti, BTC_val_\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 21.96it/s, loss=0.376, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 21.48it/s, loss=0.376, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 21.52it/s, loss=0.37, v_num=s6ti, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 21.11it/s, loss=0.37, v_num=s6ti, BTC_val_\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 21.19it/s, loss=0.374, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:01<00:00, 20.76it/s, loss=0.374, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 21.25it/s, loss=0.362, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:01<00:00, 20.86it/s, loss=0.362, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 21.25it/s, loss=0.36, v_num=s6ti, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:01<00:00, 20.81it/s, loss=0.36, v_num=s6ti, BTC_val_\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 21.11it/s, loss=0.352, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:01<00:00, 20.72it/s, loss=0.352, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 21.01it/s, loss=0.343, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:01<00:00, 20.52it/s, loss=0.343, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 21.52it/s, loss=0.33, v_num=s6ti, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 21.11it/s, loss=0.33, v_num=s6ti, BTC_val_\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:01<00:00, 20.66it/s, loss=0.341, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:01<00:00, 20.14it/s, loss=0.341, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:01<00:00, 20.91it/s, loss=0.337, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:01<00:00, 20.51it/s, loss=0.337, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 21.71it/s, loss=0.336, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 21.06it/s, loss=0.336, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 21.59it/s, loss=0.353, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 21.18it/s, loss=0.353, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:01<00:00, 20.68it/s, loss=0.341, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:01<00:00, 20.27it/s, loss=0.341, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 20.95it/s, loss=0.314, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 21.16it/s, loss=0.314, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 21.14it/s, loss=0.316, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:01<00:00, 20.66it/s, loss=0.316, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 21.99it/s, loss=0.336, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 21.56it/s, loss=0.336, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 21.12it/s, loss=0.306, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:01<00:00, 20.69it/s, loss=0.306, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 21.76it/s, loss=0.333, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 21.31it/s, loss=0.333, v_num=s6ti, BTC_val\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 21.14it/s, loss=0.309, v_num=s6ti, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.513. Signaling Trainer to stop.\n",
      "Epoch 41: 100%|█| 21/21 [00:01<00:00, 20.72it/s, loss=0.309, v_num=s6ti, BTC_val\n",
      "Epoch 41: 100%|█| 21/21 [00:01<00:00, 20.66it/s, loss=0.309, v_num=s6ti, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 35.71it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6969696879386902,\n",
      " 'BTC_test_f1': 0.682692289352417,\n",
      " 'ETH_test_acc': 0.6060606241226196,\n",
      " 'ETH_test_f1': 0.6046082973480225,\n",
      " 'test_loss': 0.6049979329109192}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 95635\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100206-2eaqs6ti/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100206-2eaqs6ti/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.8576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.85584\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.86714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.8662\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.30847\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 41\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 840\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625900576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.46154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.31579\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.53846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.35\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.58677\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.80488\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.80476\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.90244\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.90097\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.3175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.69697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.68269\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.60606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.60461\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.605\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▂▃▄▅▆▆▆▆▇▆▇▇▇▆▇▇▇▇▇█▇▇▇▇▇▇████▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▂▃▄▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇████▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▁▂▂▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▇██▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▁▂▂▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▇██▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ███▇▇▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁█▁▅▃▆▃▁▅▅▃▆▃▁▁▅▃▁▃▆▅▁▆▅▅▅▅▃▅▁▃▁▅▃▅▃▅▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▃█▁▄▂▇▃▁▄▆▃▇▃▁▁▆▄▃▃▇▅▁▆▆▆▆▆▃▅▁▄▁▆▃▆▃▆▄▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▂▂▁▂▂▁▃▂▂█▃▃█▃▂▂▅▆▆▂▇█▂█▆█▅▅▃▃▂▅▃▆▂▅▂▅▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▂▁▃▁▃▁▁█▃▃█▃▁▁▅▆▆▁▇█▁█▆█▅▅▃▃▁▅▃▆▃▅▃▅▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▂▂▂▂▂▂▂▂▂▁▁▂▁▂▂▃▂▁▁▄▁▁▃▁▂▁▂▂▃▄█▂▅▂▆▃▃▃▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▂▂▃▇▆▇█▅▇▆▇█▆▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▂▃▄▇▆▇█▆▇▆▇█▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▃▂▆▆▅▆█▅▇▇▅▆▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▃▂▆▆▅▆█▅▇▇▅▆▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▇█▆▃▅▃▂▄▁▂▃▃▃▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/2eaqs6ti\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:03:28.646591: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/s8bg4cpu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100327-s8bg4cpu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 219 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 387   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.178     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 21.76it/s, loss=1.11, v_num=4cpu, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.104\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 21.34it/s, loss=1.11, v_num=4cpu, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 22.82it/s, loss=1.08, v_num=4cpu, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 22.39it/s, loss=1.08, v_num=4cpu, BTC_val_a\u001b[A\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 21.94it/s, loss=1.08, v_num=4cpu, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 22.14it/s, loss=1.08, v_num=4cpu, BTC_val_a\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:01<00:00, 20.95it/s, loss=1.06, v_num=4cpu, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 1.093\n",
      "Epoch 3: 100%|█| 21/21 [00:01<00:00, 20.53it/s, loss=1.06, v_num=4cpu, BTC_val_a\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 21.35it/s, loss=1.02, v_num=4cpu, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:01<00:00, 20.92it/s, loss=1.02, v_num=4cpu, BTC_val_a\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:01<00:00, 20.38it/s, loss=0.943, v_num=4cpu, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:01<00:00, 20.00it/s, loss=0.943, v_num=4cpu, BTC_val_\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 21.07it/s, loss=0.87, v_num=4cpu, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:01<00:00, 20.64it/s, loss=0.87, v_num=4cpu, BTC_val_a\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:01<00:00, 20.95it/s, loss=0.845, v_num=4cpu, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:01<00:00, 20.56it/s, loss=0.845, v_num=4cpu, BTC_val_\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:01<00:00, 19.92it/s, loss=0.802, v_num=4cpu, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.122 >= min_delta = 0.003. New best score: 0.971\n",
      "Epoch 8: 100%|█| 21/21 [00:01<00:00, 19.49it/s, loss=0.802, v_num=4cpu, BTC_val_\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 21.24it/s, loss=0.803, v_num=4cpu, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:01<00:00, 20.84it/s, loss=0.803, v_num=4cpu, BTC_val_\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:01<00:00, 20.17it/s, loss=0.805, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:01<00:00, 19.70it/s, loss=0.805, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 21.04it/s, loss=0.751, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 0.961\n",
      "Epoch 11: 100%|█| 21/21 [00:01<00:00, 20.62it/s, loss=0.751, v_num=4cpu, BTC_val\n",
      "Epoch 12: 100%|█| 21/21 [00:01<00:00, 20.89it/s, loss=0.765, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:01<00:00, 20.44it/s, loss=0.765, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:01<00:00, 20.76it/s, loss=0.746, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.062 >= min_delta = 0.003. New best score: 0.899\n",
      "Epoch 13: 100%|█| 21/21 [00:01<00:00, 20.36it/s, loss=0.746, v_num=4cpu, BTC_val\n",
      "Epoch 14: 100%|█| 21/21 [00:01<00:00, 20.11it/s, loss=0.741, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:01<00:00, 19.66it/s, loss=0.741, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 22.33it/s, loss=0.716, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.895\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 21.88it/s, loss=0.716, v_num=4cpu, BTC_val\n",
      "Epoch 16: 100%|█| 21/21 [00:01<00:00, 20.73it/s, loss=0.696, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:01<00:00, 20.28it/s, loss=0.696, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 21.13it/s, loss=0.747, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:01<00:00, 20.71it/s, loss=0.747, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 21.10it/s, loss=0.726, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:01<00:00, 20.66it/s, loss=0.726, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 21.18it/s, loss=0.703, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:01<00:00, 20.77it/s, loss=0.703, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:01<00:00, 20.50it/s, loss=0.696, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:01<00:00, 20.08it/s, loss=0.696, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 21.77it/s, loss=0.68, v_num=4cpu, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 21.33it/s, loss=0.68, v_num=4cpu, BTC_val_\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:01<00:00, 20.67it/s, loss=0.679, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:01<00:00, 20.26it/s, loss=0.679, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 21.82it/s, loss=0.671, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 21.41it/s, loss=0.671, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:01<00:00, 20.66it/s, loss=0.666, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:01<00:00, 20.08it/s, loss=0.666, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 21.94it/s, loss=0.654, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 21.51it/s, loss=0.654, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:01<00:00, 20.23it/s, loss=0.646, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:01<00:00, 19.84it/s, loss=0.646, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 21.45it/s, loss=0.631, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.889\n",
      "Epoch 27: 100%|█| 21/21 [00:01<00:00, 20.84it/s, loss=0.631, v_num=4cpu, BTC_val\n",
      "Epoch 28: 100%|█| 21/21 [00:01<00:00, 20.95it/s, loss=0.628, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.020 >= min_delta = 0.003. New best score: 0.869\n",
      "Epoch 28: 100%|█| 21/21 [00:01<00:00, 20.51it/s, loss=0.628, v_num=4cpu, BTC_val\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 22.04it/s, loss=0.662, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 21.57it/s, loss=0.662, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:01<00:00, 20.59it/s, loss=0.624, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 0.858\n",
      "Epoch 30: 100%|█| 21/21 [00:01<00:00, 19.98it/s, loss=0.624, v_num=4cpu, BTC_val\n",
      "Epoch 31: 100%|█| 21/21 [00:01<00:00, 20.85it/s, loss=0.607, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:01<00:00, 20.34it/s, loss=0.607, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:01<00:00, 20.39it/s, loss=0.594, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:01<00:00, 20.01it/s, loss=0.594, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 21.23it/s, loss=0.579, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:01<00:00, 20.82it/s, loss=0.579, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:01<00:00, 20.56it/s, loss=0.58, v_num=4cpu, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:01<00:00, 20.18it/s, loss=0.58, v_num=4cpu, BTC_val_\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 21.02it/s, loss=0.604, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:01<00:00, 20.47it/s, loss=0.604, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:01<00:00, 20.53it/s, loss=0.562, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:01<00:00, 20.15it/s, loss=0.562, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:01<00:00, 20.87it/s, loss=0.602, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.850\n",
      "Epoch 37: 100%|█| 21/21 [00:01<00:00, 20.40it/s, loss=0.602, v_num=4cpu, BTC_val\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 21.32it/s, loss=0.546, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:01<00:00, 20.91it/s, loss=0.546, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:01<00:00, 20.61it/s, loss=0.526, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.137 >= min_delta = 0.003. New best score: 0.712\n",
      "Epoch 39: 100%|█| 21/21 [00:01<00:00, 20.20it/s, loss=0.526, v_num=4cpu, BTC_val\n",
      "Epoch 40: 100%|█| 21/21 [00:01<00:00, 20.88it/s, loss=0.507, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:01<00:00, 20.41it/s, loss=0.507, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:01<00:00, 20.47it/s, loss=0.52, v_num=4cpu, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:01<00:00, 20.08it/s, loss=0.52, v_num=4cpu, BTC_val_\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:01<00:00, 20.53it/s, loss=0.539, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:01<00:00, 20.12it/s, loss=0.539, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:01<00:00, 20.83it/s, loss=0.503, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:01<00:00, 20.27it/s, loss=0.503, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 21.69it/s, loss=0.503, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 21.26it/s, loss=0.503, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 21.19it/s, loss=0.478, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:01<00:00, 20.68it/s, loss=0.478, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 21.81it/s, loss=0.494, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 21.38it/s, loss=0.494, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 21.03it/s, loss=0.479, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:01<00:00, 20.53it/s, loss=0.479, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 21.48it/s, loss=0.455, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 21.03it/s, loss=0.455, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 49: 100%|█| 21/21 [00:01<00:00, 20.03it/s, loss=0.429, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 21/21 [00:01<00:00, 19.63it/s, loss=0.429, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 50: 100%|█| 21/21 [00:00<00:00, 21.40it/s, loss=0.419, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|█| 21/21 [00:01<00:00, 20.98it/s, loss=0.419, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 51: 100%|█| 21/21 [00:01<00:00, 20.90it/s, loss=0.425, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 21/21 [00:01<00:00, 20.45it/s, loss=0.425, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 52: 100%|█| 21/21 [00:01<00:00, 20.50it/s, loss=0.424, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 21/21 [00:01<00:00, 20.10it/s, loss=0.424, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 53: 100%|█| 21/21 [00:00<00:00, 21.07it/s, loss=0.433, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 21/21 [00:01<00:00, 20.49it/s, loss=0.433, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 54: 100%|█| 21/21 [00:01<00:00, 20.50it/s, loss=0.408, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 21/21 [00:01<00:00, 20.11it/s, loss=0.408, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 55: 100%|█| 21/21 [00:01<00:00, 19.88it/s, loss=0.407, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 21/21 [00:01<00:00, 19.49it/s, loss=0.407, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 56: 100%|█| 21/21 [00:00<00:00, 22.64it/s, loss=0.403, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 21/21 [00:00<00:00, 22.19it/s, loss=0.403, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 57: 100%|█| 21/21 [00:01<00:00, 20.55it/s, loss=0.408, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 21/21 [00:01<00:00, 20.14it/s, loss=0.408, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 58: 100%|█| 21/21 [00:01<00:00, 20.19it/s, loss=0.407, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 21/21 [00:01<00:00, 19.72it/s, loss=0.407, v_num=4cpu, BTC_val\u001b[A\n",
      "Epoch 59: 100%|█| 21/21 [00:00<00:00, 21.07it/s, loss=0.364, v_num=4cpu, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.712. Signaling Trainer to stop.\n",
      "Epoch 59: 100%|█| 21/21 [00:01<00:00, 20.58it/s, loss=0.364, v_num=4cpu, BTC_val\n",
      "Epoch 59: 100%|█| 21/21 [00:01<00:00, 20.51it/s, loss=0.364, v_num=4cpu, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 36.10it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6363636255264282,\n",
      " 'BTC_test_f1': 0.6393345594406128,\n",
      " 'ETH_test_acc': 0.6060606241226196,\n",
      " 'ETH_test_f1': 0.596779465675354,\n",
      " 'test_loss': 0.7749084234237671}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 95865\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100327-s8bg4cpu/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100327-s8bg4cpu/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.84566\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.84378\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.83691\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.83256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.36277\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 59\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 70\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625900677\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 144\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.69231\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.68687\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.38462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.36667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.91324\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.80488\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.79798\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.82927\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.81886\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.40371\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.63636\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.63933\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.60606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.59678\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.77491\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▂▃▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▆▇▇▇▇▇▇▇██▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▃▃▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▇▆▇▇▇▆▇▇▇▇▇▇▇██▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▁▂▂▄▄▅▅▅▅▆▆▅▆▆▆▆▆▆▆▆▆▇▇▇▆▇▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▂▃▄▄▅▅▅▅▆▆▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ███▇▆▆▅▅▅▅▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▅▂▂▂▄▃▂▂▃▅▄▂▃▃▃▂▄▃█▄▃▅▄▅▄▅██▆▃▅▃▆▇▂▃▆▁▃▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▄▁▁▁▄▃▁▁▃▅▄▁▃▃▃▁▄▃█▅▃▅▄▅▄▄██▆▃▅▃▆▇▂▃▆▁▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▂▂▂▃▃▃▃▂▅▅▃█▆▅▂▄▆▅▇▆▅▄▆▇▇▆▇█▄▄▅▅▅▄▃▆▃▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▁▁▄▄▃▄▃▅▅▄█▆▅▁▄▆▅▇▆▅▄▆▇▅▆▇▇▅▄▅▅▅▅▃▆▄▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄▄▄▄▄▄▄▄▄▂▂▄▃▃▃█▄▃▂▂▂▃▃▃▃▂▁▂▂▄▄▂▂▂▄▆▂▅▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▂▄▅▄▃▆▄▆▅▆▅▅▅▆▇▆▇▆▇▆█▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▂▄▅▄▃▆▄▆▅▆▅▅▅▆▇▆▇▆▇▆█▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▂▄▃▅▄▆▆▅▅▆▆▇█▇▇██▆█▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▂▄▃▅▄▆▆▅▅▆▆▇▇▇▇██▆█▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▇▆▅▅▆▄▅▃▅▄▄▄▃▂▂▂▂▃▁▂▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/s8bg4cpu\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:05:08.004843: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/lde8ogy4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100506-lde8ogy4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 219 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 258   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.177     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 21.78it/s, loss=0.703, v_num=ogy4, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.713\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 21.99it/s, loss=0.703, v_num=ogy4, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 21.47it/s, loss=0.695, v_num=ogy4, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 21.04it/s, loss=0.695, v_num=ogy4, BTC_val_\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:01<00:00, 20.72it/s, loss=0.679, v_num=ogy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 0.701\n",
      "Epoch 2: 100%|█| 21/21 [00:01<00:00, 20.30it/s, loss=0.679, v_num=ogy4, BTC_val_\n",
      "Epoch 3: 100%|█| 21/21 [00:01<00:00, 20.99it/s, loss=0.659, v_num=ogy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:01<00:00, 20.56it/s, loss=0.659, v_num=ogy4, BTC_val_\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:01<00:00, 20.75it/s, loss=0.615, v_num=ogy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:01<00:00, 20.33it/s, loss=0.615, v_num=ogy4, BTC_val_\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:01<00:00, 20.86it/s, loss=0.576, v_num=ogy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.061 >= min_delta = 0.003. New best score: 0.640\n",
      "Epoch 5: 100%|█| 21/21 [00:01<00:00, 20.32it/s, loss=0.576, v_num=ogy4, BTC_val_\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 21.65it/s, loss=0.55, v_num=ogy4, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 21.23it/s, loss=0.55, v_num=ogy4, BTC_val_a\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:01<00:00, 20.52it/s, loss=0.511, v_num=ogy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 0.620\n",
      "Epoch 7: 100%|█| 21/21 [00:01<00:00, 20.01it/s, loss=0.511, v_num=ogy4, BTC_val_\n",
      "Epoch 8: 100%|█| 21/21 [00:01<00:00, 20.87it/s, loss=0.487, v_num=ogy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:01<00:00, 20.48it/s, loss=0.487, v_num=ogy4, BTC_val_\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:01<00:00, 19.35it/s, loss=0.473, v_num=ogy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.022 >= min_delta = 0.003. New best score: 0.598\n",
      "Epoch 9: 100%|█| 21/21 [00:01<00:00, 18.96it/s, loss=0.473, v_num=ogy4, BTC_val_\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 21.79it/s, loss=0.457, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 21.36it/s, loss=0.457, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:01<00:00, 20.41it/s, loss=0.471, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.585\n",
      "Epoch 11: 100%|█| 21/21 [00:01<00:00, 20.03it/s, loss=0.471, v_num=ogy4, BTC_val\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 21.98it/s, loss=0.435, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 21.51it/s, loss=0.435, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:01<00:00, 20.74it/s, loss=0.439, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.031 >= min_delta = 0.003. New best score: 0.553\n",
      "Epoch 13: 100%|█| 21/21 [00:01<00:00, 20.35it/s, loss=0.439, v_num=ogy4, BTC_val\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 22.26it/s, loss=0.441, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 21.79it/s, loss=0.441, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 21.33it/s, loss=0.425, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:01<00:00, 20.92it/s, loss=0.425, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 21.02it/s, loss=0.416, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:01<00:00, 20.49it/s, loss=0.416, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:01<00:00, 20.72it/s, loss=0.421, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:01<00:00, 20.34it/s, loss=0.421, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 21.02it/s, loss=0.41, v_num=ogy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:01<00:00, 20.58it/s, loss=0.41, v_num=ogy4, BTC_val_\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:01<00:00, 20.97it/s, loss=0.394, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:01<00:00, 20.45it/s, loss=0.394, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 21.67it/s, loss=0.388, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 21.18it/s, loss=0.388, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 21.17it/s, loss=0.378, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:01<00:00, 20.76it/s, loss=0.378, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:01<00:00, 20.57it/s, loss=0.396, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:01<00:00, 20.05it/s, loss=0.396, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 21.26it/s, loss=0.377, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:01<00:00, 20.60it/s, loss=0.377, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 21.31it/s, loss=0.382, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.545\n",
      "Epoch 24: 100%|█| 21/21 [00:01<00:00, 20.70it/s, loss=0.382, v_num=ogy4, BTC_val\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 21.07it/s, loss=0.369, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:01<00:00, 20.66it/s, loss=0.369, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 22.33it/s, loss=0.364, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 21.66it/s, loss=0.364, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:01<00:00, 20.89it/s, loss=0.366, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:01<00:00, 20.30it/s, loss=0.366, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 21.26it/s, loss=0.361, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:01<00:00, 20.83it/s, loss=0.361, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:01<00:00, 20.54it/s, loss=0.35, v_num=ogy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:01<00:00, 20.15it/s, loss=0.35, v_num=ogy4, BTC_val_\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 21.02it/s, loss=0.353, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:01<00:00, 20.62it/s, loss=0.353, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:01<00:00, 20.59it/s, loss=0.336, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:01<00:00, 20.20it/s, loss=0.336, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:01<00:00, 20.99it/s, loss=0.331, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:01<00:00, 20.58it/s, loss=0.331, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 21.76it/s, loss=0.331, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 21.27it/s, loss=0.331, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:01<00:00, 20.67it/s, loss=0.336, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:01<00:00, 20.27it/s, loss=0.336, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:01<00:00, 20.72it/s, loss=0.341, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:01<00:00, 20.33it/s, loss=0.341, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:01<00:00, 20.97it/s, loss=0.32, v_num=ogy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:01<00:00, 20.44it/s, loss=0.32, v_num=ogy4, BTC_val_\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 21.36it/s, loss=0.311, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:01<00:00, 20.94it/s, loss=0.311, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:01<00:00, 20.50it/s, loss=0.316, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:01<00:00, 20.11it/s, loss=0.316, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 21.94it/s, loss=0.316, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 21.53it/s, loss=0.316, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 21.55it/s, loss=0.317, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:01<00:00, 20.99it/s, loss=0.317, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 22.42it/s, loss=0.291, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 21.96it/s, loss=0.291, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 21.11it/s, loss=0.272, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:01<00:00, 20.68it/s, loss=0.272, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 21.73it/s, loss=0.323, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 21.31it/s, loss=0.323, v_num=ogy4, BTC_val\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 21.20it/s, loss=0.303, v_num=ogy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.545. Signaling Trainer to stop.\n",
      "Epoch 44: 100%|█| 21/21 [00:01<00:00, 20.78it/s, loss=0.303, v_num=ogy4, BTC_val\n",
      "Epoch 44: 100%|█| 21/21 [00:01<00:00, 20.72it/s, loss=0.303, v_num=ogy4, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 46.95it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6060606241226196,\n",
      " 'BTC_test_f1': 0.5662285089492798,\n",
      " 'ETH_test_acc': 0.6363636255264282,\n",
      " 'ETH_test_f1': 0.6278195381164551,\n",
      " 'test_loss': 0.623695969581604}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 96176\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100506-lde8ogy4/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100506-lde8ogy4/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.85282\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.85023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.86555\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.86319\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.30283\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 44\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 900\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 54\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625900760\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 108\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.53846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.5125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.69231\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.63889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.76901\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.90244\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.88736\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.87805\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.87322\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.28682\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.60606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.56623\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.63636\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.62782\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.6237\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▁▂▃▃▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▁▂▃▃▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▁▂▃▄▅▅▆▆▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▁▂▃▃▅▅▆▆▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ███▇▇▆▆▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▃▁▃▁▆▆▁▆▃█▁█▁▁▃▃▃▃▆▃▁▆▃▆▃▃▁▁█▆▃▆▃▁▃▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▄▁▂▁▇▅▁▇▅█▁▇▁▁▅▅▅▅▆▅▁▇▅▇▅▅▄▄█▇▅▅▅▁▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▁▁▁▁█▁▅█▁▅▂▅▁▇▁▁▅▁▁▇▂▇▂█▄▄▅▄▅▅▇▅▄█▇▂▅▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▁▁▁█▁▆█▁▆▃▆▁▇▁▁▆▂▂▇▃▇▃█▅▅▆▅▆▆▇▆▅█▇▃▆▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▃▃▃▃█▂▃▂▂▇▁▃▁▃▂▅▃▁▂▂▁▄▁▆▂▂▂▁▂▁▁▁▄▁▃▁▅▂▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▃▇▆▆▆▄▆▆▆▆█▇▇▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▃▇▆▆▆▄▆▅▆▆█▇▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▂▅▅▅▅▅▆▇▄▅▄▆▇▆█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▂▅▅▅▅▅▆▇▄▅▄▆▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ██▄▄▄▄▅▃▃▅▃▄▄▃▃▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/lde8ogy4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:06:31.498567: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/2o9wdjoy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100630-2o9wdjoy\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 219 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 387   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.178     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 22.39it/s, loss=1.11, v_num=djoy, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.116\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 21.97it/s, loss=1.11, v_num=djoy, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 22.02it/s, loss=1.09, v_num=djoy, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 21.47it/s, loss=1.09, v_num=djoy, BTC_val_a\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:01<00:00, 20.97it/s, loss=1.08, v_num=djoy, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:01<00:00, 20.58it/s, loss=1.08, v_num=djoy, BTC_val_a\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 22.04it/s, loss=1.07, v_num=djoy, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 21.56it/s, loss=1.07, v_num=djoy, BTC_val_a\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:01<00:00, 20.78it/s, loss=1.03, v_num=djoy, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:01<00:00, 20.34it/s, loss=1.03, v_num=djoy, BTC_val_a\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 21.60it/s, loss=0.949, v_num=djoy, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 21.15it/s, loss=0.949, v_num=djoy, BTC_val_\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:01<00:00, 20.97it/s, loss=0.887, v_num=djoy, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:01<00:00, 20.58it/s, loss=0.887, v_num=djoy, BTC_val_\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 21.48it/s, loss=0.859, v_num=djoy, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 21.06it/s, loss=0.859, v_num=djoy, BTC_val_\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:01<00:00, 20.01it/s, loss=0.821, v_num=djoy, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:01<00:00, 19.64it/s, loss=0.821, v_num=djoy, BTC_val_\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 21.63it/s, loss=0.805, v_num=djoy, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 21.05it/s, loss=0.805, v_num=djoy, BTC_val_\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 21.56it/s, loss=0.784, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.128 >= min_delta = 0.003. New best score: 0.988\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 21.14it/s, loss=0.784, v_num=djoy, BTC_val\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 22.27it/s, loss=0.76, v_num=djoy, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 21.80it/s, loss=0.76, v_num=djoy, BTC_val_\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:01<00:00, 20.92it/s, loss=0.781, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:01<00:00, 20.54it/s, loss=0.781, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 22.09it/s, loss=0.742, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 21.63it/s, loss=0.742, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 21.41it/s, loss=0.724, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:01<00:00, 20.99it/s, loss=0.724, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 21.76it/s, loss=0.719, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.090 >= min_delta = 0.003. New best score: 0.898\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 21.31it/s, loss=0.719, v_num=djoy, BTC_val\n",
      "Epoch 16: 100%|█| 21/21 [00:01<00:00, 20.28it/s, loss=0.698, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:01<00:00, 19.91it/s, loss=0.698, v_num=djoy, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 22.18it/s, loss=0.699, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 21.72it/s, loss=0.699, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 21.54it/s, loss=0.683, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 21.08it/s, loss=0.683, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 21.74it/s, loss=0.694, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.018 >= min_delta = 0.003. New best score: 0.879\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 21.26it/s, loss=0.694, v_num=djoy, BTC_val\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 20.68it/s, loss=0.676, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:01<00:00, 20.88it/s, loss=0.676, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 21.37it/s, loss=0.671, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:01<00:00, 20.88it/s, loss=0.671, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:01<00:00, 20.66it/s, loss=0.644, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:01<00:00, 20.26it/s, loss=0.644, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 21.25it/s, loss=0.62, v_num=djoy, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:01<00:00, 20.83it/s, loss=0.62, v_num=djoy, BTC_val_\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:01<00:00, 20.62it/s, loss=0.628, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 0.867\n",
      "Epoch 24: 100%|█| 21/21 [00:01<00:00, 20.21it/s, loss=0.628, v_num=djoy, BTC_val\n",
      "Epoch 25: 100%|█| 21/21 [00:01<00:00, 20.94it/s, loss=0.621, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.020 >= min_delta = 0.003. New best score: 0.848\n",
      "Epoch 25: 100%|█| 21/21 [00:01<00:00, 20.38it/s, loss=0.621, v_num=djoy, BTC_val\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 21.79it/s, loss=0.619, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 21.36it/s, loss=0.619, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 21.49it/s, loss=0.63, v_num=djoy, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:01<00:00, 20.95it/s, loss=0.63, v_num=djoy, BTC_val_\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 21.21it/s, loss=0.608, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:01<00:00, 20.82it/s, loss=0.608, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 21.78it/s, loss=0.596, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 21.33it/s, loss=0.596, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 21.43it/s, loss=0.585, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 21.02it/s, loss=0.585, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 21.32it/s, loss=0.569, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:01<00:00, 20.89it/s, loss=0.569, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 21.48it/s, loss=0.576, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 21.02it/s, loss=0.576, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:01<00:00, 20.81it/s, loss=0.561, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:01<00:00, 20.25it/s, loss=0.561, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 21.31it/s, loss=0.557, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:01<00:00, 20.91it/s, loss=0.557, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 21.21it/s, loss=0.535, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:01<00:00, 20.79it/s, loss=0.535, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 21.36it/s, loss=0.553, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:01<00:00, 20.82it/s, loss=0.553, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 21.47it/s, loss=0.532, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 21.04it/s, loss=0.532, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 21.20it/s, loss=0.524, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.024 >= min_delta = 0.003. New best score: 0.823\n",
      "Epoch 38: 100%|█| 21/21 [00:01<00:00, 20.80it/s, loss=0.524, v_num=djoy, BTC_val\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 21.33it/s, loss=0.5, v_num=djoy, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:01<00:00, 20.90it/s, loss=0.5, v_num=djoy, BTC_val_a\u001b[A\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 20.38it/s, loss=0.482, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:01<00:00, 20.39it/s, loss=0.482, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 21.23it/s, loss=0.495, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:01<00:00, 20.81it/s, loss=0.495, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 21.60it/s, loss=0.483, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 21.16it/s, loss=0.483, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 22.08it/s, loss=0.475, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.031 >= min_delta = 0.003. New best score: 0.792\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 21.63it/s, loss=0.475, v_num=djoy, BTC_val\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 22.15it/s, loss=0.469, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 21.54it/s, loss=0.469, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 21.29it/s, loss=0.459, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:01<00:00, 20.84it/s, loss=0.459, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 21.87it/s, loss=0.453, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 21.43it/s, loss=0.453, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 21.25it/s, loss=0.433, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:01<00:00, 20.73it/s, loss=0.433, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:01<00:00, 20.91it/s, loss=0.43, v_num=djoy, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:01<00:00, 20.47it/s, loss=0.43, v_num=djoy, BTC_val_\u001b[A\n",
      "Epoch 49: 100%|█| 21/21 [00:01<00:00, 20.93it/s, loss=0.416, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 21/21 [00:01<00:00, 20.52it/s, loss=0.416, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 50: 100%|█| 21/21 [00:00<00:00, 22.40it/s, loss=0.41, v_num=djoy, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 21/21 [00:00<00:00, 21.87it/s, loss=0.41, v_num=djoy, BTC_val_\u001b[A\n",
      "Epoch 51: 100%|█| 21/21 [00:00<00:00, 21.17it/s, loss=0.416, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 21/21 [00:01<00:00, 20.60it/s, loss=0.416, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 52: 100%|█| 21/21 [00:00<00:00, 21.61it/s, loss=0.422, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 21/21 [00:00<00:00, 21.20it/s, loss=0.422, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 53: 100%|█| 21/21 [00:00<00:00, 21.30it/s, loss=0.386, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 21/21 [00:01<00:00, 20.78it/s, loss=0.386, v_num=djoy, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|█| 21/21 [00:00<00:00, 21.79it/s, loss=0.361, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 21/21 [00:00<00:00, 21.22it/s, loss=0.361, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 55: 100%|█| 21/21 [00:01<00:00, 20.88it/s, loss=0.373, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 21/21 [00:01<00:00, 20.37it/s, loss=0.373, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 56: 100%|█| 21/21 [00:01<00:00, 20.73it/s, loss=0.355, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 21/21 [00:01<00:00, 20.36it/s, loss=0.355, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 57: 100%|█| 21/21 [00:00<00:00, 21.18it/s, loss=0.358, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 21/21 [00:01<00:00, 20.76it/s, loss=0.358, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 58: 100%|█| 21/21 [00:00<00:00, 21.60it/s, loss=0.361, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 21/21 [00:00<00:00, 21.20it/s, loss=0.361, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 59: 100%|█| 21/21 [00:01<00:00, 20.31it/s, loss=0.337, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|█| 21/21 [00:01<00:00, 19.91it/s, loss=0.337, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 60: 100%|█| 21/21 [00:00<00:00, 21.94it/s, loss=0.354, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 60: 100%|█| 21/21 [00:00<00:00, 21.51it/s, loss=0.354, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 61: 100%|█| 21/21 [00:01<00:00, 20.52it/s, loss=0.338, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|█| 21/21 [00:01<00:00, 20.09it/s, loss=0.338, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 62: 100%|█| 21/21 [00:00<00:00, 21.28it/s, loss=0.322, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 62: 100%|█| 21/21 [00:01<00:00, 20.86it/s, loss=0.322, v_num=djoy, BTC_val\u001b[A\n",
      "Epoch 63: 100%|█| 21/21 [00:01<00:00, 20.68it/s, loss=0.314, v_num=djoy, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.792. Signaling Trainer to stop.\n",
      "Epoch 63: 100%|█| 21/21 [00:01<00:00, 20.29it/s, loss=0.314, v_num=djoy, BTC_val\n",
      "Epoch 63: 100%|█| 21/21 [00:01<00:00, 20.23it/s, loss=0.314, v_num=djoy, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 47.41it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5757575631141663,\n",
      " 'BTC_test_f1': 0.561274528503418,\n",
      " 'ETH_test_acc': 0.5757575631141663,\n",
      " 'ETH_test_f1': 0.5463788509368896,\n",
      " 'test_loss': 0.9244288206100464}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 96408\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100630-2o9wdjoy/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100630-2o9wdjoy/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.87033\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.86532\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.86396\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.86093\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.31396\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 63\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1280\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625900862\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 153\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.38462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.53846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.52778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.32119\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.84375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.83199\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.79004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.41517\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.57576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.56127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.57576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.54638\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.92443\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▁▂▃▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▂▃▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▁▂▂▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▂▃▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇█▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ███▇▆▅▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▂▁▁▄▁▁▁▂▂▂▄▄▂▄▂▄▄▅▂▅▂▄▂▄▇▂▇▅▂█▅▁▇▁▄▇▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▃▁▁▄▁▁▁▃▃▃▄▄▃▄▄▄▄▅▃▆▄▄▃▄▇▃▇▆▄█▆▂▇▂▅▇▄▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▃▃▃▃▃▁▃▁▅▆█▅▆▄▅▇▅▅▅▄▆▆▅▄▅▆▅▇▆▅▆▅▅▆▆▆▇▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▂▂▂▂▂▁▂▁▅▆█▅▆▅▅█▅▅▆▅▆▆▅▄▅▆▆▇▆▆▆▅▆▆▆▆▇▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▃▃▃▃▃▄▃█▃▂▂▂▂▃▂▂▁▃▃▄▂▃▃▆▁▂▅▁▂▃▁▃▄▃▅▃▂▂▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▁▅▃▆▅▇▄▅▇▃▆▆▆▆▆▇█▆▆▆█▇▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▂▅▃▇▅▇▄▆▇▃▅▆▆▆▆▇█▆▆▆█▇▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▄▄▄▅▄▅▆▅▆▇▅▇██▇▇▅█▇███▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▄▄▃▆▄▅▆▅▆▆▅▇██▇▇▅█▇███▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▇▅▆▅▅▄▅▄▃▅▅▃▃▂▄▂▂▂▂▂▁▂▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/2o9wdjoy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:08:13.002976: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/1gjdzkcr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100811-1gjdzkcr\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 134 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 258   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "959 K     Trainable params\n",
      "0         Non-trainable params\n",
      "959 K     Total params\n",
      "3.837     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 20.92it/s, loss=0.68, v_num=zkcr, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.641\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 21.15it/s, loss=0.68, v_num=zkcr, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 23.33it/s, loss=0.582, v_num=zkcr, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.126 >= min_delta = 0.003. New best score: 0.516\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 23.46it/s, loss=0.582, v_num=zkcr, BTC_val_\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 22.43it/s, loss=0.567, v_num=zkcr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.035 >= min_delta = 0.003. New best score: 0.480\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 21.75it/s, loss=0.567, v_num=zkcr, BTC_val_\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 22.05it/s, loss=0.532, v_num=zkcr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.030 >= min_delta = 0.003. New best score: 0.451\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 21.60it/s, loss=0.532, v_num=zkcr, BTC_val_\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 21.60it/s, loss=0.517, v_num=zkcr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 0.438\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 21.12it/s, loss=0.517, v_num=zkcr, BTC_val_\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 22.40it/s, loss=0.519, v_num=zkcr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 21.94it/s, loss=0.519, v_num=zkcr, BTC_val_\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 21.50it/s, loss=0.496, v_num=zkcr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 21.07it/s, loss=0.496, v_num=zkcr, BTC_val_\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 22.02it/s, loss=0.484, v_num=zkcr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 21.59it/s, loss=0.484, v_num=zkcr, BTC_val_\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 22.02it/s, loss=0.491, v_num=zkcr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.434\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 21.57it/s, loss=0.491, v_num=zkcr, BTC_val_\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 22.58it/s, loss=0.482, v_num=zkcr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 0.422\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 21.93it/s, loss=0.482, v_num=zkcr, BTC_val_\n",
      "Epoch 10: 100%|█| 21/21 [00:01<00:00, 20.00it/s, loss=0.473, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:01<00:00, 19.62it/s, loss=0.473, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 22.22it/s, loss=0.478, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 21.76it/s, loss=0.478, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 21.21it/s, loss=0.474, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:01<00:00, 20.80it/s, loss=0.474, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 21.95it/s, loss=0.465, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 21.38it/s, loss=0.465, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 22.04it/s, loss=0.473, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 21.60it/s, loss=0.473, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 21.71it/s, loss=0.472, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 21.29it/s, loss=0.472, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 22.63it/s, loss=0.459, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.418\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 22.06it/s, loss=0.459, v_num=zkcr, BTC_val\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 21.81it/s, loss=0.468, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 21.37it/s, loss=0.468, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 21.17it/s, loss=0.463, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:01<00:00, 20.70it/s, loss=0.463, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 21.63it/s, loss=0.455, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 21.86it/s, loss=0.455, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 21.50it/s, loss=0.483, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 21.06it/s, loss=0.483, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 21.34it/s, loss=0.48, v_num=zkcr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:01<00:00, 20.89it/s, loss=0.48, v_num=zkcr, BTC_val_\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 21.62it/s, loss=0.459, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 21.19it/s, loss=0.459, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 22.38it/s, loss=0.458, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 21.93it/s, loss=0.458, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 21.97it/s, loss=0.448, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 21.50it/s, loss=0.448, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 21.62it/s, loss=0.455, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 21.18it/s, loss=0.455, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 22.06it/s, loss=0.468, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 21.38it/s, loss=0.468, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 22.46it/s, loss=0.459, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 21.88it/s, loss=0.459, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 22.21it/s, loss=0.462, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 21.72it/s, loss=0.462, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:01<00:00, 20.77it/s, loss=0.457, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:01<00:00, 20.38it/s, loss=0.457, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 21.84it/s, loss=0.442, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 21.38it/s, loss=0.442, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 21.95it/s, loss=0.445, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 21.52it/s, loss=0.445, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 22.33it/s, loss=0.453, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 21.64it/s, loss=0.453, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 21.41it/s, loss=0.441, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:01<00:00, 20.75it/s, loss=0.441, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 22.11it/s, loss=0.451, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 22.24it/s, loss=0.451, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 21.96it/s, loss=0.462, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 21.53it/s, loss=0.462, v_num=zkcr, BTC_val\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 22.67it/s, loss=0.457, v_num=zkcr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.418. Signaling Trainer to stop.\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 22.21it/s, loss=0.457, v_num=zkcr, BTC_val\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 22.14it/s, loss=0.457, v_num=zkcr, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 39.36it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6363636255264282,\n",
      " 'BTC_test_f1': 0.6192307472229004,\n",
      " 'ETH_test_acc': 0.6969696879386902,\n",
      " 'ETH_test_f1': 0.6898496150970459,\n",
      " 'test_loss': 0.5724865794181824}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 96693\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100811-1gjdzkcr/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100811-1gjdzkcr/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.78839\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.78319\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.78998\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.78592\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.45717\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 36\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 740\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 45\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625900936\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 88\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.61538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.57516\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.84615\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.84524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.43253\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.92683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.92665\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.82927\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.82763\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.26661\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.63636\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.61923\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.69697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.68985\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.57249\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▅▅▆▆▆▇▇▇▇▇▇▇▇▇██▇███▇█▇██▇▇██████▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▅▅▆▆▆▇▇▇▇▇▇█▇▇██▇███▇█▇██▇▇██████▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▆▅▆▆▆▇▇▇▇█▇██▇█████▇▇███▇████████▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▆▆▆▆▆▇▇▇▇█▇██▇█████▇▇████████████▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▅▄▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▂▂▂▂▁▁▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▃▆▆▆▆▆▆▆▆▆▆▆▆▃▆▆█▆▆▆▃▆▆▃▆▆▆▆▁▆▆▃▃▆▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▄▆▆▆▆▆▆▆▆▆▆▆▆▃▆▆█▆▆▆▄▆▆▄▆▆▆▆▁▆▆▃▄▆▄▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁████████████▅█████████████████▅██▅█▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁████████████▅█████████████████▅██▅█▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▄▃▂▂▂▂▂▂▁▄▁▁▂▃▂▁▂▂▁▃▃▂▂▂▂▃▂▂▂▃▂▂▃▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▅▅▄▅▄▂▆▅▃▃▅▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▅▆▄▆▄▂▆▅▃▃▅▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▃▁▆▆▅▆▅▂▆▃▁█▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▃▁▆▅▅▆▅▂▆▃▁█▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▇▅▅▄▅▇▄▅▇▇▃█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/1gjdzkcr\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:09:24.048362: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/a2g10m5l\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100922-a2g10m5l\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 134 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 387   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "959 K     Trainable params\n",
      "0         Non-trainable params\n",
      "959 K     Total params\n",
      "3.838     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 23.10it/s, loss=1.09, v_num=0m5l, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.051\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 23.26it/s, loss=1.09, v_num=0m5l, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 22.77it/s, loss=0.97, v_num=0m5l, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.176 >= min_delta = 0.003. New best score: 0.875\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 22.28it/s, loss=0.97, v_num=0m5l, BTC_val_a\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 22.20it/s, loss=0.899, v_num=0m5l, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.086 >= min_delta = 0.003. New best score: 0.789\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 21.76it/s, loss=0.899, v_num=0m5l, BTC_val_\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 21.66it/s, loss=0.879, v_num=0m5l, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 21.23it/s, loss=0.879, v_num=0m5l, BTC_val_\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 21.87it/s, loss=0.844, v_num=0m5l, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 21.43it/s, loss=0.844, v_num=0m5l, BTC_val_\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 22.72it/s, loss=0.829, v_num=0m5l, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 22.09it/s, loss=0.829, v_num=0m5l, BTC_val_\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 21.45it/s, loss=0.835, v_num=0m5l, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:01<00:00, 20.63it/s, loss=0.835, v_num=0m5l, BTC_val_\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:01<00:00, 20.83it/s, loss=0.801, v_num=0m5l, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:01<00:00, 20.41it/s, loss=0.801, v_num=0m5l, BTC_val_\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 22.11it/s, loss=0.787, v_num=0m5l, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 21.70it/s, loss=0.787, v_num=0m5l, BTC_val_\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 21.44it/s, loss=0.765, v_num=0m5l, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 21.03it/s, loss=0.765, v_num=0m5l, BTC_val_\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:01<00:00, 21.00it/s, loss=0.792, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:01<00:00, 20.43it/s, loss=0.792, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 21.92it/s, loss=0.786, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 21.48it/s, loss=0.786, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 21.34it/s, loss=0.761, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 21.61it/s, loss=0.761, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 21.74it/s, loss=0.782, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 21.31it/s, loss=0.782, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 22.14it/s, loss=0.786, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 21.69it/s, loss=0.786, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 22.37it/s, loss=0.768, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 21.83it/s, loss=0.768, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 21.64it/s, loss=0.753, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 21.18it/s, loss=0.753, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 22.12it/s, loss=0.747, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 21.63it/s, loss=0.747, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 21.29it/s, loss=0.753, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:01<00:00, 20.89it/s, loss=0.753, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 22.00it/s, loss=0.74, v_num=0m5l, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 21.57it/s, loss=0.74, v_num=0m5l, BTC_val_\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 21.57it/s, loss=0.743, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 21.17it/s, loss=0.743, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 22.69it/s, loss=0.751, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.032 >= min_delta = 0.003. New best score: 0.757\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 22.08it/s, loss=0.751, v_num=0m5l, BTC_val\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 21.72it/s, loss=0.758, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 21.23it/s, loss=0.758, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 21.05it/s, loss=0.749, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:01<00:00, 20.67it/s, loss=0.749, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 22.62it/s, loss=0.743, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 22.03it/s, loss=0.743, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 22.30it/s, loss=0.742, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 21.81it/s, loss=0.742, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 21.77it/s, loss=0.733, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 21.99it/s, loss=0.733, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 21.94it/s, loss=0.732, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 21.49it/s, loss=0.732, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 22.48it/s, loss=0.732, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 22.03it/s, loss=0.732, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 22.51it/s, loss=0.761, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 22.07it/s, loss=0.761, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 22.01it/s, loss=0.739, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 21.59it/s, loss=0.739, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 23.54it/s, loss=0.737, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 22.96it/s, loss=0.737, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 21.22it/s, loss=0.717, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:01<00:00, 20.71it/s, loss=0.717, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 22.70it/s, loss=0.72, v_num=0m5l, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 22.19it/s, loss=0.72, v_num=0m5l, BTC_val_\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 21.75it/s, loss=0.706, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 21.29it/s, loss=0.706, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 23.08it/s, loss=0.714, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 22.61it/s, loss=0.714, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 21.51it/s, loss=0.72, v_num=0m5l, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 21.03it/s, loss=0.72, v_num=0m5l, BTC_val_\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 22.34it/s, loss=0.709, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 21.75it/s, loss=0.709, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 22.95it/s, loss=0.691, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 22.47it/s, loss=0.691, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 22.84it/s, loss=0.695, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 22.39it/s, loss=0.695, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 22.09it/s, loss=0.697, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 21.63it/s, loss=0.697, v_num=0m5l, BTC_val\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 21.95it/s, loss=0.704, v_num=0m5l, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.757. Signaling Trainer to stop.\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 21.51it/s, loss=0.704, v_num=0m5l, BTC_val\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 21.44it/s, loss=0.704, v_num=0m5l, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 54.28it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6363636255264282,\n",
      " 'BTC_test_f1': 0.6286945939064026,\n",
      " 'ETH_test_acc': 0.6969696879386902,\n",
      " 'ETH_test_f1': 0.7008547186851501,\n",
      " 'test_loss': 0.7643070816993713}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 96936\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100922-a2g10m5l/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_100922-a2g10m5l/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.6969\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.68698\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.66189\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.65029\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.70424\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 41\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 840\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 49\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625901011\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.38462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.34432\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.46154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.4127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.8407\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.78049\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.77488\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.70732\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.71146\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.70677\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.63636\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.62869\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.69697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.70085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.76431\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▄▅▆▆▆▆▇▇█▇▇█▇▇██████▇███▇███▇██████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▄▅▆▆▆▆▇▇█▇▇█▇▇█████▇▇██▇▇███▇██████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▄▅▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇██▇███▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▄▅▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇██▇███▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▅▄▄▃▄▃▃▂▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▅█▇▆▃▂▆▆▁▅▅▂▁▅▆▅▃▂▂▂▅▂▂▃▃▃▂▂▂▅▂▃▃▃▂▂▂▃▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▅█▇▆▄▂▆▆▁▅▄▂▁▄▆▄▃▂▂▂▅▂▂▃▄▃▂▂▂▄▂▃▃▄▃▂▂▄▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▃▁▁▁▃▁▁▃▃▁▁▁▃▁▁▃▆▆▃▃█▁█▃▃▃▆▁▁▃▃▆▆▁▁▃▆▃▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▃▂▂▂▄▁▁▄▄▁▁▁▄▁▂▄▆▆▄▄█▁█▄▄▄▆▁▁▄▄▆▆▁▁▄▆▄▄▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▄▂▂▅▇▃▂▄▃▃▃▇▅▂▃▃▄▄▃▁▄▄▄▃▃▃▅▆▃▅▂▃▅▅▄▄▄▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▄▄▂▆█▄▁▆▆▅▅█▂▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▄▃▂▇▇▂▁▆▆▅▄▆▂▅██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▂▆▅█▆▁▃▆▄▆▇▄▅▆▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▂▆▅█▆▁▄▆▅▆▇▃▆▅▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅▄▅▁▂█▆▃▅▃▁▄▃▁▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/a2g10m5l\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:10:40.671413: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/17isu8jr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101039-17isu8jr\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 134 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 258   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "959 K     Trainable params\n",
      "0         Non-trainable params\n",
      "959 K     Total params\n",
      "3.837     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 23.26it/s, loss=0.675, v_num=u8jr, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.642\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 22.78it/s, loss=0.675, v_num=u8jr, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 21.61it/s, loss=0.585, v_num=u8jr, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.166 >= min_delta = 0.003. New best score: 0.476\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 21.17it/s, loss=0.585, v_num=u8jr, BTC_val_\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 21.87it/s, loss=0.57, v_num=u8jr, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 21.42it/s, loss=0.57, v_num=u8jr, BTC_val_a\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 21.42it/s, loss=0.539, v_num=u8jr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.016 >= min_delta = 0.003. New best score: 0.460\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 21.00it/s, loss=0.539, v_num=u8jr, BTC_val_\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 22.57it/s, loss=0.514, v_num=u8jr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 22.11it/s, loss=0.514, v_num=u8jr, BTC_val_\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 21.69it/s, loss=0.5, v_num=u8jr, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 21.28it/s, loss=0.5, v_num=u8jr, BTC_val_ac\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 21.66it/s, loss=0.495, v_num=u8jr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.029 >= min_delta = 0.003. New best score: 0.431\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 21.19it/s, loss=0.495, v_num=u8jr, BTC_val_\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 21.76it/s, loss=0.484, v_num=u8jr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 21.33it/s, loss=0.484, v_num=u8jr, BTC_val_\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 22.84it/s, loss=0.483, v_num=u8jr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 22.21it/s, loss=0.483, v_num=u8jr, BTC_val_\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 21.37it/s, loss=0.493, v_num=u8jr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:01<00:00, 20.64it/s, loss=0.493, v_num=u8jr, BTC_val_\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:01<00:00, 20.89it/s, loss=0.475, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:01<00:00, 20.42it/s, loss=0.475, v_num=u8jr, BTC_val\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 21.77it/s, loss=0.485, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 21.15it/s, loss=0.485, v_num=u8jr, BTC_val\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 23.22it/s, loss=0.484, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 22.70it/s, loss=0.484, v_num=u8jr, BTC_val\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:01<00:00, 20.96it/s, loss=0.46, v_num=u8jr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:01<00:00, 20.56it/s, loss=0.46, v_num=u8jr, BTC_val_\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 21.84it/s, loss=0.469, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 21.30it/s, loss=0.469, v_num=u8jr, BTC_val\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 21.33it/s, loss=0.459, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.425\n",
      "Epoch 15: 100%|█| 21/21 [00:01<00:00, 20.82it/s, loss=0.459, v_num=u8jr, BTC_val\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 21.89it/s, loss=0.475, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 21.38it/s, loss=0.475, v_num=u8jr, BTC_val\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 21.72it/s, loss=0.464, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 21.31it/s, loss=0.464, v_num=u8jr, BTC_val\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 21.84it/s, loss=0.474, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 21.36it/s, loss=0.474, v_num=u8jr, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 21.21it/s, loss=0.464, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:01<00:00, 20.78it/s, loss=0.464, v_num=u8jr, BTC_val\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 23.00it/s, loss=0.46, v_num=u8jr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 22.50it/s, loss=0.46, v_num=u8jr, BTC_val_\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 21.17it/s, loss=0.462, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:01<00:00, 20.77it/s, loss=0.462, v_num=u8jr, BTC_val\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 22.37it/s, loss=0.451, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 21.90it/s, loss=0.451, v_num=u8jr, BTC_val\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 21.41it/s, loss=0.446, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:01<00:00, 20.99it/s, loss=0.446, v_num=u8jr, BTC_val\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 21.43it/s, loss=0.449, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:01<00:00, 20.99it/s, loss=0.449, v_num=u8jr, BTC_val\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 21.65it/s, loss=0.442, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 21.18it/s, loss=0.442, v_num=u8jr, BTC_val\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 21.44it/s, loss=0.45, v_num=u8jr, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 21.01it/s, loss=0.45, v_num=u8jr, BTC_val_\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 21.24it/s, loss=0.443, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:01<00:00, 20.72it/s, loss=0.443, v_num=u8jr, BTC_val\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 22.12it/s, loss=0.444, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 21.66it/s, loss=0.444, v_num=u8jr, BTC_val\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 21.23it/s, loss=0.457, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:01<00:00, 20.82it/s, loss=0.457, v_num=u8jr, BTC_val\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 22.61it/s, loss=0.453, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 22.06it/s, loss=0.453, v_num=u8jr, BTC_val\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 21.01it/s, loss=0.441, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:01<00:00, 20.62it/s, loss=0.441, v_num=u8jr, BTC_val\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 22.30it/s, loss=0.445, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 21.76it/s, loss=0.445, v_num=u8jr, BTC_val\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 21.28it/s, loss=0.432, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:01<00:00, 20.88it/s, loss=0.432, v_num=u8jr, BTC_val\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 22.11it/s, loss=0.428, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 21.65it/s, loss=0.428, v_num=u8jr, BTC_val\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 21.69it/s, loss=0.441, v_num=u8jr, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.425. Signaling Trainer to stop.\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 21.11it/s, loss=0.441, v_num=u8jr, BTC_val\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 21.05it/s, loss=0.441, v_num=u8jr, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 52.05it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6666666865348816,\n",
      " 'BTC_test_f1': 0.6552706956863403,\n",
      " 'ETH_test_acc': 0.7272727489471436,\n",
      " 'ETH_test_f1': 0.7179487347602844,\n",
      " 'test_loss': 0.5895267724990845}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 97213\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101039-17isu8jr/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101039-17isu8jr/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.79714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.79253\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.80827\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.80595\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.44089\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 35\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 720\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 43\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625901082\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 86\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.69231\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.69048\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.92308\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.92121\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.46497\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.85366\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.84625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.85366\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.84926\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.39375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.65527\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.72727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.71795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.58953\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▅▅▆▆▇▇▇▇▇▇▇▇███▇█▇▇██▇▇██████▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▅▅▆▆▇▇▆▇▇█▇▇██▇▇█▇▇██▇███████▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▅▅▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▅▅▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▅▄▃▃▃▃▂▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ██▅█▅▅███████▅██████▅█▁█▁███████▁▅██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▇█▅█▅▅███████▅██████▃█▁█▁███████▁▅██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁███▃███████████████▆███████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁███▄███████████████▆███████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▃▃▂▃▃▁▁▁▁▁▂▂▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▄█▂▂▆▁▆▆▄▆▄█▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▄█▂▂▇▁▆▆▄▆▄█▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▂▃▁▆▂▅▆▃▃▃█▃▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▂▃▁▆▂▅▇▃▄▃█▃▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▄▆▇▃█▅▃▆▃▆▁▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/17isu8jr\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:11:52.533649: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/2leduf0x\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101151-2leduf0x\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 134 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 387   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "959 K     Trainable params\n",
      "0         Non-trainable params\n",
      "959 K     Total params\n",
      "3.838     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 22.91it/s, loss=1.09, v_num=uf0x, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.086\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 22.46it/s, loss=1.09, v_num=uf0x, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 22.50it/s, loss=1, v_num=uf0x, BTC_val_acc=\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.182 >= min_delta = 0.003. New best score: 0.904\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 22.04it/s, loss=1, v_num=uf0x, BTC_val_acc=\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 22.50it/s, loss=0.926, v_num=uf0x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 21.91it/s, loss=0.926, v_num=uf0x, BTC_val_\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 21.00it/s, loss=0.889, v_num=uf0x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.092 >= min_delta = 0.003. New best score: 0.812\n",
      "Epoch 3: 100%|█| 21/21 [00:01<00:00, 20.52it/s, loss=0.889, v_num=uf0x, BTC_val_\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 21.89it/s, loss=0.859, v_num=uf0x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 21.40it/s, loss=0.859, v_num=uf0x, BTC_val_\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 21.60it/s, loss=0.825, v_num=uf0x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.795\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 21.17it/s, loss=0.825, v_num=uf0x, BTC_val_\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 21.97it/s, loss=0.822, v_num=uf0x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 21.50it/s, loss=0.822, v_num=uf0x, BTC_val_\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 21.09it/s, loss=0.787, v_num=uf0x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.044 >= min_delta = 0.003. New best score: 0.751\n",
      "Epoch 7: 100%|█| 21/21 [00:01<00:00, 20.69it/s, loss=0.787, v_num=uf0x, BTC_val_\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 21.91it/s, loss=0.787, v_num=uf0x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 21.40it/s, loss=0.787, v_num=uf0x, BTC_val_\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 21.12it/s, loss=0.772, v_num=uf0x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:01<00:00, 20.70it/s, loss=0.772, v_num=uf0x, BTC_val_\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:01<00:00, 20.70it/s, loss=0.778, v_num=uf0x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:01<00:00, 20.33it/s, loss=0.778, v_num=uf0x, BTC_val\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 23.28it/s, loss=0.78, v_num=uf0x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 22.59it/s, loss=0.78, v_num=uf0x, BTC_val_\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 22.83it/s, loss=0.767, v_num=uf0x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 22.22it/s, loss=0.767, v_num=uf0x, BTC_val\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 21.42it/s, loss=0.777, v_num=uf0x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:01<00:00, 20.82it/s, loss=0.777, v_num=uf0x, BTC_val\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 21.54it/s, loss=0.772, v_num=uf0x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 21.04it/s, loss=0.772, v_num=uf0x, BTC_val\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 22.03it/s, loss=0.758, v_num=uf0x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 21.46it/s, loss=0.758, v_num=uf0x, BTC_val\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:01<00:00, 20.79it/s, loss=0.758, v_num=uf0x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:01<00:00, 20.38it/s, loss=0.758, v_num=uf0x, BTC_val\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:01<00:00, 20.91it/s, loss=0.749, v_num=uf0x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:01<00:00, 20.39it/s, loss=0.749, v_num=uf0x, BTC_val\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 21.46it/s, loss=0.758, v_num=uf0x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 21.04it/s, loss=0.758, v_num=uf0x, BTC_val\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 21.04it/s, loss=0.755, v_num=uf0x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:01<00:00, 20.58it/s, loss=0.755, v_num=uf0x, BTC_val\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:01<00:00, 20.48it/s, loss=0.753, v_num=uf0x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:01<00:00, 20.07it/s, loss=0.753, v_num=uf0x, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 21.89it/s, loss=0.75, v_num=uf0x, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 22.12it/s, loss=0.75, v_num=uf0x, BTC_val_\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 23.58it/s, loss=0.737, v_num=uf0x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 23.07it/s, loss=0.737, v_num=uf0x, BTC_val\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 22.49it/s, loss=0.758, v_num=uf0x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 22.04it/s, loss=0.758, v_num=uf0x, BTC_val\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 22.94it/s, loss=0.742, v_num=uf0x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 22.47it/s, loss=0.742, v_num=uf0x, BTC_val\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:01<00:00, 20.93it/s, loss=0.743, v_num=uf0x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:01<00:00, 20.49it/s, loss=0.743, v_num=uf0x, BTC_val\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 22.03it/s, loss=0.744, v_num=uf0x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 21.59it/s, loss=0.744, v_num=uf0x, BTC_val\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 22.77it/s, loss=0.737, v_num=uf0x, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.751. Signaling Trainer to stop.\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 22.29it/s, loss=0.737, v_num=uf0x, BTC_val\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 22.22it/s, loss=0.737, v_num=uf0x, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 52.00it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6363636255264282,\n",
      " 'BTC_test_f1': 0.6398990750312805,\n",
      " 'ETH_test_acc': 0.7272727489471436,\n",
      " 'ETH_test_f1': 0.7144781351089478,\n",
      " 'test_loss': 0.753261923789978}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 97461\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101151-2leduf0x/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101151-2leduf0x/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.68337\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.67261\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.65951\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.64111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.73657\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 27\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 560\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 35\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625901146\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 67\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.38462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.36508\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.46154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.40909\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.86889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.71875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.7084\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.60938\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.58172\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.73426\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.63636\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.6399\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.72727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.71448\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.75326\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▃▅▅▆▇▇▇▇█▇█▇▇██▇███▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▃▅▅▆▇▇▇▇█▇█▇▇██▇███▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▄▆▆▆▆▇▇▇▇▇▇█▇▇█▇█▇████▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▄▆▆▇▇▇▇▇█▇▇█▇▇█▇███████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▅▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▄▅▅█▅▇▂▄▄▂▄▄▁▂▁▁▁▂▄▁▁▄▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▄▆▆█▆▇▂▃▄▂▃▃▁▂▁▁▁▂▃▁▁▃▁▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▄▂▅▄▄▄▅▄▄▅▄▄▅▄█▄▄▄▄▅▄▅▅▄▄▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▃▂▆▃▃▃▅▃▃▅▃▃▅▃█▃▃▃▃▅▃▅▅▃▃▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▄▄▂▃▂▃▁▄▃▃▄▃▂▄▃▅▆▃▃▁▄▃▃▃▄▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▂▅▆█▆▃▆▇▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▁▅▅█▆▃▆▇▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▃▆▂▃▇▁▆▄▃█▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▄▆▃▃█▁▆▅▃█▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▅▇▅▁▆▃▃▅▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/2leduf0x\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:12:56.900500: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/3hhps14c\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101255-3hhps14c\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 219 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 258   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.177     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:01<00:00, 16.64it/s, loss=0.703, v_num=s14c, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.661\n",
      "Epoch 0: 100%|█| 19/19 [00:01<00:00, 16.77it/s, loss=0.703, v_num=s14c, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:01<00:00, 16.59it/s, loss=0.695, v_num=s14c, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 19/19 [00:01<00:00, 16.72it/s, loss=0.695, v_num=s14c, BTC_val_\u001b[A\n",
      "Epoch 2:  95%|▉| 18/19 [00:01<00:00, 16.40it/s, loss=0.68, v_num=s14c, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.040 >= min_delta = 0.003. New best score: 0.621\n",
      "Epoch 2: 100%|█| 19/19 [00:01<00:00, 16.58it/s, loss=0.68, v_num=s14c, BTC_val_a\n",
      "Epoch 3:  95%|▉| 18/19 [00:01<00:00, 15.23it/s, loss=0.648, v_num=s14c, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 19/19 [00:01<00:00, 15.21it/s, loss=0.648, v_num=s14c, BTC_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 18/19 [00:01<00:00, 14.52it/s, loss=0.591, v_num=s14c, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 19/19 [00:01<00:00, 14.70it/s, loss=0.591, v_num=s14c, BTC_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 18/19 [00:01<00:00, 14.54it/s, loss=0.556, v_num=s14c, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.096 >= min_delta = 0.003. New best score: 0.525\n",
      "Epoch 5: 100%|█| 19/19 [00:01<00:00, 14.71it/s, loss=0.556, v_num=s14c, BTC_val_\n",
      "Epoch 6:  95%|▉| 18/19 [00:01<00:00, 15.56it/s, loss=0.512, v_num=s14c, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 19/19 [00:01<00:00, 15.74it/s, loss=0.512, v_num=s14c, BTC_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 18/19 [00:01<00:00, 14.49it/s, loss=0.505, v_num=s14c, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 19/19 [00:01<00:00, 14.66it/s, loss=0.505, v_num=s14c, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 18/19 [00:01<00:00, 15.50it/s, loss=0.48, v_num=s14c, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 19/19 [00:01<00:00, 15.72it/s, loss=0.48, v_num=s14c, BTC_val_a\u001b[A\n",
      "Epoch 9:  95%|▉| 18/19 [00:01<00:00, 15.36it/s, loss=0.474, v_num=s14c, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:01<00:00, 15.40it/s, loss=0.474, v_num=s14c, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:01<00:00, 15.28it/s, loss=0.452, v_num=s14c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:01<00:00, 15.35it/s, loss=0.452, v_num=s14c, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:01<00:00, 14.65it/s, loss=0.446, v_num=s14c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.037 >= min_delta = 0.003. New best score: 0.487\n",
      "Epoch 11: 100%|█| 19/19 [00:01<00:00, 14.61it/s, loss=0.446, v_num=s14c, BTC_val\n",
      "Epoch 12:  95%|▉| 18/19 [00:01<00:00, 15.45it/s, loss=0.435, v_num=s14c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.065 >= min_delta = 0.003. New best score: 0.423\n",
      "Epoch 12: 100%|█| 19/19 [00:01<00:00, 15.66it/s, loss=0.435, v_num=s14c, BTC_val\n",
      "Epoch 13:  95%|▉| 18/19 [00:01<00:00, 15.72it/s, loss=0.437, v_num=s14c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:01<00:00, 15.92it/s, loss=0.437, v_num=s14c, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:01<00:00, 15.16it/s, loss=0.441, v_num=s14c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:01<00:00, 15.32it/s, loss=0.441, v_num=s14c, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:01<00:00, 15.10it/s, loss=0.468, v_num=s14c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:01<00:00, 15.25it/s, loss=0.468, v_num=s14c, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:01<00:00, 14.87it/s, loss=0.438, v_num=s14c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:01<00:00, 15.11it/s, loss=0.438, v_num=s14c, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:01<00:00, 15.18it/s, loss=0.42, v_num=s14c, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:01<00:00, 15.31it/s, loss=0.42, v_num=s14c, BTC_val_\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:01<00:00, 14.35it/s, loss=0.431, v_num=s14c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:01<00:00, 14.44it/s, loss=0.431, v_num=s14c, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:01<00:00, 14.73it/s, loss=0.408, v_num=s14c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:01<00:00, 14.89it/s, loss=0.408, v_num=s14c, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:01<00:00, 15.35it/s, loss=0.391, v_num=s14c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:01<00:00, 15.32it/s, loss=0.391, v_num=s14c, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:01<00:00, 15.15it/s, loss=0.39, v_num=s14c, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:01<00:00, 15.35it/s, loss=0.39, v_num=s14c, BTC_val_\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:01<00:00, 15.90it/s, loss=0.395, v_num=s14c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:01<00:00, 16.09it/s, loss=0.395, v_num=s14c, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:  95%|▉| 18/19 [00:01<00:00, 15.52it/s, loss=0.39, v_num=s14c, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:01<00:00, 15.73it/s, loss=0.39, v_num=s14c, BTC_val_\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:01<00:00, 15.84it/s, loss=0.379, v_num=s14c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:01<00:00, 16.01it/s, loss=0.379, v_num=s14c, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:01<00:00, 15.28it/s, loss=0.382, v_num=s14c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:01<00:00, 15.37it/s, loss=0.382, v_num=s14c, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:01<00:00, 15.29it/s, loss=0.369, v_num=s14c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:01<00:00, 15.51it/s, loss=0.369, v_num=s14c, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:01<00:00, 14.97it/s, loss=0.37, v_num=s14c, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:01<00:00, 15.13it/s, loss=0.37, v_num=s14c, BTC_val_\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:01<00:00, 15.23it/s, loss=0.366, v_num=s14c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:01<00:00, 15.35it/s, loss=0.366, v_num=s14c, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:01<00:00, 15.05it/s, loss=0.366, v_num=s14c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:01<00:00, 15.25it/s, loss=0.366, v_num=s14c, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:01<00:00, 15.22it/s, loss=0.365, v_num=s14c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:01<00:00, 15.30it/s, loss=0.365, v_num=s14c, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:01<00:00, 14.71it/s, loss=0.353, v_num=s14c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:01<00:00, 14.80it/s, loss=0.353, v_num=s14c, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:01<00:00, 14.93it/s, loss=0.358, v_num=s14c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.423. Signaling Trainer to stop.\n",
      "Epoch 32: 100%|█| 19/19 [00:01<00:00, 14.96it/s, loss=0.358, v_num=s14c, BTC_val\n",
      "Epoch 32: 100%|█| 19/19 [00:01<00:00, 14.91it/s, loss=0.358, v_num=s14c, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 26.47it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.800000011920929,\n",
      " 'BTC_test_f1': 0.7963801026344299,\n",
      " 'ETH_test_acc': 0.6666666865348816,\n",
      " 'ETH_test_f1': 0.6651785373687744,\n",
      " 'LTC_test_acc': 0.8333333134651184,\n",
      " 'LTC_test_f1': 0.8331479430198669,\n",
      " 'test_loss': 0.5322601795196533}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 97806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101255-3hhps14c/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101255-3hhps14c/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.84339\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.83905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.84514\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.84279\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.84952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.84592\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.35178\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 594\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 49\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625901224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 77\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.58333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.4958\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.91667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.89916\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.91667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.87368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.52398\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.84375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.84127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.84375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.84236\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.76562\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.76419\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.3999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.79638\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.66518\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.83333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.83315\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.53226\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▁▂▄▄▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▁▂▄▄▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇█▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▂▃▄▅▆▆▆▇▆▇▇▇▇▇▇▇█▇██▇▇▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▃▄▅▆▆▆▇▆▇▇▇▇▇▇▇▇▇██▇▇▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▂▃▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▂▃▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ███▇▆▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▅▁▁▁▃▁▁▁▅▃▃▆▅▁▁█▁▁▃▁▁▁█▅▅▃▃▁▃▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▆▁▁▁▃▁▁▁▆▄▃▇▅▁▂█▁▁▃▁▁▁█▆▆▃▃▂▃▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▆▄▆▁▁█▆▇▇▅▃███▆█████▆▆██▅██▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▃▄▃▁▁█▃▇▆▆▃███▃█████▅▅██▆██▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▁▁▁█▄▄▄▄█████▁██▄██▄▄▄██████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▁▁▁█▅▅▅▅█████▁██▅██▅▅▅██████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▅▄▅▆▃▃▃▅▃▄▂▁▁█▃▂▄▃▂▄▄▃▂▃▂▂▂▂▂▃▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▄▃▆▆▄▆▄█▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▄▃▅▆▄▆▄█▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▅▆▇▇▇█▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▅▆▇▇▇█▆██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▅▅█▆▄█▆█▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▅▄█▆▄█▆█▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▅▅▃▃▅▂▅▁▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/3hhps14c\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:14:16.010503: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/2exbhlj3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101414-2exbhlj3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 219 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 387   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.178     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:01<00:00, 15.90it/s, loss=1.11, v_num=hlj3, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.130\n",
      "Epoch 0: 100%|█| 19/19 [00:01<00:00, 16.12it/s, loss=1.11, v_num=hlj3, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:01<00:00, 16.35it/s, loss=1.09, v_num=hlj3, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.016 >= min_delta = 0.003. New best score: 1.113\n",
      "Epoch 1: 100%|█| 19/19 [00:01<00:00, 16.57it/s, loss=1.09, v_num=hlj3, BTC_val_a\n",
      "Epoch 2:  95%|▉| 18/19 [00:01<00:00, 16.35it/s, loss=1.08, v_num=hlj3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 19/19 [00:01<00:00, 16.31it/s, loss=1.08, v_num=hlj3, BTC_val_a\u001b[A\n",
      "Epoch 3:  95%|▉| 18/19 [00:01<00:00, 16.48it/s, loss=1.06, v_num=hlj3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 1.103\n",
      "Epoch 3: 100%|█| 19/19 [00:01<00:00, 16.45it/s, loss=1.06, v_num=hlj3, BTC_val_a\n",
      "Epoch 4:  95%|▉| 18/19 [00:01<00:00, 15.25it/s, loss=1.01, v_num=hlj3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.091 >= min_delta = 0.003. New best score: 1.012\n",
      "Epoch 4: 100%|█| 19/19 [00:01<00:00, 15.47it/s, loss=1.01, v_num=hlj3, BTC_val_a\n",
      "Epoch 5:  95%|▉| 18/19 [00:01<00:00, 15.36it/s, loss=0.948, v_num=hlj3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 19/19 [00:01<00:00, 15.29it/s, loss=0.948, v_num=hlj3, BTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 18/19 [00:01<00:00, 14.84it/s, loss=0.879, v_num=hlj3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.110 >= min_delta = 0.003. New best score: 0.902\n",
      "Epoch 6: 100%|█| 19/19 [00:01<00:00, 15.06it/s, loss=0.879, v_num=hlj3, BTC_val_\n",
      "Epoch 7:  95%|▉| 18/19 [00:01<00:00, 15.33it/s, loss=0.844, v_num=hlj3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 19/19 [00:01<00:00, 15.33it/s, loss=0.844, v_num=hlj3, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 18/19 [00:01<00:00, 15.47it/s, loss=0.819, v_num=hlj3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.028 >= min_delta = 0.003. New best score: 0.874\n",
      "Epoch 8: 100%|█| 19/19 [00:01<00:00, 15.67it/s, loss=0.819, v_num=hlj3, BTC_val_\n",
      "Epoch 9:  95%|▉| 18/19 [00:01<00:00, 15.53it/s, loss=0.787, v_num=hlj3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.025 >= min_delta = 0.003. New best score: 0.849\n",
      "Epoch 9: 100%|█| 19/19 [00:01<00:00, 15.51it/s, loss=0.787, v_num=hlj3, BTC_val_\n",
      "Epoch 10:  95%|▉| 18/19 [00:01<00:00, 14.86it/s, loss=0.779, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:01<00:00, 15.03it/s, loss=0.779, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:01<00:00, 15.34it/s, loss=0.774, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:01<00:00, 15.43it/s, loss=0.774, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:01<00:00, 15.80it/s, loss=0.759, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.042 >= min_delta = 0.003. New best score: 0.807\n",
      "Epoch 12: 100%|█| 19/19 [00:01<00:00, 16.00it/s, loss=0.759, v_num=hlj3, BTC_val\n",
      "Epoch 13:  95%|▉| 18/19 [00:01<00:00, 15.29it/s, loss=0.748, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:01<00:00, 15.41it/s, loss=0.748, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:01<00:00, 15.18it/s, loss=0.727, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:01<00:00, 15.39it/s, loss=0.727, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:01<00:00, 15.50it/s, loss=0.728, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:01<00:00, 15.49it/s, loss=0.728, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:01<00:00, 15.09it/s, loss=0.702, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:01<00:00, 15.27it/s, loss=0.702, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:01<00:00, 14.90it/s, loss=0.687, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:01<00:00, 15.08it/s, loss=0.687, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:01<00:00, 14.84it/s, loss=0.682, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:01<00:00, 15.01it/s, loss=0.682, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:01<00:00, 14.62it/s, loss=0.68, v_num=hlj3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:01<00:00, 14.72it/s, loss=0.68, v_num=hlj3, BTC_val_\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:01<00:00, 14.64it/s, loss=0.695, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:01<00:00, 14.76it/s, loss=0.695, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:01<00:00, 15.41it/s, loss=0.674, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.049 >= min_delta = 0.003. New best score: 0.758\n",
      "Epoch 21: 100%|█| 19/19 [00:01<00:00, 15.51it/s, loss=0.674, v_num=hlj3, BTC_val\n",
      "Epoch 22:  95%|▉| 18/19 [00:01<00:00, 14.38it/s, loss=0.653, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:01<00:00, 14.50it/s, loss=0.653, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:01<00:00, 14.90it/s, loss=0.645, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:01<00:00, 15.05it/s, loss=0.645, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:01<00:00, 15.18it/s, loss=0.654, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:01<00:00, 15.37it/s, loss=0.654, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:01<00:00, 15.14it/s, loss=0.649, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:01<00:00, 15.25it/s, loss=0.649, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:01<00:00, 14.38it/s, loss=0.646, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:01<00:00, 14.52it/s, loss=0.646, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:01<00:00, 14.50it/s, loss=0.632, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:01<00:00, 14.70it/s, loss=0.632, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:01<00:00, 14.85it/s, loss=0.611, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:01<00:00, 14.98it/s, loss=0.611, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:01<00:00, 14.92it/s, loss=0.628, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:01<00:00, 14.97it/s, loss=0.628, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:01<00:00, 15.18it/s, loss=0.597, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:01<00:00, 15.33it/s, loss=0.597, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:01<00:00, 14.83it/s, loss=0.578, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:01<00:00, 14.98it/s, loss=0.578, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:01<00:00, 15.06it/s, loss=0.584, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:01<00:00, 15.24it/s, loss=0.584, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 18/19 [00:01<00:00, 15.01it/s, loss=0.571, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:01<00:00, 15.13it/s, loss=0.571, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:01<00:00, 14.70it/s, loss=0.554, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:01<00:00, 14.88it/s, loss=0.554, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:01<00:00, 14.86it/s, loss=0.564, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:01<00:00, 15.06it/s, loss=0.564, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:01<00:00, 14.60it/s, loss=0.565, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 19/19 [00:01<00:00, 14.75it/s, loss=0.565, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 18/19 [00:01<00:00, 15.07it/s, loss=0.551, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:01<00:00, 15.15it/s, loss=0.551, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:01<00:00, 14.42it/s, loss=0.562, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 19/19 [00:01<00:00, 14.61it/s, loss=0.562, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 18/19 [00:01<00:00, 14.55it/s, loss=0.533, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 19/19 [00:01<00:00, 14.61it/s, loss=0.533, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 18/19 [00:01<00:00, 15.03it/s, loss=0.513, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 19/19 [00:01<00:00, 15.18it/s, loss=0.513, v_num=hlj3, BTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 18/19 [00:01<00:00, 14.24it/s, loss=0.512, v_num=hlj3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.758. Signaling Trainer to stop.\n",
      "Epoch 41: 100%|█| 19/19 [00:01<00:00, 14.38it/s, loss=0.512, v_num=hlj3, BTC_val\n",
      "Epoch 41: 100%|█| 19/19 [00:01<00:00, 14.33it/s, loss=0.512, v_num=hlj3, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 34.28it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5333333611488342,\n",
      " 'BTC_test_f1': 0.5282953977584839,\n",
      " 'ETH_test_acc': 0.6000000238418579,\n",
      " 'ETH_test_f1': 0.5978835821151733,\n",
      " 'LTC_test_acc': 0.5333333611488342,\n",
      " 'LTC_test_f1': 0.510315477848053,\n",
      " 'test_loss': 0.8685235977172852}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 98084\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101414-2exbhlj3/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101414-2exbhlj3/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.7699\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.76651\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.76728\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.76054\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.7734\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.76958\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.50957\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 41\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 756\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 61\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625901315\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.58333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.56515\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.67576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.67937\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.82514\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.80744\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.79688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.76552\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.85938\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.84774\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.43355\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.53333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.5283\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.59788\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.53333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.51032\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.86852\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▂▃▃▄▅▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▃▃▄▄▅▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▁▂▃▄▄▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▃▃▄▄▅▅▅▆▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▁▂▂▃▄▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇███▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▂▂▃▄▄▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇███▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ███▇▇▆▅▅▅▄▄▄▄▄▃▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▂▁▁▇▅▅▁▄▂▅▄▇▄▅█▄▅▂▅███▇█▅▇▇▅▇▇▄▄█▂█▅▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▃▁▁▆▄▄▁▄▃▄▂▇▄▅█▅▆▃▆███▇█▆▅▇▄▇▅▄▂█▄█▄▇▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▂▁▂▂▆▆▆▂▇▆▅▅▇▃▇▆▇▇▆▇█▇▆▇█▇▇▇▇▇▇▆▆▅▆▇▅▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▁▁▅▅▅▁▆▅▄▄▆▃▆▅▆▆▅▆█▆▆▇█▆▆▇▆▇▆▅▅▅▅▇▄▇▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▁▂▁▃▅▄▄▆▆▄▅▇▇▅▆▆█▅▆▇█▇▇▇█▆▆▇▆▆▇█▇▆▇▆▆▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▁▂▂▂▄▄▅▆▆▄▅▇▇▆▆▆█▆▆▇█▇▇▇█▇▆▇▇▆▇█▇▆▇▇▆▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▇▇█▇▅▆▄█▃▃▃▄▂▅▂▂▂▃▄▂▁▂▃▃▁▂▃▃▅▃▃▃▅▄▃▂█▂▅▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▄▅▄▆▇▆▅▆▇▆▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▄▅▄▆▇▆▅▆▇▆▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▁▅▅▇▄▆▆▄▇▅▅▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▁▅▅▇▄▇▇▄█▆▅▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▅▆▆▆▄▆▅▆▇▆▆▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▄▆▆▆▅▅▅▅▇▆▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▆▅▄▄▅▄▃▄▃▃▂▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/2exbhlj3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:15:48.543588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/hp2ogsw6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101547-hp2ogsw6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 219 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 258   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.177     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:01<00:00, 15.61it/s, loss=0.704, v_num=gsw6, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.676\n",
      "Epoch 0: 100%|█| 19/19 [00:01<00:00, 15.69it/s, loss=0.704, v_num=gsw6, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:01<00:00, 15.43it/s, loss=0.696, v_num=gsw6, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.019 >= min_delta = 0.003. New best score: 0.657\n",
      "Epoch 1: 100%|█| 19/19 [00:01<00:00, 15.62it/s, loss=0.696, v_num=gsw6, BTC_val_\n",
      "Epoch 2:  95%|▉| 18/19 [00:01<00:00, 14.94it/s, loss=0.686, v_num=gsw6, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 19/19 [00:01<00:00, 14.88it/s, loss=0.686, v_num=gsw6, BTC_val_\u001b[A\n",
      "Epoch 3:  95%|▉| 18/19 [00:01<00:00, 14.66it/s, loss=0.675, v_num=gsw6, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 19/19 [00:01<00:00, 14.85it/s, loss=0.675, v_num=gsw6, BTC_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 18/19 [00:01<00:00, 13.95it/s, loss=0.643, v_num=gsw6, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 19/19 [00:01<00:00, 14.08it/s, loss=0.643, v_num=gsw6, BTC_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 18/19 [00:01<00:00, 14.97it/s, loss=0.569, v_num=gsw6, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.063 >= min_delta = 0.003. New best score: 0.594\n",
      "Epoch 5: 100%|█| 19/19 [00:01<00:00, 15.03it/s, loss=0.569, v_num=gsw6, BTC_val_\n",
      "Epoch 6:  95%|▉| 18/19 [00:01<00:00, 15.51it/s, loss=0.522, v_num=gsw6, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.039 >= min_delta = 0.003. New best score: 0.555\n",
      "Epoch 6: 100%|█| 19/19 [00:01<00:00, 15.60it/s, loss=0.522, v_num=gsw6, BTC_val_\n",
      "Epoch 7:  95%|▉| 18/19 [00:01<00:00, 15.07it/s, loss=0.498, v_num=gsw6, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 19/19 [00:01<00:00, 15.21it/s, loss=0.498, v_num=gsw6, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 18/19 [00:01<00:00, 15.02it/s, loss=0.486, v_num=gsw6, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.059 >= min_delta = 0.003. New best score: 0.496\n",
      "Epoch 8: 100%|█| 19/19 [00:01<00:00, 15.22it/s, loss=0.486, v_num=gsw6, BTC_val_\n",
      "Epoch 9:  95%|▉| 18/19 [00:01<00:00, 15.32it/s, loss=0.477, v_num=gsw6, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:01<00:00, 15.37it/s, loss=0.477, v_num=gsw6, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:01<00:00, 14.78it/s, loss=0.461, v_num=gsw6, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.062 >= min_delta = 0.003. New best score: 0.434\n",
      "Epoch 10: 100%|█| 19/19 [00:01<00:00, 14.88it/s, loss=0.461, v_num=gsw6, BTC_val\n",
      "Epoch 11:  95%|▉| 18/19 [00:01<00:00, 14.49it/s, loss=0.441, v_num=gsw6, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:01<00:00, 14.68it/s, loss=0.441, v_num=gsw6, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:01<00:00, 14.73it/s, loss=0.453, v_num=gsw6, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.030 >= min_delta = 0.003. New best score: 0.404\n",
      "Epoch 12: 100%|█| 19/19 [00:01<00:00, 14.84it/s, loss=0.453, v_num=gsw6, BTC_val\n",
      "Epoch 13:  95%|▉| 18/19 [00:01<00:00, 14.16it/s, loss=0.448, v_num=gsw6, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|█| 19/19 [00:01<00:00, 14.32it/s, loss=0.448, v_num=gsw6, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:01<00:00, 14.41it/s, loss=0.438, v_num=gsw6, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:01<00:00, 14.54it/s, loss=0.438, v_num=gsw6, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:01<00:00, 15.29it/s, loss=0.424, v_num=gsw6, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:01<00:00, 15.52it/s, loss=0.424, v_num=gsw6, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:01<00:00, 15.78it/s, loss=0.42, v_num=gsw6, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:01<00:00, 15.84it/s, loss=0.42, v_num=gsw6, BTC_val_\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:01<00:00, 15.81it/s, loss=0.418, v_num=gsw6, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:01<00:00, 15.98it/s, loss=0.418, v_num=gsw6, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:01<00:00, 15.59it/s, loss=0.406, v_num=gsw6, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:01<00:00, 15.86it/s, loss=0.406, v_num=gsw6, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:01<00:00, 15.02it/s, loss=0.412, v_num=gsw6, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:01<00:00, 15.18it/s, loss=0.412, v_num=gsw6, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:01<00:00, 14.68it/s, loss=0.405, v_num=gsw6, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:01<00:00, 14.86it/s, loss=0.405, v_num=gsw6, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:01<00:00, 14.84it/s, loss=0.4, v_num=gsw6, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:01<00:00, 14.86it/s, loss=0.4, v_num=gsw6, BTC_val_a\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:01<00:00, 14.69it/s, loss=0.402, v_num=gsw6, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:01<00:00, 14.82it/s, loss=0.402, v_num=gsw6, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:01<00:00, 14.97it/s, loss=0.396, v_num=gsw6, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:01<00:00, 15.16it/s, loss=0.396, v_num=gsw6, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:01<00:00, 14.62it/s, loss=0.375, v_num=gsw6, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:01<00:00, 14.77it/s, loss=0.375, v_num=gsw6, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:01<00:00, 15.24it/s, loss=0.381, v_num=gsw6, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:01<00:00, 15.44it/s, loss=0.381, v_num=gsw6, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:01<00:00, 14.60it/s, loss=0.38, v_num=gsw6, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:01<00:00, 14.69it/s, loss=0.38, v_num=gsw6, BTC_val_\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:01<00:00, 14.16it/s, loss=0.354, v_num=gsw6, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:01<00:00, 14.23it/s, loss=0.354, v_num=gsw6, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:01<00:00, 14.90it/s, loss=0.357, v_num=gsw6, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:01<00:00, 14.89it/s, loss=0.357, v_num=gsw6, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:01<00:00, 14.58it/s, loss=0.346, v_num=gsw6, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:01<00:00, 14.73it/s, loss=0.346, v_num=gsw6, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:01<00:00, 14.23it/s, loss=0.349, v_num=gsw6, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:01<00:00, 14.41it/s, loss=0.349, v_num=gsw6, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:01<00:00, 14.72it/s, loss=0.343, v_num=gsw6, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:01<00:00, 14.87it/s, loss=0.343, v_num=gsw6, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:01<00:00, 14.83it/s, loss=0.335, v_num=gsw6, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.404. Signaling Trainer to stop.\n",
      "Epoch 32: 100%|█| 19/19 [00:01<00:00, 14.91it/s, loss=0.335, v_num=gsw6, BTC_val\n",
      "Epoch 32: 100%|█| 19/19 [00:01<00:00, 14.88it/s, loss=0.335, v_num=gsw6, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 35.24it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.7333333492279053,\n",
      " 'BTC_test_f1': 0.7321428656578064,\n",
      " 'ETH_test_acc': 0.6333333253860474,\n",
      " 'ETH_test_f1': 0.6296296119689941,\n",
      " 'LTC_test_acc': 0.800000011920929,\n",
      " 'LTC_test_f1': 0.7991071343421936,\n",
      " 'test_loss': 0.5344258546829224}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 98428\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101547-hp2ogsw6/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101547-hp2ogsw6/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.84427\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.84139\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.86439\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.86181\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.85127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.84824\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.32978\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 594\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625901397\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 77\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.42857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.83333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.72677\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.84375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.84236\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.74976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.41355\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.73333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.73214\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.63333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.62963\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.79911\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.53443\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▂▂▄▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▂▂▄▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▂▂▂▃▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▁▂▂▃▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▂▂▃▄▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇██████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▂▂▃▄▅▆▆▆▇▇▇▇▇▇▇▇█▇▇▇█▇██████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ███▇▇▅▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁█▆▆▁▃▁▆▃▁▁▁▆▆▆▁▃▆▃▁▁▃▃▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▃▁▁▃▁█▇▆▃▅▃▇▅▃▃▃▇▇▇▁▄▆▄▁▁▄▄▁▃▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▆▆▄▁▂▅▆▄▆▅██████████▇▇▇█▇█▇▇█▇██▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▃▃▄▁▂▆▃▅▆▆██████████▇▆▇█▇█▆▇█▆██▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▁▁▁▁▄▁██▁███████████▄███▄▄██▄██▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▁▁▁▁▅▁██▄███████████▅███▅▅██▅██▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▇▆▇█▆▅▄▅▃▅▂▃▁▃▂▂▂▂▁▂▂▆▃▃▂▄▃▃▃█▂▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▆▅▆▁▇▇█▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▅▅▅▂█▆█▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▄▅█▄▅▅▇▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▄▅█▄▅▅▇▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▅▂▅▃█▅▆▆▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▅▂▅▃█▅▆▆▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▄▅▂▅▁▂▁▃▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/hp2ogsw6\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:17:08.810197: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/qdapvxn0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101707-qdapvxn0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 219 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 387   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.178     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:01<00:00, 15.39it/s, loss=1.12, v_num=vxn0, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.086\n",
      "Epoch 0: 100%|█| 19/19 [00:01<00:00, 15.57it/s, loss=1.12, v_num=vxn0, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:01<00:00, 14.60it/s, loss=1.09, v_num=vxn0, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 19/19 [00:01<00:00, 14.76it/s, loss=1.09, v_num=vxn0, BTC_val_a\u001b[A\n",
      "Epoch 2:  95%|▉| 18/19 [00:01<00:00, 15.10it/s, loss=1.08, v_num=vxn0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 1.078\n",
      "Epoch 2: 100%|█| 19/19 [00:01<00:00, 15.02it/s, loss=1.08, v_num=vxn0, BTC_val_a\n",
      "Epoch 3:  95%|▉| 18/19 [00:01<00:00, 15.70it/s, loss=1.06, v_num=vxn0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.024 >= min_delta = 0.003. New best score: 1.053\n",
      "Epoch 3: 100%|█| 19/19 [00:01<00:00, 15.91it/s, loss=1.06, v_num=vxn0, BTC_val_a\n",
      "Epoch 4:  95%|▉| 18/19 [00:01<00:00, 14.70it/s, loss=1.03, v_num=vxn0, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 19/19 [00:01<00:00, 14.67it/s, loss=1.03, v_num=vxn0, BTC_val_a\u001b[A\n",
      "Epoch 5:  95%|▉| 18/19 [00:01<00:00, 15.68it/s, loss=0.926, v_num=vxn0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.084 >= min_delta = 0.003. New best score: 0.969\n",
      "Epoch 5: 100%|█| 19/19 [00:01<00:00, 15.86it/s, loss=0.926, v_num=vxn0, BTC_val_\n",
      "Epoch 6:  95%|▉| 18/19 [00:01<00:00, 14.76it/s, loss=0.867, v_num=vxn0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.068 >= min_delta = 0.003. New best score: 0.902\n",
      "Epoch 6: 100%|█| 19/19 [00:01<00:00, 14.78it/s, loss=0.867, v_num=vxn0, BTC_val_\n",
      "Epoch 7:  95%|▉| 18/19 [00:01<00:00, 14.88it/s, loss=0.844, v_num=vxn0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.019 >= min_delta = 0.003. New best score: 0.883\n",
      "Epoch 7: 100%|█| 19/19 [00:01<00:00, 15.04it/s, loss=0.844, v_num=vxn0, BTC_val_\n",
      "Epoch 8:  95%|▉| 18/19 [00:01<00:00, 14.94it/s, loss=0.826, v_num=vxn0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 19/19 [00:01<00:00, 15.13it/s, loss=0.826, v_num=vxn0, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 18/19 [00:01<00:00, 15.03it/s, loss=0.804, v_num=vxn0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:01<00:00, 15.24it/s, loss=0.804, v_num=vxn0, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:01<00:00, 14.70it/s, loss=0.801, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:01<00:00, 14.82it/s, loss=0.801, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:01<00:00, 14.94it/s, loss=0.775, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:01<00:00, 15.11it/s, loss=0.775, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:01<00:00, 14.39it/s, loss=0.747, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:01<00:00, 14.61it/s, loss=0.747, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:01<00:00, 14.93it/s, loss=0.756, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:01<00:00, 15.01it/s, loss=0.756, v_num=vxn0, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:  95%|▉| 18/19 [00:01<00:00, 15.00it/s, loss=0.743, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:01<00:00, 15.09it/s, loss=0.743, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:01<00:00, 14.26it/s, loss=0.705, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.109 >= min_delta = 0.003. New best score: 0.774\n",
      "Epoch 15: 100%|█| 19/19 [00:01<00:00, 14.44it/s, loss=0.705, v_num=vxn0, BTC_val\n",
      "Epoch 16:  95%|▉| 18/19 [00:01<00:00, 14.94it/s, loss=0.695, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:01<00:00, 15.12it/s, loss=0.695, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:01<00:00, 14.92it/s, loss=0.704, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:01<00:00, 15.01it/s, loss=0.704, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:01<00:00, 14.81it/s, loss=0.689, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.024 >= min_delta = 0.003. New best score: 0.750\n",
      "Epoch 18: 100%|█| 19/19 [00:01<00:00, 14.91it/s, loss=0.689, v_num=vxn0, BTC_val\n",
      "Epoch 19:  95%|▉| 18/19 [00:01<00:00, 14.57it/s, loss=0.678, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:01<00:00, 14.60it/s, loss=0.678, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:01<00:00, 15.15it/s, loss=0.684, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:01<00:00, 15.35it/s, loss=0.684, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:01<00:00, 15.79it/s, loss=0.664, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:01<00:00, 15.97it/s, loss=0.664, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:01<00:00, 15.27it/s, loss=0.665, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:01<00:00, 15.49it/s, loss=0.665, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:01<00:00, 15.13it/s, loss=0.671, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:01<00:00, 15.27it/s, loss=0.671, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:01<00:00, 15.08it/s, loss=0.636, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 0.747\n",
      "Epoch 24: 100%|█| 19/19 [00:01<00:00, 15.24it/s, loss=0.636, v_num=vxn0, BTC_val\n",
      "Epoch 25:  95%|▉| 18/19 [00:01<00:00, 15.43it/s, loss=0.634, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:01<00:00, 15.59it/s, loss=0.634, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:01<00:00, 14.77it/s, loss=0.634, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:01<00:00, 14.97it/s, loss=0.634, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:01<00:00, 15.27it/s, loss=0.607, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:01<00:00, 15.38it/s, loss=0.607, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:01<00:00, 14.83it/s, loss=0.595, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:01<00:00, 15.00it/s, loss=0.595, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:01<00:00, 15.11it/s, loss=0.621, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:01<00:00, 15.29it/s, loss=0.621, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:01<00:00, 14.56it/s, loss=0.595, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:01<00:00, 14.68it/s, loss=0.595, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:01<00:00, 15.64it/s, loss=0.594, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:01<00:00, 15.83it/s, loss=0.594, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:01<00:00, 15.33it/s, loss=0.588, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:01<00:00, 15.36it/s, loss=0.588, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 18/19 [00:01<00:00, 15.21it/s, loss=0.586, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:01<00:00, 15.28it/s, loss=0.586, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:01<00:00, 15.25it/s, loss=0.583, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:01<00:00, 15.44it/s, loss=0.583, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:01<00:00, 14.96it/s, loss=0.574, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:01<00:00, 15.05it/s, loss=0.574, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:01<00:00, 14.80it/s, loss=0.581, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 0.733\n",
      "Epoch 36: 100%|█| 19/19 [00:01<00:00, 14.86it/s, loss=0.581, v_num=vxn0, BTC_val\n",
      "Epoch 37:  95%|▉| 18/19 [00:01<00:00, 14.62it/s, loss=0.544, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:01<00:00, 14.75it/s, loss=0.544, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:01<00:00, 14.43it/s, loss=0.559, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 19/19 [00:01<00:00, 14.59it/s, loss=0.559, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 18/19 [00:01<00:00, 14.77it/s, loss=0.525, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 19/19 [00:01<00:00, 14.95it/s, loss=0.525, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 18/19 [00:01<00:00, 14.57it/s, loss=0.523, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 19/19 [00:01<00:00, 14.73it/s, loss=0.523, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 18/19 [00:01<00:00, 15.18it/s, loss=0.546, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 19/19 [00:01<00:00, 15.33it/s, loss=0.546, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 18/19 [00:01<00:00, 14.02it/s, loss=0.527, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 19/19 [00:01<00:00, 14.25it/s, loss=0.527, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 18/19 [00:01<00:00, 14.59it/s, loss=0.543, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 19/19 [00:01<00:00, 14.81it/s, loss=0.543, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 44:  95%|▉| 18/19 [00:01<00:00, 15.13it/s, loss=0.52, v_num=vxn0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 19/19 [00:01<00:00, 15.32it/s, loss=0.52, v_num=vxn0, BTC_val_\u001b[A\n",
      "Epoch 45:  95%|▉| 18/19 [00:01<00:00, 14.71it/s, loss=0.494, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 19/19 [00:01<00:00, 14.88it/s, loss=0.494, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 18/19 [00:01<00:00, 14.85it/s, loss=0.519, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 19/19 [00:01<00:00, 15.02it/s, loss=0.519, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 18/19 [00:01<00:00, 14.47it/s, loss=0.492, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.728\n",
      "Epoch 47: 100%|█| 19/19 [00:01<00:00, 14.64it/s, loss=0.492, v_num=vxn0, BTC_val\n",
      "Epoch 48:  95%|▉| 18/19 [00:01<00:00, 14.44it/s, loss=0.492, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 19/19 [00:01<00:00, 14.65it/s, loss=0.492, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 49:  95%|▉| 18/19 [00:01<00:00, 15.06it/s, loss=0.472, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 19/19 [00:01<00:00, 15.15it/s, loss=0.472, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 50:  95%|▉| 18/19 [00:01<00:00, 14.48it/s, loss=0.449, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 19/19 [00:01<00:00, 14.63it/s, loss=0.449, v_num=vxn0, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51:  95%|▉| 18/19 [00:01<00:00, 14.81it/s, loss=0.439, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 19/19 [00:01<00:00, 14.90it/s, loss=0.439, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 52:  95%|▉| 18/19 [00:01<00:00, 14.19it/s, loss=0.436, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 19/19 [00:01<00:00, 14.38it/s, loss=0.436, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 53:  95%|▉| 18/19 [00:01<00:00, 14.71it/s, loss=0.448, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 19/19 [00:01<00:00, 14.93it/s, loss=0.448, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 54:  95%|▉| 18/19 [00:01<00:00, 14.86it/s, loss=0.441, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 19/19 [00:01<00:00, 14.99it/s, loss=0.441, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 55:  95%|▉| 18/19 [00:01<00:00, 14.49it/s, loss=0.441, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 19/19 [00:01<00:00, 14.61it/s, loss=0.441, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 56:  95%|▉| 18/19 [00:01<00:00, 14.75it/s, loss=0.408, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 19/19 [00:01<00:00, 14.93it/s, loss=0.408, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 57:  95%|▉| 18/19 [00:01<00:00, 15.16it/s, loss=0.409, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 19/19 [00:01<00:00, 15.40it/s, loss=0.409, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 58:  95%|▉| 18/19 [00:01<00:00, 15.52it/s, loss=0.388, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 19/19 [00:01<00:00, 15.67it/s, loss=0.388, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 59:  95%|▉| 18/19 [00:01<00:00, 15.07it/s, loss=0.386, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|█| 19/19 [00:01<00:00, 15.28it/s, loss=0.386, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 60:  95%|▉| 18/19 [00:01<00:00, 14.75it/s, loss=0.358, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 60: 100%|█| 19/19 [00:01<00:00, 14.87it/s, loss=0.358, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 61:  95%|▉| 18/19 [00:01<00:00, 14.41it/s, loss=0.365, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|█| 19/19 [00:01<00:00, 14.63it/s, loss=0.365, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 62:  95%|▉| 18/19 [00:01<00:00, 14.66it/s, loss=0.357, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 62: 100%|█| 19/19 [00:01<00:00, 14.80it/s, loss=0.357, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 63:  95%|▉| 18/19 [00:01<00:00, 14.62it/s, loss=0.35, v_num=vxn0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 63: 100%|█| 19/19 [00:01<00:00, 14.82it/s, loss=0.35, v_num=vxn0, BTC_val_\u001b[A\n",
      "Epoch 64:  95%|▉| 18/19 [00:01<00:00, 14.02it/s, loss=0.354, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 64: 100%|█| 19/19 [00:01<00:00, 14.13it/s, loss=0.354, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 65:  95%|▉| 18/19 [00:01<00:00, 14.24it/s, loss=0.344, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 65: 100%|█| 19/19 [00:01<00:00, 14.40it/s, loss=0.344, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 66:  95%|▉| 18/19 [00:01<00:00, 14.68it/s, loss=0.359, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 66: 100%|█| 19/19 [00:01<00:00, 14.84it/s, loss=0.359, v_num=vxn0, BTC_val\u001b[A\n",
      "Epoch 67:  95%|▉| 18/19 [00:01<00:00, 14.55it/s, loss=0.331, v_num=vxn0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.728. Signaling Trainer to stop.\n",
      "Epoch 67: 100%|█| 19/19 [00:01<00:00, 14.71it/s, loss=0.331, v_num=vxn0, BTC_val\n",
      "Epoch 67: 100%|█| 19/19 [00:01<00:00, 14.68it/s, loss=0.331, v_num=vxn0, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 19.88it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6666666865348816,\n",
      " 'BTC_test_f1': 0.6520417928695679,\n",
      " 'ETH_test_acc': 0.5666666626930237,\n",
      " 'ETH_test_f1': 0.5655624866485596,\n",
      " 'LTC_test_acc': 0.5666666626930237,\n",
      " 'LTC_test_f1': 0.5854038000106812,\n",
      " 'test_loss': 0.9531664252281189}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 98778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101707-qdapvxn0/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101707-qdapvxn0/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.86264\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.86189\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.84777\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.8453\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.84777\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.84419\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.32159\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 67\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 95\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625901522\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.58333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.51587\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.53333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.75909\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.04125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.86787\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.86995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.82812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.83387\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.28903\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.65204\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.56667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.56556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.56667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.5854\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.95317\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▃▄▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▃▄▅▅▅▅▆▆▆▆▆▆▆▇▆▇▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▁▂▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▂▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▂▂▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▂▂▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ███▆▆▅▅▅▅▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▃▃▃▃▄▁▁▄▂▆▁▃█▃▄▄▃▅▃▂▄▄▄▃▅▂▂▅▂▄▃▄▄▄▃▃▄▄▃▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▂▂▂▂▃▁▁▅▃▆▁▃█▂▄▄▂▅▂▁▃▄▄▃▆▁▁▅▁▃▃▄▃▄▃▃▃▃▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▅▅▅▅▆▁▄▅▅▇▅▇▆▅▇▇▅█▆▆▇▇▇▇▇▆▆█▅▅▅▆▅▅▇▅▆▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▂▂▂▂▆▁▄▅▅▇▆▇▅▃▇▇▅█▆▆▇▇▇▇▇▆▆█▅▅▄▆▅▅▇▄▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▂▂▂▃▄▁▄▄▆▆▄▇▇▃▇█▃▄▆▄▇▇▆▇▇▄▆▇▄▃▄▄▆▆▆▄▆▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▁▁▃▄▂▅▅▆▆▅▇▇▃▇█▄▅▅▄▇▇▆▇▇▄▆▇▅▃▅▅▆▆▆▅▆▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▃▃▃▂▂█▃▂▂▁▂▁▁▃▁▂▃▂▃▃▁▁▁▃▂▃▃▁▃▃▃▃▃▃▂▃▃▅▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▄▃▄▅▆▆▄▇▅▇▇▇▆█▇█▇█▆▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▃▃▄▅▆▆▄▇▅▆▇▇▆▇▇▇▇█▆▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▄▅▄▅▆▅▆▅▅▆▆▇▆▆▆█▇▇█▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▄▄▃▄▆▅▆▅▅▅▆▇▆▆▅▇▇▇█▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▃▄▅▄▆▅▆▆▅▆▆▆▇▄▇▇▆▇▇▇▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▃▄▅▄▇▅▅▆▅▆▆▆▇▄▆▇▆▇▇▇▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▆▆▅▅▄▅▄▄▅▃▃▂▃▃▃▂▃▂▂▃▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/qdapvxn0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:19:12.304500: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/128967r9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101911-128967r9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 134 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 258   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "959 K     Trainable params\n",
      "0         Non-trainable params\n",
      "959 K     Total params\n",
      "3.837     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:01<00:00, 16.35it/s, loss=0.68, v_num=67r9, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.570\n",
      "Epoch 0: 100%|█| 19/19 [00:01<00:00, 16.34it/s, loss=0.68, v_num=67r9, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:01<00:00, 16.00it/s, loss=0.584, v_num=67r9, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.056 >= min_delta = 0.003. New best score: 0.514\n",
      "Epoch 1: 100%|█| 19/19 [00:01<00:00, 16.14it/s, loss=0.584, v_num=67r9, BTC_val_\n",
      "Epoch 2:  95%|▉| 18/19 [00:01<00:00, 15.62it/s, loss=0.562, v_num=67r9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.496\n",
      "Epoch 2: 100%|█| 19/19 [00:01<00:00, 15.71it/s, loss=0.562, v_num=67r9, BTC_val_\n",
      "Epoch 3:  95%|▉| 18/19 [00:01<00:00, 15.02it/s, loss=0.528, v_num=67r9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.037 >= min_delta = 0.003. New best score: 0.459\n",
      "Epoch 3: 100%|█| 19/19 [00:01<00:00, 15.09it/s, loss=0.528, v_num=67r9, BTC_val_\n",
      "Epoch 4:  95%|▉| 18/19 [00:01<00:00, 15.09it/s, loss=0.514, v_num=67r9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.051 >= min_delta = 0.003. New best score: 0.408\n",
      "Epoch 4: 100%|█| 19/19 [00:01<00:00, 15.13it/s, loss=0.514, v_num=67r9, BTC_val_\n",
      "Epoch 5:  95%|▉| 18/19 [00:01<00:00, 14.05it/s, loss=0.511, v_num=67r9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.391\n",
      "Epoch 5: 100%|█| 19/19 [00:01<00:00, 14.20it/s, loss=0.511, v_num=67r9, BTC_val_\n",
      "Epoch 6:  95%|▉| 18/19 [00:01<00:00, 15.05it/s, loss=0.488, v_num=67r9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.387\n",
      "Epoch 6: 100%|█| 19/19 [00:01<00:00, 15.15it/s, loss=0.488, v_num=67r9, BTC_val_\n",
      "Epoch 7:  95%|▉| 18/19 [00:01<00:00, 16.08it/s, loss=0.486, v_num=67r9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 19/19 [00:01<00:00, 16.27it/s, loss=0.486, v_num=67r9, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 18/19 [00:01<00:00, 15.25it/s, loss=0.486, v_num=67r9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 19/19 [00:01<00:00, 15.41it/s, loss=0.486, v_num=67r9, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 18/19 [00:01<00:00, 15.55it/s, loss=0.474, v_num=67r9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:01<00:00, 15.71it/s, loss=0.474, v_num=67r9, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:01<00:00, 15.52it/s, loss=0.477, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:01<00:00, 15.57it/s, loss=0.477, v_num=67r9, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:01<00:00, 15.84it/s, loss=0.478, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:01<00:00, 15.94it/s, loss=0.478, v_num=67r9, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:01<00:00, 16.16it/s, loss=0.469, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 0.384\n",
      "Epoch 12: 100%|█| 19/19 [00:01<00:00, 16.29it/s, loss=0.469, v_num=67r9, BTC_val\n",
      "Epoch 13:  95%|▉| 18/19 [00:01<00:00, 15.88it/s, loss=0.482, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:01<00:00, 15.96it/s, loss=0.482, v_num=67r9, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:01<00:00, 15.63it/s, loss=0.465, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:01<00:00, 15.79it/s, loss=0.465, v_num=67r9, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:01<00:00, 15.54it/s, loss=0.476, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:01<00:00, 15.71it/s, loss=0.476, v_num=67r9, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:01<00:00, 15.71it/s, loss=0.479, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:01<00:00, 15.84it/s, loss=0.479, v_num=67r9, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:01<00:00, 15.97it/s, loss=0.47, v_num=67r9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:01<00:00, 16.13it/s, loss=0.47, v_num=67r9, BTC_val_\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:01<00:00, 15.36it/s, loss=0.473, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:01<00:00, 15.41it/s, loss=0.473, v_num=67r9, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:01<00:00, 15.91it/s, loss=0.464, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:01<00:00, 16.09it/s, loss=0.464, v_num=67r9, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:01<00:00, 15.71it/s, loss=0.464, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:01<00:00, 15.86it/s, loss=0.464, v_num=67r9, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:01<00:00, 15.81it/s, loss=0.462, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:01<00:00, 15.94it/s, loss=0.462, v_num=67r9, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:01<00:00, 15.71it/s, loss=0.459, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:01<00:00, 15.87it/s, loss=0.459, v_num=67r9, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:01<00:00, 15.90it/s, loss=0.46, v_num=67r9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:01<00:00, 15.94it/s, loss=0.46, v_num=67r9, BTC_val_\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:01<00:00, 16.17it/s, loss=0.461, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:01<00:00, 16.03it/s, loss=0.461, v_num=67r9, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:01<00:00, 15.96it/s, loss=0.455, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:01<00:00, 16.09it/s, loss=0.455, v_num=67r9, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:01<00:00, 15.84it/s, loss=0.451, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:01<00:00, 15.94it/s, loss=0.451, v_num=67r9, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:01<00:00, 16.02it/s, loss=0.454, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:01<00:00, 16.17it/s, loss=0.454, v_num=67r9, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:01<00:00, 15.61it/s, loss=0.449, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:01<00:00, 15.73it/s, loss=0.449, v_num=67r9, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:01<00:00, 15.18it/s, loss=0.457, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:01<00:00, 15.19it/s, loss=0.457, v_num=67r9, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:01<00:00, 15.50it/s, loss=0.443, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:01<00:00, 15.62it/s, loss=0.443, v_num=67r9, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:01<00:00, 15.77it/s, loss=0.437, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:01<00:00, 15.77it/s, loss=0.437, v_num=67r9, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:01<00:00, 16.31it/s, loss=0.445, v_num=67r9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.384. Signaling Trainer to stop.\n",
      "Epoch 32: 100%|█| 19/19 [00:01<00:00, 16.40it/s, loss=0.445, v_num=67r9, BTC_val\n",
      "Epoch 32: 100%|█| 19/19 [00:01<00:00, 16.35it/s, loss=0.445, v_num=67r9, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 26.93it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6333333253860474,\n",
      " 'BTC_test_f1': 0.6296296715736389,\n",
      " 'ETH_test_acc': 0.7333333492279053,\n",
      " 'ETH_test_f1': 0.7222222089767456,\n",
      " 'LTC_test_acc': 0.800000011920929,\n",
      " 'LTC_test_f1': 0.7991071343421936,\n",
      " 'test_loss': 0.5125181078910828}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 99208\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101911-128967r9/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_101911-128967r9/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.78565\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.78183\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.79703\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.79339\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.79353\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.79057\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.4504\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 594\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 48\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625901599\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 77\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.74825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.91667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.87368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.91667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.87368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.41385\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.74976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.87488\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.84375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.84127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.40856\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.63333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.62963\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.73333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.72222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.79911\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.51252\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▅▆▆▇▇▇▇▇▇▇▇▇█▇▇▇████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▅▆▆▇▇▇▇▇▇▇▇▇█▇▇▇████████████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▅▆▆▆▇▇▇▇▇▇▇▇▇█▇▇▇███▇███████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▅▆▆▇▇▇▇▇▇▇▇▇▇█▇▇▇███▇███████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▅▆▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇██████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▆▃▆█▆▆▆▆██▆▆▆▆▆▆▆▃▆▆▆▃▁▃▆▃▆▆▆▁▁▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▆▄▆█▆▆▆▆██▆▆▆▆▆▆▆▄▆▆▆▄▂▄▆▄▆▆▆▂▂▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▄▄▁█▄██████████████████▄██▄██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▃▅▁█▅██████████████████▅██▅██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▄▄▄█▄▄█▄█▄▄▄▄▄▄▄▄█▄▄▄█▄▄▄█▁▄██▄▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▄▄█▄▄█▄█▄▄▄▄▄▄▄▄█▄▄▄█▄▄▄█▁▄██▄▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▆▅▄▂▁▁▂▁▂▃▁▁▁▁▂▁▂▁▂▂▁▂▃▁▂▄▂▂▃▃▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▆▁▅▅▇▆▇▇▃█▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▆▁▅▅▇▆▇█▃█▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▅▂▁▂▃█▃▆▂▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▅▂▁▂▃█▂▆▂▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▅▁▂▅▁█▁▅▂▁▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▅▂▃▅▂█▁▅▃▁▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▄█▆▄▄▁▅▁▅▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/128967r9\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:20:26.599303: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/3eribpjb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102025-3eribpjb\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 134 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 387   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "959 K     Trainable params\n",
      "0         Non-trainable params\n",
      "959 K     Total params\n",
      "3.838     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:01<00:00, 16.36it/s, loss=1.1, v_num=bpjb, BTC_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.070\n",
      "Epoch 0: 100%|█| 19/19 [00:01<00:00, 16.44it/s, loss=1.1, v_num=bpjb, BTC_val_ac\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:01<00:00, 16.95it/s, loss=1, v_num=bpjb, BTC_val_acc=\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.037 >= min_delta = 0.003. New best score: 1.034\n",
      "Epoch 1: 100%|█| 19/19 [00:01<00:00, 17.15it/s, loss=1, v_num=bpjb, BTC_val_acc=\n",
      "Epoch 2:  95%|▉| 18/19 [00:01<00:00, 17.09it/s, loss=0.943, v_num=bpjb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.055 >= min_delta = 0.003. New best score: 0.979\n",
      "Epoch 2: 100%|█| 19/19 [00:01<00:00, 17.32it/s, loss=0.943, v_num=bpjb, BTC_val_\n",
      "Epoch 3:  95%|▉| 18/19 [00:01<00:00, 16.86it/s, loss=0.919, v_num=bpjb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.029 >= min_delta = 0.003. New best score: 0.950\n",
      "Epoch 3: 100%|█| 19/19 [00:01<00:00, 17.06it/s, loss=0.919, v_num=bpjb, BTC_val_\n",
      "Epoch 4:  95%|▉| 18/19 [00:01<00:00, 16.42it/s, loss=0.874, v_num=bpjb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.119 >= min_delta = 0.003. New best score: 0.831\n",
      "Epoch 4: 100%|█| 19/19 [00:01<00:00, 16.46it/s, loss=0.874, v_num=bpjb, BTC_val_\n",
      "Epoch 5:  95%|▉| 18/19 [00:01<00:00, 15.14it/s, loss=0.819, v_num=bpjb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 19/19 [00:01<00:00, 15.35it/s, loss=0.819, v_num=bpjb, BTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 18/19 [00:01<00:00, 15.08it/s, loss=0.821, v_num=bpjb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.055 >= min_delta = 0.003. New best score: 0.776\n",
      "Epoch 6: 100%|█| 19/19 [00:01<00:00, 15.21it/s, loss=0.821, v_num=bpjb, BTC_val_\n",
      "Epoch 7:  95%|▉| 18/19 [00:01<00:00, 15.72it/s, loss=0.819, v_num=bpjb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 19/19 [00:01<00:00, 15.90it/s, loss=0.819, v_num=bpjb, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 18/19 [00:01<00:00, 15.57it/s, loss=0.815, v_num=bpjb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 19/19 [00:01<00:00, 15.56it/s, loss=0.815, v_num=bpjb, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 18/19 [00:01<00:00, 15.47it/s, loss=0.775, v_num=bpjb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:01<00:00, 15.61it/s, loss=0.775, v_num=bpjb, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:01<00:00, 15.40it/s, loss=0.79, v_num=bpjb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.029 >= min_delta = 0.003. New best score: 0.747\n",
      "Epoch 10: 100%|█| 19/19 [00:01<00:00, 15.55it/s, loss=0.79, v_num=bpjb, BTC_val_\n",
      "Epoch 11:  95%|▉| 18/19 [00:01<00:00, 15.10it/s, loss=0.776, v_num=bpjb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:01<00:00, 15.20it/s, loss=0.776, v_num=bpjb, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:01<00:00, 15.52it/s, loss=0.792, v_num=bpjb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:01<00:00, 15.65it/s, loss=0.792, v_num=bpjb, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:01<00:00, 14.96it/s, loss=0.788, v_num=bpjb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:01<00:00, 15.07it/s, loss=0.788, v_num=bpjb, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:01<00:00, 15.23it/s, loss=0.77, v_num=bpjb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:01<00:00, 15.40it/s, loss=0.77, v_num=bpjb, BTC_val_\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:01<00:00, 15.01it/s, loss=0.752, v_num=bpjb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:01<00:00, 15.16it/s, loss=0.752, v_num=bpjb, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:01<00:00, 15.09it/s, loss=0.771, v_num=bpjb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:01<00:00, 15.27it/s, loss=0.771, v_num=bpjb, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:01<00:00, 15.32it/s, loss=0.771, v_num=bpjb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:01<00:00, 15.42it/s, loss=0.771, v_num=bpjb, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:01<00:00, 15.83it/s, loss=0.752, v_num=bpjb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:01<00:00, 16.02it/s, loss=0.752, v_num=bpjb, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:01<00:00, 15.78it/s, loss=0.749, v_num=bpjb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:01<00:00, 15.94it/s, loss=0.749, v_num=bpjb, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:01<00:00, 15.45it/s, loss=0.764, v_num=bpjb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:01<00:00, 15.52it/s, loss=0.764, v_num=bpjb, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:01<00:00, 15.76it/s, loss=0.75, v_num=bpjb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:01<00:00, 15.84it/s, loss=0.75, v_num=bpjb, BTC_val_\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:01<00:00, 14.92it/s, loss=0.756, v_num=bpjb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:01<00:00, 15.10it/s, loss=0.756, v_num=bpjb, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:01<00:00, 15.62it/s, loss=0.733, v_num=bpjb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:01<00:00, 15.75it/s, loss=0.733, v_num=bpjb, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:01<00:00, 15.27it/s, loss=0.744, v_num=bpjb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:01<00:00, 15.45it/s, loss=0.744, v_num=bpjb, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:01<00:00, 15.22it/s, loss=0.74, v_num=bpjb, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:01<00:00, 15.40it/s, loss=0.74, v_num=bpjb, BTC_val_\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:01<00:00, 15.59it/s, loss=0.743, v_num=bpjb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:01<00:00, 15.72it/s, loss=0.743, v_num=bpjb, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:01<00:00, 15.42it/s, loss=0.743, v_num=bpjb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:01<00:00, 15.38it/s, loss=0.743, v_num=bpjb, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:01<00:00, 15.33it/s, loss=0.744, v_num=bpjb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:01<00:00, 15.37it/s, loss=0.744, v_num=bpjb, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:01<00:00, 15.80it/s, loss=0.735, v_num=bpjb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:01<00:00, 15.97it/s, loss=0.735, v_num=bpjb, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:01<00:00, 15.46it/s, loss=0.754, v_num=bpjb, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.747. Signaling Trainer to stop.\n",
      "Epoch 30: 100%|█| 19/19 [00:01<00:00, 15.47it/s, loss=0.754, v_num=bpjb, BTC_val\n",
      "Epoch 30: 100%|█| 19/19 [00:01<00:00, 15.41it/s, loss=0.754, v_num=bpjb, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.16it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5666666626930237,\n",
      " 'BTC_test_f1': 0.5485632419586182,\n",
      " 'ETH_test_acc': 0.6333333253860474,\n",
      " 'ETH_test_f1': 0.6002699136734009,\n",
      " 'LTC_test_acc': 0.6333333253860474,\n",
      " 'LTC_test_f1': 0.4700460731983185,\n",
      " 'test_loss': 0.7460851669311523}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 99490\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102025-3eribpjb/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102025-3eribpjb/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.66842\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.66144\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.65792\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.64956\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.65617\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.64309\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.75259\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 558\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 46\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625901671\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 73\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.2963\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.53333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.45714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.82262\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.79919\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.73438\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.7367\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.73438\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.71933\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.60845\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.56667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.54856\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.63333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.60027\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.63333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.47005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.74609\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▄▅▅▆▇▇▇▇█▇▇▇▇██▇█████████████▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▃▅▅▆▇▇▇▇█▇▇▇▇██▇██████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▅▅▆▇▇▇▇▇▇█▇▇▇▇██▇██▇█████▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▄▅▆▇▇▇▇▇▇▇▇▇▇▇██▇██▇▇▇█▇█▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▄▅▅▆▆▇▇▇▇▇▇▇▇▇█▇█▇████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▄▅▆▆▇▇▇▇▇▇▇▇▇▇█▇█▇██▇█▇█████▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▅▄▄▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▃▃█▁▆█▄▆█▁▁▆▆█▆▃█▃▃▁▃▃▃▃▃▃▄▁▃▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▃▃▇▂▄▅▅▇▇▁▁▆▄▇▆▂█▃▃▁▃▃▃▃▃▃▅▁▃▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▅▂▁▅▄▅█▇▇▇▇▇▇▇▇▇▅▇█▇▇▇█▇██▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▅▂▁▅▂▃█▆▅▅▄▅▅▅▅▄▃▅▇▅▅▆▇▅▇▇▅▄▇▆▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▃▄▃█▁▁▆█▃█▄▃▃▃▃▆▃▃▆▃▃▄▄▄▄▄▃▃▃▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▃▅▃█▁▁▇█▃█▅▃▃▃▃▆▃▃▇▃▃▅▅▅▅▅▃▃▃▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▇▆▅▃▄▂▂▂▂▁▂▄▂▃▁▂▂▂▂▂▂▂▃▁▂▁▂▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▅▃▅▄▅▁▄▅▄▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▅▃▅▄▅▁▄▅▄▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▅▄▅▇▁▇▆▇▂█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▄▃▅▆▁▇▅▆▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▃▃▆▅▂▁▅█▄▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▃▃▅▅▂▁▅█▄▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▅▄▄▄█▄▂▄▆▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/3eribpjb\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:21:39.465540: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/1m24f9pd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102137-1m24f9pd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 134 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 258   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "959 K     Trainable params\n",
      "0         Non-trainable params\n",
      "959 K     Total params\n",
      "3.837     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:01<00:00, 16.11it/s, loss=0.69, v_num=f9pd, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.594\n",
      "Epoch 0: 100%|█| 19/19 [00:01<00:00, 16.34it/s, loss=0.69, v_num=f9pd, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:01<00:00, 17.06it/s, loss=0.591, v_num=f9pd, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.084 >= min_delta = 0.003. New best score: 0.510\n",
      "Epoch 1: 100%|█| 19/19 [00:01<00:00, 16.87it/s, loss=0.591, v_num=f9pd, BTC_val_\n",
      "Epoch 2:  95%|▉| 18/19 [00:01<00:00, 15.59it/s, loss=0.576, v_num=f9pd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 19/19 [00:01<00:00, 15.67it/s, loss=0.576, v_num=f9pd, BTC_val_\u001b[A\n",
      "Epoch 3:  95%|▉| 18/19 [00:01<00:00, 15.40it/s, loss=0.555, v_num=f9pd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.027 >= min_delta = 0.003. New best score: 0.484\n",
      "Epoch 3: 100%|█| 19/19 [00:01<00:00, 15.50it/s, loss=0.555, v_num=f9pd, BTC_val_\n",
      "Epoch 4:  95%|▉| 18/19 [00:01<00:00, 15.69it/s, loss=0.523, v_num=f9pd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.066 >= min_delta = 0.003. New best score: 0.418\n",
      "Epoch 4: 100%|█| 19/19 [00:01<00:00, 15.81it/s, loss=0.523, v_num=f9pd, BTC_val_\n",
      "Epoch 5:  95%|▉| 18/19 [00:01<00:00, 14.62it/s, loss=0.494, v_num=f9pd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 19/19 [00:01<00:00, 14.84it/s, loss=0.494, v_num=f9pd, BTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 18/19 [00:01<00:00, 15.52it/s, loss=0.505, v_num=f9pd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 19/19 [00:01<00:00, 15.61it/s, loss=0.505, v_num=f9pd, BTC_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 18/19 [00:01<00:00, 15.90it/s, loss=0.491, v_num=f9pd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.062 >= min_delta = 0.003. New best score: 0.355\n",
      "Epoch 7: 100%|█| 19/19 [00:01<00:00, 15.85it/s, loss=0.491, v_num=f9pd, BTC_val_\n",
      "Epoch 8:  95%|▉| 18/19 [00:01<00:00, 15.17it/s, loss=0.497, v_num=f9pd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 19/19 [00:01<00:00, 15.35it/s, loss=0.497, v_num=f9pd, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 18/19 [00:01<00:00, 15.90it/s, loss=0.486, v_num=f9pd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:01<00:00, 16.02it/s, loss=0.486, v_num=f9pd, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:01<00:00, 15.84it/s, loss=0.478, v_num=f9pd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:01<00:00, 16.03it/s, loss=0.478, v_num=f9pd, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:01<00:00, 15.58it/s, loss=0.479, v_num=f9pd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:01<00:00, 15.73it/s, loss=0.479, v_num=f9pd, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:01<00:00, 15.47it/s, loss=0.471, v_num=f9pd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:01<00:00, 15.67it/s, loss=0.471, v_num=f9pd, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:01<00:00, 15.28it/s, loss=0.473, v_num=f9pd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:01<00:00, 15.38it/s, loss=0.473, v_num=f9pd, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:01<00:00, 15.55it/s, loss=0.465, v_num=f9pd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:01<00:00, 15.62it/s, loss=0.465, v_num=f9pd, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:01<00:00, 15.17it/s, loss=0.463, v_num=f9pd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:01<00:00, 15.18it/s, loss=0.463, v_num=f9pd, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:01<00:00, 15.10it/s, loss=0.46, v_num=f9pd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:01<00:00, 15.26it/s, loss=0.46, v_num=f9pd, BTC_val_\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:01<00:00, 15.75it/s, loss=0.473, v_num=f9pd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:01<00:00, 15.87it/s, loss=0.473, v_num=f9pd, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:01<00:00, 15.19it/s, loss=0.466, v_num=f9pd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:01<00:00, 15.29it/s, loss=0.466, v_num=f9pd, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:01<00:00, 15.80it/s, loss=0.458, v_num=f9pd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:01<00:00, 15.96it/s, loss=0.458, v_num=f9pd, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:01<00:00, 15.56it/s, loss=0.459, v_num=f9pd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:01<00:00, 15.62it/s, loss=0.459, v_num=f9pd, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:01<00:00, 15.11it/s, loss=0.453, v_num=f9pd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:01<00:00, 15.28it/s, loss=0.453, v_num=f9pd, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:01<00:00, 15.82it/s, loss=0.455, v_num=f9pd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:01<00:00, 15.74it/s, loss=0.455, v_num=f9pd, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:01<00:00, 15.71it/s, loss=0.455, v_num=f9pd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:01<00:00, 15.72it/s, loss=0.455, v_num=f9pd, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:01<00:00, 15.88it/s, loss=0.453, v_num=f9pd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:01<00:00, 16.00it/s, loss=0.453, v_num=f9pd, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:01<00:00, 15.70it/s, loss=0.448, v_num=f9pd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:01<00:00, 15.88it/s, loss=0.448, v_num=f9pd, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:01<00:00, 15.59it/s, loss=0.449, v_num=f9pd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:01<00:00, 15.70it/s, loss=0.449, v_num=f9pd, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:01<00:00, 15.36it/s, loss=0.466, v_num=f9pd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.355. Signaling Trainer to stop.\n",
      "Epoch 27: 100%|█| 19/19 [00:01<00:00, 15.54it/s, loss=0.466, v_num=f9pd, BTC_val\n",
      "Epoch 27: 100%|█| 19/19 [00:01<00:00, 15.49it/s, loss=0.466, v_num=f9pd, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 30.92it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6666666865348816,\n",
      " 'BTC_test_f1': 0.6606335043907166,\n",
      " 'ETH_test_acc': 0.7333333492279053,\n",
      " 'ETH_test_f1': 0.7129186391830444,\n",
      " 'LTC_test_acc': 0.800000011920929,\n",
      " 'LTC_test_f1': 0.7963800430297852,\n",
      " 'test_loss': 0.4768936038017273}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 99726\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102137-1m24f9pd/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102137-1m24f9pd/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.77953\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.77596\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.78565\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.78266\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.79178\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.7882\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.46302\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 27\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 504\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 43\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625901740\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 66\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.74825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.91667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.87368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.91667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.87368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.43304\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.70312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.70305\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.77578\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.76562\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.76419\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.52597\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.66063\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.73333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.71292\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.79638\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.47689\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▅▆▆▆▇▇▇▇▇▇▇▇▇███▇██████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▅▆▆▆▇▇▇▇▇▇▇▇▇▇██▇██████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▅▅▅▆▇▆▇▇▇▇█▇▇▇██▇▇█████▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▅▅▅▆▇▆▇▇▇▇█▇▇███▇▇█████▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▅▆▆▇▇▇▇▇▇▇▇██▇██▇██▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▆▆▆▇▇▇▇▇▇▇▇█████▇██████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▅▄▃▂▃▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▆▃▃▆▆▆▆▆▃▆▆▆▆▆▃▃▆█▆▆▃▆▆▃▃▃▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▆▄▄▆▆▆▆▆▄▆▆▆▆▆▄▄▆█▆▆▄▆▆▄▄▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▄▁▄███████████▄▄███████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▅▁▃███████████▅▅███████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▄▄▄▄▄▄▄█▄██████▄▄▄▄▄█▁▄█▄▄▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▄▄▄▄▄▄█▄██████▄▄▄▄▄█▁▄█▄▄▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▆▅▅▃▃▃▁▂▂▂▂▂▁▃▃▃▂▃▃▃▃▂▂▃▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▃▁▃▄▇▄█▅▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▄▂▄▄▇▄█▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▅█▁█▆▇█▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▅█▁█▆▇█▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▆▇▁█▂▃▅█▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▆▇▁█▂▃▅█▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▃█▁▅▂▃▁▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/1m24f9pd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:22:48.124379: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/16a6jl3c\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102246-16a6jl3c\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 134 K \n",
      "1  | batch_norm1        | BatchNorm2d | 512   \n",
      "2  | lstm_2             | LSTM        | 395 K \n",
      "3  | batch_norm2        | BatchNorm2d | 512   \n",
      "4  | lstm_3             | LSTM        | 395 K \n",
      "5  | batch_norm3        | BatchNorm2d | 512   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 32.9 K\n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 387   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "959 K     Trainable params\n",
      "0         Non-trainable params\n",
      "959 K     Total params\n",
      "3.838     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:01<00:00, 16.82it/s, loss=1.1, v_num=jl3c, BTC_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.045\n",
      "Epoch 0: 100%|█| 19/19 [00:01<00:00, 17.05it/s, loss=1.1, v_num=jl3c, BTC_val_ac\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:01<00:00, 15.80it/s, loss=0.999, v_num=jl3c, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.069 >= min_delta = 0.003. New best score: 0.976\n",
      "Epoch 1: 100%|█| 19/19 [00:01<00:00, 15.80it/s, loss=0.999, v_num=jl3c, BTC_val_\n",
      "Epoch 2:  95%|▉| 18/19 [00:01<00:00, 15.92it/s, loss=0.926, v_num=jl3c, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 19/19 [00:01<00:00, 16.07it/s, loss=0.926, v_num=jl3c, BTC_val_\u001b[A\n",
      "Epoch 3:  95%|▉| 18/19 [00:01<00:00, 15.06it/s, loss=0.882, v_num=jl3c, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.086 >= min_delta = 0.003. New best score: 0.889\n",
      "Epoch 3: 100%|█| 19/19 [00:01<00:00, 15.13it/s, loss=0.882, v_num=jl3c, BTC_val_\n",
      "Epoch 4:  95%|▉| 18/19 [00:01<00:00, 15.07it/s, loss=0.871, v_num=jl3c, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 19/19 [00:01<00:00, 15.24it/s, loss=0.871, v_num=jl3c, BTC_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 18/19 [00:01<00:00, 15.12it/s, loss=0.861, v_num=jl3c, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.093 >= min_delta = 0.003. New best score: 0.796\n",
      "Epoch 5: 100%|█| 19/19 [00:01<00:00, 15.24it/s, loss=0.861, v_num=jl3c, BTC_val_\n",
      "Epoch 6:  95%|▉| 18/19 [00:01<00:00, 15.71it/s, loss=0.819, v_num=jl3c, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 19/19 [00:01<00:00, 15.78it/s, loss=0.819, v_num=jl3c, BTC_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 18/19 [00:01<00:00, 15.68it/s, loss=0.815, v_num=jl3c, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 19/19 [00:01<00:00, 15.82it/s, loss=0.815, v_num=jl3c, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 18/19 [00:01<00:00, 15.69it/s, loss=0.785, v_num=jl3c, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.062 >= min_delta = 0.003. New best score: 0.733\n",
      "Epoch 8: 100%|█| 19/19 [00:01<00:00, 15.90it/s, loss=0.785, v_num=jl3c, BTC_val_\n",
      "Epoch 9:  95%|▉| 18/19 [00:01<00:00, 15.47it/s, loss=0.782, v_num=jl3c, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:01<00:00, 15.64it/s, loss=0.782, v_num=jl3c, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:01<00:00, 15.51it/s, loss=0.772, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:01<00:00, 15.52it/s, loss=0.772, v_num=jl3c, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:01<00:00, 15.24it/s, loss=0.801, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:01<00:00, 15.34it/s, loss=0.801, v_num=jl3c, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:01<00:00, 15.43it/s, loss=0.79, v_num=jl3c, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.018 >= min_delta = 0.003. New best score: 0.715\n",
      "Epoch 12: 100%|█| 19/19 [00:01<00:00, 15.54it/s, loss=0.79, v_num=jl3c, BTC_val_\n",
      "Epoch 13:  95%|▉| 18/19 [00:01<00:00, 15.38it/s, loss=0.777, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:01<00:00, 15.48it/s, loss=0.777, v_num=jl3c, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:01<00:00, 15.80it/s, loss=0.769, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:01<00:00, 15.93it/s, loss=0.769, v_num=jl3c, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:01<00:00, 15.81it/s, loss=0.763, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:01<00:00, 15.89it/s, loss=0.763, v_num=jl3c, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:01<00:00, 15.81it/s, loss=0.764, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:01<00:00, 15.92it/s, loss=0.764, v_num=jl3c, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:01<00:00, 15.78it/s, loss=0.766, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:01<00:00, 15.90it/s, loss=0.766, v_num=jl3c, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:01<00:00, 15.71it/s, loss=0.771, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:01<00:00, 15.78it/s, loss=0.771, v_num=jl3c, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:01<00:00, 15.53it/s, loss=0.767, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:01<00:00, 15.63it/s, loss=0.767, v_num=jl3c, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:01<00:00, 15.73it/s, loss=0.762, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:01<00:00, 15.78it/s, loss=0.762, v_num=jl3c, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:01<00:00, 14.96it/s, loss=0.762, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:01<00:00, 15.16it/s, loss=0.762, v_num=jl3c, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:01<00:00, 15.15it/s, loss=0.741, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:01<00:00, 15.24it/s, loss=0.741, v_num=jl3c, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:01<00:00, 16.03it/s, loss=0.748, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:01<00:00, 16.20it/s, loss=0.748, v_num=jl3c, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:01<00:00, 15.55it/s, loss=0.741, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:01<00:00, 15.73it/s, loss=0.741, v_num=jl3c, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25:  95%|▉| 18/19 [00:01<00:00, 15.66it/s, loss=0.738, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:01<00:00, 15.81it/s, loss=0.738, v_num=jl3c, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:01<00:00, 15.59it/s, loss=0.755, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:01<00:00, 15.71it/s, loss=0.755, v_num=jl3c, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:01<00:00, 15.21it/s, loss=0.734, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:01<00:00, 15.19it/s, loss=0.734, v_num=jl3c, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:01<00:00, 15.74it/s, loss=0.738, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:01<00:00, 15.88it/s, loss=0.738, v_num=jl3c, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:01<00:00, 15.28it/s, loss=0.743, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:01<00:00, 15.39it/s, loss=0.743, v_num=jl3c, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:01<00:00, 15.67it/s, loss=0.732, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:01<00:00, 15.76it/s, loss=0.732, v_num=jl3c, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:01<00:00, 15.15it/s, loss=0.721, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:01<00:00, 15.37it/s, loss=0.721, v_num=jl3c, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:01<00:00, 15.51it/s, loss=0.735, v_num=jl3c, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.715. Signaling Trainer to stop.\n",
      "Epoch 32: 100%|█| 19/19 [00:01<00:00, 15.62it/s, loss=0.735, v_num=jl3c, BTC_val\n",
      "Epoch 32: 100%|█| 19/19 [00:01<00:00, 15.58it/s, loss=0.735, v_num=jl3c, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 25.49it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6000000238418579,\n",
      " 'BTC_test_f1': 0.5829131603240967,\n",
      " 'ETH_test_acc': 0.6666666865348816,\n",
      " 'ETH_test_f1': 0.6310248970985413,\n",
      " 'LTC_test_acc': 0.6333333253860474,\n",
      " 'LTC_test_f1': 0.47083336114883423,\n",
      " 'test_loss': 0.7519018650054932}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 99970\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102246-16a6jl3c/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102246-16a6jl3c/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.69291\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.68765\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.67192\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.65805\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.66142\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.64638\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.72217\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 594\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 49\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625901815\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 77\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.83333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.84127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.58333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.59259\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.83451\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.59375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.6007\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.64062\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.64202\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.78277\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.68561\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.58291\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.63102\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.63333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.47083\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.7519\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▄▅▆▆▆▇▇▇▇▇▇▇▇▇██▇▇▇▇▇█▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▄▅▆▆▆▇▇▇▇▇▇▇▇▇██▇▇▇▇██▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▄▆▆▆▆▇▇▇▇▇▇▇▇▇█▇█▇▇▇▇██▇█▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▄▆▆▆▆▇▇▇▇▇▇▇▇▇█▇█▇▇▇▇██▇█▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▄▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██▇██▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▄▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██▇██▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▅▄▄▄▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▅▄▇▅▄▇▁▅▂█▇▇▄▇▂▂▇▂▅▂▄▄▁▅▂▂▄▄▂▂▄▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▄▄▅▄▅█▁▅▂██▅▄▅▂▃█▃▆▂▃▄▁▆▃▂▄▅▃▂▅▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▂▂▁▂▅▄▅▄█▄█▅▅▅▇▇▅▇▅▅▇▅▇▅▇▅▅▇█▇▅▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▂▂▄▆▅▅▄█▃█▄▄▄▆▇▄▆▄▅▆▄▆▄▆▅▄▆█▆▄▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▂▁▂▅█▅▇▄▇▄▇▄▄▄▅█▄▅▄▇▅▄▇▄▇▇▄▇▇▅▄▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▁▃▅█▆▇▄▇▄▇▄▄▄▅█▄▅▄▇▅▄▇▄▇▇▄▇▇▅▄▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▇▇▅▅▃▄▃▁▃▁▄▁▃▃▂▂▃▂▄▄▃▄▂▃▄▃▂▄▂▂▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▅▆▇▆█▇▇▅▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▅▆▇▆█▇▇▅▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▆▅▅▅▅▆▆█▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▆▅▆▅▅▆▇█▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▄▄▆▆▇▇▅▄▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▄▄▆▆▇▇▄▄▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▄▅▁▃▂▁▂▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/16a6jl3c\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:24:03.175119: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/2ipwm1w9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102401-2ipwm1w9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 109 K \n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.532     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 43.71it/s, loss=0.723, v_num=m1w9, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.695\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 43.84it/s, loss=0.723, v_num=m1w9, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 41.59it/s, loss=0.709, v_num=m1w9, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 41.34it/s, loss=0.709, v_num=m1w9, BTC_val_\u001b[A\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 42.40it/s, loss=0.7, v_num=m1w9, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 42.08it/s, loss=0.7, v_num=m1w9, BTC_val_ac\u001b[A\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 42.42it/s, loss=0.699, v_num=m1w9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 41.82it/s, loss=0.699, v_num=m1w9, BTC_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 40.45it/s, loss=0.684, v_num=m1w9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 40.52it/s, loss=0.684, v_num=m1w9, BTC_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 39.82it/s, loss=0.699, v_num=m1w9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 39.67it/s, loss=0.699, v_num=m1w9, BTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 39.28it/s, loss=0.696, v_num=m1w9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 39.15it/s, loss=0.696, v_num=m1w9, BTC_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 41.04it/s, loss=0.691, v_num=m1w9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 40.60it/s, loss=0.691, v_num=m1w9, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 40.18it/s, loss=0.687, v_num=m1w9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 39.98it/s, loss=0.687, v_num=m1w9, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 40.28it/s, loss=0.691, v_num=m1w9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 40.38it/s, loss=0.691, v_num=m1w9, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 40.65it/s, loss=0.687, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 40.44it/s, loss=0.687, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 38.66it/s, loss=0.675, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 38.87it/s, loss=0.675, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 40.22it/s, loss=0.672, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 39.88it/s, loss=0.672, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 40.87it/s, loss=0.673, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 40.54it/s, loss=0.673, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 40.35it/s, loss=0.649, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 40.26it/s, loss=0.649, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 39.23it/s, loss=0.646, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 38.60it/s, loss=0.646, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 41.37it/s, loss=0.633, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 40.93it/s, loss=0.633, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 41.88it/s, loss=0.613, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 40.87it/s, loss=0.613, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 34.31it/s, loss=0.594, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 34.68it/s, loss=0.594, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 40.39it/s, loss=0.569, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 39.36it/s, loss=0.569, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 39.91it/s, loss=0.544, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.027 >= min_delta = 0.003. New best score: 0.668\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 40.09it/s, loss=0.544, v_num=m1w9, BTC_val\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 42.07it/s, loss=0.558, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 42.04it/s, loss=0.558, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 40.18it/s, loss=0.518, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 40.05it/s, loss=0.518, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 40.12it/s, loss=0.515, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 39.93it/s, loss=0.515, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 40.81it/s, loss=0.508, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 40.78it/s, loss=0.508, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 40.70it/s, loss=0.497, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 40.24it/s, loss=0.497, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 40.55it/s, loss=0.48, v_num=m1w9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 40.24it/s, loss=0.48, v_num=m1w9, BTC_val_\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 40.10it/s, loss=0.466, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 40.23it/s, loss=0.466, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 38.93it/s, loss=0.451, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 38.97it/s, loss=0.451, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 43.63it/s, loss=0.437, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 43.22it/s, loss=0.437, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 38.81it/s, loss=0.459, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 38.56it/s, loss=0.459, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 39.77it/s, loss=0.429, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 39.84it/s, loss=0.429, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 41.98it/s, loss=0.44, v_num=m1w9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 41.99it/s, loss=0.44, v_num=m1w9, BTC_val_\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 39.12it/s, loss=0.428, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 39.17it/s, loss=0.428, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 39.26it/s, loss=0.41, v_num=m1w9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 39.00it/s, loss=0.41, v_num=m1w9, BTC_val_\u001b[A\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 40.45it/s, loss=0.42, v_num=m1w9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 40.49it/s, loss=0.42, v_num=m1w9, BTC_val_\u001b[A\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 39.66it/s, loss=0.426, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 0.656\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 39.71it/s, loss=0.426, v_num=m1w9, BTC_val\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 40.06it/s, loss=0.403, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 39.45it/s, loss=0.403, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 20/21 [00:00<00:00, 41.58it/s, loss=0.42, v_num=m1w9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 41.20it/s, loss=0.42, v_num=m1w9, BTC_val_\u001b[A\n",
      "Epoch 39:  95%|▉| 20/21 [00:00<00:00, 39.21it/s, loss=0.39, v_num=m1w9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 39.19it/s, loss=0.39, v_num=m1w9, BTC_val_\u001b[A\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 40.04it/s, loss=0.362, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 39.46it/s, loss=0.362, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 20/21 [00:00<00:00, 42.06it/s, loss=0.393, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 41.11it/s, loss=0.393, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 20/21 [00:00<00:00, 41.32it/s, loss=0.38, v_num=m1w9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 40.36it/s, loss=0.38, v_num=m1w9, BTC_val_\u001b[A\n",
      "Epoch 43:  95%|▉| 20/21 [00:00<00:00, 41.57it/s, loss=0.379, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 40.51it/s, loss=0.379, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 44:  95%|▉| 20/21 [00:00<00:00, 40.93it/s, loss=0.365, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 40.43it/s, loss=0.365, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 45:  95%|▉| 20/21 [00:00<00:00, 40.02it/s, loss=0.369, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 39.96it/s, loss=0.369, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 20/21 [00:00<00:00, 40.27it/s, loss=0.357, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 39.96it/s, loss=0.357, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 20/21 [00:00<00:00, 40.00it/s, loss=0.364, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 40.11it/s, loss=0.364, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 48:  95%|▉| 20/21 [00:00<00:00, 40.54it/s, loss=0.335, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 40.44it/s, loss=0.335, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 49:  95%|▉| 20/21 [00:00<00:00, 40.24it/s, loss=0.353, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 21/21 [00:00<00:00, 40.16it/s, loss=0.353, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 50:  95%|▉| 20/21 [00:00<00:00, 39.47it/s, loss=0.33, v_num=m1w9, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 21/21 [00:00<00:00, 39.65it/s, loss=0.33, v_num=m1w9, BTC_val_\u001b[A\n",
      "Epoch 51:  95%|▉| 20/21 [00:00<00:00, 42.43it/s, loss=0.321, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 21/21 [00:00<00:00, 42.01it/s, loss=0.321, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 52:  95%|▉| 20/21 [00:00<00:00, 41.15it/s, loss=0.322, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 21/21 [00:00<00:00, 41.36it/s, loss=0.322, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 53:  95%|▉| 20/21 [00:00<00:00, 39.08it/s, loss=0.296, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 21/21 [00:00<00:00, 38.75it/s, loss=0.296, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 54:  95%|▉| 20/21 [00:00<00:00, 40.22it/s, loss=0.303, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 21/21 [00:00<00:00, 40.12it/s, loss=0.303, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 55:  95%|▉| 20/21 [00:00<00:00, 41.73it/s, loss=0.278, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 21/21 [00:00<00:00, 41.63it/s, loss=0.278, v_num=m1w9, BTC_val\u001b[A\n",
      "Epoch 56:  95%|▉| 20/21 [00:00<00:00, 39.14it/s, loss=0.276, v_num=m1w9, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.656. Signaling Trainer to stop.\n",
      "Epoch 56: 100%|█| 21/21 [00:00<00:00, 39.20it/s, loss=0.276, v_num=m1w9, BTC_val\n",
      "Epoch 56: 100%|█| 21/21 [00:00<00:00, 39.02it/s, loss=0.276, v_num=m1w9, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 71.55it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6060606241226196,\n",
      " 'BTC_test_f1': 0.5814634561538696,\n",
      " 'test_loss': 0.7420673966407776}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 100216\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102401-2ipwm1w9/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102401-2ipwm1w9/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.87828\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.87667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.27754\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 56\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1140\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 38\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625901879\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.53846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.53571\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.74836\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.85366\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.85287\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.25894\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.60606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.58146\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.74207\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▁▁▂▁▂▂▂▃▃▃▄▄▄▅▅▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▁▁▂▂▂▂▂▃▃▃▄▄▄▅▅▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ███▇█▇▇▇▇▇▇▇▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▃▃▃▃▃▃▃▃▃▃▃▃▃█▃▃▅▅▃▅▅▃▅▃▃▃▅▅▅▅▅▅▆▆▅▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▂▁▁▁▁▁▁▁▁▁▁▁▁▁█▄▁▆▅▁▆▅▁▅▁▁▁▅▆▅▆▆▄▇▇▄▆▄▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▂▂▂▂▂▂▂▂▃▃▄▃▃▆▁▂▃▂▂▆▁▂█▄█▇█▂▁▁▃▄▆▂▂▃▂▅▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▄▃▂▃▅▄▆▆▆▇█▇▆▇█▇▆█▅█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▄▃▂▃▅▃▆▆▆▇█▇▆▇█▇▆█▅█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▆▇▇▇▆▆▅▅▄▃▃▃▃▄▁▄▃▂▇▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/2ipwm1w9\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:24:56.346902: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/2flzwjrd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102455-2flzwjrd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 109 K \n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.532     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 42.65it/s, loss=1.12, v_num=wjrd, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.119\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 42.22it/s, loss=1.12, v_num=wjrd, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 42.10it/s, loss=1.09, v_num=wjrd, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 42.01it/s, loss=1.09, v_num=wjrd, BTC_val_a\u001b[A\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 38.02it/s, loss=1.08, v_num=wjrd, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 38.03it/s, loss=1.08, v_num=wjrd, BTC_val_a\u001b[A\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 39.88it/s, loss=1.07, v_num=wjrd, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 39.87it/s, loss=1.07, v_num=wjrd, BTC_val_a\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 41.21it/s, loss=1.05, v_num=wjrd, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 1.115\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 40.83it/s, loss=1.05, v_num=wjrd, BTC_val_a\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 36.45it/s, loss=1.06, v_num=wjrd, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 36.04it/s, loss=1.06, v_num=wjrd, BTC_val_a\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 35.22it/s, loss=1.05, v_num=wjrd, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 34.81it/s, loss=1.05, v_num=wjrd, BTC_val_a\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 39.65it/s, loss=1.03, v_num=wjrd, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 39.37it/s, loss=1.03, v_num=wjrd, BTC_val_a\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 40.53it/s, loss=1.04, v_num=wjrd, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 40.14it/s, loss=1.04, v_num=wjrd, BTC_val_a\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 37.99it/s, loss=1.02, v_num=wjrd, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 38.10it/s, loss=1.02, v_num=wjrd, BTC_val_a\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 42.61it/s, loss=1.03, v_num=wjrd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 42.47it/s, loss=1.03, v_num=wjrd, BTC_val_\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 40.11it/s, loss=1.03, v_num=wjrd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 39.70it/s, loss=1.03, v_num=wjrd, BTC_val_\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 42.02it/s, loss=1.01, v_num=wjrd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 41.90it/s, loss=1.01, v_num=wjrd, BTC_val_\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 40.02it/s, loss=1.02, v_num=wjrd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 39.78it/s, loss=1.02, v_num=wjrd, BTC_val_\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 39.85it/s, loss=1.01, v_num=wjrd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 39.79it/s, loss=1.01, v_num=wjrd, BTC_val_\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 39.07it/s, loss=0.989, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 39.32it/s, loss=0.989, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 40.92it/s, loss=0.972, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 1.106\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 40.95it/s, loss=0.972, v_num=wjrd, BTC_val\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 41.26it/s, loss=0.929, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 41.25it/s, loss=0.929, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 35.06it/s, loss=0.916, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 1.097\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 35.23it/s, loss=0.916, v_num=wjrd, BTC_val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 41.95it/s, loss=0.895, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.046 >= min_delta = 0.003. New best score: 1.052\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 41.76it/s, loss=0.895, v_num=wjrd, BTC_val\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 40.59it/s, loss=0.864, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 40.50it/s, loss=0.864, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 40.18it/s, loss=0.853, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 40.26it/s, loss=0.853, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 41.65it/s, loss=0.829, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 41.18it/s, loss=0.829, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 40.95it/s, loss=0.795, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 40.46it/s, loss=0.795, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 41.66it/s, loss=0.775, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 41.49it/s, loss=0.775, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 39.31it/s, loss=0.773, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 1.042\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 39.08it/s, loss=0.773, v_num=wjrd, BTC_val\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 37.84it/s, loss=0.761, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 38.12it/s, loss=0.761, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 40.66it/s, loss=0.752, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.045 >= min_delta = 0.003. New best score: 0.997\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 40.66it/s, loss=0.752, v_num=wjrd, BTC_val\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 41.10it/s, loss=0.758, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 41.26it/s, loss=0.758, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 40.02it/s, loss=0.725, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.980\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 40.04it/s, loss=0.725, v_num=wjrd, BTC_val\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 39.78it/s, loss=0.723, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 39.66it/s, loss=0.723, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 39.73it/s, loss=0.7, v_num=wjrd, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 39.78it/s, loss=0.7, v_num=wjrd, BTC_val_a\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 39.26it/s, loss=0.706, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 39.25it/s, loss=0.706, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 41.27it/s, loss=0.669, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 41.28it/s, loss=0.669, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 41.55it/s, loss=0.692, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 41.38it/s, loss=0.692, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 38.64it/s, loss=0.666, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 38.75it/s, loss=0.666, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 40.18it/s, loss=0.676, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 39.04it/s, loss=0.676, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 39.56it/s, loss=0.648, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 39.44it/s, loss=0.648, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 20/21 [00:00<00:00, 39.25it/s, loss=0.637, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 39.42it/s, loss=0.637, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 20/21 [00:00<00:00, 38.69it/s, loss=0.62, v_num=wjrd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 38.50it/s, loss=0.62, v_num=wjrd, BTC_val_\u001b[A\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 39.86it/s, loss=0.645, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 39.17it/s, loss=0.645, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 20/21 [00:00<00:00, 38.57it/s, loss=0.622, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 38.39it/s, loss=0.622, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 20/21 [00:00<00:00, 38.57it/s, loss=0.616, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 38.45it/s, loss=0.616, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 20/21 [00:00<00:00, 39.02it/s, loss=0.606, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 39.21it/s, loss=0.606, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 44:  95%|▉| 20/21 [00:00<00:00, 39.85it/s, loss=0.592, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 39.97it/s, loss=0.592, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 45:  95%|▉| 20/21 [00:00<00:00, 39.69it/s, loss=0.577, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 39.90it/s, loss=0.577, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 20/21 [00:00<00:00, 40.41it/s, loss=0.592, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 40.30it/s, loss=0.592, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 20/21 [00:00<00:00, 39.37it/s, loss=0.572, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 39.22it/s, loss=0.572, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 48:  95%|▉| 20/21 [00:00<00:00, 38.31it/s, loss=0.568, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 38.55it/s, loss=0.568, v_num=wjrd, BTC_val\u001b[A\n",
      "Epoch 49:  95%|▉| 20/21 [00:00<00:00, 37.22it/s, loss=0.535, v_num=wjrd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.980. Signaling Trainer to stop.\n",
      "Epoch 49: 100%|█| 21/21 [00:00<00:00, 37.21it/s, loss=0.535, v_num=wjrd, BTC_val\n",
      "Epoch 49: 100%|█| 21/21 [00:00<00:00, 37.04it/s, loss=0.535, v_num=wjrd, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 73.59it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.4545454680919647,\n",
      " 'BTC_test_f1': 0.3614130914211273,\n",
      " 'test_loss': 1.01395583152771}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 100426\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102455-2flzwjrd/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102455-2flzwjrd/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.76531\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.75914\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.53828\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 49\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 34\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625901929\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 120\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.46154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.38781\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.92683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.92674\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.35363\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.45455\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.36141\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.01396\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▆▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▆▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ██▇▇▇▇▇▇▇▇▇▇▆▆▆▆▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃█▁▃▆▆▆▆▃▆▁▆▃▃▃▃▃▆▆▃▆▃▃▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄█▁▄▆▇▆▇▄▇▁▆▅▄▄▅▄▇▇▅▆▅▄▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▃▅▅▄▃▄▅▄▄▄▄▆▄▃█▃▂▃▃▃▂▂▁▄▁▅▂▂▄▇▃█▃▃▂▃▂▅▂▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▃▂▂▂▁▂▄▄▄▂▄▆▆▆▇▅▅▆▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▃▂▂▂▁▂▃▄▄▂▄▅▅▆▆▅▅▆▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇█▇██▇▆▅▅▆▆▄▄▂▂▄▄▃▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/2flzwjrd\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:25:46.458849: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/ff7qkvw2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102545-ff7qkvw2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 109 K \n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.532     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 44.87it/s, loss=0.755, v_num=kvw2, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.693\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 44.04it/s, loss=0.755, v_num=kvw2, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 42.41it/s, loss=0.708, v_num=kvw2, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 42.35it/s, loss=0.708, v_num=kvw2, BTC_val_\u001b[A\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 39.31it/s, loss=0.692, v_num=kvw2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 39.26it/s, loss=0.692, v_num=kvw2, BTC_val_\u001b[A\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 42.01it/s, loss=0.696, v_num=kvw2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 41.53it/s, loss=0.696, v_num=kvw2, BTC_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 37.34it/s, loss=0.69, v_num=kvw2, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 37.53it/s, loss=0.69, v_num=kvw2, BTC_val_a\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 41.42it/s, loss=0.685, v_num=kvw2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 41.16it/s, loss=0.685, v_num=kvw2, BTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 39.06it/s, loss=0.687, v_num=kvw2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 39.26it/s, loss=0.687, v_num=kvw2, BTC_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 39.11it/s, loss=0.694, v_num=kvw2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 39.14it/s, loss=0.694, v_num=kvw2, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 39.79it/s, loss=0.682, v_num=kvw2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 39.52it/s, loss=0.682, v_num=kvw2, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 42.47it/s, loss=0.683, v_num=kvw2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 42.40it/s, loss=0.683, v_num=kvw2, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 40.61it/s, loss=0.683, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 40.74it/s, loss=0.683, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 43.08it/s, loss=0.669, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 43.06it/s, loss=0.669, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 43.55it/s, loss=0.676, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 43.34it/s, loss=0.676, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 41.26it/s, loss=0.664, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 41.06it/s, loss=0.664, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 42.38it/s, loss=0.647, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 42.12it/s, loss=0.647, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 40.76it/s, loss=0.644, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 40.77it/s, loss=0.644, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 40.30it/s, loss=0.624, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 40.31it/s, loss=0.624, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 39.04it/s, loss=0.596, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 39.17it/s, loss=0.596, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 34.57it/s, loss=0.581, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 34.55it/s, loss=0.581, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 40.31it/s, loss=0.552, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 40.27it/s, loss=0.552, v_num=kvw2, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 40.72it/s, loss=0.54, v_num=kvw2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.687\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 40.46it/s, loss=0.54, v_num=kvw2, BTC_val_\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 41.76it/s, loss=0.527, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 41.06it/s, loss=0.527, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 42.68it/s, loss=0.519, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 41.21it/s, loss=0.519, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 40.90it/s, loss=0.509, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 39.95it/s, loss=0.509, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 40.81it/s, loss=0.499, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.683\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 40.67it/s, loss=0.499, v_num=kvw2, BTC_val\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 38.29it/s, loss=0.491, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 38.52it/s, loss=0.491, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 41.96it/s, loss=0.505, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 41.71it/s, loss=0.505, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 40.15it/s, loss=0.482, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 39.56it/s, loss=0.482, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 39.22it/s, loss=0.469, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 39.34it/s, loss=0.469, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 39.35it/s, loss=0.465, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.677\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 39.46it/s, loss=0.465, v_num=kvw2, BTC_val\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 40.80it/s, loss=0.442, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 40.82it/s, loss=0.442, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 40.53it/s, loss=0.445, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 40.69it/s, loss=0.445, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 39.91it/s, loss=0.443, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 39.94it/s, loss=0.443, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 40.79it/s, loss=0.414, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 40.84it/s, loss=0.414, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 43.13it/s, loss=0.41, v_num=kvw2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 0.664\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 42.99it/s, loss=0.41, v_num=kvw2, BTC_val_\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 40.12it/s, loss=0.402, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 40.01it/s, loss=0.402, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 40.46it/s, loss=0.417, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 40.28it/s, loss=0.417, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 41.23it/s, loss=0.401, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 41.35it/s, loss=0.401, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 20/21 [00:00<00:00, 41.53it/s, loss=0.395, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 41.26it/s, loss=0.395, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 20/21 [00:00<00:00, 42.51it/s, loss=0.371, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 42.30it/s, loss=0.371, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 40.31it/s, loss=0.372, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 40.40it/s, loss=0.372, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 20/21 [00:00<00:00, 41.23it/s, loss=0.359, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 40.94it/s, loss=0.359, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 20/21 [00:00<00:00, 39.97it/s, loss=0.35, v_num=kvw2, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 40.11it/s, loss=0.35, v_num=kvw2, BTC_val_\u001b[A\n",
      "Epoch 43:  95%|▉| 20/21 [00:00<00:00, 41.28it/s, loss=0.335, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 40.85it/s, loss=0.335, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 44:  95%|▉| 20/21 [00:00<00:00, 39.82it/s, loss=0.348, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 39.62it/s, loss=0.348, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 45:  95%|▉| 20/21 [00:00<00:00, 38.86it/s, loss=0.315, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 38.98it/s, loss=0.315, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 20/21 [00:00<00:00, 39.82it/s, loss=0.341, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 39.87it/s, loss=0.341, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 20/21 [00:00<00:00, 41.17it/s, loss=0.322, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 40.83it/s, loss=0.322, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 48:  95%|▉| 20/21 [00:00<00:00, 40.77it/s, loss=0.319, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 40.93it/s, loss=0.319, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 49:  95%|▉| 20/21 [00:00<00:00, 44.22it/s, loss=0.309, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 21/21 [00:00<00:00, 43.97it/s, loss=0.309, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 50:  95%|▉| 20/21 [00:00<00:00, 38.97it/s, loss=0.305, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 21/21 [00:00<00:00, 38.98it/s, loss=0.305, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 51:  95%|▉| 20/21 [00:00<00:00, 41.71it/s, loss=0.306, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 21/21 [00:00<00:00, 41.62it/s, loss=0.306, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 52:  95%|▉| 20/21 [00:00<00:00, 39.95it/s, loss=0.306, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 21/21 [00:00<00:00, 39.54it/s, loss=0.306, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 53:  95%|▉| 20/21 [00:00<00:00, 41.75it/s, loss=0.282, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 21/21 [00:00<00:00, 41.45it/s, loss=0.282, v_num=kvw2, BTC_val\u001b[A\n",
      "Epoch 54:  95%|▉| 20/21 [00:00<00:00, 40.07it/s, loss=0.271, v_num=kvw2, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.664. Signaling Trainer to stop.\n",
      "Epoch 54: 100%|█| 21/21 [00:00<00:00, 39.99it/s, loss=0.271, v_num=kvw2, BTC_val\n",
      "Epoch 54: 100%|█| 21/21 [00:00<00:00, 39.81it/s, loss=0.271, v_num=kvw2, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 64.17it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6666666865348816,\n",
      " 'BTC_test_f1': 0.6666666269302368,\n",
      " 'test_loss': 0.7019655108451843}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 100620\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102545-ff7qkvw2/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102545-ff7qkvw2/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.87351\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.87208\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.272\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 54\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 36\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625901981\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 132\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.61538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.60606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.99407\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.90244\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.89487\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.22627\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.70197\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▁▂▂▂▂▃▂▃▃▄▄▄▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▃▃▃▃▄▃▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▇▇▇▇▇▇▇▇▇▆▆▅▅▅▅▄▄▄▄▄▃▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▃▃▃▃▃▃▃▃▃▃▃▃▃▅▅▁▃▃▆▃▃▅▅▃▆▅▅▅▅▅▃▅▃▅▃█▅▅▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▂▂▂▂▂▂▂▂▂▂▂▂▂▂▆▁▂▂▇▂▂▆▄▂▇▆▅▅▅▄▂▅▂▄▂█▄▄▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▂▁▂▂▂▁▁▁▂▂▃▃▁▁▁▁▂▁▄▂▁▂▄▂▁▂▄▆▅▇▂▄▃█▂█▇▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▃▁▄▄▂▃▄▆▅▆▆▆▅▆▇▆▇▇▇▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▁▅▄▃▃▄▆▅▅▆▆▆▆▇▆▇▇▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ██▇█▇█▆▅▅▄▅▅▆▄▃▃▃▃▃▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/ff7qkvw2\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:26:38.604583: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/220hlugq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102637-220hlugq\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 109 K \n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.532     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 43.48it/s, loss=1.14, v_num=lugq, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.107\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 43.62it/s, loss=1.14, v_num=lugq, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 40.30it/s, loss=1.09, v_num=lugq, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 40.24it/s, loss=1.09, v_num=lugq, BTC_val_a\u001b[A\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 40.41it/s, loss=1.06, v_num=lugq, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 40.68it/s, loss=1.06, v_num=lugq, BTC_val_a\u001b[A\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 39.60it/s, loss=1.06, v_num=lugq, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 39.68it/s, loss=1.06, v_num=lugq, BTC_val_a\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 39.60it/s, loss=1.06, v_num=lugq, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 39.46it/s, loss=1.06, v_num=lugq, BTC_val_a\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 39.81it/s, loss=1.06, v_num=lugq, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 39.75it/s, loss=1.06, v_num=lugq, BTC_val_a\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 38.14it/s, loss=1.04, v_num=lugq, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 38.23it/s, loss=1.04, v_num=lugq, BTC_val_a\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 40.33it/s, loss=1.04, v_num=lugq, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 40.60it/s, loss=1.04, v_num=lugq, BTC_val_a\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 41.44it/s, loss=1.03, v_num=lugq, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 41.37it/s, loss=1.03, v_num=lugq, BTC_val_a\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 38.83it/s, loss=1.03, v_num=lugq, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 38.77it/s, loss=1.03, v_num=lugq, BTC_val_a\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 42.05it/s, loss=1.02, v_num=lugq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 42.11it/s, loss=1.02, v_num=lugq, BTC_val_\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 40.85it/s, loss=1.04, v_num=lugq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 40.95it/s, loss=1.04, v_num=lugq, BTC_val_\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 39.57it/s, loss=1.02, v_num=lugq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 39.52it/s, loss=1.02, v_num=lugq, BTC_val_\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 41.30it/s, loss=1.01, v_num=lugq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 41.29it/s, loss=1.01, v_num=lugq, BTC_val_\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 41.54it/s, loss=1.01, v_num=lugq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 41.50it/s, loss=1.01, v_num=lugq, BTC_val_\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 41.83it/s, loss=0.992, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 41.54it/s, loss=0.992, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 40.70it/s, loss=0.981, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 39.73it/s, loss=0.981, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 40.06it/s, loss=0.962, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 39.74it/s, loss=0.962, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 37.04it/s, loss=0.959, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 37.18it/s, loss=0.959, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 41.65it/s, loss=0.928, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 41.70it/s, loss=0.928, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 39.68it/s, loss=0.917, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.045 >= min_delta = 0.003. New best score: 1.062\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 38.96it/s, loss=0.917, v_num=lugq, BTC_val\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 41.75it/s, loss=0.885, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 40.22it/s, loss=0.885, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 39.22it/s, loss=0.841, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 39.10it/s, loss=0.841, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 40.27it/s, loss=0.845, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 40.48it/s, loss=0.845, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 42.59it/s, loss=0.8, v_num=lugq, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 42.47it/s, loss=0.8, v_num=lugq, BTC_val_a\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 39.61it/s, loss=0.8, v_num=lugq, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 39.77it/s, loss=0.8, v_num=lugq, BTC_val_a\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 39.76it/s, loss=0.793, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 39.99it/s, loss=0.793, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 39.11it/s, loss=0.761, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 39.12it/s, loss=0.761, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 39.34it/s, loss=0.767, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 39.51it/s, loss=0.767, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 40.02it/s, loss=0.773, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 40.06it/s, loss=0.773, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 41.95it/s, loss=0.752, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.042 >= min_delta = 0.003. New best score: 1.020\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 41.96it/s, loss=0.752, v_num=lugq, BTC_val\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 40.94it/s, loss=0.719, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 40.98it/s, loss=0.719, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 39.57it/s, loss=0.746, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 39.59it/s, loss=0.746, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 38.93it/s, loss=0.699, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 38.55it/s, loss=0.699, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 39.43it/s, loss=0.702, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 1.015\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 39.03it/s, loss=0.702, v_num=lugq, BTC_val\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 41.79it/s, loss=0.698, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 42.00it/s, loss=0.698, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 39.19it/s, loss=0.696, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 39.09it/s, loss=0.696, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 41.86it/s, loss=0.7, v_num=lugq, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 1.002\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 41.41it/s, loss=0.7, v_num=lugq, BTC_val_a\n",
      "Epoch 38:  95%|▉| 20/21 [00:00<00:00, 41.96it/s, loss=0.69, v_num=lugq, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.051 >= min_delta = 0.003. New best score: 0.952\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 41.86it/s, loss=0.69, v_num=lugq, BTC_val_\n",
      "Epoch 39:  95%|▉| 20/21 [00:00<00:00, 40.64it/s, loss=0.664, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 40.36it/s, loss=0.664, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 39.49it/s, loss=0.646, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 39.04it/s, loss=0.646, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 20/21 [00:00<00:00, 41.00it/s, loss=0.631, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 40.85it/s, loss=0.631, v_num=lugq, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42:  95%|▉| 20/21 [00:00<00:00, 42.30it/s, loss=0.646, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 42.22it/s, loss=0.646, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 20/21 [00:00<00:00, 39.27it/s, loss=0.643, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 39.08it/s, loss=0.643, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 44:  95%|▉| 20/21 [00:00<00:00, 39.72it/s, loss=0.615, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 39.83it/s, loss=0.615, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 45:  95%|▉| 20/21 [00:00<00:00, 39.93it/s, loss=0.612, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 39.78it/s, loss=0.612, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 20/21 [00:00<00:00, 38.92it/s, loss=0.614, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 39.06it/s, loss=0.614, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 20/21 [00:00<00:00, 40.18it/s, loss=0.6, v_num=lugq, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 40.19it/s, loss=0.6, v_num=lugq, BTC_val_a\u001b[A\n",
      "Epoch 48:  95%|▉| 20/21 [00:00<00:00, 40.62it/s, loss=0.614, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 40.59it/s, loss=0.614, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 49:  95%|▉| 20/21 [00:00<00:00, 41.35it/s, loss=0.579, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 21/21 [00:00<00:00, 40.87it/s, loss=0.579, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 50:  95%|▉| 20/21 [00:00<00:00, 39.06it/s, loss=0.566, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 21/21 [00:00<00:00, 38.18it/s, loss=0.566, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 51:  95%|▉| 20/21 [00:00<00:00, 40.90it/s, loss=0.577, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 21/21 [00:00<00:00, 40.30it/s, loss=0.577, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 52:  95%|▉| 20/21 [00:00<00:00, 39.73it/s, loss=0.555, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 21/21 [00:00<00:00, 39.60it/s, loss=0.555, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 53:  95%|▉| 20/21 [00:00<00:00, 40.89it/s, loss=0.514, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 21/21 [00:00<00:00, 40.86it/s, loss=0.514, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 54:  95%|▉| 20/21 [00:00<00:00, 42.12it/s, loss=0.557, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 21/21 [00:00<00:00, 42.12it/s, loss=0.557, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 55:  95%|▉| 20/21 [00:00<00:00, 42.40it/s, loss=0.532, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 21/21 [00:00<00:00, 42.31it/s, loss=0.532, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 56:  95%|▉| 20/21 [00:00<00:00, 42.38it/s, loss=0.517, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 21/21 [00:00<00:00, 42.22it/s, loss=0.517, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 57:  95%|▉| 20/21 [00:00<00:00, 39.33it/s, loss=0.517, v_num=lugq, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 21/21 [00:00<00:00, 39.61it/s, loss=0.517, v_num=lugq, BTC_val\u001b[A\n",
      "Epoch 58:  95%|▉| 20/21 [00:00<00:00, 42.67it/s, loss=0.5, v_num=lugq, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.952. Signaling Trainer to stop.\n",
      "Epoch 58: 100%|█| 21/21 [00:00<00:00, 42.37it/s, loss=0.5, v_num=lugq, BTC_val_a\n",
      "Epoch 58: 100%|█| 21/21 [00:00<00:00, 42.15it/s, loss=0.5, v_num=lugq, BTC_val_a\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 72.23it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.4545454680919647,\n",
      " 'BTC_test_f1': 0.4300858974456787,\n",
      " 'test_loss': 0.9747630953788757}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 100825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102637-220hlugq/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102637-220hlugq/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.77884\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.77492\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.50229\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 58\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1180\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 39\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902036\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 141\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.38462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.28294\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.82812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.80592\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.47961\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.45455\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.43009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.97476\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▃▂▃▃▃▃▃▃▃▄▄▅▅▅▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▂▂▃▃▃▃▃▄▄▄▄▅▅▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▇▇▇▇▇▇▇▇▇▆▆▆▆▅▅▄▄▄▄▃▄▃▃▃▃▃▂▃▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▃▃▃▃▃▃▃▃▃▃▃▃▃▆▃▃▃▆▄▃▃▃█▆██▆▃▆▄▄▆█▆█▄▄▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▂▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▅▄▁▁▁▇▅▇█▆▁▅▄▄▅▇▅▇▄▄▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▂▃▃▃▃▃▃▃▃▃▃▃▃▄▂▃▃▅▂▂▃▅▃▁▂▁▁▂█▂▂▂▂▂▃▁▃▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▂▃▃▁▃▄▄▄▄▄▄▆▄▅▆▆▇▅▇▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▂▃▁▃▄▃▄▄▄▄▆▄▅▆▆▇▅▇▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▇▆█▆▆▆▆▅▅▅▄▅▄▃▄▂▅▃▁▂▄▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/220hlugq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:27:32.258055: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/2llmjaik\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102730-2llmjaik\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 43.84it/s, loss=0.705, v_num=jaik, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.680\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 42.96it/s, loss=0.705, v_num=jaik, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 43.98it/s, loss=0.671, v_num=jaik, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.042 >= min_delta = 0.003. New best score: 0.637\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 43.87it/s, loss=0.671, v_num=jaik, BTC_val_\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 41.73it/s, loss=0.625, v_num=jaik, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.025 >= min_delta = 0.003. New best score: 0.612\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 41.35it/s, loss=0.625, v_num=jaik, BTC_val_\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 39.85it/s, loss=0.612, v_num=jaik, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.608\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 39.82it/s, loss=0.612, v_num=jaik, BTC_val_\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 40.67it/s, loss=0.586, v_num=jaik, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.018 >= min_delta = 0.003. New best score: 0.590\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 40.75it/s, loss=0.586, v_num=jaik, BTC_val_\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 41.83it/s, loss=0.557, v_num=jaik, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.016 >= min_delta = 0.003. New best score: 0.574\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 41.74it/s, loss=0.557, v_num=jaik, BTC_val_\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 43.15it/s, loss=0.552, v_num=jaik, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 43.13it/s, loss=0.552, v_num=jaik, BTC_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 40.86it/s, loss=0.528, v_num=jaik, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 40.83it/s, loss=0.528, v_num=jaik, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 40.66it/s, loss=0.53, v_num=jaik, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 40.74it/s, loss=0.53, v_num=jaik, BTC_val_a\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 41.42it/s, loss=0.533, v_num=jaik, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 40.76it/s, loss=0.533, v_num=jaik, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 40.66it/s, loss=0.511, v_num=jaik, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 40.59it/s, loss=0.511, v_num=jaik, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 41.43it/s, loss=0.502, v_num=jaik, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 40.68it/s, loss=0.502, v_num=jaik, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 40.03it/s, loss=0.487, v_num=jaik, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 40.22it/s, loss=0.487, v_num=jaik, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 40.37it/s, loss=0.486, v_num=jaik, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 40.49it/s, loss=0.486, v_num=jaik, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 40.24it/s, loss=0.49, v_num=jaik, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 40.23it/s, loss=0.49, v_num=jaik, BTC_val_\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 40.97it/s, loss=0.484, v_num=jaik, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 40.37it/s, loss=0.484, v_num=jaik, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 43.36it/s, loss=0.479, v_num=jaik, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 43.50it/s, loss=0.479, v_num=jaik, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 42.38it/s, loss=0.489, v_num=jaik, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 42.05it/s, loss=0.489, v_num=jaik, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 41.30it/s, loss=0.47, v_num=jaik, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 41.17it/s, loss=0.47, v_num=jaik, BTC_val_\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 41.89it/s, loss=0.481, v_num=jaik, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 41.46it/s, loss=0.481, v_num=jaik, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 41.78it/s, loss=0.457, v_num=jaik, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 41.56it/s, loss=0.457, v_num=jaik, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 34.77it/s, loss=0.494, v_num=jaik, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 35.09it/s, loss=0.494, v_num=jaik, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 42.51it/s, loss=0.469, v_num=jaik, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 42.18it/s, loss=0.469, v_num=jaik, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 43.43it/s, loss=0.485, v_num=jaik, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 43.42it/s, loss=0.485, v_num=jaik, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 41.27it/s, loss=0.477, v_num=jaik, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 41.01it/s, loss=0.477, v_num=jaik, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 41.80it/s, loss=0.478, v_num=jaik, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.574. Signaling Trainer to stop.\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 41.34it/s, loss=0.478, v_num=jaik, BTC_val\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 41.03it/s, loss=0.478, v_num=jaik, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 65.47it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.7575757503509521,\n",
      " 'BTC_test_f1': 0.7518796920776367,\n",
      " 'test_loss': 0.5874589085578918}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 101013\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102730-2llmjaik/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102730-2llmjaik/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.77884\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.77466\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.47581\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 520\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902072\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 62\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.69231\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.69048\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.65246\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.7561\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.73718\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.55964\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.75758\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.75188\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.58746\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▃▄▅▅▆▆▇▇▇▇█▇█▇████████▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▃▄▅▅▆▆▇▇▇▇█▇████▇█████▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▂▁▂▁▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▅██████████▅██▅▅██▁██▁███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▆██████████▆██▆▆██▄██▄███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▆▄▃▃▂▁▂▂▄▃▅▃▆▅▇▇█▆█▆▇▅█▆▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▃▁▃▂▅▆▅█▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▃▁▂▂▅▆▅█▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ██▅█▄▆▄▁▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/2llmjaik\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:28:06.900221: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/2bzg1exm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102805-2bzg1exm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 45.11it/s, loss=1.11, v_num=1exm, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.102\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 45.09it/s, loss=1.11, v_num=1exm, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 43.79it/s, loss=1.1, v_num=1exm, BTC_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.065 >= min_delta = 0.003. New best score: 1.037\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 43.64it/s, loss=1.1, v_num=1exm, BTC_val_ac\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 40.79it/s, loss=1.03, v_num=1exm, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.173 >= min_delta = 0.003. New best score: 0.864\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 40.71it/s, loss=1.03, v_num=1exm, BTC_val_a\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 41.85it/s, loss=0.986, v_num=1exm, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 0.861\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 42.03it/s, loss=0.986, v_num=1exm, BTC_val_\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 41.12it/s, loss=0.942, v_num=1exm, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 40.77it/s, loss=0.942, v_num=1exm, BTC_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 41.57it/s, loss=0.913, v_num=1exm, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 0.840\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 41.53it/s, loss=0.913, v_num=1exm, BTC_val_\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 44.42it/s, loss=0.862, v_num=1exm, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.137 >= min_delta = 0.003. New best score: 0.703\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 43.53it/s, loss=0.862, v_num=1exm, BTC_val_\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 42.89it/s, loss=0.829, v_num=1exm, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 42.94it/s, loss=0.829, v_num=1exm, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 41.82it/s, loss=0.808, v_num=1exm, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 41.46it/s, loss=0.808, v_num=1exm, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 40.37it/s, loss=0.794, v_num=1exm, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 39.71it/s, loss=0.794, v_num=1exm, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 42.10it/s, loss=0.793, v_num=1exm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 42.06it/s, loss=0.793, v_num=1exm, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 43.42it/s, loss=0.786, v_num=1exm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 43.10it/s, loss=0.786, v_num=1exm, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 43.05it/s, loss=0.759, v_num=1exm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 43.19it/s, loss=0.759, v_num=1exm, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 42.23it/s, loss=0.775, v_num=1exm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 41.80it/s, loss=0.775, v_num=1exm, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 40.90it/s, loss=0.778, v_num=1exm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 41.11it/s, loss=0.778, v_num=1exm, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 40.48it/s, loss=0.804, v_num=1exm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 40.50it/s, loss=0.804, v_num=1exm, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 41.45it/s, loss=0.757, v_num=1exm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 41.10it/s, loss=0.757, v_num=1exm, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 41.44it/s, loss=0.782, v_num=1exm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 41.48it/s, loss=0.782, v_num=1exm, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 42.46it/s, loss=0.781, v_num=1exm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 42.38it/s, loss=0.781, v_num=1exm, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 41.96it/s, loss=0.755, v_num=1exm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 41.47it/s, loss=0.755, v_num=1exm, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 41.73it/s, loss=0.785, v_num=1exm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 41.19it/s, loss=0.785, v_num=1exm, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 36.14it/s, loss=0.796, v_num=1exm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 36.20it/s, loss=0.796, v_num=1exm, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 41.08it/s, loss=0.769, v_num=1exm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 40.99it/s, loss=0.769, v_num=1exm, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 44.52it/s, loss=0.737, v_num=1exm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 44.07it/s, loss=0.737, v_num=1exm, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 42.19it/s, loss=0.765, v_num=1exm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 42.22it/s, loss=0.765, v_num=1exm, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 41.05it/s, loss=0.743, v_num=1exm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 40.99it/s, loss=0.743, v_num=1exm, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 40.02it/s, loss=0.794, v_num=1exm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.703. Signaling Trainer to stop.\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 39.92it/s, loss=0.794, v_num=1exm, BTC_val\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 39.71it/s, loss=0.794, v_num=1exm, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 71.32it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6060606241226196,\n",
      " 'BTC_test_f1': 0.6051282286643982,\n",
      " 'test_loss': 0.8032497763633728}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 101147\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102805-2bzg1exm/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102805-2bzg1exm/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.63723\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.62296\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.79055\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 540\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 22\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902107\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.53846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.44848\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.90678\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.63415\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.62733\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.80067\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.60606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.60513\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.80325\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▄▄▅▅▇▇▇▇▇▇█▇▇▇█▇▇█▇▇████▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▄▄▅▅▇▇▇▇▇▇█▇▇▇█▇▇█▇▇████▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ██▆▆▅▄▃▃▂▂▂▂▁▂▂▂▁▂▂▁▂▂▂▁▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▆▆▃█▆▇▇▆▂▇▅▇▆▇▆▂▅▅▃▆▅▅▃▅▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▅▆▄█▆▇▇▆▃▇▄▇▆▇▆▃▄▄▄▆▄▄▄▄▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▆▆▃▃▃▃▁▂▂█▂▃▂▄▂▃█▃▃█▄▄▄▃▄▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▇▆█▆▇▆▄▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▆▅█▅▇▆▄▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▄▂▂▂▁▆▄▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/2bzg1exm\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:28:42.834271: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/txlkl19a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102841-txlkl19a\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 43.53it/s, loss=0.713, v_num=l19a, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.690\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 43.11it/s, loss=0.713, v_num=l19a, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 44.38it/s, loss=0.668, v_num=l19a, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.036 >= min_delta = 0.003. New best score: 0.653\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 44.25it/s, loss=0.668, v_num=l19a, BTC_val_\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 40.93it/s, loss=0.631, v_num=l19a, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.057 >= min_delta = 0.003. New best score: 0.597\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 41.02it/s, loss=0.631, v_num=l19a, BTC_val_\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 41.58it/s, loss=0.612, v_num=l19a, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 41.25it/s, loss=0.612, v_num=l19a, BTC_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 41.27it/s, loss=0.575, v_num=l19a, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 40.32it/s, loss=0.575, v_num=l19a, BTC_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 41.11it/s, loss=0.572, v_num=l19a, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 40.55it/s, loss=0.572, v_num=l19a, BTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 42.52it/s, loss=0.527, v_num=l19a, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.022 >= min_delta = 0.003. New best score: 0.574\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 42.31it/s, loss=0.527, v_num=l19a, BTC_val_\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 41.33it/s, loss=0.522, v_num=l19a, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 41.12it/s, loss=0.522, v_num=l19a, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 40.76it/s, loss=0.502, v_num=l19a, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 40.86it/s, loss=0.502, v_num=l19a, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 44.20it/s, loss=0.477, v_num=l19a, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 43.69it/s, loss=0.477, v_num=l19a, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 43.74it/s, loss=0.497, v_num=l19a, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 43.70it/s, loss=0.497, v_num=l19a, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 43.58it/s, loss=0.511, v_num=l19a, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 43.11it/s, loss=0.511, v_num=l19a, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 40.84it/s, loss=0.499, v_num=l19a, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 40.93it/s, loss=0.499, v_num=l19a, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 41.81it/s, loss=0.486, v_num=l19a, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 41.76it/s, loss=0.486, v_num=l19a, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 41.78it/s, loss=0.502, v_num=l19a, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 41.90it/s, loss=0.502, v_num=l19a, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 40.94it/s, loss=0.503, v_num=l19a, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 41.04it/s, loss=0.503, v_num=l19a, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 42.10it/s, loss=0.497, v_num=l19a, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 41.99it/s, loss=0.497, v_num=l19a, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 40.52it/s, loss=0.47, v_num=l19a, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 40.30it/s, loss=0.47, v_num=l19a, BTC_val_\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 41.95it/s, loss=0.474, v_num=l19a, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 42.08it/s, loss=0.474, v_num=l19a, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 44.54it/s, loss=0.481, v_num=l19a, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 44.58it/s, loss=0.481, v_num=l19a, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 42.96it/s, loss=0.472, v_num=l19a, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 42.84it/s, loss=0.472, v_num=l19a, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 35.97it/s, loss=0.479, v_num=l19a, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 36.14it/s, loss=0.479, v_num=l19a, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 39.74it/s, loss=0.465, v_num=l19a, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 39.21it/s, loss=0.465, v_num=l19a, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 40.35it/s, loss=0.489, v_num=l19a, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 40.49it/s, loss=0.489, v_num=l19a, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 40.90it/s, loss=0.472, v_num=l19a, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 40.94it/s, loss=0.472, v_num=l19a, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 41.69it/s, loss=0.462, v_num=l19a, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 41.63it/s, loss=0.462, v_num=l19a, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 41.88it/s, loss=0.472, v_num=l19a, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.574. Signaling Trainer to stop.\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 41.45it/s, loss=0.472, v_num=l19a, BTC_val\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 41.21it/s, loss=0.472, v_num=l19a, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 67.95it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6969696879386902,\n",
      " 'BTC_test_f1': 0.6898496747016907,\n",
      " 'test_loss': 0.5915910005569458}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 101332\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102841-txlkl19a/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102841-txlkl19a/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.78123\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.77472\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.47078\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 540\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 22\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.61538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.60606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.65521\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.92683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.92665\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.36346\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.69697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.68985\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.59159\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▄▅▆▆▇▇▇▇█▇▇▇▇▇███████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▃▅▆▆▆▇▇▇██▇▇█▇████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▅▄▄▃▃▂▁▂▂▂▂▂▂▂▁▁▂▁▁▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▆▆▆▃▆▆▆▆▆▆▆▆▆▃▃▁▆█▆▃▃▆▆▆▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▆▇▇▅▇▇▇▇▇▇▇▇▇▅▅▄▇█▇▅▅▇▇▇▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▆▄▂▃▃▂▁▁▂▅▅▃▆▄▂▃▇▄▃▄▃█▄▃▃▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▂▃▂▆▁▁▅▅▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▂▂▂▆▁▁▅▅▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▅▇▁█▆▃▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/txlkl19a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:29:19.021188: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/cfn6008z\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102917-cfn6008z\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 45.55it/s, loss=1.11, v_num=008z, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.101\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 44.51it/s, loss=1.11, v_num=008z, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 41.62it/s, loss=1.07, v_num=008z, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.073 >= min_delta = 0.003. New best score: 1.028\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 41.35it/s, loss=1.07, v_num=008z, BTC_val_a\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 43.79it/s, loss=1.02, v_num=008z, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.152 >= min_delta = 0.003. New best score: 0.876\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 43.78it/s, loss=1.02, v_num=008z, BTC_val_a\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 43.13it/s, loss=0.989, v_num=008z, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 42.65it/s, loss=0.989, v_num=008z, BTC_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 39.97it/s, loss=0.96, v_num=008z, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 39.43it/s, loss=0.96, v_num=008z, BTC_val_a\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 42.08it/s, loss=0.939, v_num=008z, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 41.98it/s, loss=0.939, v_num=008z, BTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 41.19it/s, loss=0.891, v_num=008z, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.064 >= min_delta = 0.003. New best score: 0.812\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 40.67it/s, loss=0.891, v_num=008z, BTC_val_\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 44.07it/s, loss=0.877, v_num=008z, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 43.99it/s, loss=0.877, v_num=008z, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 42.62it/s, loss=0.859, v_num=008z, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.087 >= min_delta = 0.003. New best score: 0.725\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 42.07it/s, loss=0.859, v_num=008z, BTC_val_\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 41.05it/s, loss=0.83, v_num=008z, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 40.67it/s, loss=0.83, v_num=008z, BTC_val_a\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 40.68it/s, loss=0.831, v_num=008z, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 0.716\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 39.86it/s, loss=0.831, v_num=008z, BTC_val\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 44.38it/s, loss=0.823, v_num=008z, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 43.87it/s, loss=0.823, v_num=008z, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 40.28it/s, loss=0.802, v_num=008z, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 40.42it/s, loss=0.802, v_num=008z, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 41.78it/s, loss=0.793, v_num=008z, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 41.87it/s, loss=0.793, v_num=008z, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 40.85it/s, loss=0.78, v_num=008z, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 40.68it/s, loss=0.78, v_num=008z, BTC_val_\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 40.21it/s, loss=0.759, v_num=008z, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 40.37it/s, loss=0.759, v_num=008z, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 43.92it/s, loss=0.753, v_num=008z, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 43.96it/s, loss=0.753, v_num=008z, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 41.82it/s, loss=0.763, v_num=008z, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 41.58it/s, loss=0.763, v_num=008z, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 41.88it/s, loss=0.771, v_num=008z, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 41.76it/s, loss=0.771, v_num=008z, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 40.43it/s, loss=0.789, v_num=008z, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 40.37it/s, loss=0.789, v_num=008z, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 41.32it/s, loss=0.773, v_num=008z, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 41.26it/s, loss=0.773, v_num=008z, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 35.02it/s, loss=0.751, v_num=008z, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 35.28it/s, loss=0.751, v_num=008z, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 41.37it/s, loss=0.778, v_num=008z, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 41.37it/s, loss=0.778, v_num=008z, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 44.51it/s, loss=0.751, v_num=008z, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 44.45it/s, loss=0.751, v_num=008z, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 38.91it/s, loss=0.741, v_num=008z, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 39.15it/s, loss=0.741, v_num=008z, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 44.33it/s, loss=0.731, v_num=008z, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 43.63it/s, loss=0.731, v_num=008z, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 42.00it/s, loss=0.741, v_num=008z, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 42.02it/s, loss=0.741, v_num=008z, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 43.35it/s, loss=0.742, v_num=008z, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 43.17it/s, loss=0.742, v_num=008z, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 42.27it/s, loss=0.739, v_num=008z, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 42.07it/s, loss=0.739, v_num=008z, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 41.61it/s, loss=0.746, v_num=008z, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 41.47it/s, loss=0.746, v_num=008z, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 43.06it/s, loss=0.774, v_num=008z, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.716. Signaling Trainer to stop.\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 42.61it/s, loss=0.774, v_num=008z, BTC_val\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 42.37it/s, loss=0.774, v_num=008z, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 92.08it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5757575631141663,\n",
      " 'BTC_test_f1': 0.5743847489356995,\n",
      " 'test_loss': 0.8615686893463135}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 101512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102917-cfn6008z/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102917-cfn6008z/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.67144\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.66343\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.77366\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 620\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902181\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 74\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.53846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.44848\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.93129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.7561\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.73437\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.73547\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.57576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.57438\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.86157\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▃▄▅▅▆▆▆▇▇▇▇▇▇███▇▇▇█▇████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▄▄▅▅▆▆▆▇▇▇▇▇▇████▇██▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▆▅▅▄▄▃▃▃▃▂▂▂▂▁▂▂▂▂▁▂▁▁▁▁▁▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▅▅█▂▅▂█▅▅▅▇▅▄▅▄▂▁▅▇▄▁▅▁▂▂▂▄▂▂▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▄▄█▃▆▄█▆▆▅▇▅▄▅▅▃▁▆▇▃▂▆▁▃▃▂▃▂▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▄▃▃▃▃▂▂▁▂▁▂▃▂▃▃█▂▁▂▃▂▃▃▄▄▃▃▄▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▄▄▁▄▄▄▇▃▅▆▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▄▁▁▄▅▅▇▄▅▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇█▇▅▅▁▁▄▃▂▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/cfn6008z\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:29:57.539757: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/3ncs3u0x\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102956-3ncs3u0x\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 109 K \n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.532     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 41.78it/s, loss=0.732, v_num=3u0x, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.693\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 41.61it/s, loss=0.732, v_num=3u0x, ETH_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 41.93it/s, loss=0.705, v_num=3u0x, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 42.10it/s, loss=0.705, v_num=3u0x, ETH_val_\u001b[A\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 40.19it/s, loss=0.707, v_num=3u0x, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 40.29it/s, loss=0.707, v_num=3u0x, ETH_val_\u001b[A\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 37.94it/s, loss=0.699, v_num=3u0x, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 38.05it/s, loss=0.699, v_num=3u0x, ETH_val_\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 39.13it/s, loss=0.695, v_num=3u0x, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 38.79it/s, loss=0.695, v_num=3u0x, ETH_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 41.16it/s, loss=0.69, v_num=3u0x, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.018 >= min_delta = 0.003. New best score: 0.674\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 40.55it/s, loss=0.69, v_num=3u0x, ETH_val_a\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 41.91it/s, loss=0.693, v_num=3u0x, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 41.50it/s, loss=0.693, v_num=3u0x, ETH_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 39.40it/s, loss=0.7, v_num=3u0x, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 39.40it/s, loss=0.7, v_num=3u0x, ETH_val_ac\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 39.44it/s, loss=0.69, v_num=3u0x, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 39.37it/s, loss=0.69, v_num=3u0x, ETH_val_a\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 41.34it/s, loss=0.684, v_num=3u0x, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 41.10it/s, loss=0.684, v_num=3u0x, ETH_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 40.77it/s, loss=0.683, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 40.69it/s, loss=0.683, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 38.45it/s, loss=0.69, v_num=3u0x, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 38.22it/s, loss=0.69, v_num=3u0x, ETH_val_\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 40.74it/s, loss=0.675, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.669\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 40.62it/s, loss=0.675, v_num=3u0x, ETH_val\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 40.04it/s, loss=0.669, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 40.22it/s, loss=0.669, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 39.22it/s, loss=0.682, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.660\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 39.26it/s, loss=0.682, v_num=3u0x, ETH_val\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 39.13it/s, loss=0.666, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 38.15it/s, loss=0.666, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 38.85it/s, loss=0.661, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 39.15it/s, loss=0.661, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 40.72it/s, loss=0.651, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 40.70it/s, loss=0.651, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 35.91it/s, loss=0.634, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 35.91it/s, loss=0.634, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 39.89it/s, loss=0.615, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 39.86it/s, loss=0.615, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 38.80it/s, loss=0.594, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 38.76it/s, loss=0.594, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 42.03it/s, loss=0.558, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 42.10it/s, loss=0.558, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 40.83it/s, loss=0.545, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.074 >= min_delta = 0.003. New best score: 0.586\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 40.66it/s, loss=0.545, v_num=3u0x, ETH_val\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 40.28it/s, loss=0.516, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 40.23it/s, loss=0.516, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 39.59it/s, loss=0.508, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 39.10it/s, loss=0.508, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 40.97it/s, loss=0.489, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.020 >= min_delta = 0.003. New best score: 0.565\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 40.90it/s, loss=0.489, v_num=3u0x, ETH_val\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 41.44it/s, loss=0.471, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.023 >= min_delta = 0.003. New best score: 0.542\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 41.14it/s, loss=0.471, v_num=3u0x, ETH_val\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 39.49it/s, loss=0.462, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 39.58it/s, loss=0.462, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 42.64it/s, loss=0.474, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 42.57it/s, loss=0.474, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 40.55it/s, loss=0.444, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.034 >= min_delta = 0.003. New best score: 0.508\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 40.48it/s, loss=0.444, v_num=3u0x, ETH_val\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 39.06it/s, loss=0.468, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 38.92it/s, loss=0.468, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 39.26it/s, loss=0.441, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 38.30it/s, loss=0.441, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 40.89it/s, loss=0.433, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.032 >= min_delta = 0.003. New best score: 0.476\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 40.72it/s, loss=0.433, v_num=3u0x, ETH_val\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 40.02it/s, loss=0.421, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 39.84it/s, loss=0.421, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 39.69it/s, loss=0.424, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.025 >= min_delta = 0.003. New best score: 0.451\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 39.36it/s, loss=0.424, v_num=3u0x, ETH_val\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 40.59it/s, loss=0.403, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.032 >= min_delta = 0.003. New best score: 0.419\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 40.67it/s, loss=0.403, v_num=3u0x, ETH_val\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 39.89it/s, loss=0.392, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.406\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 39.98it/s, loss=0.392, v_num=3u0x, ETH_val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 40.42it/s, loss=0.379, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 0.394\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 40.50it/s, loss=0.379, v_num=3u0x, ETH_val\n",
      "Epoch 38:  95%|▉| 20/21 [00:00<00:00, 40.84it/s, loss=0.391, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 40.94it/s, loss=0.391, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 39:  95%|▉| 20/21 [00:00<00:00, 40.39it/s, loss=0.381, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 39.22it/s, loss=0.381, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 41.61it/s, loss=0.392, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 41.43it/s, loss=0.392, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 41:  95%|▉| 20/21 [00:00<00:00, 40.14it/s, loss=0.355, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.058 >= min_delta = 0.003. New best score: 0.336\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 39.80it/s, loss=0.355, v_num=3u0x, ETH_val\n",
      "Epoch 42:  95%|▉| 20/21 [00:00<00:00, 40.97it/s, loss=0.345, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.023 >= min_delta = 0.003. New best score: 0.313\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 40.53it/s, loss=0.345, v_num=3u0x, ETH_val\n",
      "Epoch 43:  95%|▉| 20/21 [00:00<00:00, 42.39it/s, loss=0.322, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 42.27it/s, loss=0.322, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 44:  95%|▉| 20/21 [00:00<00:00, 41.16it/s, loss=0.313, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 41.02it/s, loss=0.313, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 45:  95%|▉| 20/21 [00:00<00:00, 41.20it/s, loss=0.331, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 40.75it/s, loss=0.331, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 46:  95%|▉| 20/21 [00:00<00:00, 39.21it/s, loss=0.321, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 39.30it/s, loss=0.321, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 47:  95%|▉| 20/21 [00:00<00:00, 38.53it/s, loss=0.312, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 38.64it/s, loss=0.312, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 48:  95%|▉| 20/21 [00:00<00:00, 39.37it/s, loss=0.285, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 39.36it/s, loss=0.285, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 49:  95%|▉| 20/21 [00:00<00:00, 39.94it/s, loss=0.298, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 21/21 [00:00<00:00, 40.06it/s, loss=0.298, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 50:  95%|▉| 20/21 [00:00<00:00, 41.06it/s, loss=0.283, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 21/21 [00:00<00:00, 40.92it/s, loss=0.283, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 51:  95%|▉| 20/21 [00:00<00:00, 39.50it/s, loss=0.279, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 21/21 [00:00<00:00, 39.06it/s, loss=0.279, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 52:  95%|▉| 20/21 [00:00<00:00, 39.25it/s, loss=0.3, v_num=3u0x, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 21/21 [00:00<00:00, 39.22it/s, loss=0.3, v_num=3u0x, ETH_val_a\u001b[A\n",
      "Epoch 53:  95%|▉| 20/21 [00:00<00:00, 39.68it/s, loss=0.272, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 21/21 [00:00<00:00, 39.51it/s, loss=0.272, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 54:  95%|▉| 20/21 [00:00<00:00, 41.79it/s, loss=0.233, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 21/21 [00:00<00:00, 41.75it/s, loss=0.233, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 55:  95%|▉| 20/21 [00:00<00:00, 40.54it/s, loss=0.245, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 21/21 [00:00<00:00, 40.60it/s, loss=0.245, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 56:  95%|▉| 20/21 [00:00<00:00, 39.33it/s, loss=0.252, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 21/21 [00:00<00:00, 39.45it/s, loss=0.252, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 57:  95%|▉| 20/21 [00:00<00:00, 40.14it/s, loss=0.216, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 21/21 [00:00<00:00, 39.87it/s, loss=0.216, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 58:  95%|▉| 20/21 [00:00<00:00, 40.43it/s, loss=0.218, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 21/21 [00:00<00:00, 40.61it/s, loss=0.218, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 59:  95%|▉| 20/21 [00:00<00:00, 40.92it/s, loss=0.216, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|█| 21/21 [00:00<00:00, 40.65it/s, loss=0.216, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 60:  95%|▉| 20/21 [00:00<00:00, 39.45it/s, loss=0.223, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 60: 100%|█| 21/21 [00:00<00:00, 39.38it/s, loss=0.223, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 61:  95%|▉| 20/21 [00:00<00:00, 39.82it/s, loss=0.229, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|█| 21/21 [00:00<00:00, 40.05it/s, loss=0.229, v_num=3u0x, ETH_val\u001b[A\n",
      "Epoch 62:  95%|▉| 20/21 [00:00<00:00, 40.14it/s, loss=0.209, v_num=3u0x, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.313. Signaling Trainer to stop.\n",
      "Epoch 62: 100%|█| 21/21 [00:00<00:00, 40.23it/s, loss=0.209, v_num=3u0x, ETH_val\n",
      "Epoch 62: 100%|█| 21/21 [00:00<00:00, 40.04it/s, loss=0.209, v_num=3u0x, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 80.44it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.6060606241226196,\n",
      " 'ETH_test_f1': 0.6001864075660706,\n",
      " 'test_loss': 0.7000057101249695}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 101709\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102956-3ncs3u0x/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_102956-3ncs3u0x/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.90374\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.90208\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.20769\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 62\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1260\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 41\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902237\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 151\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.61538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.57516\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.51931\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.89062\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.8893\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.23217\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.60606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.60019\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.70001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▂▂▂▂▂▂▂▃▂▃▃▄▄▅▅▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▂▂▂▂▃▂▃▃▃▃▄▄▅▅▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ███▇▇█▇▇▇▇▇▇▆▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▄▄▄▄█▄▄▁▄▄▅▃▄▄▅▃▆▄▅▅▅▅▅▇▅▅██▅▆▅▆▆▅▇▅▅▆▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▂▂▂▂█▂▂▁▂▂▅▂▂▂▅▂▆▂▄▄▆▅▅▇▅▅██▄▆▅▆▆▄▇▄▅▆▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▅▅▅▄▅▅▅▄▄▅▅▅▅▄▅▃▅▄▄▃▄▂▂▂▃▁▁▅▂▅▁▃▃▂█▆▁▆▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▃▂▂▄▃▁▄▄▅▇▅▆▆▆▇▆▆▆▆▇█▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▃▁▂▄▃▁▄▄▅▇▅▅▆▆▇▆▆▆▆▇█▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇█▇▇▇█▆▇▅▄▄▅▅▄▄▅▄▃▃▂▁▂▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/3ncs3u0x\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:30:53.675844: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/3u14o79e\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103052-3u14o79e\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 109 K \n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.532     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 41.89it/s, loss=1.15, v_num=o79e, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.107\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 41.34it/s, loss=1.15, v_num=o79e, ETH_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 42.14it/s, loss=1.11, v_num=o79e, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 42.36it/s, loss=1.11, v_num=o79e, ETH_val_a\u001b[A\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 39.80it/s, loss=1.11, v_num=o79e, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 39.86it/s, loss=1.11, v_num=o79e, ETH_val_a\u001b[A\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 42.21it/s, loss=1.09, v_num=o79e, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 41.94it/s, loss=1.09, v_num=o79e, ETH_val_a\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 40.96it/s, loss=1.1, v_num=o79e, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 40.83it/s, loss=1.1, v_num=o79e, ETH_val_ac\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 39.68it/s, loss=1.09, v_num=o79e, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 39.72it/s, loss=1.09, v_num=o79e, ETH_val_a\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 40.98it/s, loss=1.1, v_num=o79e, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 40.66it/s, loss=1.1, v_num=o79e, ETH_val_ac\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 39.99it/s, loss=1.1, v_num=o79e, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 39.87it/s, loss=1.1, v_num=o79e, ETH_val_ac\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 41.61it/s, loss=1.08, v_num=o79e, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 41.65it/s, loss=1.08, v_num=o79e, ETH_val_a\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 40.82it/s, loss=1.08, v_num=o79e, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 40.64it/s, loss=1.08, v_num=o79e, ETH_val_a\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 40.32it/s, loss=1.07, v_num=o79e, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 39.89it/s, loss=1.07, v_num=o79e, ETH_val_\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 39.48it/s, loss=1.07, v_num=o79e, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 39.17it/s, loss=1.07, v_num=o79e, ETH_val_\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 41.32it/s, loss=1.06, v_num=o79e, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 41.37it/s, loss=1.06, v_num=o79e, ETH_val_\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 41.68it/s, loss=1.06, v_num=o79e, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 41.51it/s, loss=1.06, v_num=o79e, ETH_val_\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 41.34it/s, loss=1.06, v_num=o79e, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 41.29it/s, loss=1.06, v_num=o79e, ETH_val_\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 41.37it/s, loss=1.02, v_num=o79e, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 41.31it/s, loss=1.02, v_num=o79e, ETH_val_\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 41.28it/s, loss=1.01, v_num=o79e, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 1.099\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 41.21it/s, loss=1.01, v_num=o79e, ETH_val_\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 41.38it/s, loss=0.985, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 41.03it/s, loss=0.985, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 32.80it/s, loss=0.949, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 33.10it/s, loss=0.949, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 41.84it/s, loss=0.915, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 41.17it/s, loss=0.915, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 41.80it/s, loss=0.871, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 41.80it/s, loss=0.871, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 39.27it/s, loss=0.884, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 39.23it/s, loss=0.884, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 40.28it/s, loss=0.842, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 40.38it/s, loss=0.842, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 41.38it/s, loss=0.826, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 41.46it/s, loss=0.826, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 39.97it/s, loss=0.799, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.282 >= min_delta = 0.003. New best score: 0.817\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 40.05it/s, loss=0.799, v_num=o79e, ETH_val\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 41.61it/s, loss=0.78, v_num=o79e, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 41.52it/s, loss=0.78, v_num=o79e, ETH_val_\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 39.62it/s, loss=0.796, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 38.99it/s, loss=0.796, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 40.00it/s, loss=0.793, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 39.71it/s, loss=0.793, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 42.04it/s, loss=0.751, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 42.24it/s, loss=0.751, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 44.54it/s, loss=0.767, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 44.05it/s, loss=0.767, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 41.34it/s, loss=0.732, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 40.89it/s, loss=0.732, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 41.65it/s, loss=0.734, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 41.07it/s, loss=0.734, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 40.66it/s, loss=0.717, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 39.88it/s, loss=0.717, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 41.23it/s, loss=0.726, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.020 >= min_delta = 0.003. New best score: 0.797\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 41.14it/s, loss=0.726, v_num=o79e, ETH_val\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 41.36it/s, loss=0.703, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 41.01it/s, loss=0.703, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 39.94it/s, loss=0.71, v_num=o79e, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 39.77it/s, loss=0.71, v_num=o79e, ETH_val_\u001b[A\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 41.22it/s, loss=0.678, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 41.52it/s, loss=0.678, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 41.40it/s, loss=0.673, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.116 >= min_delta = 0.003. New best score: 0.682\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 41.01it/s, loss=0.673, v_num=o79e, ETH_val\n",
      "Epoch 38:  95%|▉| 20/21 [00:00<00:00, 41.39it/s, loss=0.659, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 41.21it/s, loss=0.659, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 39:  95%|▉| 20/21 [00:00<00:00, 41.85it/s, loss=0.659, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 41.61it/s, loss=0.659, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 41.66it/s, loss=0.638, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 41.90it/s, loss=0.638, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 41:  95%|▉| 20/21 [00:00<00:00, 40.74it/s, loss=0.621, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 40.75it/s, loss=0.621, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 42:  95%|▉| 20/21 [00:00<00:00, 39.67it/s, loss=0.648, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 39.71it/s, loss=0.648, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 43:  95%|▉| 20/21 [00:00<00:00, 40.94it/s, loss=0.637, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 41.08it/s, loss=0.637, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 44:  95%|▉| 20/21 [00:00<00:00, 40.64it/s, loss=0.616, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 40.65it/s, loss=0.616, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 45:  95%|▉| 20/21 [00:00<00:00, 42.06it/s, loss=0.593, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 41.99it/s, loss=0.593, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 46:  95%|▉| 20/21 [00:00<00:00, 39.98it/s, loss=0.589, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 39.67it/s, loss=0.589, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 47:  95%|▉| 20/21 [00:00<00:00, 41.51it/s, loss=0.578, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 41.69it/s, loss=0.578, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 48:  95%|▉| 20/21 [00:00<00:00, 40.93it/s, loss=0.565, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 41.14it/s, loss=0.565, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 49:  95%|▉| 20/21 [00:00<00:00, 40.47it/s, loss=0.587, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 21/21 [00:00<00:00, 40.37it/s, loss=0.587, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 50:  95%|▉| 20/21 [00:00<00:00, 42.33it/s, loss=0.581, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 21/21 [00:00<00:00, 41.70it/s, loss=0.581, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 51:  95%|▉| 20/21 [00:00<00:00, 39.58it/s, loss=0.56, v_num=o79e, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 21/21 [00:00<00:00, 39.56it/s, loss=0.56, v_num=o79e, ETH_val_\u001b[A\n",
      "Epoch 52:  95%|▉| 20/21 [00:00<00:00, 41.44it/s, loss=0.554, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 21/21 [00:00<00:00, 41.42it/s, loss=0.554, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 53:  95%|▉| 20/21 [00:00<00:00, 41.40it/s, loss=0.535, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 21/21 [00:00<00:00, 41.56it/s, loss=0.535, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 54:  95%|▉| 20/21 [00:00<00:00, 40.84it/s, loss=0.528, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 21/21 [00:00<00:00, 40.66it/s, loss=0.528, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 55:  95%|▉| 20/21 [00:00<00:00, 43.01it/s, loss=0.509, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 21/21 [00:00<00:00, 43.06it/s, loss=0.509, v_num=o79e, ETH_val\u001b[A\n",
      "Epoch 56:  95%|▉| 20/21 [00:00<00:00, 40.56it/s, loss=0.499, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 21/21 [00:00<00:00, 40.59it/s, loss=0.499, v_num=o79e, ETH_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57:  95%|▉| 20/21 [00:00<00:00, 43.48it/s, loss=0.514, v_num=o79e, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.682. Signaling Trainer to stop.\n",
      "Epoch 57: 100%|█| 21/21 [00:00<00:00, 43.30it/s, loss=0.514, v_num=o79e, ETH_val\n",
      "Epoch 57: 100%|█| 21/21 [00:00<00:00, 43.08it/s, loss=0.514, v_num=o79e, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 69.92it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.5757575631141663,\n",
      " 'ETH_test_f1': 0.5166666507720947,\n",
      " 'test_loss': 0.9012503623962402}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 101892\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103052-3u14o79e/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103052-3u14o79e/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.78679\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.7816\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.51438\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 57\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 38\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902290\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 139\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.53846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.51667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.00111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.75064\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.64978\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.57576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.51667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.90125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▁▁▂▂▂▂▂▂▃▃▄▄▄▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇█▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▂▂▃▃▃▃▃▃▃▄▄▅▅▅▆▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇██▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ███▇▇▇▇▇▇▇▇▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▁▁▁▁▁▁▁▁▁▁▃▃▁▃▃▃▇▆▇▆▇▆▆▆▆▄▆▆▆▇▆█▆▇▄▄█▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▂▂▁▂▄▂▇▆▇▆▇▆▅▆▆▅▅▅▅▇▅█▅▇▅▅█▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄▅▅▅▄▅▆▄▄▅▆▄▄▅▅▅▄▁▂▂▂▁▅▁▂▁█▂▂▂▁▃▁▁▂▅▆▁▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▂▁▁▁▂▃▆▆▆▆▅▄▇▆▇▅▆▆█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▂▁▁▁▂▃▆▆▅▆▅▄▆▆▇▅▆▆█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ████▇█▇▄▄▃▅▅▅▂▂▄▃▃▂▂▂▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/3u14o79e\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:31:47.338034: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/3m3ffipd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103146-3m3ffipd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 109 K \n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.532     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 43.87it/s, loss=0.71, v_num=fipd, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.689\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 43.70it/s, loss=0.71, v_num=fipd, ETH_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 45.42it/s, loss=0.702, v_num=fipd, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.685\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 44.85it/s, loss=0.702, v_num=fipd, ETH_val_\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 41.74it/s, loss=0.702, v_num=fipd, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.681\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 41.79it/s, loss=0.702, v_num=fipd, ETH_val_\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 40.63it/s, loss=0.696, v_num=fipd, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 40.90it/s, loss=0.696, v_num=fipd, ETH_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 40.71it/s, loss=0.702, v_num=fipd, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 40.64it/s, loss=0.702, v_num=fipd, ETH_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 42.91it/s, loss=0.692, v_num=fipd, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 42.62it/s, loss=0.692, v_num=fipd, ETH_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 41.09it/s, loss=0.686, v_num=fipd, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 41.19it/s, loss=0.686, v_num=fipd, ETH_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 40.98it/s, loss=0.685, v_num=fipd, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 40.76it/s, loss=0.685, v_num=fipd, ETH_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 40.30it/s, loss=0.69, v_num=fipd, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 40.49it/s, loss=0.69, v_num=fipd, ETH_val_a\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 41.24it/s, loss=0.685, v_num=fipd, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 40.96it/s, loss=0.685, v_num=fipd, ETH_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 43.31it/s, loss=0.681, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 43.00it/s, loss=0.681, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 39.97it/s, loss=0.677, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 0.669\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 39.88it/s, loss=0.677, v_num=fipd, ETH_val\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 41.65it/s, loss=0.679, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 41.53it/s, loss=0.679, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 40.23it/s, loss=0.672, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 40.24it/s, loss=0.672, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 42.64it/s, loss=0.67, v_num=fipd, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 42.51it/s, loss=0.67, v_num=fipd, ETH_val_\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 42.45it/s, loss=0.658, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 41.77it/s, loss=0.658, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 42.25it/s, loss=0.628, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 42.23it/s, loss=0.628, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 42.08it/s, loss=0.605, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 41.22it/s, loss=0.605, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 35.63it/s, loss=0.589, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 35.60it/s, loss=0.589, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 43.39it/s, loss=0.57, v_num=fipd, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 43.23it/s, loss=0.57, v_num=fipd, ETH_val_\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 39.33it/s, loss=0.541, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.663\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 39.04it/s, loss=0.541, v_num=fipd, ETH_val\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 43.65it/s, loss=0.507, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.092 >= min_delta = 0.003. New best score: 0.571\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 43.43it/s, loss=0.507, v_num=fipd, ETH_val\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 41.73it/s, loss=0.51, v_num=fipd, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 41.67it/s, loss=0.51, v_num=fipd, ETH_val_\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 41.67it/s, loss=0.483, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 41.44it/s, loss=0.483, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 41.16it/s, loss=0.477, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 40.95it/s, loss=0.477, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 41.39it/s, loss=0.473, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 41.58it/s, loss=0.473, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 41.37it/s, loss=0.442, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 41.38it/s, loss=0.442, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 42.33it/s, loss=0.454, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 42.04it/s, loss=0.454, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 42.09it/s, loss=0.444, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.033 >= min_delta = 0.003. New best score: 0.537\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 41.93it/s, loss=0.444, v_num=fipd, ETH_val\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 42.32it/s, loss=0.444, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 42.02it/s, loss=0.444, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 41.51it/s, loss=0.413, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.045 >= min_delta = 0.003. New best score: 0.492\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 41.38it/s, loss=0.413, v_num=fipd, ETH_val\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 39.60it/s, loss=0.395, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 39.44it/s, loss=0.395, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 43.26it/s, loss=0.412, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 43.05it/s, loss=0.412, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 43.73it/s, loss=0.41, v_num=fipd, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 43.57it/s, loss=0.41, v_num=fipd, ETH_val_\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 41.84it/s, loss=0.392, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 41.28it/s, loss=0.392, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 38.39it/s, loss=0.381, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 38.34it/s, loss=0.381, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 41.92it/s, loss=0.387, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 41.41it/s, loss=0.387, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 40.78it/s, loss=0.382, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.047 >= min_delta = 0.003. New best score: 0.445\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 40.81it/s, loss=0.382, v_num=fipd, ETH_val\n",
      "Epoch 38:  95%|▉| 20/21 [00:00<00:00, 41.40it/s, loss=0.37, v_num=fipd, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.034 >= min_delta = 0.003. New best score: 0.412\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 41.32it/s, loss=0.37, v_num=fipd, ETH_val_\n",
      "Epoch 39:  95%|▉| 20/21 [00:00<00:00, 42.09it/s, loss=0.356, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 41.81it/s, loss=0.356, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 41.89it/s, loss=0.347, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.076 >= min_delta = 0.003. New best score: 0.336\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 41.78it/s, loss=0.347, v_num=fipd, ETH_val\n",
      "Epoch 41:  95%|▉| 20/21 [00:00<00:00, 39.37it/s, loss=0.328, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 39.35it/s, loss=0.328, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 42:  95%|▉| 20/21 [00:00<00:00, 39.81it/s, loss=0.346, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 39.56it/s, loss=0.346, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 43:  95%|▉| 20/21 [00:00<00:00, 41.21it/s, loss=0.324, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 41.23it/s, loss=0.324, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 44:  95%|▉| 20/21 [00:00<00:00, 42.06it/s, loss=0.325, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.319\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 42.01it/s, loss=0.325, v_num=fipd, ETH_val\n",
      "Epoch 45:  95%|▉| 20/21 [00:00<00:00, 41.98it/s, loss=0.312, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 41.81it/s, loss=0.312, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 46:  95%|▉| 20/21 [00:00<00:00, 41.01it/s, loss=0.277, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.028 >= min_delta = 0.003. New best score: 0.290\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 41.05it/s, loss=0.277, v_num=fipd, ETH_val\n",
      "Epoch 47:  95%|▉| 20/21 [00:00<00:00, 40.66it/s, loss=0.29, v_num=fipd, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 40.68it/s, loss=0.29, v_num=fipd, ETH_val_\u001b[A\n",
      "Epoch 48:  95%|▉| 20/21 [00:00<00:00, 41.85it/s, loss=0.298, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 41.79it/s, loss=0.298, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 49:  95%|▉| 20/21 [00:00<00:00, 42.30it/s, loss=0.265, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 21/21 [00:00<00:00, 42.07it/s, loss=0.265, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 50:  95%|▉| 20/21 [00:00<00:00, 43.86it/s, loss=0.288, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 21/21 [00:00<00:00, 43.44it/s, loss=0.288, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 51:  95%|▉| 20/21 [00:00<00:00, 40.13it/s, loss=0.283, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 21/21 [00:00<00:00, 40.31it/s, loss=0.283, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 52:  95%|▉| 20/21 [00:00<00:00, 41.51it/s, loss=0.26, v_num=fipd, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 21/21 [00:00<00:00, 41.24it/s, loss=0.26, v_num=fipd, ETH_val_\u001b[A\n",
      "Epoch 53:  95%|▉| 20/21 [00:00<00:00, 43.29it/s, loss=0.247, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 21/21 [00:00<00:00, 43.38it/s, loss=0.247, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 54:  95%|▉| 20/21 [00:00<00:00, 41.12it/s, loss=0.237, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 21/21 [00:00<00:00, 40.95it/s, loss=0.237, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 55:  95%|▉| 20/21 [00:00<00:00, 39.33it/s, loss=0.236, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 21/21 [00:00<00:00, 39.49it/s, loss=0.236, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 56:  95%|▉| 20/21 [00:00<00:00, 42.59it/s, loss=0.234, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 21/21 [00:00<00:00, 42.84it/s, loss=0.234, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 57:  95%|▉| 20/21 [00:00<00:00, 42.09it/s, loss=0.218, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.023 >= min_delta = 0.003. New best score: 0.267\n",
      "Epoch 57: 100%|█| 21/21 [00:00<00:00, 41.89it/s, loss=0.218, v_num=fipd, ETH_val\n",
      "Epoch 58:  95%|▉| 20/21 [00:00<00:00, 39.99it/s, loss=0.222, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 21/21 [00:00<00:00, 40.14it/s, loss=0.222, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 59:  95%|▉| 20/21 [00:00<00:00, 41.46it/s, loss=0.216, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|█| 21/21 [00:00<00:00, 41.43it/s, loss=0.216, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 60:  95%|▉| 20/21 [00:00<00:00, 40.98it/s, loss=0.189, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.022 >= min_delta = 0.003. New best score: 0.245\n",
      "Epoch 60: 100%|█| 21/21 [00:00<00:00, 40.58it/s, loss=0.189, v_num=fipd, ETH_val\n",
      "Epoch 61:  95%|▉| 20/21 [00:00<00:00, 42.91it/s, loss=0.197, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|█| 21/21 [00:00<00:00, 42.81it/s, loss=0.197, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 62:  95%|▉| 20/21 [00:00<00:00, 41.72it/s, loss=0.191, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 62: 100%|█| 21/21 [00:00<00:00, 41.88it/s, loss=0.191, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 63:  95%|▉| 20/21 [00:00<00:00, 39.59it/s, loss=0.193, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 63: 100%|█| 21/21 [00:00<00:00, 39.28it/s, loss=0.193, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 64:  95%|▉| 20/21 [00:00<00:00, 40.41it/s, loss=0.214, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 64: 100%|█| 21/21 [00:00<00:00, 40.50it/s, loss=0.214, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 65:  95%|▉| 20/21 [00:00<00:00, 42.60it/s, loss=0.197, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 65: 100%|█| 21/21 [00:00<00:00, 42.60it/s, loss=0.197, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 66:  95%|▉| 20/21 [00:00<00:00, 41.47it/s, loss=0.173, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 66: 100%|█| 21/21 [00:00<00:00, 41.41it/s, loss=0.173, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 67:  95%|▉| 20/21 [00:00<00:00, 40.14it/s, loss=0.159, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 67: 100%|█| 21/21 [00:00<00:00, 40.19it/s, loss=0.159, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 68:  95%|▉| 20/21 [00:00<00:00, 38.89it/s, loss=0.147, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 68: 100%|█| 21/21 [00:00<00:00, 38.40it/s, loss=0.147, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 69:  95%|▉| 20/21 [00:00<00:00, 41.77it/s, loss=0.152, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 69: 100%|█| 21/21 [00:00<00:00, 41.86it/s, loss=0.152, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 70:  95%|▉| 20/21 [00:00<00:00, 41.19it/s, loss=0.154, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 70: 100%|█| 21/21 [00:00<00:00, 41.24it/s, loss=0.154, v_num=fipd, ETH_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71:  95%|▉| 20/21 [00:00<00:00, 39.78it/s, loss=0.16, v_num=fipd, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 71: 100%|█| 21/21 [00:00<00:00, 39.11it/s, loss=0.16, v_num=fipd, ETH_val_\u001b[A\n",
      "Epoch 72:  95%|▉| 20/21 [00:00<00:00, 41.62it/s, loss=0.127, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 72: 100%|█| 21/21 [00:00<00:00, 40.71it/s, loss=0.127, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 73:  95%|▉| 20/21 [00:00<00:00, 42.97it/s, loss=0.127, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 73: 100%|█| 21/21 [00:00<00:00, 42.12it/s, loss=0.127, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 74:  95%|▉| 20/21 [00:00<00:00, 40.25it/s, loss=0.135, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 74: 100%|█| 21/21 [00:00<00:00, 39.86it/s, loss=0.135, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 75:  95%|▉| 20/21 [00:00<00:00, 40.19it/s, loss=0.129, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 75: 100%|█| 21/21 [00:00<00:00, 40.32it/s, loss=0.129, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 76:  95%|▉| 20/21 [00:00<00:00, 41.46it/s, loss=0.12, v_num=fipd, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 76: 100%|█| 21/21 [00:00<00:00, 41.18it/s, loss=0.12, v_num=fipd, ETH_val_\u001b[A\n",
      "Epoch 77:  95%|▉| 20/21 [00:00<00:00, 40.45it/s, loss=0.131, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 77: 100%|█| 21/21 [00:00<00:00, 40.17it/s, loss=0.131, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 78:  95%|▉| 20/21 [00:00<00:00, 40.71it/s, loss=0.124, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 78: 100%|█| 21/21 [00:00<00:00, 40.66it/s, loss=0.124, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 79:  95%|▉| 20/21 [00:00<00:00, 40.88it/s, loss=0.132, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 79: 100%|█| 21/21 [00:00<00:00, 40.57it/s, loss=0.132, v_num=fipd, ETH_val\u001b[A\n",
      "Epoch 80:  95%|▉| 20/21 [00:00<00:00, 42.95it/s, loss=0.135, v_num=fipd, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.245. Signaling Trainer to stop.\n",
      "Epoch 80: 100%|█| 21/21 [00:00<00:00, 42.55it/s, loss=0.135, v_num=fipd, ETH_val\n",
      "Epoch 80: 100%|█| 21/21 [00:00<00:00, 42.34it/s, loss=0.135, v_num=fipd, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|███████████████████████████████████| 1/1 [00:00<00:00, 101.90it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.6060606241226196,\n",
      " 'ETH_test_f1': 0.6001864075660706,\n",
      " 'test_loss': 0.8413621783256531}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 102068\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103146-3m3ffipd/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103146-3m3ffipd/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.94352\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.94267\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.13362\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 80\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1620\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 49\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902355\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 194\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.76923\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.76364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.44696\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.95122\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.95048\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.15389\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.60606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.60019\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.84136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▁▁▂▂▂▂▃▃▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▁▁▂▂▂▂▃▃▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ████████▇▇▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▅▅▅▄▁▁▅▅▅▄▅▅▅▇▇▇▅▅▆▆▅▇▇▅▇▆▇▇▇▇▅▆▇▇▇▇▆█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▃▃▃▃▁▁▃▃▃▃▃▃▅▇▇▇▃▅▆▅▅▇▆▄▇▆▇▇▇▇▄▅▇▆▇▆▆█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▅▃▃▂▅▂▁█▃▄▂▂▁▁▆▄▂▄▃▃▄▁▅▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▂▂▂▁▃▃▄▄▅▅▆▅▅▆▅▆▆▆▆▇▆▇▇▇▇▇▇▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▂▂▂▁▂▃▄▄▅▅▆▅▅▅▅▆▆▆▆▇▆▇▇▇▇▇▇▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ██████▇▇▅▆▆▅▆▄▅▅▅▆▄▂▄▄▄▃▃▃▂▂▂▁▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/3m3ffipd\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:32:52.624285: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/3dh8gocq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103251-3dh8gocq\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 109 K \n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.532     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 42.95it/s, loss=1.16, v_num=gocq, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.111\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 43.10it/s, loss=1.16, v_num=gocq, ETH_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 41.35it/s, loss=1.11, v_num=gocq, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 41.23it/s, loss=1.11, v_num=gocq, ETH_val_a\u001b[A\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 40.63it/s, loss=1.1, v_num=gocq, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 40.54it/s, loss=1.1, v_num=gocq, ETH_val_ac\u001b[A\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 41.50it/s, loss=1.1, v_num=gocq, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 41.69it/s, loss=1.1, v_num=gocq, ETH_val_ac\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 38.59it/s, loss=1.09, v_num=gocq, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 38.16it/s, loss=1.09, v_num=gocq, ETH_val_a\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 41.02it/s, loss=1.08, v_num=gocq, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 41.18it/s, loss=1.08, v_num=gocq, ETH_val_a\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 41.04it/s, loss=1.09, v_num=gocq, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 40.93it/s, loss=1.09, v_num=gocq, ETH_val_a\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 41.05it/s, loss=1.08, v_num=gocq, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 40.70it/s, loss=1.08, v_num=gocq, ETH_val_a\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 39.29it/s, loss=1.07, v_num=gocq, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 39.33it/s, loss=1.07, v_num=gocq, ETH_val_a\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 41.50it/s, loss=1.08, v_num=gocq, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 41.33it/s, loss=1.08, v_num=gocq, ETH_val_a\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 40.45it/s, loss=1.08, v_num=gocq, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 40.40it/s, loss=1.08, v_num=gocq, ETH_val_\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 41.65it/s, loss=1.06, v_num=gocq, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 41.56it/s, loss=1.06, v_num=gocq, ETH_val_\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 40.96it/s, loss=1.06, v_num=gocq, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 41.02it/s, loss=1.06, v_num=gocq, ETH_val_\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 41.44it/s, loss=1.06, v_num=gocq, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 40.92it/s, loss=1.06, v_num=gocq, ETH_val_\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 39.91it/s, loss=1.06, v_num=gocq, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 40.11it/s, loss=1.06, v_num=gocq, ETH_val_\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 42.33it/s, loss=1.05, v_num=gocq, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 42.11it/s, loss=1.05, v_num=gocq, ETH_val_\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 41.16it/s, loss=1.03, v_num=gocq, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 40.97it/s, loss=1.03, v_num=gocq, ETH_val_\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 40.36it/s, loss=1.03, v_num=gocq, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 39.96it/s, loss=1.03, v_num=gocq, ETH_val_\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 34.89it/s, loss=1.02, v_num=gocq, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 34.86it/s, loss=1.02, v_num=gocq, ETH_val_\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 41.15it/s, loss=0.962, v_num=gocq, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 40.85it/s, loss=0.962, v_num=gocq, ETH_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 40.64it/s, loss=0.918, v_num=gocq, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 1.111. Signaling Trainer to stop.\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 40.49it/s, loss=0.918, v_num=gocq, ETH_val\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 40.30it/s, loss=0.918, v_num=gocq, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 65.78it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.42424243688583374,\n",
      " 'ETH_test_f1': 0.19858156144618988,\n",
      " 'test_loss': 1.0923391580581665}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 102318\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103251-3dh8gocq/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103251-3dh8gocq/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.55131\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.53861\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.91955\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 420\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 19\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902390\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.30769\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.15686\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.34086\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.4878\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.47429\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.0494\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.42424\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.19858\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.09234\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▆▆▅▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▃▄▄▄▄▄▅▄▄▅▅▅▅▆▆▆▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▆▆▆▆▆▆▆▆▅▅▅▅▅▄▄▄▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc █▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 █▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▂▄▅▆▇▆▅▆▇▇▆▅▄▅▄▄▄▃█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▃▁▃██▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▃▁▃███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▆▄▁▂▄▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/3dh8gocq\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:33:26.340211: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/270k60ov\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103325-270k60ov\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 46.83it/s, loss=0.702, v_num=60ov, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.695\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 45.91it/s, loss=0.702, v_num=60ov, ETH_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 46.65it/s, loss=0.674, v_num=60ov, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.068 >= min_delta = 0.003. New best score: 0.627\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 46.41it/s, loss=0.674, v_num=60ov, ETH_val_\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 40.32it/s, loss=0.636, v_num=60ov, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.123 >= min_delta = 0.003. New best score: 0.504\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 40.42it/s, loss=0.636, v_num=60ov, ETH_val_\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 42.73it/s, loss=0.631, v_num=60ov, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.057 >= min_delta = 0.003. New best score: 0.447\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 42.39it/s, loss=0.631, v_num=60ov, ETH_val_\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 40.53it/s, loss=0.589, v_num=60ov, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.104 >= min_delta = 0.003. New best score: 0.343\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 39.84it/s, loss=0.589, v_num=60ov, ETH_val_\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 41.21it/s, loss=0.564, v_num=60ov, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 40.99it/s, loss=0.564, v_num=60ov, ETH_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 42.13it/s, loss=0.557, v_num=60ov, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.032 >= min_delta = 0.003. New best score: 0.311\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 42.09it/s, loss=0.557, v_num=60ov, ETH_val_\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 42.59it/s, loss=0.532, v_num=60ov, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.032 >= min_delta = 0.003. New best score: 0.279\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 42.09it/s, loss=0.532, v_num=60ov, ETH_val_\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 42.16it/s, loss=0.505, v_num=60ov, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 41.94it/s, loss=0.505, v_num=60ov, ETH_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 42.69it/s, loss=0.527, v_num=60ov, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 42.05it/s, loss=0.527, v_num=60ov, ETH_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 40.65it/s, loss=0.506, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 40.34it/s, loss=0.506, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 43.22it/s, loss=0.513, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 42.94it/s, loss=0.513, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 40.31it/s, loss=0.509, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 40.42it/s, loss=0.509, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 42.50it/s, loss=0.492, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 42.26it/s, loss=0.492, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 41.90it/s, loss=0.495, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 42.01it/s, loss=0.495, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 44.42it/s, loss=0.485, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 44.20it/s, loss=0.485, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 42.95it/s, loss=0.507, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 42.76it/s, loss=0.507, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 42.72it/s, loss=0.495, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 42.18it/s, loss=0.495, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 42.46it/s, loss=0.489, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 42.25it/s, loss=0.489, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 42.64it/s, loss=0.475, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 42.38it/s, loss=0.475, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 41.12it/s, loss=0.483, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 40.93it/s, loss=0.483, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 38.95it/s, loss=0.478, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 39.14it/s, loss=0.478, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 41.30it/s, loss=0.476, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 41.25it/s, loss=0.476, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 43.39it/s, loss=0.481, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 43.40it/s, loss=0.481, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 43.17it/s, loss=0.465, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.273\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 43.04it/s, loss=0.465, v_num=60ov, ETH_val\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 42.30it/s, loss=0.483, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 42.20it/s, loss=0.483, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 40.79it/s, loss=0.488, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 40.85it/s, loss=0.488, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 41.86it/s, loss=0.471, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.265\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 42.01it/s, loss=0.471, v_num=60ov, ETH_val\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 43.17it/s, loss=0.477, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 43.34it/s, loss=0.477, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 42.69it/s, loss=0.48, v_num=60ov, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 42.49it/s, loss=0.48, v_num=60ov, ETH_val_\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 41.46it/s, loss=0.476, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 41.01it/s, loss=0.476, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 43.75it/s, loss=0.464, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 43.16it/s, loss=0.464, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 41.79it/s, loss=0.459, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 41.85it/s, loss=0.459, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 42.39it/s, loss=0.459, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 42.31it/s, loss=0.459, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 42.08it/s, loss=0.453, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 41.62it/s, loss=0.453, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 41.96it/s, loss=0.436, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 41.71it/s, loss=0.436, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 41.16it/s, loss=0.454, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 0.261\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 40.82it/s, loss=0.454, v_num=60ov, ETH_val\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 41.53it/s, loss=0.456, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 41.42it/s, loss=0.456, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 38:  95%|▉| 20/21 [00:00<00:00, 41.21it/s, loss=0.463, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 41.14it/s, loss=0.463, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 39:  95%|▉| 20/21 [00:00<00:00, 41.72it/s, loss=0.444, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.255\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 41.27it/s, loss=0.444, v_num=60ov, ETH_val\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 42.05it/s, loss=0.45, v_num=60ov, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 41.73it/s, loss=0.45, v_num=60ov, ETH_val_\u001b[A\n",
      "Epoch 41:  95%|▉| 20/21 [00:00<00:00, 43.74it/s, loss=0.444, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 43.39it/s, loss=0.444, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 42:  95%|▉| 20/21 [00:00<00:00, 43.34it/s, loss=0.453, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 41.83it/s, loss=0.453, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 43:  95%|▉| 20/21 [00:00<00:00, 42.60it/s, loss=0.443, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 40.66it/s, loss=0.443, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 44:  95%|▉| 20/21 [00:00<00:00, 43.04it/s, loss=0.442, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 41.86it/s, loss=0.442, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 45:  95%|▉| 20/21 [00:00<00:00, 43.99it/s, loss=0.455, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 43.83it/s, loss=0.455, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 46:  95%|▉| 20/21 [00:00<00:00, 41.89it/s, loss=0.441, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 41.96it/s, loss=0.441, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 47:  95%|▉| 20/21 [00:00<00:00, 45.27it/s, loss=0.44, v_num=60ov, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 45.30it/s, loss=0.44, v_num=60ov, ETH_val_\u001b[A\n",
      "Epoch 48:  95%|▉| 20/21 [00:00<00:00, 41.58it/s, loss=0.444, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 41.54it/s, loss=0.444, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 49:  95%|▉| 20/21 [00:00<00:00, 44.35it/s, loss=0.439, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 0.245\n",
      "Epoch 49: 100%|█| 21/21 [00:00<00:00, 44.27it/s, loss=0.439, v_num=60ov, ETH_val\n",
      "Epoch 50:  95%|▉| 20/21 [00:00<00:00, 40.39it/s, loss=0.443, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 21/21 [00:00<00:00, 39.96it/s, loss=0.443, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 51:  95%|▉| 20/21 [00:00<00:00, 42.83it/s, loss=0.426, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 21/21 [00:00<00:00, 42.04it/s, loss=0.426, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 52:  95%|▉| 20/21 [00:00<00:00, 40.48it/s, loss=0.443, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 21/21 [00:00<00:00, 40.62it/s, loss=0.443, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 53:  95%|▉| 20/21 [00:00<00:00, 41.99it/s, loss=0.428, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 21/21 [00:00<00:00, 41.93it/s, loss=0.428, v_num=60ov, ETH_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54:  95%|▉| 20/21 [00:00<00:00, 39.95it/s, loss=0.421, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 21/21 [00:00<00:00, 39.98it/s, loss=0.421, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 55:  95%|▉| 20/21 [00:00<00:00, 42.52it/s, loss=0.418, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 21/21 [00:00<00:00, 42.52it/s, loss=0.418, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 56:  95%|▉| 20/21 [00:00<00:00, 42.26it/s, loss=0.425, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 21/21 [00:00<00:00, 42.27it/s, loss=0.425, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 57:  95%|▉| 20/21 [00:00<00:00, 42.29it/s, loss=0.432, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 21/21 [00:00<00:00, 41.75it/s, loss=0.432, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 58:  95%|▉| 20/21 [00:00<00:00, 41.72it/s, loss=0.439, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 21/21 [00:00<00:00, 41.49it/s, loss=0.439, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 59:  95%|▉| 20/21 [00:00<00:00, 43.34it/s, loss=0.429, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|█| 21/21 [00:00<00:00, 42.82it/s, loss=0.429, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 60:  95%|▉| 20/21 [00:00<00:00, 43.82it/s, loss=0.415, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.015 >= min_delta = 0.003. New best score: 0.229\n",
      "Epoch 60: 100%|█| 21/21 [00:00<00:00, 43.86it/s, loss=0.415, v_num=60ov, ETH_val\n",
      "Epoch 61:  95%|▉| 20/21 [00:00<00:00, 40.66it/s, loss=0.412, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|█| 21/21 [00:00<00:00, 40.37it/s, loss=0.412, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 62:  95%|▉| 20/21 [00:00<00:00, 44.26it/s, loss=0.42, v_num=60ov, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 62: 100%|█| 21/21 [00:00<00:00, 43.75it/s, loss=0.42, v_num=60ov, ETH_val_\u001b[A\n",
      "Epoch 63:  95%|▉| 20/21 [00:00<00:00, 42.96it/s, loss=0.414, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 63: 100%|█| 21/21 [00:00<00:00, 43.08it/s, loss=0.414, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 64:  95%|▉| 20/21 [00:00<00:00, 43.41it/s, loss=0.421, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 64: 100%|█| 21/21 [00:00<00:00, 43.14it/s, loss=0.421, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 65:  95%|▉| 20/21 [00:00<00:00, 42.96it/s, loss=0.413, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 65: 100%|█| 21/21 [00:00<00:00, 42.46it/s, loss=0.413, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 66:  95%|▉| 20/21 [00:00<00:00, 44.05it/s, loss=0.409, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 66: 100%|█| 21/21 [00:00<00:00, 43.92it/s, loss=0.409, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 67:  95%|▉| 20/21 [00:00<00:00, 40.92it/s, loss=0.411, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 67: 100%|█| 21/21 [00:00<00:00, 40.82it/s, loss=0.411, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 68:  95%|▉| 20/21 [00:00<00:00, 43.31it/s, loss=0.401, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 68: 100%|█| 21/21 [00:00<00:00, 43.38it/s, loss=0.401, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 69:  95%|▉| 20/21 [00:00<00:00, 40.25it/s, loss=0.401, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 69: 100%|█| 21/21 [00:00<00:00, 39.96it/s, loss=0.401, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 70:  95%|▉| 20/21 [00:00<00:00, 44.50it/s, loss=0.395, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 70: 100%|█| 21/21 [00:00<00:00, 44.33it/s, loss=0.395, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 71:  95%|▉| 20/21 [00:00<00:00, 42.03it/s, loss=0.405, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 71: 100%|█| 21/21 [00:00<00:00, 41.92it/s, loss=0.405, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 72:  95%|▉| 20/21 [00:00<00:00, 42.80it/s, loss=0.395, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 72: 100%|█| 21/21 [00:00<00:00, 42.68it/s, loss=0.395, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 73:  95%|▉| 20/21 [00:00<00:00, 43.31it/s, loss=0.39, v_num=60ov, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 73: 100%|█| 21/21 [00:00<00:00, 43.23it/s, loss=0.39, v_num=60ov, ETH_val_\u001b[A\n",
      "Epoch 74:  95%|▉| 20/21 [00:00<00:00, 41.25it/s, loss=0.393, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 74: 100%|█| 21/21 [00:00<00:00, 41.34it/s, loss=0.393, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 75:  95%|▉| 20/21 [00:00<00:00, 42.52it/s, loss=0.383, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 75: 100%|█| 21/21 [00:00<00:00, 42.42it/s, loss=0.383, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 76:  95%|▉| 20/21 [00:00<00:00, 41.27it/s, loss=0.384, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 76: 100%|█| 21/21 [00:00<00:00, 40.54it/s, loss=0.384, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 77:  95%|▉| 20/21 [00:00<00:00, 41.91it/s, loss=0.394, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 77: 100%|█| 21/21 [00:00<00:00, 41.86it/s, loss=0.394, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 78:  95%|▉| 20/21 [00:00<00:00, 42.47it/s, loss=0.381, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 78: 100%|█| 21/21 [00:00<00:00, 42.35it/s, loss=0.381, v_num=60ov, ETH_val\u001b[A\n",
      "Epoch 79:  95%|▉| 20/21 [00:00<00:00, 45.20it/s, loss=0.38, v_num=60ov, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 79: 100%|█| 21/21 [00:00<00:00, 45.03it/s, loss=0.38, v_num=60ov, ETH_val_\u001b[A\n",
      "Epoch 80:  95%|▉| 20/21 [00:00<00:00, 45.56it/s, loss=0.379, v_num=60ov, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.229. Signaling Trainer to stop.\n",
      "Epoch 80: 100%|█| 21/21 [00:00<00:00, 45.15it/s, loss=0.379, v_num=60ov, ETH_val\n",
      "Epoch 80: 100%|█| 21/21 [00:00<00:00, 44.66it/s, loss=0.379, v_num=60ov, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 71.04it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.7878788113594055,\n",
      " 'ETH_test_f1': 0.7746341824531555,\n",
      " 'test_loss': 0.4350806772708893}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 102466\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103325-270k60ov/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103325-270k60ov/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.83373\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.83133\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.38206\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 80\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1620\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 48\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902453\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 194\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.92308\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.92121\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.34894\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.85366\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.85145\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.34033\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.78788\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.77463\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.43508\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▄▅▆▆▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▄▅▆▆▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▆▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▅▃▂▃▂▂▂▃▂▂▂▁▂▂▁▁▁▁▂▁▂▂▁▁▁▁▁▂▂▂▂▂▂▂▃▂▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▂▁▇▅▅▆▄▆▂▄▆▅▅▆▅▆▄▆█▅▆▅▆▇▅▃▃▆▅▃▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▂▁▇▅▅▆▄▆▂▄▆▅▅▆▅▆▄▆█▅▆▅▆▇▅▃▃▆▅▃▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇█▄▄▄▄▅▄▇▆▅▅▅▃▅▄▅▃▁▄▃▄▄▂▄▇▆▄▆▇▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/270k60ov\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:34:29.109129: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/3kcp4ux8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103427-3kcp4ux8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 44.57it/s, loss=1.11, v_num=4ux8, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.119\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 44.41it/s, loss=1.11, v_num=4ux8, ETH_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 42.30it/s, loss=1.1, v_num=4ux8, ETH_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.049 >= min_delta = 0.003. New best score: 1.070\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 41.98it/s, loss=1.1, v_num=4ux8, ETH_val_ac\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 41.10it/s, loss=1.04, v_num=4ux8, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.153 >= min_delta = 0.003. New best score: 0.917\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 40.55it/s, loss=1.04, v_num=4ux8, ETH_val_a\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 43.02it/s, loss=0.995, v_num=4ux8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.045 >= min_delta = 0.003. New best score: 0.872\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 42.82it/s, loss=0.995, v_num=4ux8, ETH_val_\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 39.89it/s, loss=0.94, v_num=4ux8, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 39.71it/s, loss=0.94, v_num=4ux8, ETH_val_a\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 41.06it/s, loss=0.927, v_num=4ux8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.084 >= min_delta = 0.003. New best score: 0.788\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 40.94it/s, loss=0.927, v_num=4ux8, ETH_val_\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 43.06it/s, loss=0.907, v_num=4ux8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.016 >= min_delta = 0.003. New best score: 0.772\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 42.73it/s, loss=0.907, v_num=4ux8, ETH_val_\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 42.34it/s, loss=0.925, v_num=4ux8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 42.38it/s, loss=0.925, v_num=4ux8, ETH_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 39.92it/s, loss=0.863, v_num=4ux8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 39.99it/s, loss=0.863, v_num=4ux8, ETH_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 38.90it/s, loss=0.864, v_num=4ux8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.029 >= min_delta = 0.003. New best score: 0.744\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 39.03it/s, loss=0.864, v_num=4ux8, ETH_val_\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 41.66it/s, loss=0.832, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.031 >= min_delta = 0.003. New best score: 0.713\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 41.67it/s, loss=0.832, v_num=4ux8, ETH_val\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 40.30it/s, loss=0.85, v_num=4ux8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.018 >= min_delta = 0.003. New best score: 0.695\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 40.38it/s, loss=0.85, v_num=4ux8, ETH_val_\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 38.54it/s, loss=0.817, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 38.68it/s, loss=0.817, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 41.59it/s, loss=0.81, v_num=4ux8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 41.20it/s, loss=0.81, v_num=4ux8, ETH_val_\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 41.52it/s, loss=0.835, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 40.99it/s, loss=0.835, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 41.16it/s, loss=0.837, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 41.08it/s, loss=0.837, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 40.93it/s, loss=0.798, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 41.01it/s, loss=0.798, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 41.71it/s, loss=0.787, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 41.49it/s, loss=0.787, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 40.98it/s, loss=0.814, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 40.74it/s, loss=0.814, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 42.57it/s, loss=0.817, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 42.01it/s, loss=0.817, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 40.99it/s, loss=0.793, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 40.92it/s, loss=0.793, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 36.10it/s, loss=0.795, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 36.24it/s, loss=0.795, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 41.01it/s, loss=0.791, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 41.08it/s, loss=0.791, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 40.54it/s, loss=0.782, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 40.64it/s, loss=0.782, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 41.91it/s, loss=0.787, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 41.53it/s, loss=0.787, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 40.86it/s, loss=0.77, v_num=4ux8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 40.86it/s, loss=0.77, v_num=4ux8, ETH_val_\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 42.78it/s, loss=0.792, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 42.12it/s, loss=0.792, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 40.98it/s, loss=0.761, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 40.97it/s, loss=0.761, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 41.45it/s, loss=0.78, v_num=4ux8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 41.33it/s, loss=0.78, v_num=4ux8, ETH_val_\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 40.93it/s, loss=0.773, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 41.00it/s, loss=0.773, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 41.15it/s, loss=0.774, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 40.69it/s, loss=0.774, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 39.53it/s, loss=0.763, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.685\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 39.24it/s, loss=0.763, v_num=4ux8, ETH_val\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 40.77it/s, loss=0.772, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 40.61it/s, loss=0.772, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 40.21it/s, loss=0.791, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 39.82it/s, loss=0.791, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 42.50it/s, loss=0.779, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 42.52it/s, loss=0.779, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 38.09it/s, loss=0.758, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 37.73it/s, loss=0.758, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 39.26it/s, loss=0.76, v_num=4ux8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 39.28it/s, loss=0.76, v_num=4ux8, ETH_val_\u001b[A\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 39.74it/s, loss=0.759, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 39.96it/s, loss=0.759, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 38:  95%|▉| 20/21 [00:00<00:00, 43.82it/s, loss=0.765, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 43.32it/s, loss=0.765, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 39:  95%|▉| 20/21 [00:00<00:00, 41.55it/s, loss=0.746, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 41.07it/s, loss=0.746, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 40.39it/s, loss=0.768, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 40.40it/s, loss=0.768, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 41:  95%|▉| 20/21 [00:00<00:00, 42.37it/s, loss=0.765, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 41.92it/s, loss=0.765, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 42:  95%|▉| 20/21 [00:00<00:00, 43.29it/s, loss=0.74, v_num=4ux8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 42.77it/s, loss=0.74, v_num=4ux8, ETH_val_\u001b[A\n",
      "Epoch 43:  95%|▉| 20/21 [00:00<00:00, 39.45it/s, loss=0.744, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 39.48it/s, loss=0.744, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 44:  95%|▉| 20/21 [00:00<00:00, 41.69it/s, loss=0.739, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 40.95it/s, loss=0.739, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 45:  95%|▉| 20/21 [00:00<00:00, 41.28it/s, loss=0.729, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 41.42it/s, loss=0.729, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 46:  95%|▉| 20/21 [00:00<00:00, 41.10it/s, loss=0.731, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 41.29it/s, loss=0.731, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 47:  95%|▉| 20/21 [00:00<00:00, 41.60it/s, loss=0.737, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 40.92it/s, loss=0.737, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 48:  95%|▉| 20/21 [00:00<00:00, 39.97it/s, loss=0.722, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 39.74it/s, loss=0.722, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 49:  95%|▉| 20/21 [00:00<00:00, 40.15it/s, loss=0.737, v_num=4ux8, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 21/21 [00:00<00:00, 40.19it/s, loss=0.737, v_num=4ux8, ETH_val\u001b[A\n",
      "Epoch 50:  95%|▉| 20/21 [00:00<00:00, 42.21it/s, loss=0.73, v_num=4ux8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 21/21 [00:00<00:00, 41.93it/s, loss=0.73, v_num=4ux8, ETH_val_\u001b[A\n",
      "Epoch 51:  95%|▉| 20/21 [00:00<00:00, 39.99it/s, loss=0.73, v_num=4ux8, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.685. Signaling Trainer to stop.\n",
      "Epoch 51: 100%|█| 21/21 [00:00<00:00, 39.89it/s, loss=0.73, v_num=4ux8, ETH_val_\n",
      "Epoch 51: 100%|█| 21/21 [00:00<00:00, 39.71it/s, loss=0.73, v_num=4ux8, ETH_val_\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 93.53it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.7272727489471436,\n",
      " 'ETH_test_f1': 0.7273833751678467,\n",
      " 'test_loss': 0.7255769371986389}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 102723\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103427-3kcp4ux8/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103427-3kcp4ux8/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.67462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.66812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.72837\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 51\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1040\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 35\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902502\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 124\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.61538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.61279\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.70214\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.70732\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.68496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.75104\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.72727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.72738\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.72558\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▂▃▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇██▇▇█▇██▇▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▃▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ██▇▆▅▄▅▄▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▆█▆▄█▇█▇▇▆▇▇▆█▆▇▇▇▇▇▆▇▇▇█▆▆▇▇▇▇▇▇▆▇▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▅█▆▄█▇█▇▇▅▇▇▅█▅▇▇▇▇▇▅▇▇▇█▅▅▇▇▇▇▇▇▅▇▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▇▅▄▃▂▃▂▁▁▃▂▃▃▃▃▃▃▃▂▂▃▂▃▁▃▃▃▂▂▂▂▂▂▂▂▁▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▄▂▅▇█▇▃▆▅▇▆▄▄█▅▇▇▃█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▃▂▅▆█▆▃▅▅▆▆▄▃█▅▇▆▂▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▇▇▆▅▄▅▆▄▃▃▃▄▅▁▄▄▄▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/3kcp4ux8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:35:18.265029: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/3gptidyf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103516-3gptidyf\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 44.38it/s, loss=0.711, v_num=idyf, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.695\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 44.30it/s, loss=0.711, v_num=idyf, ETH_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 43.42it/s, loss=0.688, v_num=idyf, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.027 >= min_delta = 0.003. New best score: 0.668\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 43.09it/s, loss=0.688, v_num=idyf, ETH_val_\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 41.09it/s, loss=0.648, v_num=idyf, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.172 >= min_delta = 0.003. New best score: 0.496\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 41.25it/s, loss=0.648, v_num=idyf, ETH_val_\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 39.06it/s, loss=0.615, v_num=idyf, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.074 >= min_delta = 0.003. New best score: 0.422\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 39.30it/s, loss=0.615, v_num=idyf, ETH_val_\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 41.39it/s, loss=0.59, v_num=idyf, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.044 >= min_delta = 0.003. New best score: 0.378\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 41.18it/s, loss=0.59, v_num=idyf, ETH_val_a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 45.52it/s, loss=0.564, v_num=idyf, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.032 >= min_delta = 0.003. New best score: 0.345\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 45.07it/s, loss=0.564, v_num=idyf, ETH_val_\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 41.72it/s, loss=0.553, v_num=idyf, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 41.75it/s, loss=0.553, v_num=idyf, ETH_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 40.37it/s, loss=0.532, v_num=idyf, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.054 >= min_delta = 0.003. New best score: 0.291\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 40.39it/s, loss=0.532, v_num=idyf, ETH_val_\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 40.18it/s, loss=0.518, v_num=idyf, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 40.05it/s, loss=0.518, v_num=idyf, ETH_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 40.10it/s, loss=0.521, v_num=idyf, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 40.28it/s, loss=0.521, v_num=idyf, ETH_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 42.84it/s, loss=0.5, v_num=idyf, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 42.46it/s, loss=0.5, v_num=idyf, ETH_val_a\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 41.13it/s, loss=0.501, v_num=idyf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 41.08it/s, loss=0.501, v_num=idyf, ETH_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 42.60it/s, loss=0.492, v_num=idyf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.029 >= min_delta = 0.003. New best score: 0.263\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 42.30it/s, loss=0.492, v_num=idyf, ETH_val\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 43.48it/s, loss=0.488, v_num=idyf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 43.22it/s, loss=0.488, v_num=idyf, ETH_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 43.34it/s, loss=0.483, v_num=idyf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 42.79it/s, loss=0.483, v_num=idyf, ETH_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 40.11it/s, loss=0.485, v_num=idyf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 40.37it/s, loss=0.485, v_num=idyf, ETH_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 42.09it/s, loss=0.483, v_num=idyf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 41.77it/s, loss=0.483, v_num=idyf, ETH_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 42.47it/s, loss=0.48, v_num=idyf, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 41.97it/s, loss=0.48, v_num=idyf, ETH_val_\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 42.32it/s, loss=0.485, v_num=idyf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 42.00it/s, loss=0.485, v_num=idyf, ETH_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 41.18it/s, loss=0.499, v_num=idyf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 40.62it/s, loss=0.499, v_num=idyf, ETH_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 43.28it/s, loss=0.475, v_num=idyf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 43.41it/s, loss=0.475, v_num=idyf, ETH_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 36.90it/s, loss=0.486, v_num=idyf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 36.90it/s, loss=0.486, v_num=idyf, ETH_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 42.29it/s, loss=0.479, v_num=idyf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 42.17it/s, loss=0.479, v_num=idyf, ETH_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 41.77it/s, loss=0.477, v_num=idyf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 41.44it/s, loss=0.477, v_num=idyf, ETH_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 43.44it/s, loss=0.473, v_num=idyf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 43.38it/s, loss=0.473, v_num=idyf, ETH_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 40.98it/s, loss=0.463, v_num=idyf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 40.93it/s, loss=0.463, v_num=idyf, ETH_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 42.13it/s, loss=0.472, v_num=idyf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 41.72it/s, loss=0.472, v_num=idyf, ETH_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 43.09it/s, loss=0.47, v_num=idyf, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 42.10it/s, loss=0.47, v_num=idyf, ETH_val_\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 42.51it/s, loss=0.474, v_num=idyf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 42.67it/s, loss=0.474, v_num=idyf, ETH_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 42.29it/s, loss=0.478, v_num=idyf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 42.03it/s, loss=0.478, v_num=idyf, ETH_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 43.54it/s, loss=0.472, v_num=idyf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 43.08it/s, loss=0.472, v_num=idyf, ETH_val\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 42.39it/s, loss=0.468, v_num=idyf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 42.54it/s, loss=0.468, v_num=idyf, ETH_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 42.19it/s, loss=0.475, v_num=idyf, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.263. Signaling Trainer to stop.\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 42.20it/s, loss=0.475, v_num=idyf, ETH_val\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 41.99it/s, loss=0.475, v_num=idyf, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 72.54it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.7272727489471436,\n",
      " 'ETH_test_f1': 0.7179487347602844,\n",
      " 'test_loss': 0.4729567766189575}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 102911\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103516-3gptidyf/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103516-3gptidyf/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.79316\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.78946\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.47469\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 660\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902541\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 79\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.92308\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.92121\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.27913\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.76562\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.76511\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.51642\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.72727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.71795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.47296\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▂▄▅▆▆▆▇▇▇▇▇███▇▇██▇██▇██████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▄▅▆▆▆▇▇▇▇▇███▇▇█▇██████████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▅▄▄▃▃▃▃▂▂▂▂▁▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▂▇▇██▇████▇█████████▇███████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▃▇▇██▇████▇█████████▇███████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ██▅▄▃▂▃▁▁▂▂▂▁▁▁▁▁▁▂▂▂▂▁▂▁▁▁▁▁▂▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▂▄▆▅▅▇▅█▅▇▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▃▄▆▅▅▇▅█▅█▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇█▅▁▃▅▃▄▁▂▁▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/3gptidyf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:35:57.282635: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/1esfng47\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103556-1esfng47\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 44.69it/s, loss=1.13, v_num=ng47, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.102\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 44.43it/s, loss=1.13, v_num=ng47, ETH_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 43.02it/s, loss=1.11, v_num=ng47, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.032 >= min_delta = 0.003. New best score: 1.070\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 42.80it/s, loss=1.11, v_num=ng47, ETH_val_a\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 43.20it/s, loss=1.08, v_num=ng47, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.091 >= min_delta = 0.003. New best score: 0.978\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 42.39it/s, loss=1.08, v_num=ng47, ETH_val_a\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 40.40it/s, loss=1.01, v_num=ng47, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.086 >= min_delta = 0.003. New best score: 0.892\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 40.37it/s, loss=1.01, v_num=ng47, ETH_val_a\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 42.40it/s, loss=0.985, v_num=ng47, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 42.23it/s, loss=0.985, v_num=ng47, ETH_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 41.89it/s, loss=0.956, v_num=ng47, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 42.10it/s, loss=0.956, v_num=ng47, ETH_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 44.25it/s, loss=0.913, v_num=ng47, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.049 >= min_delta = 0.003. New best score: 0.844\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 43.50it/s, loss=0.913, v_num=ng47, ETH_val_\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 43.35it/s, loss=0.894, v_num=ng47, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.106 >= min_delta = 0.003. New best score: 0.738\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 43.34it/s, loss=0.894, v_num=ng47, ETH_val_\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 42.82it/s, loss=0.856, v_num=ng47, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 42.63it/s, loss=0.856, v_num=ng47, ETH_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 42.02it/s, loss=0.855, v_num=ng47, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 41.85it/s, loss=0.855, v_num=ng47, ETH_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 44.09it/s, loss=0.831, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 43.76it/s, loss=0.831, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 43.88it/s, loss=0.844, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.734\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 42.78it/s, loss=0.844, v_num=ng47, ETH_val\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 41.08it/s, loss=0.824, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.028 >= min_delta = 0.003. New best score: 0.706\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 40.66it/s, loss=0.824, v_num=ng47, ETH_val\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 40.68it/s, loss=0.852, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 40.34it/s, loss=0.852, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 42.79it/s, loss=0.829, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 42.85it/s, loss=0.829, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 43.31it/s, loss=0.817, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 43.00it/s, loss=0.817, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 41.27it/s, loss=0.787, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 41.44it/s, loss=0.787, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 42.42it/s, loss=0.787, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 42.20it/s, loss=0.787, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 42.85it/s, loss=0.828, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 42.99it/s, loss=0.828, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 42.63it/s, loss=0.818, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 42.68it/s, loss=0.818, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 45.19it/s, loss=0.811, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 44.84it/s, loss=0.811, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 35.90it/s, loss=0.806, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 36.21it/s, loss=0.806, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 40.89it/s, loss=0.766, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 41.07it/s, loss=0.766, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 41.04it/s, loss=0.788, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.024 >= min_delta = 0.003. New best score: 0.683\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 40.56it/s, loss=0.788, v_num=ng47, ETH_val\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 42.60it/s, loss=0.768, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 42.35it/s, loss=0.768, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 41.61it/s, loss=0.776, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 41.53it/s, loss=0.776, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 42.56it/s, loss=0.781, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 42.68it/s, loss=0.781, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 43.88it/s, loss=0.783, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 43.65it/s, loss=0.783, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 44.91it/s, loss=0.783, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 44.92it/s, loss=0.783, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 42.20it/s, loss=0.769, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 41.16it/s, loss=0.769, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 43.53it/s, loss=0.768, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 43.41it/s, loss=0.768, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 42.27it/s, loss=0.78, v_num=ng47, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 42.06it/s, loss=0.78, v_num=ng47, ETH_val_\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 41.86it/s, loss=0.766, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 42.01it/s, loss=0.766, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 43.62it/s, loss=0.775, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 43.74it/s, loss=0.775, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 43.63it/s, loss=0.746, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 43.60it/s, loss=0.746, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 44.00it/s, loss=0.75, v_num=ng47, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 43.58it/s, loss=0.75, v_num=ng47, ETH_val_\u001b[A\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 43.79it/s, loss=0.757, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 43.75it/s, loss=0.757, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 40.06it/s, loss=0.764, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 40.03it/s, loss=0.764, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 38:  95%|▉| 20/21 [00:00<00:00, 42.21it/s, loss=0.746, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 42.20it/s, loss=0.746, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 39:  95%|▉| 20/21 [00:00<00:00, 43.50it/s, loss=0.739, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 43.49it/s, loss=0.739, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 43.16it/s, loss=0.737, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 43.14it/s, loss=0.737, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 41:  95%|▉| 20/21 [00:00<00:00, 42.18it/s, loss=0.739, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 41.82it/s, loss=0.739, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 42:  95%|▉| 20/21 [00:00<00:00, 42.89it/s, loss=0.751, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 42.88it/s, loss=0.751, v_num=ng47, ETH_val\u001b[A\n",
      "Epoch 43:  95%|▉| 20/21 [00:00<00:00, 41.57it/s, loss=0.729, v_num=ng47, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.683. Signaling Trainer to stop.\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 41.22it/s, loss=0.729, v_num=ng47, ETH_val\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 41.03it/s, loss=0.729, v_num=ng47, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 87.79it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.7272727489471436,\n",
      " 'ETH_test_f1': 0.7111111283302307,\n",
      " 'test_loss': 0.6490672826766968}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 103063\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103556-1esfng47/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103556-1esfng47/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.67462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.65685\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.72606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 43\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 880\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902585\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.53846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.51623\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.73042\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.6875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.70295\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.7736\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.72727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.71111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.64907\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▂▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇██▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▃▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇██▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ██▇▆▅▅▄▄▃▃▃▃▃▃▃▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁█▄▆▃▆▄█▆▄▄▆▄▄▄▄▆▄▄▆▆▆▄█▄▄▆▄▆▄▆▆▆▆▆▆▆▆▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁█▅▆▄▇▅▇▆▅▅▆▅▅▅▅▆▅▅▆▆▆▅█▅▅▆▅▆▅▆▆▆▆▆▆▆▆▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▇▆▅▅▅▄▂▂▄▂▁▄▂▃▂▂▂▄▂▂▁▃▂▂▂▂▄▁▃▂▂▂▂▂▃▂▁▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▅▅▆▇▆▆▆▅▅▄▄▇█▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▄▅▅▇▇▆▇▆▅▅▂██▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▄▄▃▁▃▃▃▃▃▅▅▁▂▄▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/1esfng47\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:36:41.526997: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/12o3nwlr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103640-12o3nwlr\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 109 K \n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.532     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:00<00:00, 44.05it/s, loss=0.732, v_num=nwlr, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.721\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 43.37it/s, loss=0.732, v_num=nwlr, LTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 43.93it/s, loss=0.697, v_num=nwlr, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.050 >= min_delta = 0.003. New best score: 0.672\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 42.86it/s, loss=0.697, v_num=nwlr, LTC_val_\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 40.87it/s, loss=0.695, v_num=nwlr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.045 >= min_delta = 0.003. New best score: 0.627\n",
      "Epoch 2: 100%|█| 19/19 [00:00<00:00, 40.52it/s, loss=0.695, v_num=nwlr, LTC_val_\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 41.21it/s, loss=0.693, v_num=nwlr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 19/19 [00:00<00:00, 40.74it/s, loss=0.693, v_num=nwlr, LTC_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 18/19 [00:00<00:00, 40.60it/s, loss=0.689, v_num=nwlr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 19/19 [00:00<00:00, 40.71it/s, loss=0.689, v_num=nwlr, LTC_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 18/19 [00:00<00:00, 40.79it/s, loss=0.688, v_num=nwlr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 19/19 [00:00<00:00, 39.93it/s, loss=0.688, v_num=nwlr, LTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 41.84it/s, loss=0.682, v_num=nwlr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.018 >= min_delta = 0.003. New best score: 0.609\n",
      "Epoch 6: 100%|█| 19/19 [00:00<00:00, 41.45it/s, loss=0.682, v_num=nwlr, LTC_val_\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 42.65it/s, loss=0.679, v_num=nwlr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 19/19 [00:00<00:00, 42.22it/s, loss=0.679, v_num=nwlr, LTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 40.33it/s, loss=0.671, v_num=nwlr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 0.598\n",
      "Epoch 8: 100%|█| 19/19 [00:00<00:00, 40.61it/s, loss=0.671, v_num=nwlr, LTC_val_\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 40.07it/s, loss=0.664, v_num=nwlr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:00<00:00, 40.31it/s, loss=0.664, v_num=nwlr, LTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 39.84it/s, loss=0.666, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:00<00:00, 39.97it/s, loss=0.666, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 41.89it/s, loss=0.663, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.039 >= min_delta = 0.003. New best score: 0.558\n",
      "Epoch 11: 100%|█| 19/19 [00:00<00:00, 41.40it/s, loss=0.663, v_num=nwlr, LTC_val\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 41.94it/s, loss=0.649, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:00<00:00, 41.96it/s, loss=0.649, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 40.79it/s, loss=0.636, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:00<00:00, 40.62it/s, loss=0.636, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 40.36it/s, loss=0.616, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:00<00:00, 40.28it/s, loss=0.616, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 43.18it/s, loss=0.611, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.106 >= min_delta = 0.003. New best score: 0.452\n",
      "Epoch 15: 100%|█| 19/19 [00:00<00:00, 43.09it/s, loss=0.611, v_num=nwlr, LTC_val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 40.15it/s, loss=0.575, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.059 >= min_delta = 0.003. New best score: 0.393\n",
      "Epoch 16: 100%|█| 19/19 [00:00<00:00, 40.10it/s, loss=0.575, v_num=nwlr, LTC_val\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 41.10it/s, loss=0.541, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:00<00:00, 41.15it/s, loss=0.541, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 41.57it/s, loss=0.535, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:00<00:00, 40.09it/s, loss=0.535, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 35.55it/s, loss=0.525, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:00<00:00, 35.86it/s, loss=0.525, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 40.44it/s, loss=0.497, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.102 >= min_delta = 0.003. New best score: 0.292\n",
      "Epoch 20: 100%|█| 19/19 [00:00<00:00, 39.94it/s, loss=0.497, v_num=nwlr, LTC_val\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 40.32it/s, loss=0.475, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:00<00:00, 39.47it/s, loss=0.475, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:00<00:00, 42.19it/s, loss=0.492, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:00<00:00, 42.25it/s, loss=0.492, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 44.31it/s, loss=0.503, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:00<00:00, 43.79it/s, loss=0.503, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 40.99it/s, loss=0.444, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:00<00:00, 41.04it/s, loss=0.444, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 40.73it/s, loss=0.465, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:00<00:00, 40.52it/s, loss=0.465, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:00<00:00, 41.65it/s, loss=0.47, v_num=nwlr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 0.282\n",
      "Epoch 26: 100%|█| 19/19 [00:00<00:00, 41.16it/s, loss=0.47, v_num=nwlr, LTC_val_\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 42.00it/s, loss=0.463, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:00<00:00, 42.18it/s, loss=0.463, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 44.09it/s, loss=0.413, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:00<00:00, 44.32it/s, loss=0.413, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 41.77it/s, loss=0.412, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.050 >= min_delta = 0.003. New best score: 0.232\n",
      "Epoch 29: 100%|█| 19/19 [00:00<00:00, 41.53it/s, loss=0.412, v_num=nwlr, LTC_val\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 40.44it/s, loss=0.419, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.042 >= min_delta = 0.003. New best score: 0.190\n",
      "Epoch 30: 100%|█| 19/19 [00:00<00:00, 40.35it/s, loss=0.419, v_num=nwlr, LTC_val\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 41.24it/s, loss=0.395, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.042 >= min_delta = 0.003. New best score: 0.148\n",
      "Epoch 31: 100%|█| 19/19 [00:00<00:00, 39.86it/s, loss=0.395, v_num=nwlr, LTC_val\n",
      "Epoch 32:  95%|▉| 18/19 [00:00<00:00, 43.71it/s, loss=0.401, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:00<00:00, 43.56it/s, loss=0.401, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 18/19 [00:00<00:00, 41.88it/s, loss=0.39, v_num=nwlr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:00<00:00, 41.97it/s, loss=0.39, v_num=nwlr, LTC_val_\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:00<00:00, 39.63it/s, loss=0.381, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:00<00:00, 39.29it/s, loss=0.381, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:00<00:00, 43.67it/s, loss=0.369, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:00<00:00, 43.87it/s, loss=0.369, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:00<00:00, 42.99it/s, loss=0.374, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 19/19 [00:00<00:00, 43.01it/s, loss=0.374, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 18/19 [00:00<00:00, 43.89it/s, loss=0.37, v_num=nwlr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:00<00:00, 43.21it/s, loss=0.37, v_num=nwlr, LTC_val_\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:00<00:00, 39.92it/s, loss=0.365, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 19/19 [00:00<00:00, 40.15it/s, loss=0.365, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 18/19 [00:00<00:00, 41.77it/s, loss=0.354, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 19/19 [00:00<00:00, 41.69it/s, loss=0.354, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 18/19 [00:00<00:00, 39.44it/s, loss=0.351, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 19/19 [00:00<00:00, 39.84it/s, loss=0.351, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 18/19 [00:00<00:00, 43.63it/s, loss=0.361, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 19/19 [00:00<00:00, 42.52it/s, loss=0.361, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 18/19 [00:00<00:00, 42.04it/s, loss=0.325, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 19/19 [00:00<00:00, 41.90it/s, loss=0.325, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 18/19 [00:00<00:00, 42.54it/s, loss=0.325, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 19/19 [00:00<00:00, 42.05it/s, loss=0.325, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 44:  95%|▉| 18/19 [00:00<00:00, 39.96it/s, loss=0.32, v_num=nwlr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 19/19 [00:00<00:00, 40.13it/s, loss=0.32, v_num=nwlr, LTC_val_\u001b[A\n",
      "Epoch 45:  95%|▉| 18/19 [00:00<00:00, 41.69it/s, loss=0.32, v_num=nwlr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 19/19 [00:00<00:00, 41.36it/s, loss=0.32, v_num=nwlr, LTC_val_\u001b[A\n",
      "Epoch 46:  95%|▉| 18/19 [00:00<00:00, 40.30it/s, loss=0.317, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 19/19 [00:00<00:00, 40.57it/s, loss=0.317, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 18/19 [00:00<00:00, 40.22it/s, loss=0.302, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 19/19 [00:00<00:00, 40.34it/s, loss=0.302, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 48:  95%|▉| 18/19 [00:00<00:00, 43.47it/s, loss=0.29, v_num=nwlr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 19/19 [00:00<00:00, 43.50it/s, loss=0.29, v_num=nwlr, LTC_val_\u001b[A\n",
      "Epoch 49:  95%|▉| 18/19 [00:00<00:00, 42.36it/s, loss=0.272, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 19/19 [00:00<00:00, 42.28it/s, loss=0.272, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 50:  95%|▉| 18/19 [00:00<00:00, 40.37it/s, loss=0.251, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 19/19 [00:00<00:00, 40.36it/s, loss=0.251, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 51:  95%|▉| 18/19 [00:00<00:00, 41.21it/s, loss=0.265, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.142\n",
      "Epoch 51: 100%|█| 19/19 [00:00<00:00, 40.88it/s, loss=0.265, v_num=nwlr, LTC_val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52:  95%|▉| 18/19 [00:00<00:00, 40.67it/s, loss=0.277, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 19/19 [00:00<00:00, 39.55it/s, loss=0.277, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 53:  95%|▉| 18/19 [00:00<00:00, 42.33it/s, loss=0.276, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 19/19 [00:00<00:00, 42.12it/s, loss=0.276, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 54:  95%|▉| 18/19 [00:00<00:00, 42.86it/s, loss=0.239, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 19/19 [00:00<00:00, 42.72it/s, loss=0.239, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 55:  95%|▉| 18/19 [00:00<00:00, 40.27it/s, loss=0.217, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 0.130\n",
      "Epoch 55: 100%|█| 19/19 [00:00<00:00, 40.49it/s, loss=0.217, v_num=nwlr, LTC_val\n",
      "Epoch 56:  95%|▉| 18/19 [00:00<00:00, 41.45it/s, loss=0.239, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 19/19 [00:00<00:00, 41.50it/s, loss=0.239, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 57:  95%|▉| 18/19 [00:00<00:00, 40.67it/s, loss=0.24, v_num=nwlr, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 19/19 [00:00<00:00, 40.59it/s, loss=0.24, v_num=nwlr, LTC_val_\u001b[A\n",
      "Epoch 58:  95%|▉| 18/19 [00:00<00:00, 42.34it/s, loss=0.231, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 19/19 [00:00<00:00, 42.53it/s, loss=0.231, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 59:  95%|▉| 18/19 [00:00<00:00, 41.88it/s, loss=0.213, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|█| 19/19 [00:00<00:00, 41.90it/s, loss=0.213, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 60:  95%|▉| 18/19 [00:00<00:00, 38.78it/s, loss=0.219, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 60: 100%|█| 19/19 [00:00<00:00, 39.06it/s, loss=0.219, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 61:  95%|▉| 18/19 [00:00<00:00, 40.14it/s, loss=0.197, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|█| 19/19 [00:00<00:00, 40.40it/s, loss=0.197, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 62:  95%|▉| 18/19 [00:00<00:00, 41.74it/s, loss=0.209, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 62: 100%|█| 19/19 [00:00<00:00, 41.76it/s, loss=0.209, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 63:  95%|▉| 18/19 [00:00<00:00, 39.54it/s, loss=0.192, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 63: 100%|█| 19/19 [00:00<00:00, 39.77it/s, loss=0.192, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 64:  95%|▉| 18/19 [00:00<00:00, 42.40it/s, loss=0.191, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 64: 100%|█| 19/19 [00:00<00:00, 41.95it/s, loss=0.191, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 65:  95%|▉| 18/19 [00:00<00:00, 41.34it/s, loss=0.179, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 65: 100%|█| 19/19 [00:00<00:00, 41.28it/s, loss=0.179, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 66:  95%|▉| 18/19 [00:00<00:00, 43.30it/s, loss=0.177, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 66: 100%|█| 19/19 [00:00<00:00, 43.06it/s, loss=0.177, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 67:  95%|▉| 18/19 [00:00<00:00, 42.28it/s, loss=0.183, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 67: 100%|█| 19/19 [00:00<00:00, 42.39it/s, loss=0.183, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 68:  95%|▉| 18/19 [00:00<00:00, 42.29it/s, loss=0.191, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 68: 100%|█| 19/19 [00:00<00:00, 42.12it/s, loss=0.191, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 69:  95%|▉| 18/19 [00:00<00:00, 40.96it/s, loss=0.189, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 69: 100%|█| 19/19 [00:00<00:00, 41.21it/s, loss=0.189, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 70:  95%|▉| 18/19 [00:00<00:00, 40.46it/s, loss=0.172, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 70: 100%|█| 19/19 [00:00<00:00, 40.44it/s, loss=0.172, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 71:  95%|▉| 18/19 [00:00<00:00, 42.93it/s, loss=0.176, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 71: 100%|█| 19/19 [00:00<00:00, 42.70it/s, loss=0.176, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 72:  95%|▉| 18/19 [00:00<00:00, 41.02it/s, loss=0.181, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 72: 100%|█| 19/19 [00:00<00:00, 40.88it/s, loss=0.181, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 73:  95%|▉| 18/19 [00:00<00:00, 39.83it/s, loss=0.169, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 73: 100%|█| 19/19 [00:00<00:00, 39.99it/s, loss=0.169, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 74:  95%|▉| 18/19 [00:00<00:00, 42.79it/s, loss=0.157, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 74: 100%|█| 19/19 [00:00<00:00, 42.65it/s, loss=0.157, v_num=nwlr, LTC_val\u001b[A\n",
      "Epoch 75:  95%|▉| 18/19 [00:00<00:00, 40.76it/s, loss=0.139, v_num=nwlr, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.130. Signaling Trainer to stop.\n",
      "Epoch 75: 100%|█| 19/19 [00:00<00:00, 40.67it/s, loss=0.139, v_num=nwlr, LTC_val\n",
      "Epoch 75: 100%|█| 19/19 [00:00<00:00, 40.46it/s, loss=0.139, v_num=nwlr, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 74.29it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.6333333253860474,\n",
      " 'LTC_test_f1': 0.6122208833694458,\n",
      " 'test_loss': 1.2266250848770142}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 103241\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103640-12o3nwlr/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103640-12o3nwlr/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.94241\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.94099\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.13326\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 43\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902643\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 179\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.69748\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.36341\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.98276\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.98233\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.10482\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.63333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.61222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.22663\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▁▁▁▂▃▃▃▃▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▃▃▃▃▄▄▄▄▅▆▆▆▆▆▇▆▆▇▇▇▇▇▇▇▇█▇▇███████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ███▇▇▇▇▇▇▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▂▇▇▁▇▇▇▇▇▅▅▅▆▅▇▇█▅▇▇▇▇▇▇▇▆▅█▇█▇▆▇▇█▆▇▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▂▆▄▁▄▄▄▄▄▅▅▅▆▅▇▇█▅▇▇▇▇▇▇▇▆▅█▆█▆▆▆▆█▆▆▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▇▇▇█▇▆▆▆▄▆▅▆▅▆▃▃▂▆▃▂▂▃▂▂▂▄█▁▂▁▃▄▂▃▂▃▂▅▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▄▁▃▂▃▄▅▄▅▅▆▅▆▆▆▆▇▇▇▇▇▇█▆▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▄▁▃▂▃▄▅▄▅▅▆▅▆▆▆▆▇▇▇▇▇▇█▆▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇█▇▇▇▇▆▇▅▆▄▅▅▄▅▅▄▂▃▂▃▃▁▃▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/12o3nwlr\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:37:40.100149: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/1nqjlgop\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103738-1nqjlgop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 109 K \n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.532     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:00<00:00, 44.30it/s, loss=1.13, v_num=lgop, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.118\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 44.34it/s, loss=1.13, v_num=lgop, LTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 44.23it/s, loss=1.12, v_num=lgop, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 43.87it/s, loss=1.12, v_num=lgop, LTC_val_a\u001b[A\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 40.31it/s, loss=1.1, v_num=lgop, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.038 >= min_delta = 0.003. New best score: 1.080\n",
      "Epoch 2: 100%|█| 19/19 [00:00<00:00, 39.69it/s, loss=1.1, v_num=lgop, LTC_val_ac\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 39.74it/s, loss=1.1, v_num=lgop, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 19/19 [00:00<00:00, 39.87it/s, loss=1.1, v_num=lgop, LTC_val_ac\u001b[A\n",
      "Epoch 4:  95%|▉| 18/19 [00:00<00:00, 38.34it/s, loss=1.11, v_num=lgop, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.047 >= min_delta = 0.003. New best score: 1.034\n",
      "Epoch 4: 100%|█| 19/19 [00:00<00:00, 38.51it/s, loss=1.11, v_num=lgop, LTC_val_a\n",
      "Epoch 5:  95%|▉| 18/19 [00:00<00:00, 40.51it/s, loss=1.1, v_num=lgop, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 19/19 [00:00<00:00, 40.63it/s, loss=1.1, v_num=lgop, LTC_val_ac\u001b[A\n",
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 40.68it/s, loss=1.1, v_num=lgop, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 19/19 [00:00<00:00, 39.63it/s, loss=1.1, v_num=lgop, LTC_val_ac\u001b[A\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 40.45it/s, loss=1.1, v_num=lgop, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 19/19 [00:00<00:00, 40.33it/s, loss=1.1, v_num=lgop, LTC_val_ac\u001b[A\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 40.50it/s, loss=1.09, v_num=lgop, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 19/19 [00:00<00:00, 40.40it/s, loss=1.09, v_num=lgop, LTC_val_a\u001b[A\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 38.64it/s, loss=1.08, v_num=lgop, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:00<00:00, 38.53it/s, loss=1.08, v_num=lgop, LTC_val_a\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 40.59it/s, loss=1.09, v_num=lgop, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:00<00:00, 40.51it/s, loss=1.09, v_num=lgop, LTC_val_\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 38.41it/s, loss=1.09, v_num=lgop, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:00<00:00, 38.58it/s, loss=1.09, v_num=lgop, LTC_val_\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 41.03it/s, loss=1.07, v_num=lgop, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:00<00:00, 41.04it/s, loss=1.07, v_num=lgop, LTC_val_\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 39.29it/s, loss=1.07, v_num=lgop, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:00<00:00, 39.14it/s, loss=1.07, v_num=lgop, LTC_val_\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 39.54it/s, loss=1.07, v_num=lgop, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:00<00:00, 39.02it/s, loss=1.07, v_num=lgop, LTC_val_\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 35.91it/s, loss=1.05, v_num=lgop, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:00<00:00, 35.61it/s, loss=1.05, v_num=lgop, LTC_val_\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 37.98it/s, loss=1.07, v_num=lgop, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:00<00:00, 38.18it/s, loss=1.07, v_num=lgop, LTC_val_\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 39.11it/s, loss=1.03, v_num=lgop, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:00<00:00, 38.94it/s, loss=1.03, v_num=lgop, LTC_val_\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 41.25it/s, loss=1.02, v_num=lgop, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 1.022\n",
      "Epoch 18: 100%|█| 19/19 [00:00<00:00, 41.28it/s, loss=1.02, v_num=lgop, LTC_val_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 34.27it/s, loss=0.999, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:00<00:00, 34.36it/s, loss=0.999, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 39.37it/s, loss=0.989, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:00<00:00, 39.19it/s, loss=0.989, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 41.70it/s, loss=0.97, v_num=lgop, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:00<00:00, 41.92it/s, loss=0.97, v_num=lgop, LTC_val_\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:00<00:00, 41.88it/s, loss=0.95, v_num=lgop, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:00<00:00, 41.84it/s, loss=0.95, v_num=lgop, LTC_val_\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 39.64it/s, loss=0.886, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:00<00:00, 39.81it/s, loss=0.886, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 41.18it/s, loss=0.86, v_num=lgop, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:00<00:00, 41.27it/s, loss=0.86, v_num=lgop, LTC_val_\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 40.21it/s, loss=0.841, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.203 >= min_delta = 0.003. New best score: 0.819\n",
      "Epoch 25: 100%|█| 19/19 [00:00<00:00, 40.56it/s, loss=0.841, v_num=lgop, LTC_val\n",
      "Epoch 26:  95%|▉| 18/19 [00:00<00:00, 43.02it/s, loss=0.809, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:00<00:00, 41.81it/s, loss=0.809, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 40.66it/s, loss=0.813, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:00<00:00, 40.61it/s, loss=0.813, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 40.04it/s, loss=0.814, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:00<00:00, 40.27it/s, loss=0.814, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 39.03it/s, loss=0.799, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.097 >= min_delta = 0.003. New best score: 0.722\n",
      "Epoch 29: 100%|█| 19/19 [00:00<00:00, 39.05it/s, loss=0.799, v_num=lgop, LTC_val\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 40.23it/s, loss=0.804, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.035 >= min_delta = 0.003. New best score: 0.686\n",
      "Epoch 30: 100%|█| 19/19 [00:00<00:00, 40.15it/s, loss=0.804, v_num=lgop, LTC_val\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 40.30it/s, loss=0.764, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:00<00:00, 40.59it/s, loss=0.764, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:00<00:00, 39.31it/s, loss=0.76, v_num=lgop, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:00<00:00, 39.34it/s, loss=0.76, v_num=lgop, LTC_val_\u001b[A\n",
      "Epoch 33:  95%|▉| 18/19 [00:00<00:00, 39.65it/s, loss=0.728, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:00<00:00, 39.34it/s, loss=0.728, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:00<00:00, 40.61it/s, loss=0.703, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:00<00:00, 40.53it/s, loss=0.703, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:00<00:00, 39.94it/s, loss=0.712, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:00<00:00, 39.83it/s, loss=0.712, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:00<00:00, 40.28it/s, loss=0.69, v_num=lgop, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 19/19 [00:00<00:00, 40.03it/s, loss=0.69, v_num=lgop, LTC_val_\u001b[A\n",
      "Epoch 37:  95%|▉| 18/19 [00:00<00:00, 38.90it/s, loss=0.703, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:00<00:00, 39.05it/s, loss=0.703, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:00<00:00, 39.80it/s, loss=0.712, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 19/19 [00:00<00:00, 39.88it/s, loss=0.712, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 18/19 [00:00<00:00, 41.06it/s, loss=0.693, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 19/19 [00:00<00:00, 41.13it/s, loss=0.693, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 18/19 [00:00<00:00, 38.72it/s, loss=0.695, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 19/19 [00:00<00:00, 38.75it/s, loss=0.695, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 18/19 [00:00<00:00, 39.87it/s, loss=0.666, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 19/19 [00:00<00:00, 40.10it/s, loss=0.666, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 18/19 [00:00<00:00, 39.55it/s, loss=0.653, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 19/19 [00:00<00:00, 39.37it/s, loss=0.653, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 18/19 [00:00<00:00, 41.21it/s, loss=0.643, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 19/19 [00:00<00:00, 40.29it/s, loss=0.643, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 44:  95%|▉| 18/19 [00:00<00:00, 42.05it/s, loss=0.636, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 19/19 [00:00<00:00, 42.10it/s, loss=0.636, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 45:  95%|▉| 18/19 [00:00<00:00, 39.69it/s, loss=0.648, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 19/19 [00:00<00:00, 39.72it/s, loss=0.648, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 18/19 [00:00<00:00, 40.00it/s, loss=0.619, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 19/19 [00:00<00:00, 40.03it/s, loss=0.619, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 18/19 [00:00<00:00, 42.77it/s, loss=0.611, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 19/19 [00:00<00:00, 42.56it/s, loss=0.611, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 48:  95%|▉| 18/19 [00:00<00:00, 39.30it/s, loss=0.598, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 19/19 [00:00<00:00, 39.45it/s, loss=0.598, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 49:  95%|▉| 18/19 [00:00<00:00, 40.10it/s, loss=0.611, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 19/19 [00:00<00:00, 40.10it/s, loss=0.611, v_num=lgop, LTC_val\u001b[A\n",
      "Epoch 50:  95%|▉| 18/19 [00:00<00:00, 39.07it/s, loss=0.584, v_num=lgop, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.686. Signaling Trainer to stop.\n",
      "Epoch 50: 100%|█| 19/19 [00:00<00:00, 39.02it/s, loss=0.584, v_num=lgop, LTC_val\n",
      "Epoch 50: 100%|█| 19/19 [00:00<00:00, 38.83it/s, loss=0.584, v_num=lgop, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 90.33it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.6000000238418579,\n",
      " 'LTC_test_f1': 0.559259295463562,\n",
      " 'test_loss': 0.9269394874572754}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 103424\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103738-1nqjlgop/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103738-1nqjlgop/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.74782\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.73957\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.58313\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 918\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902691\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 120\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.58333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.54444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.01843\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.7931\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.79513\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.55703\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.55926\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.92694\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▂▁▂▂▂▂▂▃▃▂▃▃▄▃▄▄▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▁▂▂▃▂▂▃▃▃▃▃▄▃▄▄▄▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇█▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ███████▇▇▇▇▇▇▇▇▆▆▆▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▃▅▄▆▄▆▅▄▃▁▄▃▄▅▄▄▄▄▄▆▄▄█▇▅▆▄▄▇▆▄██▆█▄▄▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▂▃▂▄▂▄▃▃▂▁▃▂▄▄▃▄▄▃▄▆▃▄█▆▅▅▃▄▇▆▄▇█▆█▄▄▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄▄▄▄▃▃▃▃▄▄▄▄▄▄▃▄▄▃▆▃▂▄▃▁▁▃▂▇▅▄▂▄▂▃▄▄▆█▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▂▁▂▃▃▄▄▄▆▆▆▇▇▅▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▂▁▂▃▃▄▄▄▆▆▆▇▇▅▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ████▇▇▆▆▅▃▄▃▃▂▃▁▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/1nqjlgop\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:38:27.082814: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/f28psazl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103825-f28psazl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 109 K \n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.532     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:00<00:00, 41.38it/s, loss=0.735, v_num=sazl, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.646\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 41.01it/s, loss=0.735, v_num=sazl, LTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 44.12it/s, loss=0.696, v_num=sazl, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 43.42it/s, loss=0.696, v_num=sazl, LTC_val_\u001b[A\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 40.70it/s, loss=0.713, v_num=sazl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 19/19 [00:00<00:00, 40.84it/s, loss=0.713, v_num=sazl, LTC_val_\u001b[A\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 40.73it/s, loss=0.705, v_num=sazl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 19/19 [00:00<00:00, 40.87it/s, loss=0.705, v_num=sazl, LTC_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 18/19 [00:00<00:00, 41.07it/s, loss=0.69, v_num=sazl, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 19/19 [00:00<00:00, 41.08it/s, loss=0.69, v_num=sazl, LTC_val_a\u001b[A\n",
      "Epoch 5:  95%|▉| 18/19 [00:00<00:00, 39.32it/s, loss=0.695, v_num=sazl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.036 >= min_delta = 0.003. New best score: 0.609\n",
      "Epoch 5: 100%|█| 19/19 [00:00<00:00, 39.19it/s, loss=0.695, v_num=sazl, LTC_val_\n",
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 39.54it/s, loss=0.69, v_num=sazl, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 19/19 [00:00<00:00, 39.60it/s, loss=0.69, v_num=sazl, LTC_val_a\u001b[A\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 40.25it/s, loss=0.684, v_num=sazl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 19/19 [00:00<00:00, 40.40it/s, loss=0.684, v_num=sazl, LTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 42.34it/s, loss=0.681, v_num=sazl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.605\n",
      "Epoch 8: 100%|█| 19/19 [00:00<00:00, 42.22it/s, loss=0.681, v_num=sazl, LTC_val_\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 40.68it/s, loss=0.676, v_num=sazl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:00<00:00, 40.81it/s, loss=0.676, v_num=sazl, LTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 42.57it/s, loss=0.666, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.596\n",
      "Epoch 10: 100%|█| 19/19 [00:00<00:00, 42.73it/s, loss=0.666, v_num=sazl, LTC_val\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 40.88it/s, loss=0.673, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.023 >= min_delta = 0.003. New best score: 0.573\n",
      "Epoch 11: 100%|█| 19/19 [00:00<00:00, 41.02it/s, loss=0.673, v_num=sazl, LTC_val\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 42.76it/s, loss=0.667, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.569\n",
      "Epoch 12: 100%|█| 19/19 [00:00<00:00, 42.59it/s, loss=0.667, v_num=sazl, LTC_val\n",
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 41.19it/s, loss=0.652, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:00<00:00, 41.02it/s, loss=0.652, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 40.82it/s, loss=0.646, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:00<00:00, 40.88it/s, loss=0.646, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 43.46it/s, loss=0.625, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.024 >= min_delta = 0.003. New best score: 0.544\n",
      "Epoch 15: 100%|█| 19/19 [00:00<00:00, 43.37it/s, loss=0.625, v_num=sazl, LTC_val\n",
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 39.93it/s, loss=0.609, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:00<00:00, 40.03it/s, loss=0.609, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 40.73it/s, loss=0.605, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:00<00:00, 40.99it/s, loss=0.605, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 40.64it/s, loss=0.578, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:00<00:00, 40.41it/s, loss=0.578, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 35.30it/s, loss=0.559, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.136 >= min_delta = 0.003. New best score: 0.408\n",
      "Epoch 19: 100%|█| 19/19 [00:00<00:00, 35.22it/s, loss=0.559, v_num=sazl, LTC_val\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 42.94it/s, loss=0.539, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:00<00:00, 43.07it/s, loss=0.539, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 41.62it/s, loss=0.519, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.092 >= min_delta = 0.003. New best score: 0.316\n",
      "Epoch 21: 100%|█| 19/19 [00:00<00:00, 40.89it/s, loss=0.519, v_num=sazl, LTC_val\n",
      "Epoch 22:  95%|▉| 18/19 [00:00<00:00, 42.57it/s, loss=0.532, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:00<00:00, 42.80it/s, loss=0.532, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 41.49it/s, loss=0.484, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.109 >= min_delta = 0.003. New best score: 0.207\n",
      "Epoch 23: 100%|█| 19/19 [00:00<00:00, 41.47it/s, loss=0.484, v_num=sazl, LTC_val\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 40.19it/s, loss=0.489, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:00<00:00, 40.26it/s, loss=0.489, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 42.35it/s, loss=0.472, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:00<00:00, 42.22it/s, loss=0.472, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:00<00:00, 41.14it/s, loss=0.465, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:00<00:00, 41.24it/s, loss=0.465, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 39.49it/s, loss=0.452, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:00<00:00, 39.58it/s, loss=0.452, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 39.68it/s, loss=0.448, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:00<00:00, 39.56it/s, loss=0.448, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 42.27it/s, loss=0.427, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:00<00:00, 40.76it/s, loss=0.427, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 39.18it/s, loss=0.416, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.041 >= min_delta = 0.003. New best score: 0.167\n",
      "Epoch 30: 100%|█| 19/19 [00:00<00:00, 39.12it/s, loss=0.416, v_num=sazl, LTC_val\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 41.27it/s, loss=0.425, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:00<00:00, 41.25it/s, loss=0.425, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:00<00:00, 42.25it/s, loss=0.424, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:00<00:00, 42.50it/s, loss=0.424, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 18/19 [00:00<00:00, 39.94it/s, loss=0.398, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:00<00:00, 40.02it/s, loss=0.398, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:00<00:00, 43.69it/s, loss=0.422, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:00<00:00, 43.33it/s, loss=0.422, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:00<00:00, 42.05it/s, loss=0.417, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:00<00:00, 42.26it/s, loss=0.417, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:00<00:00, 40.26it/s, loss=0.392, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 19/19 [00:00<00:00, 40.52it/s, loss=0.392, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 18/19 [00:00<00:00, 41.17it/s, loss=0.39, v_num=sazl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:00<00:00, 41.28it/s, loss=0.39, v_num=sazl, LTC_val_\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:00<00:00, 41.29it/s, loss=0.379, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 19/19 [00:00<00:00, 41.29it/s, loss=0.379, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 18/19 [00:00<00:00, 41.01it/s, loss=0.369, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 19/19 [00:00<00:00, 40.47it/s, loss=0.369, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 18/19 [00:00<00:00, 39.99it/s, loss=0.362, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 19/19 [00:00<00:00, 39.98it/s, loss=0.362, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 18/19 [00:00<00:00, 44.38it/s, loss=0.36, v_num=sazl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 19/19 [00:00<00:00, 44.42it/s, loss=0.36, v_num=sazl, LTC_val_\u001b[A\n",
      "Epoch 42:  95%|▉| 18/19 [00:00<00:00, 41.08it/s, loss=0.358, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 19/19 [00:00<00:00, 41.33it/s, loss=0.358, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 18/19 [00:00<00:00, 40.21it/s, loss=0.327, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 19/19 [00:00<00:00, 40.27it/s, loss=0.327, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 44:  95%|▉| 18/19 [00:00<00:00, 38.29it/s, loss=0.341, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 19/19 [00:00<00:00, 38.12it/s, loss=0.341, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 45:  95%|▉| 18/19 [00:00<00:00, 42.14it/s, loss=0.344, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 19/19 [00:00<00:00, 42.27it/s, loss=0.344, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 18/19 [00:00<00:00, 40.19it/s, loss=0.325, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 19/19 [00:00<00:00, 40.14it/s, loss=0.325, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 18/19 [00:00<00:00, 40.91it/s, loss=0.33, v_num=sazl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 19/19 [00:00<00:00, 40.87it/s, loss=0.33, v_num=sazl, LTC_val_\u001b[A\n",
      "Epoch 48:  95%|▉| 18/19 [00:00<00:00, 41.49it/s, loss=0.315, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 19/19 [00:00<00:00, 41.60it/s, loss=0.315, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 49:  95%|▉| 18/19 [00:00<00:00, 42.03it/s, loss=0.316, v_num=sazl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 19/19 [00:00<00:00, 41.99it/s, loss=0.316, v_num=sazl, LTC_val\u001b[A\n",
      "Epoch 50:  95%|▉| 18/19 [00:00<00:00, 40.72it/s, loss=0.29, v_num=sazl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.167. Signaling Trainer to stop.\n",
      "Epoch 50: 100%|█| 19/19 [00:00<00:00, 40.84it/s, loss=0.29, v_num=sazl, LTC_val_\n",
      "Epoch 50: 100%|█| 19/19 [00:00<00:00, 40.60it/s, loss=0.29, v_num=sazl, LTC_val_\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 85.93it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.5,\n",
      " 'LTC_test_f1': 0.4223363399505615,\n",
      " 'test_loss': 1.1077232360839844}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 103601\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103825-f28psazl/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103825-f28psazl/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.87609\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.8741\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.2838\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 918\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902737\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 120\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.83333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.77778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.33647\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.7931\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.78676\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.39769\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.42234\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.10772\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▁▁▂▁▂▂▂▃▃▃▄▄▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▂▂▂▂▃▃▃▄▃▃▄▄▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇█▇▇▇▇▇▇▇▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▇▁▁▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇█▃▅▇▇▇██▅███▇█▆█▇██▇▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▄▁▁▄▄▄▄▄▄▄▄▄▄▄▄▄▆▇█▃▅▇▇▇██▅███▆█▆█▇██▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▆▆█▆▅▅▆▅▅▅▅▅▅▅▅▃▄▂▁█▅▃▃▂▁▁▅▁▁▂▃▁▄▂▂▂▂▃▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▁▂▄▃▅▅▆▅▆▇█▆▇█▇▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▁▃▄▃▅▅▆▆▆▇█▆▇█▇▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ██▇▇▇▆▅▃▅▄▂▁▃▂▁▃▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/f28psazl\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:39:14.207290: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/ih4b7ygh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103912-ih4b7ygh\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 109 K \n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.532     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:00<00:00, 42.71it/s, loss=1.15, v_num=7ygh, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.129\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 42.77it/s, loss=1.15, v_num=7ygh, LTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 42.63it/s, loss=1.12, v_num=7ygh, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 42.15it/s, loss=1.12, v_num=7ygh, LTC_val_a\u001b[A\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 40.70it/s, loss=1.09, v_num=7ygh, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 19/19 [00:00<00:00, 40.82it/s, loss=1.09, v_num=7ygh, LTC_val_a\u001b[A\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 41.33it/s, loss=1.09, v_num=7ygh, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 19/19 [00:00<00:00, 40.09it/s, loss=1.09, v_num=7ygh, LTC_val_a\u001b[A\n",
      "Epoch 4:  95%|▉| 18/19 [00:00<00:00, 40.69it/s, loss=1.1, v_num=7ygh, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 19/19 [00:00<00:00, 39.19it/s, loss=1.1, v_num=7ygh, LTC_val_ac\u001b[A\n",
      "Epoch 5:  95%|▉| 18/19 [00:00<00:00, 39.43it/s, loss=1.1, v_num=7ygh, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 19/19 [00:00<00:00, 39.42it/s, loss=1.1, v_num=7ygh, LTC_val_ac\u001b[A\n",
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 39.76it/s, loss=1.09, v_num=7ygh, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 19/19 [00:00<00:00, 40.03it/s, loss=1.09, v_num=7ygh, LTC_val_a\u001b[A\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 41.75it/s, loss=1.08, v_num=7ygh, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 19/19 [00:00<00:00, 41.83it/s, loss=1.08, v_num=7ygh, LTC_val_a\u001b[A\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 41.18it/s, loss=1.1, v_num=7ygh, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 1.117\n",
      "Epoch 8: 100%|█| 19/19 [00:00<00:00, 40.91it/s, loss=1.1, v_num=7ygh, LTC_val_ac\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 40.16it/s, loss=1.09, v_num=7ygh, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:00<00:00, 39.94it/s, loss=1.09, v_num=7ygh, LTC_val_a\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 42.29it/s, loss=1.08, v_num=7ygh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:00<00:00, 42.28it/s, loss=1.08, v_num=7ygh, LTC_val_\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 39.65it/s, loss=1.08, v_num=7ygh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:00<00:00, 39.56it/s, loss=1.08, v_num=7ygh, LTC_val_\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 41.51it/s, loss=1.07, v_num=7ygh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:00<00:00, 41.70it/s, loss=1.07, v_num=7ygh, LTC_val_\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 39.70it/s, loss=1.06, v_num=7ygh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:00<00:00, 39.77it/s, loss=1.06, v_num=7ygh, LTC_val_\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 40.26it/s, loss=1.06, v_num=7ygh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:00<00:00, 40.40it/s, loss=1.06, v_num=7ygh, LTC_val_\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 40.21it/s, loss=1.06, v_num=7ygh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.066 >= min_delta = 0.003. New best score: 1.051\n",
      "Epoch 15: 100%|█| 19/19 [00:00<00:00, 40.30it/s, loss=1.06, v_num=7ygh, LTC_val_\n",
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 40.42it/s, loss=1.05, v_num=7ygh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:00<00:00, 39.89it/s, loss=1.05, v_num=7ygh, LTC_val_\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 42.31it/s, loss=1.05, v_num=7ygh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:00<00:00, 42.55it/s, loss=1.05, v_num=7ygh, LTC_val_\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 43.06it/s, loss=1.03, v_num=7ygh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:00<00:00, 43.27it/s, loss=1.03, v_num=7ygh, LTC_val_\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 33.51it/s, loss=1, v_num=7ygh, LTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:00<00:00, 33.97it/s, loss=1, v_num=7ygh, LTC_val_acc\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 39.06it/s, loss=1.01, v_num=7ygh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:00<00:00, 38.92it/s, loss=1.01, v_num=7ygh, LTC_val_\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 40.20it/s, loss=0.996, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:00<00:00, 40.43it/s, loss=0.996, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:00<00:00, 42.22it/s, loss=0.967, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:00<00:00, 42.46it/s, loss=0.967, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 40.66it/s, loss=0.945, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:00<00:00, 40.84it/s, loss=0.945, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 40.35it/s, loss=0.904, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:00<00:00, 40.39it/s, loss=0.904, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 42.19it/s, loss=0.896, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.161 >= min_delta = 0.003. New best score: 0.890\n",
      "Epoch 25: 100%|█| 19/19 [00:00<00:00, 42.27it/s, loss=0.896, v_num=7ygh, LTC_val\n",
      "Epoch 26:  95%|▉| 18/19 [00:00<00:00, 40.51it/s, loss=0.88, v_num=7ygh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.186 >= min_delta = 0.003. New best score: 0.703\n",
      "Epoch 26: 100%|█| 19/19 [00:00<00:00, 40.92it/s, loss=0.88, v_num=7ygh, LTC_val_\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 40.24it/s, loss=0.827, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:00<00:00, 40.32it/s, loss=0.827, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 41.03it/s, loss=0.818, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:00<00:00, 40.69it/s, loss=0.818, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 41.32it/s, loss=0.794, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:00<00:00, 41.59it/s, loss=0.794, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 39.81it/s, loss=0.776, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.074 >= min_delta = 0.003. New best score: 0.629\n",
      "Epoch 30: 100%|█| 19/19 [00:00<00:00, 40.11it/s, loss=0.776, v_num=7ygh, LTC_val\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 42.45it/s, loss=0.785, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:00<00:00, 40.78it/s, loss=0.785, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:00<00:00, 42.09it/s, loss=0.751, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:00<00:00, 42.06it/s, loss=0.751, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 18/19 [00:00<00:00, 39.68it/s, loss=0.755, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:00<00:00, 39.39it/s, loss=0.755, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:00<00:00, 38.46it/s, loss=0.734, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.089 >= min_delta = 0.003. New best score: 0.540\n",
      "Epoch 34: 100%|█| 19/19 [00:00<00:00, 38.68it/s, loss=0.734, v_num=7ygh, LTC_val\n",
      "Epoch 35:  95%|▉| 18/19 [00:00<00:00, 41.38it/s, loss=0.732, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:00<00:00, 41.53it/s, loss=0.732, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:00<00:00, 41.28it/s, loss=0.739, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 19/19 [00:00<00:00, 41.40it/s, loss=0.739, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 18/19 [00:00<00:00, 40.91it/s, loss=0.712, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:00<00:00, 40.98it/s, loss=0.712, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:00<00:00, 39.50it/s, loss=0.7, v_num=7ygh, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 19/19 [00:00<00:00, 39.89it/s, loss=0.7, v_num=7ygh, LTC_val_a\u001b[A\n",
      "Epoch 39:  95%|▉| 18/19 [00:00<00:00, 42.11it/s, loss=0.691, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 19/19 [00:00<00:00, 41.72it/s, loss=0.691, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 18/19 [00:00<00:00, 42.03it/s, loss=0.684, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 19/19 [00:00<00:00, 42.14it/s, loss=0.684, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 18/19 [00:00<00:00, 40.94it/s, loss=0.668, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 19/19 [00:00<00:00, 40.78it/s, loss=0.668, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 18/19 [00:00<00:00, 41.10it/s, loss=0.666, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 19/19 [00:00<00:00, 40.92it/s, loss=0.666, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 18/19 [00:00<00:00, 39.85it/s, loss=0.664, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 19/19 [00:00<00:00, 40.10it/s, loss=0.664, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 44:  95%|▉| 18/19 [00:00<00:00, 42.03it/s, loss=0.662, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 19/19 [00:00<00:00, 41.88it/s, loss=0.662, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 45:  95%|▉| 18/19 [00:00<00:00, 42.40it/s, loss=0.634, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.043 >= min_delta = 0.003. New best score: 0.497\n",
      "Epoch 45: 100%|█| 19/19 [00:00<00:00, 42.29it/s, loss=0.634, v_num=7ygh, LTC_val\n",
      "Epoch 46:  95%|▉| 18/19 [00:00<00:00, 40.14it/s, loss=0.621, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 19/19 [00:00<00:00, 40.27it/s, loss=0.621, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 18/19 [00:00<00:00, 41.01it/s, loss=0.608, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 19/19 [00:00<00:00, 40.74it/s, loss=0.608, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 48:  95%|▉| 18/19 [00:00<00:00, 43.80it/s, loss=0.592, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 19/19 [00:00<00:00, 43.34it/s, loss=0.592, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 49:  95%|▉| 18/19 [00:00<00:00, 40.79it/s, loss=0.593, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 19/19 [00:00<00:00, 40.68it/s, loss=0.593, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 50:  95%|▉| 18/19 [00:00<00:00, 41.33it/s, loss=0.604, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 19/19 [00:00<00:00, 41.22it/s, loss=0.604, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 51:  95%|▉| 18/19 [00:00<00:00, 39.03it/s, loss=0.593, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 19/19 [00:00<00:00, 38.95it/s, loss=0.593, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 52:  95%|▉| 18/19 [00:00<00:00, 43.35it/s, loss=0.597, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 19/19 [00:00<00:00, 43.43it/s, loss=0.597, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 53:  95%|▉| 18/19 [00:00<00:00, 40.88it/s, loss=0.59, v_num=7ygh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 19/19 [00:00<00:00, 40.82it/s, loss=0.59, v_num=7ygh, LTC_val_\u001b[A\n",
      "Epoch 54:  95%|▉| 18/19 [00:00<00:00, 40.25it/s, loss=0.567, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 19/19 [00:00<00:00, 40.15it/s, loss=0.567, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 55:  95%|▉| 18/19 [00:00<00:00, 40.92it/s, loss=0.545, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 19/19 [00:00<00:00, 40.41it/s, loss=0.545, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 56:  95%|▉| 18/19 [00:00<00:00, 42.69it/s, loss=0.55, v_num=7ygh, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 19/19 [00:00<00:00, 42.66it/s, loss=0.55, v_num=7ygh, LTC_val_\u001b[A\n",
      "Epoch 57:  95%|▉| 18/19 [00:00<00:00, 41.10it/s, loss=0.539, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 19/19 [00:00<00:00, 40.59it/s, loss=0.539, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 58:  95%|▉| 18/19 [00:00<00:00, 40.68it/s, loss=0.535, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 19/19 [00:00<00:00, 40.71it/s, loss=0.535, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 59:  95%|▉| 18/19 [00:00<00:00, 43.36it/s, loss=0.515, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|█| 19/19 [00:00<00:00, 43.48it/s, loss=0.515, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 60:  95%|▉| 18/19 [00:00<00:00, 41.47it/s, loss=0.559, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 60: 100%|█| 19/19 [00:00<00:00, 41.69it/s, loss=0.559, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 61:  95%|▉| 18/19 [00:00<00:00, 43.20it/s, loss=0.521, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|█| 19/19 [00:00<00:00, 43.01it/s, loss=0.521, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 62:  95%|▉| 18/19 [00:00<00:00, 40.15it/s, loss=0.474, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 62: 100%|█| 19/19 [00:00<00:00, 40.36it/s, loss=0.474, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 63:  95%|▉| 18/19 [00:00<00:00, 41.74it/s, loss=0.501, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|█| 19/19 [00:00<00:00, 41.87it/s, loss=0.501, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 64:  95%|▉| 18/19 [00:00<00:00, 40.40it/s, loss=0.495, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 64: 100%|█| 19/19 [00:00<00:00, 40.77it/s, loss=0.495, v_num=7ygh, LTC_val\u001b[A\n",
      "Epoch 65:  95%|▉| 18/19 [00:00<00:00, 41.89it/s, loss=0.476, v_num=7ygh, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.497. Signaling Trainer to stop.\n",
      "Epoch 65: 100%|█| 19/19 [00:00<00:00, 41.91it/s, loss=0.476, v_num=7ygh, LTC_val\n",
      "Epoch 65: 100%|█| 19/19 [00:00<00:00, 41.66it/s, loss=0.476, v_num=7ygh, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 74.02it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.6333333253860474,\n",
      " 'LTC_test_f1': 0.5786994099617004,\n",
      " 'test_loss': 0.9927167296409607}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 103778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103912-ih4b7ygh/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_103912-ih4b7ygh/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.79668\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.79407\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.47274\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 65\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1188\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 39\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902791\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 155\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.41667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.35556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.43242\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.71875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.68742\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.68115\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.63333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.5787\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.99272\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▇█▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▇█▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ██▇█▇▇▇▇▇▇▇▇▇▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▃▁▁▁▁▅▁▁▁▅▅▁▁▃▁▅▇▄▆▃▇▆▆▄▄▄▇█▄▆▄▄▅▅▅▅▃▄▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▂▁▁▁▁▄▁▁▁▂▂▁▁▃▁▄▇▄▇▃█▇▇▄▄▄▇█▄▆▄▄▅▅▅▅▃▃▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▆▆▆▆▆▆▆▆▇▅▅▆▆▆▇▄▃▅▂▇▂▃▄▅▅▅▂▁▆▄▇█▅▆▅▄▇▇▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▂▂▂▃▃▄▄▆▅▆▅▆▅▇▆▆▇▇█▆█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▂▂▂▂▃▄▃▅▅▆▅▆▅▇▆▆▇▇█▆█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▇██▇▇▇▇▄▇▄▄▂▄▃▃▃▂▂▁▃▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/ih4b7ygh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:40:07.998085: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/2zmgjjnl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104006-2zmgjjnl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:00<00:00, 44.42it/s, loss=0.711, v_num=jjnl, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.674\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 44.25it/s, loss=0.711, v_num=jjnl, LTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 43.62it/s, loss=0.678, v_num=jjnl, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.074 >= min_delta = 0.003. New best score: 0.599\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 43.67it/s, loss=0.678, v_num=jjnl, LTC_val_\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 41.02it/s, loss=0.647, v_num=jjnl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.183 >= min_delta = 0.003. New best score: 0.416\n",
      "Epoch 2: 100%|█| 19/19 [00:00<00:00, 40.95it/s, loss=0.647, v_num=jjnl, LTC_val_\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 42.08it/s, loss=0.613, v_num=jjnl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 19/19 [00:00<00:00, 40.55it/s, loss=0.613, v_num=jjnl, LTC_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 18/19 [00:00<00:00, 40.62it/s, loss=0.59, v_num=jjnl, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.404\n",
      "Epoch 4: 100%|█| 19/19 [00:00<00:00, 39.95it/s, loss=0.59, v_num=jjnl, LTC_val_a\n",
      "Epoch 5:  95%|▉| 18/19 [00:00<00:00, 43.73it/s, loss=0.578, v_num=jjnl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 19/19 [00:00<00:00, 43.23it/s, loss=0.578, v_num=jjnl, LTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 42.35it/s, loss=0.551, v_num=jjnl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 19/19 [00:00<00:00, 41.48it/s, loss=0.551, v_num=jjnl, LTC_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 43.26it/s, loss=0.554, v_num=jjnl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.162 >= min_delta = 0.003. New best score: 0.242\n",
      "Epoch 7: 100%|█| 19/19 [00:00<00:00, 43.02it/s, loss=0.554, v_num=jjnl, LTC_val_\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 40.78it/s, loss=0.552, v_num=jjnl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 0.220\n",
      "Epoch 8: 100%|█| 19/19 [00:00<00:00, 40.58it/s, loss=0.552, v_num=jjnl, LTC_val_\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 41.11it/s, loss=0.509, v_num=jjnl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:00<00:00, 40.89it/s, loss=0.509, v_num=jjnl, LTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 40.29it/s, loss=0.508, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:00<00:00, 39.88it/s, loss=0.508, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 40.34it/s, loss=0.524, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:00<00:00, 39.96it/s, loss=0.524, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 41.26it/s, loss=0.508, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:00<00:00, 41.27it/s, loss=0.508, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 43.56it/s, loss=0.488, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:00<00:00, 43.32it/s, loss=0.488, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 42.90it/s, loss=0.502, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:00<00:00, 42.74it/s, loss=0.502, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 43.82it/s, loss=0.492, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.037 >= min_delta = 0.003. New best score: 0.183\n",
      "Epoch 15: 100%|█| 19/19 [00:00<00:00, 43.68it/s, loss=0.492, v_num=jjnl, LTC_val\n",
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 42.14it/s, loss=0.482, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:00<00:00, 42.17it/s, loss=0.482, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 42.75it/s, loss=0.477, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:00<00:00, 41.98it/s, loss=0.477, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 40.14it/s, loss=0.487, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:00<00:00, 40.46it/s, loss=0.487, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 42.45it/s, loss=0.495, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:00<00:00, 42.59it/s, loss=0.495, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 42.25it/s, loss=0.478, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:00<00:00, 42.39it/s, loss=0.478, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 39.60it/s, loss=0.468, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:00<00:00, 39.52it/s, loss=0.468, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:00<00:00, 36.64it/s, loss=0.467, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:00<00:00, 36.73it/s, loss=0.467, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 40.35it/s, loss=0.462, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:00<00:00, 40.51it/s, loss=0.462, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 38.87it/s, loss=0.475, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:00<00:00, 38.49it/s, loss=0.475, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 40.58it/s, loss=0.455, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:00<00:00, 40.76it/s, loss=0.455, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:00<00:00, 40.68it/s, loss=0.469, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.170\n",
      "Epoch 26: 100%|█| 19/19 [00:00<00:00, 40.83it/s, loss=0.469, v_num=jjnl, LTC_val\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 42.36it/s, loss=0.499, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:00<00:00, 42.58it/s, loss=0.499, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 44.57it/s, loss=0.481, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:00<00:00, 43.94it/s, loss=0.481, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 39.25it/s, loss=0.468, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:00<00:00, 39.27it/s, loss=0.468, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 40.98it/s, loss=0.464, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:00<00:00, 40.83it/s, loss=0.464, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 41.86it/s, loss=0.474, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:00<00:00, 41.35it/s, loss=0.474, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:00<00:00, 41.99it/s, loss=0.472, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:00<00:00, 41.93it/s, loss=0.472, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 18/19 [00:00<00:00, 43.33it/s, loss=0.471, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:00<00:00, 43.09it/s, loss=0.471, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:00<00:00, 41.40it/s, loss=0.463, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:00<00:00, 41.56it/s, loss=0.463, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:00<00:00, 41.64it/s, loss=0.465, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:00<00:00, 41.75it/s, loss=0.465, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:00<00:00, 40.74it/s, loss=0.452, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.163\n",
      "Epoch 36: 100%|█| 19/19 [00:00<00:00, 40.87it/s, loss=0.452, v_num=jjnl, LTC_val\n",
      "Epoch 37:  95%|▉| 18/19 [00:00<00:00, 41.59it/s, loss=0.488, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:00<00:00, 41.37it/s, loss=0.488, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:00<00:00, 42.23it/s, loss=0.49, v_num=jjnl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 19/19 [00:00<00:00, 42.27it/s, loss=0.49, v_num=jjnl, LTC_val_\u001b[A\n",
      "Epoch 39:  95%|▉| 18/19 [00:00<00:00, 40.20it/s, loss=0.448, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 19/19 [00:00<00:00, 40.19it/s, loss=0.448, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 18/19 [00:00<00:00, 41.48it/s, loss=0.453, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 19/19 [00:00<00:00, 41.79it/s, loss=0.453, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 18/19 [00:00<00:00, 43.60it/s, loss=0.46, v_num=jjnl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 19/19 [00:00<00:00, 43.28it/s, loss=0.46, v_num=jjnl, LTC_val_\u001b[A\n",
      "Epoch 42:  95%|▉| 18/19 [00:00<00:00, 42.08it/s, loss=0.447, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 19/19 [00:00<00:00, 42.10it/s, loss=0.447, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 18/19 [00:00<00:00, 41.74it/s, loss=0.456, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 19/19 [00:00<00:00, 41.82it/s, loss=0.456, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 44:  95%|▉| 18/19 [00:00<00:00, 40.23it/s, loss=0.446, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 19/19 [00:00<00:00, 40.19it/s, loss=0.446, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 45:  95%|▉| 18/19 [00:00<00:00, 41.32it/s, loss=0.453, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 19/19 [00:00<00:00, 41.31it/s, loss=0.453, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 18/19 [00:00<00:00, 41.07it/s, loss=0.442, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 19/19 [00:00<00:00, 40.93it/s, loss=0.442, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 18/19 [00:00<00:00, 40.24it/s, loss=0.438, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 19/19 [00:00<00:00, 40.20it/s, loss=0.438, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 48:  95%|▉| 18/19 [00:00<00:00, 43.43it/s, loss=0.443, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 19/19 [00:00<00:00, 41.72it/s, loss=0.443, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 49:  95%|▉| 18/19 [00:00<00:00, 41.50it/s, loss=0.441, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 19/19 [00:00<00:00, 41.44it/s, loss=0.441, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 50:  95%|▉| 18/19 [00:00<00:00, 41.18it/s, loss=0.442, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 19/19 [00:00<00:00, 40.94it/s, loss=0.442, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 51:  95%|▉| 18/19 [00:00<00:00, 39.68it/s, loss=0.434, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 19/19 [00:00<00:00, 39.81it/s, loss=0.434, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 52:  95%|▉| 18/19 [00:00<00:00, 42.75it/s, loss=0.45, v_num=jjnl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 19/19 [00:00<00:00, 43.06it/s, loss=0.45, v_num=jjnl, LTC_val_\u001b[A\n",
      "Epoch 53:  95%|▉| 18/19 [00:00<00:00, 42.68it/s, loss=0.44, v_num=jjnl, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 19/19 [00:00<00:00, 43.02it/s, loss=0.44, v_num=jjnl, LTC_val_\u001b[A\n",
      "Epoch 54:  95%|▉| 18/19 [00:00<00:00, 43.27it/s, loss=0.434, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 19/19 [00:00<00:00, 43.19it/s, loss=0.434, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 55:  95%|▉| 18/19 [00:00<00:00, 39.93it/s, loss=0.465, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 19/19 [00:00<00:00, 40.18it/s, loss=0.465, v_num=jjnl, LTC_val\u001b[A\n",
      "Epoch 56:  95%|▉| 18/19 [00:00<00:00, 41.07it/s, loss=0.455, v_num=jjnl, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.163. Signaling Trainer to stop.\n",
      "Epoch 56: 100%|█| 19/19 [00:00<00:00, 41.34it/s, loss=0.455, v_num=jjnl, LTC_val\n",
      "Epoch 56: 100%|█| 19/19 [00:00<00:00, 41.15it/s, loss=0.455, v_num=jjnl, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 71.68it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.7333333492279053,\n",
      " 'LTC_test_f1': 0.7285068035125732,\n",
      " 'test_loss': 0.5483573079109192}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 103988\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104006-2zmgjjnl/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104006-2zmgjjnl/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.78883\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.78566\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.45582\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 56\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1026\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 34\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902840\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 134\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.91667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.87368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.27085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.76562\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.752\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.52971\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.73333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.72851\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.54836\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▃▄▅▆▆▆▇▇▇▇▇▇▇▇█▇▇▇▇▇▇█▇█▇▇███▇████████▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▃▄▆▆▆▆▇▇▇▇▇▇▇▇█▇▇▇▇▇▇███▇▇███▇████████▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▃▂▂▂▂▁▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▅▅█▇▇▇█▁▇▇▇██▅█▇██▅▇█▇▅▇▇▇▇▇█▇█▇▇▇▇█▇▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▁█▆▆▆█▂▆▆▆██▅█▆██▅▆█▆▅▆▆▆▆▆█▆█▆▆▆▆█▆▆█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▇▄▄▄▂▂▆▃▂▃▁▁▄▂▃▁▃▄▃▂▃▄▃▅▂▃▃▂▃▁▃▂▃▃▂▂▃▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▅▅▃▇▅▅▇▆▅▄█▅▇▆▇▃▅▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▅▆▃█▅▅▇▆▅▅█▆▇▆▇▄▆▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▅▄▆▃▄▄▃▂▄▆▁▄▂▂▁▆▂▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/2zmgjjnl\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:40:56.159701: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/36o1cj5y\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104054-36o1cj5y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:00<00:00, 45.20it/s, loss=1.13, v_num=cj5y, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.103\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 44.89it/s, loss=1.13, v_num=cj5y, LTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 48.23it/s, loss=1.1, v_num=cj5y, LTC_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 46.90it/s, loss=1.1, v_num=cj5y, LTC_val_ac\u001b[A\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 40.31it/s, loss=1.06, v_num=cj5y, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.058 >= min_delta = 0.003. New best score: 1.045\n",
      "Epoch 2: 100%|█| 19/19 [00:00<00:00, 39.18it/s, loss=1.06, v_num=cj5y, LTC_val_a\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 44.05it/s, loss=1.02, v_num=cj5y, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 1.040\n",
      "Epoch 3: 100%|█| 19/19 [00:00<00:00, 43.42it/s, loss=1.02, v_num=cj5y, LTC_val_a\n",
      "Epoch 4:  95%|▉| 18/19 [00:00<00:00, 41.37it/s, loss=0.976, v_num=cj5y, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.145 >= min_delta = 0.003. New best score: 0.894\n",
      "Epoch 4: 100%|█| 19/19 [00:00<00:00, 41.59it/s, loss=0.976, v_num=cj5y, LTC_val_\n",
      "Epoch 5:  95%|▉| 18/19 [00:00<00:00, 40.82it/s, loss=0.947, v_num=cj5y, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.036 >= min_delta = 0.003. New best score: 0.859\n",
      "Epoch 5: 100%|█| 19/19 [00:00<00:00, 40.91it/s, loss=0.947, v_num=cj5y, LTC_val_\n",
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 42.75it/s, loss=0.921, v_num=cj5y, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.069 >= min_delta = 0.003. New best score: 0.790\n",
      "Epoch 6: 100%|█| 19/19 [00:00<00:00, 42.74it/s, loss=0.921, v_num=cj5y, LTC_val_\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 42.69it/s, loss=0.899, v_num=cj5y, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.015 >= min_delta = 0.003. New best score: 0.775\n",
      "Epoch 7: 100%|█| 19/19 [00:00<00:00, 42.56it/s, loss=0.899, v_num=cj5y, LTC_val_\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 42.86it/s, loss=0.869, v_num=cj5y, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.102 >= min_delta = 0.003. New best score: 0.673\n",
      "Epoch 8: 100%|█| 19/19 [00:00<00:00, 43.14it/s, loss=0.869, v_num=cj5y, LTC_val_\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 44.90it/s, loss=0.844, v_num=cj5y, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:00<00:00, 44.99it/s, loss=0.844, v_num=cj5y, LTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 42.83it/s, loss=0.835, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:00<00:00, 42.22it/s, loss=0.835, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 41.94it/s, loss=0.857, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:00<00:00, 42.11it/s, loss=0.857, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 41.91it/s, loss=0.854, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:00<00:00, 41.72it/s, loss=0.854, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 43.04it/s, loss=0.792, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:00<00:00, 42.46it/s, loss=0.792, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 43.39it/s, loss=0.806, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:00<00:00, 43.03it/s, loss=0.806, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 44.88it/s, loss=0.788, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.669\n",
      "Epoch 15: 100%|█| 19/19 [00:00<00:00, 44.27it/s, loss=0.788, v_num=cj5y, LTC_val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 39.41it/s, loss=0.828, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:00<00:00, 39.44it/s, loss=0.828, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 39.87it/s, loss=0.814, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:00<00:00, 40.08it/s, loss=0.814, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 41.98it/s, loss=0.809, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:00<00:00, 42.07it/s, loss=0.809, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 41.08it/s, loss=0.791, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.065 >= min_delta = 0.003. New best score: 0.604\n",
      "Epoch 19: 100%|█| 19/19 [00:00<00:00, 41.01it/s, loss=0.791, v_num=cj5y, LTC_val\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 42.33it/s, loss=0.8, v_num=cj5y, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:00<00:00, 42.54it/s, loss=0.8, v_num=cj5y, LTC_val_a\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 41.52it/s, loss=0.801, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:00<00:00, 41.26it/s, loss=0.801, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:00<00:00, 34.99it/s, loss=0.819, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:00<00:00, 35.40it/s, loss=0.819, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 51.46it/s, loss=0.797, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.599\n",
      "Epoch 23: 100%|█| 19/19 [00:00<00:00, 51.48it/s, loss=0.797, v_num=cj5y, LTC_val\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 42.03it/s, loss=0.778, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:00<00:00, 41.91it/s, loss=0.778, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 41.68it/s, loss=0.763, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:00<00:00, 41.77it/s, loss=0.763, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:00<00:00, 44.51it/s, loss=0.782, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:00<00:00, 44.11it/s, loss=0.782, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 43.07it/s, loss=0.793, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:00<00:00, 42.36it/s, loss=0.793, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 40.51it/s, loss=0.78, v_num=cj5y, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:00<00:00, 40.38it/s, loss=0.78, v_num=cj5y, LTC_val_\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 40.94it/s, loss=0.761, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:00<00:00, 40.66it/s, loss=0.761, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 42.18it/s, loss=0.772, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:00<00:00, 42.00it/s, loss=0.772, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 41.70it/s, loss=0.767, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:00<00:00, 41.05it/s, loss=0.767, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:00<00:00, 44.54it/s, loss=0.768, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:00<00:00, 44.61it/s, loss=0.768, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 18/19 [00:00<00:00, 43.53it/s, loss=0.773, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:00<00:00, 43.15it/s, loss=0.773, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:00<00:00, 43.64it/s, loss=0.76, v_num=cj5y, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:00<00:00, 43.25it/s, loss=0.76, v_num=cj5y, LTC_val_\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:00<00:00, 41.36it/s, loss=0.765, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:00<00:00, 41.59it/s, loss=0.765, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:00<00:00, 43.69it/s, loss=0.745, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 19/19 [00:00<00:00, 43.62it/s, loss=0.745, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 18/19 [00:00<00:00, 43.20it/s, loss=0.73, v_num=cj5y, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:00<00:00, 43.30it/s, loss=0.73, v_num=cj5y, LTC_val_\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:00<00:00, 43.74it/s, loss=0.757, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 19/19 [00:00<00:00, 43.75it/s, loss=0.757, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 18/19 [00:00<00:00, 40.72it/s, loss=0.775, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 19/19 [00:00<00:00, 40.81it/s, loss=0.775, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 18/19 [00:00<00:00, 41.86it/s, loss=0.78, v_num=cj5y, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 19/19 [00:00<00:00, 42.07it/s, loss=0.78, v_num=cj5y, LTC_val_\u001b[A\n",
      "Epoch 41:  95%|▉| 18/19 [00:00<00:00, 42.44it/s, loss=0.744, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 19/19 [00:00<00:00, 42.13it/s, loss=0.744, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 18/19 [00:00<00:00, 40.22it/s, loss=0.745, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 19/19 [00:00<00:00, 40.31it/s, loss=0.745, v_num=cj5y, LTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 18/19 [00:00<00:00, 41.57it/s, loss=0.752, v_num=cj5y, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.599. Signaling Trainer to stop.\n",
      "Epoch 43: 100%|█| 19/19 [00:00<00:00, 40.89it/s, loss=0.752, v_num=cj5y, LTC_val\n",
      "Epoch 43: 100%|█| 19/19 [00:00<00:00, 40.64it/s, loss=0.752, v_num=cj5y, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 95.38it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.5666666626930237,\n",
      " 'LTC_test_f1': 0.42543861269950867,\n",
      " 'test_loss': 0.8672093152999878}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 104154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104054-36o1cj5y/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104054-36o1cj5y/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.64747\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.64618\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.75448\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 43\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 792\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 28\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902882\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.58333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.73286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.57812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.57662\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.85243\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.56667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.42544\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.86721\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▂▃▄▅▅▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇██▇███████▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▂▃▄▅▅▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇██▇▇▇█████▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▇▆▅▅▄▄▃▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▁▁▁▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▁▄▄▇▇▅▅█▅▅▅█▅█▇▇▅▇█▅▇▇▅▇█▇▇▅▇▅▅▇█▅▄▅▅▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▁▄▅▇▇▅▅█▅▇▅█▇█▇▇▅▇█▅▇▇▅▇█▇▇▅▇▅▅▇█▇▆▇▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ██▇▇▅▅▄▃▂▃▃▄▂▃▂▂▃▂▁▃▂▁▁▁▂▂▂▃▂▂▁▁▂▂▃▃▃▃▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▃▂▄▄▂▁▄▆▅▅▅▇█▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▄▁▄▄▄▂▄▇▆▆▆▇█▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▆▇▅▇█▅▅▃▄▅▂▁▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/36o1cj5y\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:41:38.738931: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/1sdtafbd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104137-1sdtafbd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:00<00:00, 47.62it/s, loss=0.715, v_num=afbd, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.710\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 47.50it/s, loss=0.715, v_num=afbd, LTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 38.48it/s, loss=0.679, v_num=afbd, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.701\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 38.53it/s, loss=0.679, v_num=afbd, LTC_val_\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 41.72it/s, loss=0.641, v_num=afbd, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 19/19 [00:00<00:00, 41.83it/s, loss=0.641, v_num=afbd, LTC_val_\u001b[A\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 43.51it/s, loss=0.599, v_num=afbd, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.263 >= min_delta = 0.003. New best score: 0.439\n",
      "Epoch 3: 100%|█| 19/19 [00:00<00:00, 43.15it/s, loss=0.599, v_num=afbd, LTC_val_\n",
      "Epoch 4:  95%|▉| 18/19 [00:00<00:00, 37.91it/s, loss=0.584, v_num=afbd, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.037 >= min_delta = 0.003. New best score: 0.402\n",
      "Epoch 4: 100%|█| 19/19 [00:00<00:00, 37.88it/s, loss=0.584, v_num=afbd, LTC_val_\n",
      "Epoch 5:  95%|▉| 18/19 [00:00<00:00, 40.17it/s, loss=0.557, v_num=afbd, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 19/19 [00:00<00:00, 40.27it/s, loss=0.557, v_num=afbd, LTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 43.46it/s, loss=0.541, v_num=afbd, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 19/19 [00:00<00:00, 43.46it/s, loss=0.541, v_num=afbd, LTC_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 41.26it/s, loss=0.545, v_num=afbd, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.194 >= min_delta = 0.003. New best score: 0.208\n",
      "Epoch 7: 100%|█| 19/19 [00:00<00:00, 41.21it/s, loss=0.545, v_num=afbd, LTC_val_\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 44.96it/s, loss=0.539, v_num=afbd, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 19/19 [00:00<00:00, 44.53it/s, loss=0.539, v_num=afbd, LTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 41.03it/s, loss=0.514, v_num=afbd, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:00<00:00, 40.95it/s, loss=0.514, v_num=afbd, LTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 39.99it/s, loss=0.502, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 0.194\n",
      "Epoch 10: 100%|█| 19/19 [00:00<00:00, 40.16it/s, loss=0.502, v_num=afbd, LTC_val\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 40.79it/s, loss=0.512, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:00<00:00, 40.70it/s, loss=0.512, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 41.07it/s, loss=0.514, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:00<00:00, 40.58it/s, loss=0.514, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 39.51it/s, loss=0.493, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:00<00:00, 39.57it/s, loss=0.493, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 42.43it/s, loss=0.499, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.190\n",
      "Epoch 14: 100%|█| 19/19 [00:00<00:00, 42.37it/s, loss=0.499, v_num=afbd, LTC_val\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 42.91it/s, loss=0.508, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:00<00:00, 42.87it/s, loss=0.508, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 43.25it/s, loss=0.505, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:00<00:00, 43.28it/s, loss=0.505, v_num=afbd, LTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 42.12it/s, loss=0.501, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:00<00:00, 42.22it/s, loss=0.501, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 43.54it/s, loss=0.471, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:00<00:00, 42.88it/s, loss=0.471, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 41.18it/s, loss=0.469, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:00<00:00, 41.17it/s, loss=0.469, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 41.52it/s, loss=0.485, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:00<00:00, 41.34it/s, loss=0.485, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 44.34it/s, loss=0.467, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:00<00:00, 44.48it/s, loss=0.467, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:00<00:00, 35.76it/s, loss=0.469, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:00<00:00, 36.16it/s, loss=0.469, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 42.19it/s, loss=0.467, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:00<00:00, 41.64it/s, loss=0.467, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 41.15it/s, loss=0.478, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:00<00:00, 41.00it/s, loss=0.478, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 39.55it/s, loss=0.478, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:00<00:00, 39.63it/s, loss=0.478, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:00<00:00, 40.98it/s, loss=0.476, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:00<00:00, 40.74it/s, loss=0.476, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 41.80it/s, loss=0.475, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:00<00:00, 41.59it/s, loss=0.475, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 44.87it/s, loss=0.469, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:00<00:00, 45.11it/s, loss=0.469, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 41.20it/s, loss=0.452, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:00<00:00, 41.49it/s, loss=0.452, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 40.95it/s, loss=0.466, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:00<00:00, 40.52it/s, loss=0.466, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 40.37it/s, loss=0.464, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:00<00:00, 40.63it/s, loss=0.464, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:00<00:00, 40.24it/s, loss=0.461, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.019 >= min_delta = 0.003. New best score: 0.170\n",
      "Epoch 32: 100%|█| 19/19 [00:00<00:00, 39.65it/s, loss=0.461, v_num=afbd, LTC_val\n",
      "Epoch 33:  95%|▉| 18/19 [00:00<00:00, 38.57it/s, loss=0.465, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:00<00:00, 38.56it/s, loss=0.465, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:00<00:00, 43.23it/s, loss=0.444, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:00<00:00, 43.22it/s, loss=0.444, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:00<00:00, 40.71it/s, loss=0.444, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:00<00:00, 40.79it/s, loss=0.444, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:00<00:00, 39.97it/s, loss=0.453, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 19/19 [00:00<00:00, 40.00it/s, loss=0.453, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 18/19 [00:00<00:00, 39.85it/s, loss=0.468, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:00<00:00, 39.63it/s, loss=0.468, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:00<00:00, 39.66it/s, loss=0.458, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 19/19 [00:00<00:00, 39.21it/s, loss=0.458, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 18/19 [00:00<00:00, 41.98it/s, loss=0.472, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 19/19 [00:00<00:00, 41.96it/s, loss=0.472, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 18/19 [00:00<00:00, 39.90it/s, loss=0.449, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 19/19 [00:00<00:00, 39.84it/s, loss=0.449, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 18/19 [00:00<00:00, 42.74it/s, loss=0.467, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 19/19 [00:00<00:00, 42.50it/s, loss=0.467, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 18/19 [00:00<00:00, 42.05it/s, loss=0.441, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 19/19 [00:00<00:00, 40.88it/s, loss=0.441, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 18/19 [00:00<00:00, 46.20it/s, loss=0.463, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 19/19 [00:00<00:00, 45.60it/s, loss=0.463, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 44:  95%|▉| 18/19 [00:00<00:00, 40.85it/s, loss=0.435, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 19/19 [00:00<00:00, 40.81it/s, loss=0.435, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 45:  95%|▉| 18/19 [00:00<00:00, 40.64it/s, loss=0.453, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 19/19 [00:00<00:00, 40.61it/s, loss=0.453, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 18/19 [00:00<00:00, 42.69it/s, loss=0.448, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 19/19 [00:00<00:00, 42.80it/s, loss=0.448, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 18/19 [00:00<00:00, 42.90it/s, loss=0.448, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 19/19 [00:00<00:00, 42.97it/s, loss=0.448, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 48:  95%|▉| 18/19 [00:00<00:00, 41.59it/s, loss=0.441, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 19/19 [00:00<00:00, 41.38it/s, loss=0.441, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 49:  95%|▉| 18/19 [00:00<00:00, 41.28it/s, loss=0.441, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 19/19 [00:00<00:00, 41.32it/s, loss=0.441, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 50:  95%|▉| 18/19 [00:00<00:00, 42.41it/s, loss=0.438, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 19/19 [00:00<00:00, 42.35it/s, loss=0.438, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 51:  95%|▉| 18/19 [00:00<00:00, 43.49it/s, loss=0.457, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 19/19 [00:00<00:00, 41.90it/s, loss=0.457, v_num=afbd, LTC_val\u001b[A\n",
      "Epoch 52:  95%|▉| 18/19 [00:00<00:00, 39.79it/s, loss=0.424, v_num=afbd, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.170. Signaling Trainer to stop.\n",
      "Epoch 52: 100%|█| 19/19 [00:00<00:00, 40.09it/s, loss=0.424, v_num=afbd, LTC_val\n",
      "Epoch 52: 100%|█| 19/19 [00:00<00:00, 39.89it/s, loss=0.424, v_num=afbd, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 75.04it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.8333333134651184,\n",
      " 'LTC_test_f1': 0.8331479430198669,\n",
      " 'test_loss': 0.48648399114608765}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 104368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104137-1sdtafbd/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104137-1sdtafbd/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.81414\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.811\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.42803\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 954\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902930\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.25599\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.81084\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.40894\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.83333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.83315\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.48648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▃▄▆▆▆▆▇▇▇▇▇▆▇▇▇██▇▇▇█▇█▇▇███▇▇███▇█▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▃▄▆▆▆▆▇▇▇▇▇▇▇▇▇██▇▇▇███▇▇███▇████▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▂▄▇▄▄▇▇█▇██▇█▇▅▇▇▇▆▇▇▇▇█▆▇▇▆▇▇▇▇▇▆██▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▃▄▇▄▄▇▇█▇██▇█▆▅▇▇▆▆▇▇▇▇█▆▆▇▆▇▇▇▇▆▆██▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ███▄▆▇▂▂▁▃▁▁▃▂▃▅▃▂▄▄▃▃▂▃▁▄▄▂▅▄▂▂▃▄▅▂▂▃▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▅▅▂▆▅▇▆▄█▅█▇▆▅█▇▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▅▄▂▆▅▇▆▄█▅█▇▆▅█▇▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▄▅▇▅▅▅▃▅▁▃▃▃▂▄▂▁▅▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/1sdtafbd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:42:26.668022: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/3fdub9iv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104225-3fdub9iv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:00<00:00, 45.89it/s, loss=1.14, v_num=b9iv, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.096\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 45.31it/s, loss=1.14, v_num=b9iv, LTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 41.34it/s, loss=1.11, v_num=b9iv, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 41.17it/s, loss=1.11, v_num=b9iv, LTC_val_a\u001b[A\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 41.72it/s, loss=1.08, v_num=b9iv, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.061 >= min_delta = 0.003. New best score: 1.035\n",
      "Epoch 2: 100%|█| 19/19 [00:00<00:00, 41.46it/s, loss=1.08, v_num=b9iv, LTC_val_a\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 39.73it/s, loss=1.02, v_num=b9iv, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.122 >= min_delta = 0.003. New best score: 0.913\n",
      "Epoch 3: 100%|█| 19/19 [00:00<00:00, 39.87it/s, loss=1.02, v_num=b9iv, LTC_val_a\n",
      "Epoch 4:  95%|▉| 18/19 [00:00<00:00, 39.04it/s, loss=0.963, v_num=b9iv, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 19/19 [00:00<00:00, 39.11it/s, loss=0.963, v_num=b9iv, LTC_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 18/19 [00:00<00:00, 39.80it/s, loss=0.951, v_num=b9iv, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.036 >= min_delta = 0.003. New best score: 0.877\n",
      "Epoch 5: 100%|█| 19/19 [00:00<00:00, 39.14it/s, loss=0.951, v_num=b9iv, LTC_val_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 45.17it/s, loss=0.916, v_num=b9iv, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.096 >= min_delta = 0.003. New best score: 0.781\n",
      "Epoch 6: 100%|█| 19/19 [00:00<00:00, 45.13it/s, loss=0.916, v_num=b9iv, LTC_val_\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 40.98it/s, loss=0.892, v_num=b9iv, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.035 >= min_delta = 0.003. New best score: 0.746\n",
      "Epoch 7: 100%|█| 19/19 [00:00<00:00, 41.23it/s, loss=0.892, v_num=b9iv, LTC_val_\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 45.03it/s, loss=0.863, v_num=b9iv, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 19/19 [00:00<00:00, 43.55it/s, loss=0.863, v_num=b9iv, LTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 43.89it/s, loss=0.848, v_num=b9iv, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:00<00:00, 43.95it/s, loss=0.848, v_num=b9iv, LTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 40.65it/s, loss=0.878, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:00<00:00, 40.10it/s, loss=0.878, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 42.05it/s, loss=0.853, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:00<00:00, 41.79it/s, loss=0.853, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 40.77it/s, loss=0.829, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:00<00:00, 40.87it/s, loss=0.829, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 42.83it/s, loss=0.829, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:00<00:00, 42.83it/s, loss=0.829, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 43.82it/s, loss=0.837, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.019 >= min_delta = 0.003. New best score: 0.727\n",
      "Epoch 14: 100%|█| 19/19 [00:00<00:00, 43.92it/s, loss=0.837, v_num=b9iv, LTC_val\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 43.00it/s, loss=0.818, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.099 >= min_delta = 0.003. New best score: 0.628\n",
      "Epoch 15: 100%|█| 19/19 [00:00<00:00, 43.01it/s, loss=0.818, v_num=b9iv, LTC_val\n",
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 43.45it/s, loss=0.807, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:00<00:00, 43.35it/s, loss=0.807, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 42.88it/s, loss=0.805, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:00<00:00, 41.62it/s, loss=0.805, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 44.65it/s, loss=0.79, v_num=b9iv, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:00<00:00, 44.41it/s, loss=0.79, v_num=b9iv, LTC_val_\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 43.04it/s, loss=0.786, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.031 >= min_delta = 0.003. New best score: 0.598\n",
      "Epoch 19: 100%|█| 19/19 [00:00<00:00, 42.87it/s, loss=0.786, v_num=b9iv, LTC_val\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 41.88it/s, loss=0.787, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:00<00:00, 41.99it/s, loss=0.787, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 35.35it/s, loss=0.811, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:00<00:00, 35.64it/s, loss=0.811, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:00<00:00, 41.02it/s, loss=0.808, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:00<00:00, 40.70it/s, loss=0.808, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 42.87it/s, loss=0.766, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:00<00:00, 42.15it/s, loss=0.766, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 41.59it/s, loss=0.79, v_num=b9iv, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:00<00:00, 41.07it/s, loss=0.79, v_num=b9iv, LTC_val_\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 41.95it/s, loss=0.769, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:00<00:00, 42.02it/s, loss=0.769, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:00<00:00, 41.87it/s, loss=0.766, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:00<00:00, 41.93it/s, loss=0.766, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 42.58it/s, loss=0.772, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:00<00:00, 41.96it/s, loss=0.772, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 43.87it/s, loss=0.767, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.585\n",
      "Epoch 28: 100%|█| 19/19 [00:00<00:00, 43.84it/s, loss=0.767, v_num=b9iv, LTC_val\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 42.70it/s, loss=0.775, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:00<00:00, 41.96it/s, loss=0.775, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 41.60it/s, loss=0.781, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:00<00:00, 41.34it/s, loss=0.781, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 42.03it/s, loss=0.777, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:00<00:00, 42.30it/s, loss=0.777, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:00<00:00, 43.55it/s, loss=0.774, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:00<00:00, 43.72it/s, loss=0.774, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 18/19 [00:00<00:00, 43.86it/s, loss=0.806, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:00<00:00, 43.94it/s, loss=0.806, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:00<00:00, 43.81it/s, loss=0.772, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:00<00:00, 43.84it/s, loss=0.772, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:00<00:00, 41.51it/s, loss=0.762, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:00<00:00, 41.60it/s, loss=0.762, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:00<00:00, 41.64it/s, loss=0.763, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 19/19 [00:00<00:00, 41.59it/s, loss=0.763, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 18/19 [00:00<00:00, 44.01it/s, loss=0.763, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:00<00:00, 43.88it/s, loss=0.763, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:00<00:00, 40.09it/s, loss=0.749, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 19/19 [00:00<00:00, 40.40it/s, loss=0.749, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 18/19 [00:00<00:00, 39.18it/s, loss=0.761, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 19/19 [00:00<00:00, 39.40it/s, loss=0.761, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 18/19 [00:00<00:00, 41.72it/s, loss=0.763, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 19/19 [00:00<00:00, 41.90it/s, loss=0.763, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 18/19 [00:00<00:00, 43.02it/s, loss=0.76, v_num=b9iv, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 19/19 [00:00<00:00, 43.09it/s, loss=0.76, v_num=b9iv, LTC_val_\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42:  95%|▉| 18/19 [00:00<00:00, 43.06it/s, loss=0.746, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 19/19 [00:00<00:00, 42.53it/s, loss=0.746, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 18/19 [00:00<00:00, 40.65it/s, loss=0.738, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 19/19 [00:00<00:00, 40.20it/s, loss=0.738, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 44:  95%|▉| 18/19 [00:00<00:00, 42.68it/s, loss=0.75, v_num=b9iv, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 19/19 [00:00<00:00, 42.83it/s, loss=0.75, v_num=b9iv, LTC_val_\u001b[A\n",
      "Epoch 45:  95%|▉| 18/19 [00:00<00:00, 43.92it/s, loss=0.754, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 19/19 [00:00<00:00, 43.51it/s, loss=0.754, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 18/19 [00:00<00:00, 42.39it/s, loss=0.744, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 19/19 [00:00<00:00, 40.82it/s, loss=0.744, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 18/19 [00:00<00:00, 43.75it/s, loss=0.748, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 19/19 [00:00<00:00, 43.84it/s, loss=0.748, v_num=b9iv, LTC_val\u001b[A\n",
      "Epoch 48:  95%|▉| 18/19 [00:00<00:00, 42.65it/s, loss=0.761, v_num=b9iv, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.585. Signaling Trainer to stop.\n",
      "Epoch 48: 100%|█| 19/19 [00:00<00:00, 42.66it/s, loss=0.761, v_num=b9iv, LTC_val\n",
      "Epoch 48: 100%|█| 19/19 [00:00<00:00, 42.45it/s, loss=0.761, v_num=b9iv, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 91.94it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.5666666626930237,\n",
      " 'LTC_test_f1': 0.42473119497299194,\n",
      " 'test_loss': 0.8801459074020386}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 104536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104225-3fdub9iv/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104225-3fdub9iv/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.637\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.63113\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.75897\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 48\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 882\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625902975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 115\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.58333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.6609\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.60938\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.6087\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.87117\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.56667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.42473\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.88015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▂▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██▇███▇██▇████████▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▂▃▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇▇▇██▇████████▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▇▆▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▂▁▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▃▁▅▅▆▅▅▅▅▅▅█▅▆▅▆▆▆▆▆▇▇▇▇▇▅▅▅▆▆▆▇▅▆▅▆▆▅▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▂▁▅▅▇▅▆▆▅▅▅█▅▆▅▆▇▆▇▇▇▇▇▇▇▅▅▅▆▇▆▇▆▇▆▆▇▅▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ██▇▅▅▄▃▃▄▃▃▄▃▄▂▂▁▂▂▁▁▂▁▁▂▂▂▂▁▂▂▁▂▂▃▁▂▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▂▃▆▃▇▅▇▅▇▆▆▅█▆▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▁▃▆▃▇▅▇▅▇▆▇▅█▆▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▇▅▄▆▂▅▄▄▄▅▃▄▁▄▃▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/3fdub9iv\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:43:11.929691: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/34uufrvo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104310-34uufrvo\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 109 K \n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.532     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 26.47it/s, loss=0.713, v_num=frvo, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.720\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 26.59it/s, loss=0.713, v_num=frvo, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 25.61it/s, loss=0.703, v_num=frvo, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 0.709\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 25.67it/s, loss=0.703, v_num=frvo, BTC_val_\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 26.02it/s, loss=0.698, v_num=frvo, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 25.72it/s, loss=0.698, v_num=frvo, BTC_val_\u001b[A\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 26.01it/s, loss=0.699, v_num=frvo, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 26.07it/s, loss=0.699, v_num=frvo, BTC_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 25.83it/s, loss=0.696, v_num=frvo, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.701\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 25.89it/s, loss=0.696, v_num=frvo, BTC_val_\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 24.94it/s, loss=0.687, v_num=frvo, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 24.99it/s, loss=0.687, v_num=frvo, BTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 25.44it/s, loss=0.689, v_num=frvo, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 0.680\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 25.15it/s, loss=0.689, v_num=frvo, BTC_val_\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 25.85it/s, loss=0.692, v_num=frvo, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 25.67it/s, loss=0.692, v_num=frvo, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 26.29it/s, loss=0.677, v_num=frvo, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 26.33it/s, loss=0.677, v_num=frvo, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 23.01it/s, loss=0.672, v_num=frvo, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 22.72it/s, loss=0.672, v_num=frvo, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 25.99it/s, loss=0.676, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 25.98it/s, loss=0.676, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 25.46it/s, loss=0.647, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 25.52it/s, loss=0.647, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 24.74it/s, loss=0.615, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 24.87it/s, loss=0.615, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 25.86it/s, loss=0.582, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.035 >= min_delta = 0.003. New best score: 0.645\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 25.84it/s, loss=0.582, v_num=frvo, BTC_val\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 24.72it/s, loss=0.544, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.050 >= min_delta = 0.003. New best score: 0.595\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 24.82it/s, loss=0.544, v_num=frvo, BTC_val\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 25.40it/s, loss=0.545, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.057 >= min_delta = 0.003. New best score: 0.539\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 25.26it/s, loss=0.545, v_num=frvo, BTC_val\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 25.04it/s, loss=0.5, v_num=frvo, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 24.93it/s, loss=0.5, v_num=frvo, BTC_val_a\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 25.44it/s, loss=0.493, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.040 >= min_delta = 0.003. New best score: 0.498\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 25.30it/s, loss=0.493, v_num=frvo, BTC_val\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 25.29it/s, loss=0.496, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 25.07it/s, loss=0.496, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 24.87it/s, loss=0.474, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 24.73it/s, loss=0.474, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 25.99it/s, loss=0.471, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 25.99it/s, loss=0.471, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 25.43it/s, loss=0.456, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 25.49it/s, loss=0.456, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 25.53it/s, loss=0.443, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 25.74it/s, loss=0.443, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 24.55it/s, loss=0.441, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 24.41it/s, loss=0.441, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 25.34it/s, loss=0.437, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 25.48it/s, loss=0.437, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 25.11it/s, loss=0.433, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 25.22it/s, loss=0.433, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 24.66it/s, loss=0.425, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 24.63it/s, loss=0.425, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 24.83it/s, loss=0.405, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 24.92it/s, loss=0.405, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 25.26it/s, loss=0.418, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 25.38it/s, loss=0.418, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 24.97it/s, loss=0.416, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 24.85it/s, loss=0.416, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 24.83it/s, loss=0.404, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 24.88it/s, loss=0.404, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 25.52it/s, loss=0.398, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 25.56it/s, loss=0.398, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 25.75it/s, loss=0.379, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 25.56it/s, loss=0.379, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 24.77it/s, loss=0.373, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 24.87it/s, loss=0.373, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 25.15it/s, loss=0.377, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 25.03it/s, loss=0.377, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 24.84it/s, loss=0.379, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 24.66it/s, loss=0.379, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 26.09it/s, loss=0.364, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 26.10it/s, loss=0.364, v_num=frvo, BTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 25.14it/s, loss=0.356, v_num=frvo, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.498. Signaling Trainer to stop.\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 25.16it/s, loss=0.356, v_num=frvo, BTC_val\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 25.07it/s, loss=0.356, v_num=frvo, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 40.52it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6666666865348816,\n",
      " 'BTC_test_f1': 0.6458536386489868,\n",
      " 'ETH_test_acc': 0.6666666865348816,\n",
      " 'ETH_test_f1': 0.661696195602417,\n",
      " 'test_loss': 0.5624074935913086}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 104711\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104310-34uufrvo/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104310-34uufrvo/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.82975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.82805\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.86794\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.86625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.35547\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 37\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 760\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625903030\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 91\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.46154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.46154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.76923\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.76364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.56032\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.85938\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.85767\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.84375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.8359\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.41434\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.64585\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.6617\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.56241\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▁▂▁▁▂▂▂▃▃▂▃▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇█▇▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▁▂▁▁▂▂▂▃▃▂▃▄▅▆▆▆▆▆▆▇▇▇▇▇▇▇█▇▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▁▁▁▂▂▂▁▂▃▃▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇██▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▁▁▂▂▂▂▂▂▃▃▃▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇██▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █████▇██▇▇▇▇▆▅▅▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▃▁▁▁▁▁▃▃▃█▁█▃▆▃▆█▁▆▆▆▃█▆▆▆▃▆▁▃█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▂▁▁▁▁▁▂▂▂▇▁█▂▅▂▅█▄▅▆▆▅▇▇▆▇▅▇▁▅█▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▂▂▂▂▂▂▁▂▂▂▂▂▃▅██▂▆▂██▆▆▆█▆█▅█▃█▃▇▆▃▅▃▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▄▅██▁▆▃██▆▆▆█▆█▅█▃█▄▇▆▃▅▃▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▃▃▃▃▃▃▂▂▃▃▃▄▃▂▂▁▅▁▂▁▂▂▁▁▁▁▁▂▁▃▁▂▁▂█▂▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▂▄▁▅▄▆▇▅▆▅█▅▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▂▄▁▄▄▆▇▅▆▅█▅▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▃▃▃▄▆██▅█▇▆███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▃▃▂▄▆██▅█▇▆▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▇▇▇▇▅▂▃▄▂▁▅▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/34uufrvo\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:44:08.070354: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/2uq6m291\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104406-2uq6m291\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 109 K \n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.532     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 25.30it/s, loss=1.12, v_num=m291, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.207\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 25.37it/s, loss=1.12, v_num=m291, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 25.56it/s, loss=1.1, v_num=m291, BTC_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 25.53it/s, loss=1.1, v_num=m291, BTC_val_ac\u001b[A\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 26.44it/s, loss=1.08, v_num=m291, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 26.24it/s, loss=1.08, v_num=m291, BTC_val_a\u001b[A\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 25.65it/s, loss=1.08, v_num=m291, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 25.51it/s, loss=1.08, v_num=m291, BTC_val_a\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 24.71it/s, loss=1.08, v_num=m291, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 1.204\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 24.74it/s, loss=1.08, v_num=m291, BTC_val_a\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 25.22it/s, loss=1.08, v_num=m291, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 1.194\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 25.31it/s, loss=1.08, v_num=m291, BTC_val_a\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 24.34it/s, loss=1.06, v_num=m291, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 1.186\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 23.64it/s, loss=1.06, v_num=m291, BTC_val_a\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 25.47it/s, loss=1.06, v_num=m291, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 25.33it/s, loss=1.06, v_num=m291, BTC_val_a\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 22.88it/s, loss=1.05, v_num=m291, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 22.44it/s, loss=1.05, v_num=m291, BTC_val_a\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 24.12it/s, loss=1.04, v_num=m291, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 1.182\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 24.10it/s, loss=1.04, v_num=m291, BTC_val_a\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 24.18it/s, loss=1.03, v_num=m291, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 23.68it/s, loss=1.03, v_num=m291, BTC_val_\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 23.54it/s, loss=1, v_num=m291, BTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 23.61it/s, loss=1, v_num=m291, BTC_val_acc\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 26.09it/s, loss=0.969, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.028 >= min_delta = 0.003. New best score: 1.154\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 26.01it/s, loss=0.969, v_num=m291, BTC_val\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 24.42it/s, loss=0.92, v_num=m291, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 23.84it/s, loss=0.92, v_num=m291, BTC_val_\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 24.43it/s, loss=0.87, v_num=m291, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 23.93it/s, loss=0.87, v_num=m291, BTC_val_\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 23.81it/s, loss=0.861, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 1.142\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 23.84it/s, loss=0.861, v_num=m291, BTC_val\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 23.85it/s, loss=0.824, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.096 >= min_delta = 0.003. New best score: 1.047\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 23.94it/s, loss=0.824, v_num=m291, BTC_val\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 24.52it/s, loss=0.81, v_num=m291, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.078 >= min_delta = 0.003. New best score: 0.969\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 24.56it/s, loss=0.81, v_num=m291, BTC_val_\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 24.58it/s, loss=0.792, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 24.10it/s, loss=0.792, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 25.54it/s, loss=0.786, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.062 >= min_delta = 0.003. New best score: 0.907\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 25.52it/s, loss=0.786, v_num=m291, BTC_val\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 25.22it/s, loss=0.786, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.026 >= min_delta = 0.003. New best score: 0.881\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 25.30it/s, loss=0.786, v_num=m291, BTC_val\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 23.75it/s, loss=0.743, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 23.29it/s, loss=0.743, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 24.00it/s, loss=0.746, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 24.01it/s, loss=0.746, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 25.11it/s, loss=0.732, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 25.12it/s, loss=0.732, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 24.61it/s, loss=0.733, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 24.67it/s, loss=0.733, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 26.26it/s, loss=0.732, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 25.59it/s, loss=0.732, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 24.10it/s, loss=0.698, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 23.58it/s, loss=0.698, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 23.71it/s, loss=0.696, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 23.82it/s, loss=0.696, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 25.88it/s, loss=0.703, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 25.90it/s, loss=0.703, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 23.49it/s, loss=0.687, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 23.44it/s, loss=0.687, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 24.02it/s, loss=0.707, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 24.00it/s, loss=0.707, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 23.70it/s, loss=0.682, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 23.20it/s, loss=0.682, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 24.92it/s, loss=0.653, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 25.00it/s, loss=0.653, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 24.03it/s, loss=0.645, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 23.97it/s, loss=0.645, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 23.84it/s, loss=0.63, v_num=m291, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 23.32it/s, loss=0.63, v_num=m291, BTC_val_\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 24.11it/s, loss=0.644, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 23.57it/s, loss=0.644, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 23.58it/s, loss=0.635, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 22.94it/s, loss=0.635, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 23.38it/s, loss=0.606, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 23.48it/s, loss=0.606, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 20/21 [00:00<00:00, 23.90it/s, loss=0.618, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.029 >= min_delta = 0.003. New best score: 0.852\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 23.95it/s, loss=0.618, v_num=m291, BTC_val\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 25.02it/s, loss=0.613, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.083 >= min_delta = 0.003. New best score: 0.769\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 24.39it/s, loss=0.613, v_num=m291, BTC_val\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 23.77it/s, loss=0.598, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 23.69it/s, loss=0.598, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 20/21 [00:00<00:00, 24.60it/s, loss=0.588, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 24.63it/s, loss=0.588, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 20/21 [00:00<00:00, 23.00it/s, loss=0.58, v_num=m291, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 23.06it/s, loss=0.58, v_num=m291, BTC_val_\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 24.01it/s, loss=0.569, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 23.46it/s, loss=0.569, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 24.94it/s, loss=0.553, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 24.09it/s, loss=0.553, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 25.70it/s, loss=0.558, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 25.08it/s, loss=0.558, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 25.01it/s, loss=0.547, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 24.52it/s, loss=0.547, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 20/21 [00:00<00:00, 24.18it/s, loss=0.546, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 24.27it/s, loss=0.546, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 24.88it/s, loss=0.546, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 24.37it/s, loss=0.546, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 49:  95%|▉| 20/21 [00:00<00:00, 24.87it/s, loss=0.523, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 21/21 [00:00<00:00, 24.83it/s, loss=0.523, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 50: 100%|█| 21/21 [00:00<00:00, 24.01it/s, loss=0.516, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 21/21 [00:00<00:00, 23.50it/s, loss=0.516, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 51:  95%|▉| 20/21 [00:00<00:00, 24.58it/s, loss=0.5, v_num=m291, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 21/21 [00:00<00:00, 24.60it/s, loss=0.5, v_num=m291, BTC_val_a\u001b[A\n",
      "Epoch 52: 100%|█| 21/21 [00:00<00:00, 25.20it/s, loss=0.503, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 21/21 [00:00<00:00, 24.66it/s, loss=0.503, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 53: 100%|█| 21/21 [00:00<00:00, 25.43it/s, loss=0.501, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 21/21 [00:00<00:00, 24.72it/s, loss=0.501, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 54:  95%|▉| 20/21 [00:00<00:00, 24.49it/s, loss=0.485, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 21/21 [00:00<00:00, 24.55it/s, loss=0.485, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 55:  95%|▉| 20/21 [00:00<00:00, 24.78it/s, loss=0.482, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 21/21 [00:00<00:00, 24.88it/s, loss=0.482, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 56:  95%|▉| 20/21 [00:00<00:00, 24.97it/s, loss=0.472, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 21/21 [00:00<00:00, 24.77it/s, loss=0.472, v_num=m291, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57:  95%|▉| 20/21 [00:00<00:00, 24.49it/s, loss=0.447, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 21/21 [00:00<00:00, 24.20it/s, loss=0.447, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 58: 100%|█| 21/21 [00:00<00:00, 25.46it/s, loss=0.435, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 21/21 [00:00<00:00, 24.84it/s, loss=0.435, v_num=m291, BTC_val\u001b[A\n",
      "Epoch 59: 100%|█| 21/21 [00:00<00:00, 25.12it/s, loss=0.442, v_num=m291, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.769. Signaling Trainer to stop.\n",
      "Epoch 59: 100%|█| 21/21 [00:00<00:00, 24.57it/s, loss=0.442, v_num=m291, BTC_val\n",
      "Epoch 59: 100%|█| 21/21 [00:00<00:00, 24.46it/s, loss=0.442, v_num=m291, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 54.26it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5757575631141663,\n",
      " 'BTC_test_f1': 0.5727816224098206,\n",
      " 'ETH_test_acc': 0.6969696879386902,\n",
      " 'ETH_test_f1': 0.6633418202400208,\n",
      " 'test_loss': 0.8994266986846924}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 104905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104406-2uq6m291/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104406-2uq6m291/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.81543\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.81056\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.81305\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.8086\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.44371\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 59\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625903106\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 144\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.76923\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.7746\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.46154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.41481\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.07726\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.87805\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.88751\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.78049\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.78831\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.3664\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.57576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.57278\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.69697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.66334\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.89943\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▂▂▂▂▃▃▄▄▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▂▂▂▃▃▃▄▄▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▂▂▂▂▃▃▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ████▇▇▇▇▆▆▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▂▂▄▂▂▂▃▅▂▂▄▃▄▄▃▂▄▂▃▄▃▄▃▃▄▆▅▄▅▄▄▄▅▄▃▆▅▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▃▁▁▁▂▄▁▂▄▃▄▄▃▁▄▂▃▄▃▄▃▃▄▆▅▄▅▄▅▅▅▄▃▆▅▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▁▁▂▃▃▁▂▃▃▃▄▄▆▆▃▁▆▄▆▆▃▆▃▃▆█▇▆▆▇▆▇▇▇▇█▇▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▁▁▂▂▁▃▂▂▄▅▅▅▅▂▁▅▅▅▅▄▅▄▄▅█▇▆▅▇▆▇▇▇▇█▇▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄▄▄▄▃▄▃▃▃▃▃▃▃▂▂▄█▂▄▃▂▆▃▇▇▂▁▂▄▂▃█▄▃▄▄▃▅▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▂▁▂▃▄▅▇▅▄▆▅▄▅▆▅▆▆▆▇▇▆███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▁▂▃▃▄▇▅▄▆▆▄▅▆▅▆▆▅▇▆▆███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▁▂▃▄▃▅▅▄▅▅▆▆▆▅█▆▇▇▇▇▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▁▂▃▄▂▅▅▄▅▅▅▆▆▅█▇▇▇▇▇▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇█▇▇▆▆▄▅▆▄▆▄▃▃▄▂▃▂▂▁▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/2uq6m291\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:45:24.293383: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/bltdy27d\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104522-bltdy27d\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 109 K \n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.532     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 25.78it/s, loss=0.707, v_num=y27d, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.690\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 25.23it/s, loss=0.707, v_num=y27d, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 25.27it/s, loss=0.702, v_num=y27d, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 24.68it/s, loss=0.702, v_num=y27d, BTC_val_\u001b[A\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 22.87it/s, loss=0.695, v_num=y27d, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 22.93it/s, loss=0.695, v_num=y27d, BTC_val_\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 23.46it/s, loss=0.692, v_num=y27d, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 23.00it/s, loss=0.692, v_num=y27d, BTC_val_\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 24.25it/s, loss=0.696, v_num=y27d, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 23.77it/s, loss=0.696, v_num=y27d, BTC_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 24.36it/s, loss=0.689, v_num=y27d, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.681\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 24.37it/s, loss=0.689, v_num=y27d, BTC_val_\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 23.45it/s, loss=0.684, v_num=y27d, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 23.02it/s, loss=0.684, v_num=y27d, BTC_val_\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 23.40it/s, loss=0.683, v_num=y27d, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 22.73it/s, loss=0.683, v_num=y27d, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 22.79it/s, loss=0.678, v_num=y27d, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 0.669\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 22.90it/s, loss=0.678, v_num=y27d, BTC_val_\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 23.20it/s, loss=0.666, v_num=y27d, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 22.73it/s, loss=0.666, v_num=y27d, BTC_val_\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 24.97it/s, loss=0.652, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 24.39it/s, loss=0.652, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 23.45it/s, loss=0.616, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 23.42it/s, loss=0.616, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 23.79it/s, loss=0.602, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 23.73it/s, loss=0.602, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 24.16it/s, loss=0.56, v_num=y27d, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.082 >= min_delta = 0.003. New best score: 0.586\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 24.06it/s, loss=0.56, v_num=y27d, BTC_val_\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 25.75it/s, loss=0.532, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 25.68it/s, loss=0.532, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 24.97it/s, loss=0.511, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 24.90it/s, loss=0.511, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 25.17it/s, loss=0.504, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 24.64it/s, loss=0.504, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 25.26it/s, loss=0.491, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 24.71it/s, loss=0.491, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 25.51it/s, loss=0.468, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.026 >= min_delta = 0.003. New best score: 0.560\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 24.97it/s, loss=0.468, v_num=y27d, BTC_val\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 23.73it/s, loss=0.46, v_num=y27d, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 23.59it/s, loss=0.46, v_num=y27d, BTC_val_\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 23.89it/s, loss=0.449, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 23.43it/s, loss=0.449, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 24.40it/s, loss=0.46, v_num=y27d, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 24.46it/s, loss=0.46, v_num=y27d, BTC_val_\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 24.88it/s, loss=0.462, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 24.24it/s, loss=0.462, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 25.12it/s, loss=0.442, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 24.49it/s, loss=0.442, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 24.32it/s, loss=0.431, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.552\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 24.33it/s, loss=0.431, v_num=y27d, BTC_val\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 25.15it/s, loss=0.421, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.548\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 24.61it/s, loss=0.421, v_num=y27d, BTC_val\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 24.27it/s, loss=0.429, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 24.26it/s, loss=0.429, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 25.48it/s, loss=0.417, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 25.06it/s, loss=0.417, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 24.50it/s, loss=0.399, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 24.44it/s, loss=0.399, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 24.86it/s, loss=0.414, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 24.91it/s, loss=0.414, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 25.09it/s, loss=0.386, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 24.90it/s, loss=0.386, v_num=y27d, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 25.01it/s, loss=0.395, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 24.42it/s, loss=0.395, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 24.45it/s, loss=0.405, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 24.47it/s, loss=0.405, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 24.44it/s, loss=0.395, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.543\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 23.81it/s, loss=0.395, v_num=y27d, BTC_val\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 24.99it/s, loss=0.372, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 24.46it/s, loss=0.372, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 25.11it/s, loss=0.379, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 24.58it/s, loss=0.379, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 25.44it/s, loss=0.367, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 24.89it/s, loss=0.367, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 24.73it/s, loss=0.355, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 24.67it/s, loss=0.355, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 20/21 [00:00<00:00, 24.50it/s, loss=0.344, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 24.61it/s, loss=0.344, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 20/21 [00:00<00:00, 24.84it/s, loss=0.345, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 24.69it/s, loss=0.345, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 23.99it/s, loss=0.336, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.531\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 23.44it/s, loss=0.336, v_num=y27d, BTC_val\n",
      "Epoch 41:  95%|▉| 20/21 [00:00<00:00, 24.89it/s, loss=0.344, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 24.93it/s, loss=0.344, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 20/21 [00:00<00:00, 24.63it/s, loss=0.325, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 24.69it/s, loss=0.325, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 24.29it/s, loss=0.316, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 23.74it/s, loss=0.316, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 25.73it/s, loss=0.304, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 24.99it/s, loss=0.304, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 45:  95%|▉| 20/21 [00:00<00:00, 24.14it/s, loss=0.317, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 24.21it/s, loss=0.317, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 20/21 [00:00<00:00, 24.62it/s, loss=0.296, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.023 >= min_delta = 0.003. New best score: 0.508\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 24.46it/s, loss=0.296, v_num=y27d, BTC_val\n",
      "Epoch 47:  95%|▉| 20/21 [00:00<00:00, 25.42it/s, loss=0.288, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 25.40it/s, loss=0.288, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 48:  95%|▉| 20/21 [00:00<00:00, 24.47it/s, loss=0.288, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 24.53it/s, loss=0.288, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 49: 100%|█| 21/21 [00:00<00:00, 25.16it/s, loss=0.27, v_num=y27d, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 21/21 [00:00<00:00, 24.62it/s, loss=0.27, v_num=y27d, BTC_val_\u001b[A\n",
      "Epoch 50: 100%|█| 21/21 [00:00<00:00, 24.81it/s, loss=0.277, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 21/21 [00:00<00:00, 24.23it/s, loss=0.277, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 51: 100%|█| 21/21 [00:00<00:00, 25.44it/s, loss=0.254, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 21/21 [00:00<00:00, 24.89it/s, loss=0.254, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 52:  95%|▉| 20/21 [00:00<00:00, 24.11it/s, loss=0.262, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 21/21 [00:00<00:00, 24.28it/s, loss=0.262, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 53: 100%|█| 21/21 [00:00<00:00, 25.43it/s, loss=0.252, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 21/21 [00:00<00:00, 24.88it/s, loss=0.252, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 54: 100%|█| 21/21 [00:00<00:00, 25.02it/s, loss=0.253, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 21/21 [00:00<00:00, 24.46it/s, loss=0.253, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 55:  95%|▉| 20/21 [00:00<00:00, 23.87it/s, loss=0.233, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 21/21 [00:00<00:00, 23.95it/s, loss=0.233, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 56:  95%|▉| 20/21 [00:00<00:00, 24.74it/s, loss=0.245, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 21/21 [00:00<00:00, 24.77it/s, loss=0.245, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 57: 100%|█| 21/21 [00:00<00:00, 24.35it/s, loss=0.227, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 21/21 [00:00<00:00, 23.85it/s, loss=0.227, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 58:  95%|▉| 20/21 [00:00<00:00, 24.61it/s, loss=0.218, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 21/21 [00:00<00:00, 24.52it/s, loss=0.218, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 59: 100%|█| 21/21 [00:00<00:00, 24.71it/s, loss=0.209, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|█| 21/21 [00:00<00:00, 24.22it/s, loss=0.209, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 60: 100%|█| 21/21 [00:00<00:00, 25.01it/s, loss=0.197, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 60: 100%|█| 21/21 [00:00<00:00, 24.46it/s, loss=0.197, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 61:  95%|▉| 20/21 [00:00<00:00, 24.62it/s, loss=0.191, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|█| 21/21 [00:00<00:00, 24.69it/s, loss=0.191, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 62: 100%|█| 21/21 [00:00<00:00, 24.50it/s, loss=0.185, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 62: 100%|█| 21/21 [00:00<00:00, 24.01it/s, loss=0.185, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 63:  95%|▉| 20/21 [00:00<00:00, 24.76it/s, loss=0.174, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 63: 100%|█| 21/21 [00:00<00:00, 24.73it/s, loss=0.174, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 64: 100%|█| 21/21 [00:00<00:00, 24.97it/s, loss=0.194, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 64: 100%|█| 21/21 [00:00<00:00, 24.45it/s, loss=0.194, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 65:  95%|▉| 20/21 [00:00<00:00, 24.60it/s, loss=0.171, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 65: 100%|█| 21/21 [00:00<00:00, 24.60it/s, loss=0.171, v_num=y27d, BTC_val\u001b[A\n",
      "Epoch 66: 100%|█| 21/21 [00:00<00:00, 25.55it/s, loss=0.183, v_num=y27d, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.508. Signaling Trainer to stop.\n",
      "Epoch 66: 100%|█| 21/21 [00:00<00:00, 25.00it/s, loss=0.183, v_num=y27d, BTC_val\n",
      "Epoch 66: 100%|█| 21/21 [00:00<00:00, 24.91it/s, loss=0.183, v_num=y27d, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 52.95it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5757575631141663,\n",
      " 'BTC_test_f1': 0.5722222328186035,\n",
      " 'ETH_test_acc': 0.6969696879386902,\n",
      " 'ETH_test_f1': 0.6590908765792847,\n",
      " 'test_loss': 0.6439937949180603}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 105163\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104522-bltdy27d/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104522-bltdy27d/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.91567\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.91438\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.92761\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.92636\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.18139\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 66\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1340\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 66\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625903189\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.69231\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.675\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.84615\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.8375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.8067\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.92683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.92683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.145\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.57576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.57222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.69697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.65909\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.64399\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▁▂▂▂▂▃▄▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▁▂▂▂▂▃▄▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▁▁▂▂▂▃▃▄▅▅▆▆▆▆▆▆▆▇▆▆▆▇▇▇▇▇▇▇▇▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▁▁▁▁▂▃▃▄▅▅▆▆▆▆▆▆▆▇▆▆▆▇▇▇▇▇▇▇▇▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ██████▇▇▆▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▃▁▁▃▃▃▃▃▃▁█▆▁█▃▃▁▆▃▁█▆▃▆▁▆▃▃▁▁█▆▃▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▄▁▂▁▁▅▅▅▄▄▄▁█▆▁█▅▅▄▆▅▄█▇▅▇▄▇▅▅▄▄█▇▅▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▁▁▁▁▅▁▁█▂▄▇▁▁▁▅▅▁█▅█▇█▇▅▄▇▄▇▇▄▄▇▇▇▅▅▅▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▁▁▁▆▁▁█▃▅▇▁▁▁▆▆▁█▆█▇█▇▆▅▇▅▇▇▅▅▇▇▇▆▆▆▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▂▃▂▂▂▂▃▄▂▂▂▁▆▃▅▁▂█▁▂▁▄▂▂▁▃▄▁▁▁▂▂▄▃▁▃▂▅▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▂▂▁▃▃▄▅▄▅▅▅▆▅▅▆▅▆▆▅▄▆▇▆▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▂▂▁▃▃▄▅▄▅▅▅▆▅▅▆▅▆▆▅▅▆▇▆▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▂▁▄▃▅▆▆▆▇▆▆▆▆▇▆▇▇▇▆▆██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▂▁▄▃▅▆▆▆▇▆▆▆▆▇▆▇▇▇▆▆██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ████▇▆▅▅▄▆▄▄▅▄▄▄▃▃▄▄▂▃▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/bltdy27d\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:46:46.669612: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/pz6ryfrv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104645-pz6ryfrv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 109 K \n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.532     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 26.75it/s, loss=1.15, v_num=yfrv, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.173\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 26.61it/s, loss=1.15, v_num=yfrv, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 26.33it/s, loss=1.1, v_num=yfrv, BTC_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 26.36it/s, loss=1.1, v_num=yfrv, BTC_val_ac\u001b[A\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 25.00it/s, loss=1.08, v_num=yfrv, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 24.68it/s, loss=1.08, v_num=yfrv, BTC_val_a\u001b[A\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 24.75it/s, loss=1.08, v_num=yfrv, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 24.65it/s, loss=1.08, v_num=yfrv, BTC_val_a\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 25.40it/s, loss=1.08, v_num=yfrv, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 25.23it/s, loss=1.08, v_num=yfrv, BTC_val_a\u001b[A\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 24.05it/s, loss=1.08, v_num=yfrv, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 24.11it/s, loss=1.08, v_num=yfrv, BTC_val_a\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 26.04it/s, loss=1.06, v_num=yfrv, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 25.23it/s, loss=1.06, v_num=yfrv, BTC_val_a\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 25.63it/s, loss=1.07, v_num=yfrv, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 25.70it/s, loss=1.07, v_num=yfrv, BTC_val_a\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 22.82it/s, loss=1.07, v_num=yfrv, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 22.94it/s, loss=1.07, v_num=yfrv, BTC_val_a\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 25.21it/s, loss=1.05, v_num=yfrv, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 25.23it/s, loss=1.05, v_num=yfrv, BTC_val_a\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 25.09it/s, loss=1.04, v_num=yfrv, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 25.17it/s, loss=1.04, v_num=yfrv, BTC_val_\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 25.27it/s, loss=1.03, v_num=yfrv, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 25.45it/s, loss=1.03, v_num=yfrv, BTC_val_\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 25.54it/s, loss=1.01, v_num=yfrv, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 25.66it/s, loss=1.01, v_num=yfrv, BTC_val_\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 25.26it/s, loss=0.954, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 25.30it/s, loss=0.954, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 24.62it/s, loss=0.92, v_num=yfrv, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.035 >= min_delta = 0.003. New best score: 1.139\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 24.74it/s, loss=0.92, v_num=yfrv, BTC_val_\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 25.91it/s, loss=0.861, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 25.78it/s, loss=0.861, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 25.32it/s, loss=0.845, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 25.42it/s, loss=0.845, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 25.93it/s, loss=0.82, v_num=yfrv, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 25.39it/s, loss=0.82, v_num=yfrv, BTC_val_\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 25.52it/s, loss=0.829, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 25.50it/s, loss=0.829, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 23.88it/s, loss=0.801, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 23.93it/s, loss=0.801, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 24.67it/s, loss=0.769, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 24.63it/s, loss=0.769, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 25.17it/s, loss=0.758, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.126 >= min_delta = 0.003. New best score: 1.013\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 25.30it/s, loss=0.758, v_num=yfrv, BTC_val\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 24.65it/s, loss=0.738, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 24.63it/s, loss=0.738, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 24.68it/s, loss=0.715, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 24.66it/s, loss=0.715, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 24.73it/s, loss=0.748, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 24.70it/s, loss=0.748, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 25.64it/s, loss=0.72, v_num=yfrv, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.066 >= min_delta = 0.003. New best score: 0.947\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 24.96it/s, loss=0.72, v_num=yfrv, BTC_val_\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 25.99it/s, loss=0.715, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.148 >= min_delta = 0.003. New best score: 0.799\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 25.85it/s, loss=0.715, v_num=yfrv, BTC_val\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 25.87it/s, loss=0.739, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 25.21it/s, loss=0.739, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 24.79it/s, loss=0.708, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 24.87it/s, loss=0.708, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 24.66it/s, loss=0.681, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 24.70it/s, loss=0.681, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 25.22it/s, loss=0.692, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 25.30it/s, loss=0.692, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 25.26it/s, loss=0.662, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 25.02it/s, loss=0.662, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 24.15it/s, loss=0.674, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 24.21it/s, loss=0.674, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 26.45it/s, loss=0.645, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 26.45it/s, loss=0.645, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 25.89it/s, loss=0.643, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 25.48it/s, loss=0.643, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 24.91it/s, loss=0.654, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 25.00it/s, loss=0.654, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 24.99it/s, loss=0.639, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 25.04it/s, loss=0.639, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 24.63it/s, loss=0.617, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 24.76it/s, loss=0.617, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 25.20it/s, loss=0.625, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 24.67it/s, loss=0.625, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 25.18it/s, loss=0.615, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 24.50it/s, loss=0.615, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 23.64it/s, loss=0.586, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 23.68it/s, loss=0.586, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 20/21 [00:00<00:00, 25.45it/s, loss=0.588, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 25.22it/s, loss=0.588, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 20/21 [00:00<00:00, 24.86it/s, loss=0.581, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 24.80it/s, loss=0.581, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 20/21 [00:00<00:00, 25.06it/s, loss=0.588, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 25.09it/s, loss=0.588, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 44:  95%|▉| 20/21 [00:00<00:00, 24.56it/s, loss=0.581, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 24.41it/s, loss=0.581, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 45:  95%|▉| 20/21 [00:00<00:00, 24.57it/s, loss=0.548, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 24.49it/s, loss=0.548, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 20/21 [00:00<00:00, 25.00it/s, loss=0.566, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.089 >= min_delta = 0.003. New best score: 0.710\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 25.07it/s, loss=0.566, v_num=yfrv, BTC_val\n",
      "Epoch 47:  95%|▉| 20/21 [00:00<00:00, 25.50it/s, loss=0.545, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 25.50it/s, loss=0.545, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 25.65it/s, loss=0.538, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 25.07it/s, loss=0.538, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 49:  95%|▉| 20/21 [00:00<00:00, 25.05it/s, loss=0.535, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 21/21 [00:00<00:00, 25.15it/s, loss=0.535, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 50:  95%|▉| 20/21 [00:00<00:00, 24.97it/s, loss=0.514, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 21/21 [00:00<00:00, 25.11it/s, loss=0.514, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 51: 100%|█| 21/21 [00:00<00:00, 24.59it/s, loss=0.523, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 21/21 [00:00<00:00, 24.06it/s, loss=0.523, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 52: 100%|█| 21/21 [00:00<00:00, 24.41it/s, loss=0.514, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 21/21 [00:00<00:00, 23.90it/s, loss=0.514, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 53:  95%|▉| 20/21 [00:00<00:00, 24.80it/s, loss=0.485, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 21/21 [00:00<00:00, 24.86it/s, loss=0.485, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 54: 100%|█| 21/21 [00:00<00:00, 25.54it/s, loss=0.489, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 21/21 [00:00<00:00, 24.99it/s, loss=0.489, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 55:  95%|▉| 20/21 [00:00<00:00, 23.88it/s, loss=0.486, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 21/21 [00:00<00:00, 24.04it/s, loss=0.486, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 56:  95%|▉| 20/21 [00:00<00:00, 24.73it/s, loss=0.463, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 21/21 [00:00<00:00, 24.66it/s, loss=0.463, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 57: 100%|█| 21/21 [00:00<00:00, 24.53it/s, loss=0.448, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 21/21 [00:00<00:00, 24.04it/s, loss=0.448, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 58: 100%|█| 21/21 [00:00<00:00, 25.21it/s, loss=0.443, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 21/21 [00:00<00:00, 24.64it/s, loss=0.443, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 59:  95%|▉| 20/21 [00:00<00:00, 24.52it/s, loss=0.454, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|█| 21/21 [00:00<00:00, 24.56it/s, loss=0.454, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 60: 100%|█| 21/21 [00:00<00:00, 25.85it/s, loss=0.444, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 60: 100%|█| 21/21 [00:00<00:00, 25.32it/s, loss=0.444, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 61:  95%|▉| 20/21 [00:00<00:00, 25.18it/s, loss=0.434, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|█| 21/21 [00:00<00:00, 25.20it/s, loss=0.434, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 62: 100%|█| 21/21 [00:00<00:00, 25.29it/s, loss=0.419, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 62: 100%|█| 21/21 [00:00<00:00, 24.66it/s, loss=0.419, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 63: 100%|█| 21/21 [00:00<00:00, 25.52it/s, loss=0.402, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 63: 100%|█| 21/21 [00:00<00:00, 25.01it/s, loss=0.402, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 64:  95%|▉| 20/21 [00:00<00:00, 24.84it/s, loss=0.394, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 64: 100%|█| 21/21 [00:00<00:00, 24.92it/s, loss=0.394, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 65:  95%|▉| 20/21 [00:00<00:00, 24.29it/s, loss=0.405, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 65: 100%|█| 21/21 [00:00<00:00, 24.50it/s, loss=0.405, v_num=yfrv, BTC_val\u001b[A\n",
      "Epoch 66:  95%|▉| 20/21 [00:00<00:00, 25.29it/s, loss=0.387, v_num=yfrv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.710. Signaling Trainer to stop.\n",
      "Epoch 66: 100%|█| 21/21 [00:00<00:00, 25.33it/s, loss=0.387, v_num=yfrv, BTC_val\n",
      "Epoch 66: 100%|█| 21/21 [00:00<00:00, 25.23it/s, loss=0.387, v_num=yfrv, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 57.80it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5757575631141663,\n",
      " 'BTC_test_f1': 0.577189564704895,\n",
      " 'ETH_test_acc': 0.6363636255264282,\n",
      " 'ETH_test_f1': 0.6256667375564575,\n",
      " 'test_loss': 0.8655917644500732}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 105416\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104645-pz6ryfrv/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104645-pz6ryfrv/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.84646\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.83999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.83294\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.82752\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.38647\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 66\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1340\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625903269\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.76923\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.76435\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.69231\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.68889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.78099\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.97561\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.97616\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.82927\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.8246\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.32187\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.57576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.57719\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.63636\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.62567\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.86559\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▂▂▃▃▃▃▄▅▅▅▅▆▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇██▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▃▃▃▃▄▄▅▅▅▆▆▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▂▁▂▂▂▂▃▄▄▅▅▅▆▆▅▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▂▂▃▃▃▃▄▅▅▅▆▆▆▆▆▆▆▆▇▆▇▆▇▇▇▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ██▇▇▇▇▇▇▆▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▁▃▁▁▂▁▃▃▅▃▅▂▃▃▃▂▆▅▅▃▃▃▅▂▆█▇▇▆▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁▁▄▁▁▃▂▄▄▅▄▅▃▄▄▄▃▆▆▅▄▄▄▅▃▆█▇▇▆▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▂▃▃▃▃▃▃▃▁▃▃▃▄▄▅▅▄▅▅▅▅▅▆▅▆▆▅▆▅▅▆▇▆▆▇▅██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▃▂▂▂▂▂▂▂▁▂▂▂▄▄▅▅▄▅▅▅▅▅▆▅▆▆▅▆▅▅▆▇▆▆▇▅██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄▅▄▄▅▅▅▅▅▄▇█▅▄▄▂▃▂▂▃▃▄▃▄▂▃▄▄▂▂▃▄▂▂▂▂▂▂▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▄▂▂▂▅▅▃▄▃▄▃▅▄▅▆▅▅▆▆▅▆▇▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▄▂▂▂▅▄▃▄▃▄▂▅▄▅▆▅▅▆▆▅▆▇▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▃▃▃▁▃▄▆▆▇▅▆▆▅▆▆▆▆▆▆█▆▆▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▃▃▃▁▃▄▆▆▇▅▆▆▅▅▆▆▆▆▆█▆▆▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▇██▇▅▅▇▄▆▄▅▄▄▄▃▃▄▃▂▃▃▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/pz6ryfrv\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:48:06.529808: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/1lvio7ns\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104805-1lvio7ns\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 29.58it/s, loss=0.706, v_num=o7ns, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.681\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 28.86it/s, loss=0.706, v_num=o7ns, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 27.06it/s, loss=0.668, v_num=o7ns, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.091 >= min_delta = 0.003. New best score: 0.590\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 26.83it/s, loss=0.668, v_num=o7ns, BTC_val_\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 26.57it/s, loss=0.627, v_num=o7ns, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.073 >= min_delta = 0.003. New best score: 0.518\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 26.20it/s, loss=0.627, v_num=o7ns, BTC_val_\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 25.86it/s, loss=0.591, v_num=o7ns, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.018 >= min_delta = 0.003. New best score: 0.500\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 25.71it/s, loss=0.591, v_num=o7ns, BTC_val_\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 25.19it/s, loss=0.554, v_num=o7ns, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.045 >= min_delta = 0.003. New best score: 0.455\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 25.05it/s, loss=0.554, v_num=o7ns, BTC_val_\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 25.62it/s, loss=0.533, v_num=o7ns, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.447\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 25.64it/s, loss=0.533, v_num=o7ns, BTC_val_\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 25.72it/s, loss=0.525, v_num=o7ns, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 25.84it/s, loss=0.525, v_num=o7ns, BTC_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 25.31it/s, loss=0.527, v_num=o7ns, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 25.33it/s, loss=0.527, v_num=o7ns, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 27.13it/s, loss=0.51, v_num=o7ns, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 27.28it/s, loss=0.51, v_num=o7ns, BTC_val_a\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 27.47it/s, loss=0.487, v_num=o7ns, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 27.48it/s, loss=0.487, v_num=o7ns, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 23.74it/s, loss=0.484, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 23.52it/s, loss=0.484, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 25.65it/s, loss=0.481, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 25.72it/s, loss=0.481, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 27.41it/s, loss=0.495, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 27.17it/s, loss=0.495, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 26.21it/s, loss=0.475, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 26.07it/s, loss=0.475, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 25.93it/s, loss=0.472, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 25.80it/s, loss=0.472, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 25.74it/s, loss=0.478, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 25.61it/s, loss=0.478, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 26.69it/s, loss=0.469, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 26.70it/s, loss=0.469, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 26.19it/s, loss=0.488, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 26.29it/s, loss=0.488, v_num=o7ns, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 27.04it/s, loss=0.488, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 26.88it/s, loss=0.488, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 26.90it/s, loss=0.47, v_num=o7ns, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 26.70it/s, loss=0.47, v_num=o7ns, BTC_val_\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 25.57it/s, loss=0.465, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 25.61it/s, loss=0.465, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 25.65it/s, loss=0.478, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 25.64it/s, loss=0.478, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 25.88it/s, loss=0.469, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 0.435\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 25.63it/s, loss=0.469, v_num=o7ns, BTC_val\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 26.49it/s, loss=0.46, v_num=o7ns, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 26.53it/s, loss=0.46, v_num=o7ns, BTC_val_\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 26.33it/s, loss=0.466, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 26.28it/s, loss=0.466, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 26.65it/s, loss=0.467, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 26.56it/s, loss=0.467, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 27.27it/s, loss=0.455, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 27.24it/s, loss=0.455, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 26.60it/s, loss=0.473, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 26.46it/s, loss=0.473, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 26.11it/s, loss=0.455, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 26.02it/s, loss=0.455, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 26.41it/s, loss=0.453, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 26.36it/s, loss=0.453, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 27.63it/s, loss=0.467, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 27.40it/s, loss=0.467, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 26.62it/s, loss=0.455, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 26.31it/s, loss=0.455, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 26.83it/s, loss=0.454, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 26.45it/s, loss=0.454, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 26.49it/s, loss=0.459, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 26.22it/s, loss=0.459, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 26.50it/s, loss=0.461, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 26.50it/s, loss=0.461, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 26.37it/s, loss=0.463, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.430\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 26.39it/s, loss=0.463, v_num=o7ns, BTC_val\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 26.42it/s, loss=0.445, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 26.08it/s, loss=0.445, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 26.10it/s, loss=0.456, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 25.76it/s, loss=0.456, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 20/21 [00:00<00:00, 25.58it/s, loss=0.443, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 25.40it/s, loss=0.443, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 20/21 [00:00<00:00, 26.14it/s, loss=0.454, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 26.09it/s, loss=0.454, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 27.04it/s, loss=0.453, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 27.04it/s, loss=0.453, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 20/21 [00:00<00:00, 26.70it/s, loss=0.443, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 21/21 [00:00<00:00, 26.37it/s, loss=0.443, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 20/21 [00:00<00:00, 26.13it/s, loss=0.45, v_num=o7ns, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 21/21 [00:00<00:00, 26.12it/s, loss=0.45, v_num=o7ns, BTC_val_\u001b[A\n",
      "Epoch 43:  95%|▉| 20/21 [00:00<00:00, 25.75it/s, loss=0.45, v_num=o7ns, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 21/21 [00:00<00:00, 25.85it/s, loss=0.45, v_num=o7ns, BTC_val_\u001b[A\n",
      "Epoch 44:  95%|▉| 20/21 [00:00<00:00, 26.50it/s, loss=0.446, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 21/21 [00:00<00:00, 26.23it/s, loss=0.446, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 45:  95%|▉| 20/21 [00:00<00:00, 26.23it/s, loss=0.441, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 21/21 [00:00<00:00, 26.23it/s, loss=0.441, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 20/21 [00:00<00:00, 25.61it/s, loss=0.434, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 21/21 [00:00<00:00, 25.69it/s, loss=0.434, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 20/21 [00:00<00:00, 25.82it/s, loss=0.439, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 21/21 [00:00<00:00, 25.08it/s, loss=0.439, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 48:  95%|▉| 20/21 [00:00<00:00, 26.06it/s, loss=0.438, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 21/21 [00:00<00:00, 26.08it/s, loss=0.438, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 49:  95%|▉| 20/21 [00:00<00:00, 25.99it/s, loss=0.431, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 21/21 [00:00<00:00, 25.99it/s, loss=0.431, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 50:  95%|▉| 20/21 [00:00<00:00, 26.33it/s, loss=0.442, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 21/21 [00:00<00:00, 26.33it/s, loss=0.442, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 51:  95%|▉| 20/21 [00:00<00:00, 26.64it/s, loss=0.44, v_num=o7ns, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 21/21 [00:00<00:00, 26.62it/s, loss=0.44, v_num=o7ns, BTC_val_\u001b[A\n",
      "Epoch 52:  95%|▉| 20/21 [00:00<00:00, 27.28it/s, loss=0.441, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 21/21 [00:00<00:00, 26.99it/s, loss=0.441, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 53:  95%|▉| 20/21 [00:00<00:00, 24.15it/s, loss=0.43, v_num=o7ns, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 21/21 [00:00<00:00, 24.04it/s, loss=0.43, v_num=o7ns, BTC_val_\u001b[A\n",
      "Epoch 54:  95%|▉| 20/21 [00:00<00:00, 25.15it/s, loss=0.424, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 21/21 [00:00<00:00, 25.00it/s, loss=0.424, v_num=o7ns, BTC_val\u001b[A\n",
      "Epoch 55:  95%|▉| 20/21 [00:00<00:00, 25.95it/s, loss=0.443, v_num=o7ns, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.430. Signaling Trainer to stop.\n",
      "Epoch 55: 100%|█| 21/21 [00:00<00:00, 25.99it/s, loss=0.443, v_num=o7ns, BTC_val\n",
      "Epoch 55: 100%|█| 21/21 [00:00<00:00, 25.86it/s, loss=0.443, v_num=o7ns, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 39.20it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6969696879386902,\n",
      " 'BTC_test_f1': 0.6898496747016907,\n",
      " 'ETH_test_acc': 0.7272727489471436,\n",
      " 'ETH_test_f1': 0.7179487347602844,\n",
      " 'test_loss': 0.5354040265083313}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 105630\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104805-1lvio7ns/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104805-1lvio7ns/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.80668\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.80248\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.79634\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.79313\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.44344\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 55\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1120\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625903338\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 134\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.61538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.60606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.92308\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.92121\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.47248\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.85366\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.8373\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.82927\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.7986\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.37952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.69697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.68985\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.72727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.71795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.5354\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▃▄▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███▇███▇████▇█████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▃▄▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████▇██████████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▃▅▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇██▇▇███▇██████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▄▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██▇▇███▇██████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▄▄▄▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁████▃███▆▆█▆████▆█▆██▆▆█▆▆███▆█▆▆▆▆█▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁████▅███▇▆█▆████▆█▆██▆▆█▆▆███▇█▆▆▆▆█▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▇███▅██████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▇███▆██████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▅▃▂▁▃▂▂▄▃▂▂▄▂▂▂▁▂▂▁▁▁▂▂▁▁▂▂▄▂▄▁▁▂▂▂▃▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▆▄▄▆▆▅█▄▂▄▅▆▁▅▆▅▅▃▅▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▆▃▃▆▆▅█▄▂▄▅▆▁▅▆▅▆▃▆▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▂▅▁▇▂▃▃█▃▄▄▇▃▂▃▅▃▄▇▆▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▆▁▇▃▂▃█▃▅▄▇▄▂▃▅▃▄▇▆▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▃▆▅▅▄▄▁▅▅▅▃▄█▅▆▄▄▄▂▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/1lvio7ns\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:49:14.842691: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/1uf31qsw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104913-1uf31qsw\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 26.77it/s, loss=1.12, v_num=1qsw, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.111\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 26.68it/s, loss=1.12, v_num=1qsw, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 26.46it/s, loss=1.09, v_num=1qsw, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.104 >= min_delta = 0.003. New best score: 1.006\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 26.29it/s, loss=1.09, v_num=1qsw, BTC_val_a\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 25.08it/s, loss=1.03, v_num=1qsw, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.102 >= min_delta = 0.003. New best score: 0.904\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 25.17it/s, loss=1.03, v_num=1qsw, BTC_val_a\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 25.43it/s, loss=0.972, v_num=1qsw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 25.07it/s, loss=0.972, v_num=1qsw, BTC_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 26.74it/s, loss=0.932, v_num=1qsw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.016 >= min_delta = 0.003. New best score: 0.888\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 26.69it/s, loss=0.932, v_num=1qsw, BTC_val_\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 25.53it/s, loss=0.883, v_num=1qsw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.095 >= min_delta = 0.003. New best score: 0.793\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 25.31it/s, loss=0.883, v_num=1qsw, BTC_val_\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 27.40it/s, loss=0.835, v_num=1qsw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 27.13it/s, loss=0.835, v_num=1qsw, BTC_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 24.23it/s, loss=0.825, v_num=1qsw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 24.09it/s, loss=0.825, v_num=1qsw, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 25.28it/s, loss=0.797, v_num=1qsw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 25.35it/s, loss=0.797, v_num=1qsw, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 26.18it/s, loss=0.827, v_num=1qsw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 26.09it/s, loss=0.827, v_num=1qsw, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 23.10it/s, loss=0.803, v_num=1qsw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 23.16it/s, loss=0.803, v_num=1qsw, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 25.34it/s, loss=0.824, v_num=1qsw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 25.27it/s, loss=0.824, v_num=1qsw, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 25.65it/s, loss=0.796, v_num=1qsw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.033 >= min_delta = 0.003. New best score: 0.759\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 25.21it/s, loss=0.796, v_num=1qsw, BTC_val\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 25.60it/s, loss=0.79, v_num=1qsw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 25.70it/s, loss=0.79, v_num=1qsw, BTC_val_\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 26.12it/s, loss=0.782, v_num=1qsw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.752\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 26.05it/s, loss=0.782, v_num=1qsw, BTC_val\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 24.58it/s, loss=0.786, v_num=1qsw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.744\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 24.51it/s, loss=0.786, v_num=1qsw, BTC_val\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 25.90it/s, loss=0.775, v_num=1qsw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 25.65it/s, loss=0.775, v_num=1qsw, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 26.43it/s, loss=0.774, v_num=1qsw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 26.14it/s, loss=0.774, v_num=1qsw, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 24.21it/s, loss=0.756, v_num=1qsw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 24.22it/s, loss=0.756, v_num=1qsw, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 25.61it/s, loss=0.765, v_num=1qsw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 25.33it/s, loss=0.765, v_num=1qsw, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 25.36it/s, loss=0.761, v_num=1qsw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 25.48it/s, loss=0.761, v_num=1qsw, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 25.54it/s, loss=0.79, v_num=1qsw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 25.57it/s, loss=0.79, v_num=1qsw, BTC_val_\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 25.39it/s, loss=0.757, v_num=1qsw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 25.37it/s, loss=0.757, v_num=1qsw, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 25.34it/s, loss=0.754, v_num=1qsw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 25.26it/s, loss=0.754, v_num=1qsw, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 25.32it/s, loss=0.78, v_num=1qsw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 25.36it/s, loss=0.78, v_num=1qsw, BTC_val_\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 24.93it/s, loss=0.76, v_num=1qsw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 24.92it/s, loss=0.76, v_num=1qsw, BTC_val_\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 25.26it/s, loss=0.74, v_num=1qsw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 25.04it/s, loss=0.74, v_num=1qsw, BTC_val_\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 25.09it/s, loss=0.765, v_num=1qsw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 25.19it/s, loss=0.765, v_num=1qsw, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 24.63it/s, loss=0.766, v_num=1qsw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 24.70it/s, loss=0.766, v_num=1qsw, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 26.15it/s, loss=0.765, v_num=1qsw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 26.06it/s, loss=0.765, v_num=1qsw, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 26.02it/s, loss=0.75, v_num=1qsw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 25.81it/s, loss=0.75, v_num=1qsw, BTC_val_\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 25.56it/s, loss=0.747, v_num=1qsw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 25.60it/s, loss=0.747, v_num=1qsw, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 24.95it/s, loss=0.746, v_num=1qsw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 24.89it/s, loss=0.746, v_num=1qsw, BTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 26.27it/s, loss=0.759, v_num=1qsw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 26.20it/s, loss=0.759, v_num=1qsw, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 24.48it/s, loss=0.735, v_num=1qsw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 24.31it/s, loss=0.735, v_num=1qsw, BTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 24.65it/s, loss=0.737, v_num=1qsw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.744. Signaling Trainer to stop.\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 24.78it/s, loss=0.737, v_num=1qsw, BTC_val\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 24.70it/s, loss=0.737, v_num=1qsw, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 56.94it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6666666865348816,\n",
      " 'BTC_test_f1': 0.6551132202148438,\n",
      " 'ETH_test_acc': 0.7272727489471436,\n",
      " 'ETH_test_f1': 0.6967593431472778,\n",
      " 'test_loss': 0.7356162071228027}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 105911\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104913-1uf31qsw/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_104913-1uf31qsw/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.68099\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.67118\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.66905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.64768\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.7357\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 35\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 720\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 38\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625903391\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 86\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.46154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.45238\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.46154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.4127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.91415\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.58537\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.56757\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.65854\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.65697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.7278\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.65511\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.72727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.69676\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.73562\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▁▃▄▅▆▆▇▇▇▇▇▇▇▇▇▇██▇█▇█████▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▁▃▄▅▆▆▆▇▇▇▇▇▇▇▇▇██▇█▇▇████▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▂▄▄▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇██▇█▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▃▄▆▆▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇██▇██▇█▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ██▆▅▅▄▃▃▂▃▂▃▂▂▂▂▂▂▁▂▁▂▁▁▂▁▁▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▅▅▃▇▅▇█▅▇▆▇▆▆▆▆▆▆▂▂▂▅▆▂▂▃▂▁▃▂▅▂▃▃▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▄▄▃▇▅▇█▅▇▅▇▅▆▅▆▆▆▂▂▂▅▅▂▂▃▂▁▄▂▄▂▄▃▃▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▄▄▄▄▆▆█▆▄▆▆▆▄▆▆▆▄▆▆▇▆▄▆▄▄▄▄▄▄▄█▄▄▆▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▄▄▄▄▆▆█▆▄▆▆▆▄▆▆▆▄▆▆▇▆▄▆▄▄▄▄▄▄▄█▄▄▆▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▆▄▅▄▂▂▃▃▄▃▃▁▃▁▁▂▆▆▂▁▃▃▂▃▄▆▄▂▄▃▃▃▃▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▂▁▁▆▇█▅▅▆▇▅▂▇▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▂▂▂▆▇█▆▆▅▇▅▁█▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▃▅▄▃▃▄▆▆▃▄█▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▃▅▅▃▃▅▆▆▃▃█▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▇▄▂▅▆▄▁▂▄▄▃▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/1uf31qsw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:50:07.071132: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/18899wlx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105005-18899wlx\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 20/21 [00:00<00:00, 27.83it/s, loss=0.704, v_num=9wlx, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.677\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 27.78it/s, loss=0.704, v_num=9wlx, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 26.10it/s, loss=0.673, v_num=9wlx, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.110 >= min_delta = 0.003. New best score: 0.567\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 26.17it/s, loss=0.673, v_num=9wlx, BTC_val_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 26.20it/s, loss=0.616, v_num=9wlx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.024 >= min_delta = 0.003. New best score: 0.543\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 26.16it/s, loss=0.616, v_num=9wlx, BTC_val_\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 26.00it/s, loss=0.589, v_num=9wlx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.530\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 26.00it/s, loss=0.589, v_num=9wlx, BTC_val_\n",
      "Epoch 4:  95%|▉| 20/21 [00:00<00:00, 25.62it/s, loss=0.578, v_num=9wlx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.066 >= min_delta = 0.003. New best score: 0.463\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 25.43it/s, loss=0.578, v_num=9wlx, BTC_val_\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 25.67it/s, loss=0.549, v_num=9wlx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.456\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 25.59it/s, loss=0.549, v_num=9wlx, BTC_val_\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 26.13it/s, loss=0.525, v_num=9wlx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 26.07it/s, loss=0.525, v_num=9wlx, BTC_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 26.21it/s, loss=0.506, v_num=9wlx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 26.16it/s, loss=0.506, v_num=9wlx, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 20/21 [00:00<00:00, 26.48it/s, loss=0.526, v_num=9wlx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 26.25it/s, loss=0.526, v_num=9wlx, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 20/21 [00:00<00:00, 25.15it/s, loss=0.492, v_num=9wlx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 25.14it/s, loss=0.492, v_num=9wlx, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 23.76it/s, loss=0.506, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 23.81it/s, loss=0.506, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 26.31it/s, loss=0.49, v_num=9wlx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 26.29it/s, loss=0.49, v_num=9wlx, BTC_val_\u001b[A\n",
      "Epoch 12:  95%|▉| 20/21 [00:00<00:00, 25.75it/s, loss=0.483, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 25.75it/s, loss=0.483, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 20/21 [00:00<00:00, 25.93it/s, loss=0.48, v_num=9wlx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 25.68it/s, loss=0.48, v_num=9wlx, BTC_val_\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 25.93it/s, loss=0.472, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 25.76it/s, loss=0.472, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 20/21 [00:00<00:00, 25.32it/s, loss=0.481, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 25.22it/s, loss=0.481, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 20/21 [00:00<00:00, 26.38it/s, loss=0.476, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 26.43it/s, loss=0.476, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 20/21 [00:00<00:00, 25.72it/s, loss=0.483, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 25.59it/s, loss=0.483, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 26.32it/s, loss=0.477, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 26.22it/s, loss=0.477, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 26.31it/s, loss=0.474, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 26.17it/s, loss=0.474, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 20/21 [00:00<00:00, 26.27it/s, loss=0.484, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 0.435\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 26.19it/s, loss=0.484, v_num=9wlx, BTC_val\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 26.44it/s, loss=0.485, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 26.34it/s, loss=0.485, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 25.34it/s, loss=0.463, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 25.26it/s, loss=0.463, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 20/21 [00:00<00:00, 25.49it/s, loss=0.472, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 25.26it/s, loss=0.472, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 20/21 [00:00<00:00, 26.12it/s, loss=0.462, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 26.12it/s, loss=0.462, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 24.82it/s, loss=0.463, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 24.81it/s, loss=0.463, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 20/21 [00:00<00:00, 25.79it/s, loss=0.461, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 21/21 [00:00<00:00, 25.82it/s, loss=0.461, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 20/21 [00:00<00:00, 26.80it/s, loss=0.462, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 21/21 [00:00<00:00, 26.78it/s, loss=0.462, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 20/21 [00:00<00:00, 25.75it/s, loss=0.463, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 21/21 [00:00<00:00, 25.65it/s, loss=0.463, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 20/21 [00:00<00:00, 24.58it/s, loss=0.464, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 21/21 [00:00<00:00, 24.47it/s, loss=0.464, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 20/21 [00:00<00:00, 23.89it/s, loss=0.46, v_num=9wlx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 21/21 [00:00<00:00, 23.78it/s, loss=0.46, v_num=9wlx, BTC_val_\u001b[A\n",
      "Epoch 31:  95%|▉| 20/21 [00:00<00:00, 24.13it/s, loss=0.465, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 21/21 [00:00<00:00, 24.08it/s, loss=0.465, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 20/21 [00:00<00:00, 24.77it/s, loss=0.447, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 21/21 [00:00<00:00, 24.71it/s, loss=0.447, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 20/21 [00:00<00:00, 23.22it/s, loss=0.454, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 21/21 [00:00<00:00, 23.39it/s, loss=0.454, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 20/21 [00:00<00:00, 24.71it/s, loss=0.457, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 21/21 [00:00<00:00, 24.61it/s, loss=0.457, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 20/21 [00:00<00:00, 24.97it/s, loss=0.447, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 21/21 [00:00<00:00, 24.93it/s, loss=0.447, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 20/21 [00:00<00:00, 24.54it/s, loss=0.445, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 21/21 [00:00<00:00, 24.42it/s, loss=0.445, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 20/21 [00:00<00:00, 24.84it/s, loss=0.456, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 21/21 [00:00<00:00, 24.78it/s, loss=0.456, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 20/21 [00:00<00:00, 26.69it/s, loss=0.454, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 21/21 [00:00<00:00, 26.35it/s, loss=0.454, v_num=9wlx, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39:  95%|▉| 20/21 [00:00<00:00, 23.75it/s, loss=0.458, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 21/21 [00:00<00:00, 23.68it/s, loss=0.458, v_num=9wlx, BTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 20/21 [00:00<00:00, 25.62it/s, loss=0.447, v_num=9wlx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.435. Signaling Trainer to stop.\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 25.62it/s, loss=0.447, v_num=9wlx, BTC_val\n",
      "Epoch 40: 100%|█| 21/21 [00:00<00:00, 25.53it/s, loss=0.447, v_num=9wlx, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 38.00it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6363636255264282,\n",
      " 'BTC_test_f1': 0.6278195381164551,\n",
      " 'ETH_test_acc': 0.7272727489471436,\n",
      " 'ETH_test_f1': 0.7179487347602844,\n",
      " 'test_loss': 0.5567930936813354}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 106128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105005-18899wlx/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105005-18899wlx/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.80111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.7963\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.8035\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.80108\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.44747\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 820\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625903447\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 98\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.61538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.60606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.92308\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.92121\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.50576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.85366\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.84926\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.82927\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.82553\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.36525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.63636\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.62782\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.72727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.71795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.55679\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▃▅▅▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇█▇▇█▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▃▅▅▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇█▇▇█▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▃▄▅▆▆▆▇▇▇▇▇█▇███▇█▇▇███████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▃▄▅▆▆▆▇▇▇▇▇█▇███▇█▇▇███████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▅▅▄▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▆▅▆▆▅▆▅█▆▆▆▆▅▅▆▅▅▆▆▆▆▆▆▅▅▆▅▆▆▆▆▅▆▅▅▅▆▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▇▆▇▇▅▇▅█▇▇▇▇▅▅▇▆▅▇▇▇▇▇▇▅▅▇▅▇▇▇▇▅▇▅▅▅▇▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁█▅▅█████████████████████▇██████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁█▆▆█████████████████████▇██████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▅▄▄▂▂▂▂▃▂▂▂▂▂▂▂▃▂▂▂▁▂▂▂▂▂▂▂▃▂▂▂▃▂▁▂▃▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▅▇▇▇▇▅▇▆█▅▅▇▅▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▅▇▆▇▇▅▇▆█▄▅▇▅▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▂▆▄█▂▄▅▆▇▅▄▇▅█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▂▆▄█▃▄▅▆▇▅▃▇▅█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▇▄▂▂▄▄▃▃▂▅▅▂▅▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/18899wlx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:51:02.968547: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/79qiu9p5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105101-79qiu9p5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 26.16it/s, loss=1.12, v_num=u9p5, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.080\n",
      "Epoch 0: 100%|█| 21/21 [00:00<00:00, 25.55it/s, loss=1.12, v_num=u9p5, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 20/21 [00:00<00:00, 25.88it/s, loss=1.06, v_num=u9p5, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.156 >= min_delta = 0.003. New best score: 0.924\n",
      "Epoch 1: 100%|█| 21/21 [00:00<00:00, 25.85it/s, loss=1.06, v_num=u9p5, BTC_val_a\n",
      "Epoch 2:  95%|▉| 20/21 [00:00<00:00, 25.12it/s, loss=1, v_num=u9p5, BTC_val_acc=\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.068 >= min_delta = 0.003. New best score: 0.856\n",
      "Epoch 2: 100%|█| 21/21 [00:00<00:00, 25.10it/s, loss=1, v_num=u9p5, BTC_val_acc=\n",
      "Epoch 3:  95%|▉| 20/21 [00:00<00:00, 23.90it/s, loss=0.952, v_num=u9p5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 21/21 [00:00<00:00, 23.87it/s, loss=0.952, v_num=u9p5, BTC_val_\u001b[A\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 25.29it/s, loss=0.912, v_num=u9p5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.843\n",
      "Epoch 4: 100%|█| 21/21 [00:00<00:00, 24.58it/s, loss=0.912, v_num=u9p5, BTC_val_\n",
      "Epoch 5:  95%|▉| 20/21 [00:00<00:00, 25.20it/s, loss=0.869, v_num=u9p5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.130 >= min_delta = 0.003. New best score: 0.713\n",
      "Epoch 5: 100%|█| 21/21 [00:00<00:00, 25.05it/s, loss=0.869, v_num=u9p5, BTC_val_\n",
      "Epoch 6:  95%|▉| 20/21 [00:00<00:00, 25.85it/s, loss=0.847, v_num=u9p5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 21/21 [00:00<00:00, 25.88it/s, loss=0.847, v_num=u9p5, BTC_val_\u001b[A\n",
      "Epoch 7:  95%|▉| 20/21 [00:00<00:00, 26.03it/s, loss=0.823, v_num=u9p5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 21/21 [00:00<00:00, 25.88it/s, loss=0.823, v_num=u9p5, BTC_val_\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 25.50it/s, loss=0.831, v_num=u9p5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 21/21 [00:00<00:00, 24.91it/s, loss=0.831, v_num=u9p5, BTC_val_\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 27.14it/s, loss=0.803, v_num=u9p5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 21/21 [00:00<00:00, 26.42it/s, loss=0.803, v_num=u9p5, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 20/21 [00:00<00:00, 24.29it/s, loss=0.815, v_num=u9p5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 21/21 [00:00<00:00, 24.34it/s, loss=0.815, v_num=u9p5, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 20/21 [00:00<00:00, 26.05it/s, loss=0.799, v_num=u9p5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 21/21 [00:00<00:00, 25.79it/s, loss=0.799, v_num=u9p5, BTC_val\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 25.29it/s, loss=0.77, v_num=u9p5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 21/21 [00:00<00:00, 24.72it/s, loss=0.77, v_num=u9p5, BTC_val_\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 25.56it/s, loss=0.775, v_num=u9p5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 21/21 [00:00<00:00, 25.01it/s, loss=0.775, v_num=u9p5, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 20/21 [00:00<00:00, 24.97it/s, loss=0.776, v_num=u9p5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 21/21 [00:00<00:00, 24.72it/s, loss=0.776, v_num=u9p5, BTC_val\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 26.48it/s, loss=0.805, v_num=u9p5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 21/21 [00:00<00:00, 25.80it/s, loss=0.805, v_num=u9p5, BTC_val\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 26.49it/s, loss=0.78, v_num=u9p5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 21/21 [00:00<00:00, 25.77it/s, loss=0.78, v_num=u9p5, BTC_val_\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 25.91it/s, loss=0.77, v_num=u9p5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 21/21 [00:00<00:00, 25.31it/s, loss=0.77, v_num=u9p5, BTC_val_\u001b[A\n",
      "Epoch 18:  95%|▉| 20/21 [00:00<00:00, 25.93it/s, loss=0.755, v_num=u9p5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 21/21 [00:00<00:00, 25.57it/s, loss=0.755, v_num=u9p5, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 20/21 [00:00<00:00, 25.37it/s, loss=0.757, v_num=u9p5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 21/21 [00:00<00:00, 25.37it/s, loss=0.757, v_num=u9p5, BTC_val\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 26.10it/s, loss=0.755, v_num=u9p5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 21/21 [00:00<00:00, 25.55it/s, loss=0.755, v_num=u9p5, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 20/21 [00:00<00:00, 26.18it/s, loss=0.769, v_num=u9p5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 21/21 [00:00<00:00, 26.16it/s, loss=0.769, v_num=u9p5, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 20/21 [00:00<00:00, 25.22it/s, loss=0.761, v_num=u9p5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 21/21 [00:00<00:00, 25.25it/s, loss=0.761, v_num=u9p5, BTC_val\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 26.77it/s, loss=0.745, v_num=u9p5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 21/21 [00:00<00:00, 26.19it/s, loss=0.745, v_num=u9p5, BTC_val\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 25.98it/s, loss=0.744, v_num=u9p5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 21/21 [00:00<00:00, 25.43it/s, loss=0.744, v_num=u9p5, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 20/21 [00:00<00:00, 25.56it/s, loss=0.797, v_num=u9p5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.713. Signaling Trainer to stop.\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 25.56it/s, loss=0.797, v_num=u9p5, BTC_val\n",
      "Epoch 25: 100%|█| 21/21 [00:00<00:00, 25.44it/s, loss=0.797, v_num=u9p5, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 47.02it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6666666865348816,\n",
      " 'BTC_test_f1': 0.6676768064498901,\n",
      " 'ETH_test_acc': 0.7575757503509521,\n",
      " 'ETH_test_f1': 0.7414215803146362,\n",
      " 'test_loss': 0.7139993906021118}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 106316\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105101-79qiu9p5/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105101-79qiu9p5/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.65553\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.64864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.62609\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.61124\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.79738\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 520\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625903491\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 62\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.61538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.57778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.53846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.52222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.80459\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.7561\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.73434\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.56098\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.54164\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.68647\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.66768\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.75758\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.74142\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▃▅▅▆▇▇▇▇▇▇▇███▇█████████▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▃▄▅▆▇▇▇▇▇▇▇███▇█████████▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▃▄▅▆▆▆▇▇▇▆▇▇▇█▇▇█████▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▃▃▅▆▆▆▇▇▇▆▇▇▇▇▇▇████▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▅▄▃▃▂▃▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▅▅▂▅▇▆█▆▅▆▇▃▆▂▅▃▅▅▃▆▃▂▂▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▄▅▃▅▇▆█▆▅▆▇▄▆▃▄▄▅▅▄▆▄▃▃▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▆▄▄▄▆▄█▄▆█▆▄▆▄▆▄▆▄▆▆▆▆▄▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▆▆▅▅▆▅█▅▆█▆▅▆▅▆▅▆▅▆▆▆▆▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▅▄▅▃▁▂▃▄▄▂▁▃▃▅▄▃▂▄▃▂▂▄▄▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▅▅▆▆▅▆▆▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▅▆▆▆▅▆▇▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▆▇▇▇█▇▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▆▇▇▇█▇▇▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▅▂▃▃▁▂▂▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/79qiu9p5\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:51:49.613288: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/3ljcqyc5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105148-3ljcqyc5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 109 K \n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.532     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|█| 19/19 [00:01<00:00, 18.87it/s, loss=0.716, v_num=qyc5, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.688\n",
      "Epoch 0: 100%|█| 19/19 [00:01<00:00, 18.33it/s, loss=0.716, v_num=qyc5, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 19.23it/s, loss=0.707, v_num=qyc5, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.032 >= min_delta = 0.003. New best score: 0.656\n",
      "Epoch 1: 100%|█| 19/19 [00:01<00:00, 18.83it/s, loss=0.707, v_num=qyc5, BTC_val_\n",
      "Epoch 2:  95%|▉| 18/19 [00:01<00:00, 17.23it/s, loss=0.701, v_num=qyc5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 19/19 [00:01<00:00, 17.31it/s, loss=0.701, v_num=qyc5, BTC_val_\u001b[A\n",
      "Epoch 3:  95%|▉| 18/19 [00:01<00:00, 17.49it/s, loss=0.697, v_num=qyc5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 19/19 [00:01<00:00, 17.56it/s, loss=0.697, v_num=qyc5, BTC_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 18/19 [00:01<00:00, 17.26it/s, loss=0.692, v_num=qyc5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 19/19 [00:01<00:00, 17.36it/s, loss=0.692, v_num=qyc5, BTC_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 18/19 [00:01<00:00, 16.78it/s, loss=0.689, v_num=qyc5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 19/19 [00:01<00:00, 16.84it/s, loss=0.689, v_num=qyc5, BTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 18/19 [00:01<00:00, 17.46it/s, loss=0.69, v_num=qyc5, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 19/19 [00:01<00:00, 17.52it/s, loss=0.69, v_num=qyc5, BTC_val_a\u001b[A\n",
      "Epoch 7:  95%|▉| 18/19 [00:01<00:00, 17.18it/s, loss=0.681, v_num=qyc5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.037 >= min_delta = 0.003. New best score: 0.619\n",
      "Epoch 7: 100%|█| 19/19 [00:01<00:00, 17.20it/s, loss=0.681, v_num=qyc5, BTC_val_\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 18.11it/s, loss=0.672, v_num=qyc5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 19/19 [00:01<00:00, 18.07it/s, loss=0.672, v_num=qyc5, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 18/19 [00:01<00:00, 17.88it/s, loss=0.667, v_num=qyc5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:01<00:00, 17.89it/s, loss=0.667, v_num=qyc5, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 18.10it/s, loss=0.65, v_num=qyc5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.019 >= min_delta = 0.003. New best score: 0.600\n",
      "Epoch 10: 100%|█| 19/19 [00:01<00:00, 18.19it/s, loss=0.65, v_num=qyc5, BTC_val_\n",
      "Epoch 11:  95%|▉| 18/19 [00:01<00:00, 17.88it/s, loss=0.612, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.036 >= min_delta = 0.003. New best score: 0.563\n",
      "Epoch 11: 100%|█| 19/19 [00:01<00:00, 17.87it/s, loss=0.612, v_num=qyc5, BTC_val\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 18.16it/s, loss=0.575, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:01<00:00, 18.26it/s, loss=0.575, v_num=qyc5, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 18.16it/s, loss=0.545, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.030 >= min_delta = 0.003. New best score: 0.534\n",
      "Epoch 13: 100%|█| 19/19 [00:01<00:00, 18.18it/s, loss=0.545, v_num=qyc5, BTC_val\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 18.87it/s, loss=0.508, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:01<00:00, 18.68it/s, loss=0.508, v_num=qyc5, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 18.33it/s, loss=0.501, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.067 >= min_delta = 0.003. New best score: 0.467\n",
      "Epoch 15: 100%|█| 19/19 [00:01<00:00, 18.31it/s, loss=0.501, v_num=qyc5, BTC_val\n",
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 18.44it/s, loss=0.48, v_num=qyc5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:01<00:00, 18.43it/s, loss=0.48, v_num=qyc5, BTC_val_\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 18.08it/s, loss=0.482, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:01<00:00, 18.21it/s, loss=0.482, v_num=qyc5, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:01<00:00, 17.49it/s, loss=0.475, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.052 >= min_delta = 0.003. New best score: 0.415\n",
      "Epoch 18: 100%|█| 19/19 [00:01<00:00, 17.59it/s, loss=0.475, v_num=qyc5, BTC_val\n",
      "Epoch 19:  95%|▉| 18/19 [00:01<00:00, 17.53it/s, loss=0.46, v_num=qyc5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:01<00:00, 17.69it/s, loss=0.46, v_num=qyc5, BTC_val_\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:01<00:00, 17.31it/s, loss=0.444, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:01<00:00, 17.42it/s, loss=0.444, v_num=qyc5, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:01<00:00, 17.53it/s, loss=0.448, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:01<00:00, 17.60it/s, loss=0.448, v_num=qyc5, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:01<00:00, 17.36it/s, loss=0.433, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:01<00:00, 17.51it/s, loss=0.433, v_num=qyc5, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:01<00:00, 17.88it/s, loss=0.441, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:01<00:00, 17.99it/s, loss=0.441, v_num=qyc5, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:01<00:00, 17.53it/s, loss=0.43, v_num=qyc5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:01<00:00, 17.64it/s, loss=0.43, v_num=qyc5, BTC_val_\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 18.28it/s, loss=0.419, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:01<00:00, 18.11it/s, loss=0.419, v_num=qyc5, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:01<00:00, 17.89it/s, loss=0.419, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:01<00:00, 18.00it/s, loss=0.419, v_num=qyc5, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:01<00:00, 17.87it/s, loss=0.415, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:01<00:00, 17.89it/s, loss=0.415, v_num=qyc5, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 18.12it/s, loss=0.405, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:01<00:00, 17.99it/s, loss=0.405, v_num=qyc5, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 18.48it/s, loss=0.401, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:01<00:00, 18.39it/s, loss=0.401, v_num=qyc5, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 18.18it/s, loss=0.389, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:01<00:00, 18.16it/s, loss=0.389, v_num=qyc5, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 18.74it/s, loss=0.392, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:01<00:00, 18.71it/s, loss=0.392, v_num=qyc5, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:01<00:00, 17.59it/s, loss=0.39, v_num=qyc5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:01<00:00, 17.71it/s, loss=0.39, v_num=qyc5, BTC_val_\u001b[A\n",
      "Epoch 33:  95%|▉| 18/19 [00:01<00:00, 17.86it/s, loss=0.371, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:01<00:00, 17.97it/s, loss=0.371, v_num=qyc5, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:00<00:00, 18.30it/s, loss=0.373, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:01<00:00, 18.16it/s, loss=0.373, v_num=qyc5, BTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:00<00:00, 18.03it/s, loss=0.364, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:01<00:00, 17.93it/s, loss=0.364, v_num=qyc5, BTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:00<00:00, 18.11it/s, loss=0.362, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 19/19 [00:01<00:00, 18.12it/s, loss=0.362, v_num=qyc5, BTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 18/19 [00:01<00:00, 17.84it/s, loss=0.349, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:01<00:00, 17.63it/s, loss=0.349, v_num=qyc5, BTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:00<00:00, 18.37it/s, loss=0.352, v_num=qyc5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.415. Signaling Trainer to stop.\n",
      "Epoch 38: 100%|█| 19/19 [00:01<00:00, 18.46it/s, loss=0.352, v_num=qyc5, BTC_val\n",
      "Epoch 38: 100%|█| 19/19 [00:01<00:00, 18.39it/s, loss=0.352, v_num=qyc5, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 36.21it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6333333253860474,\n",
      " 'BTC_test_f1': 0.6228570938110352,\n",
      " 'ETH_test_acc': 0.7333333492279053,\n",
      " 'ETH_test_f1': 0.7285068035125732,\n",
      " 'LTC_test_acc': 0.7666666507720947,\n",
      " 'LTC_test_f1': 0.7664071321487427,\n",
      " 'test_loss': 0.5270160436630249}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 106498\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105148-3ljcqyc5/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105148-3ljcqyc5/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.83027\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.828\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.86002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.85792\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.85564\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.85412\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.35522\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 38\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 702\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625903558\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.58333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.4958\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.62105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.91667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.87368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.6195\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.90625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.90625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.89062\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.89038\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.90625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.90476\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.31082\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.63333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.62286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.73333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.72851\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.76667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.76641\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.52702\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▁▂▂▂▂▃▃▃▄▄▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇██▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▁▂▂▂▂▃▃▃▄▄▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇██▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▂▁▂▂▂▂▂▃▃▃▅▅▅▆▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▁▂▂▂▂▂▃▃▃▅▅▅▆▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▁▂▂▂▂▂▃▃▃▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▁▂▂▂▂▂▃▃▃▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █████▇▇▇▇▇▇▆▅▅▄▄▃▃▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▃▁▁▁▁▁▁▃▅▆▃▁▃█▁▃▃▁▃▃▃▃▁▃▁▃▁▁▅▃▁▃▅▁▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▃▄▃▃▁▁▁▁▃▅▇▃▁▃█▂▄▄▂▄▃▄▃▁▃▂▃▂▂▆▃▂▃▆▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▃▆▄▇▄▆▅▆▁▁▆▆▄██▇██████▆█▇▇▇▆███▇▇▆▇▇▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▃▃▄▇▅▃▅▃▁▁▃▃▅██▇██████▃█▇▆▆▅███▇▆▅▆▇▇▃▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▃▃▃▃▃▃▃▃▃▃▃▃▃██▁█████▃████████▃███▃███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▅▁██▄█████▁████████▅███▅███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▇▆▇▇▇▆▆▅▇█▅▄▅▄▄▂▄▂▁▂▁▂▇▂▃▃▆▄▆▂▂▅▆▅█▆▇█▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▁▂▃▆▆▇▇▆▇▆▆▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▁▂▃▆▆▆▇▆▇▆▆▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▂▁▄▅▅▆▆▆█▇▇▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▂▁▄▅▅▆▆▆█▇▇▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▃▁▂▄▆▆▆▆▇▆██▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▂▁▂▄▆▆▆▆▇▆██▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ██▇▇▅▄▄▄▃▃▂▂▄▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/3ljcqyc5\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:52:56.089529: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/324dhtza\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105254-324dhtza\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 109 K \n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.532     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 19.96it/s, loss=1.14, v_num=htza, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.151\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 19.46it/s, loss=1.14, v_num=htza, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 19.61it/s, loss=1.11, v_num=htza, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.041 >= min_delta = 0.003. New best score: 1.109\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 19.73it/s, loss=1.11, v_num=htza, BTC_val_a\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 19.98it/s, loss=1.09, v_num=htza, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 1.104\n",
      "Epoch 2: 100%|█| 19/19 [00:00<00:00, 20.16it/s, loss=1.09, v_num=htza, BTC_val_a\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 18.41it/s, loss=1.09, v_num=htza, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 19/19 [00:01<00:00, 18.53it/s, loss=1.09, v_num=htza, BTC_val_a\u001b[A\n",
      "Epoch 4:  95%|▉| 18/19 [00:01<00:00, 17.55it/s, loss=1.09, v_num=htza, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.016 >= min_delta = 0.003. New best score: 1.088\n",
      "Epoch 4: 100%|█| 19/19 [00:01<00:00, 17.70it/s, loss=1.09, v_num=htza, BTC_val_a\n",
      "Epoch 5:  95%|▉| 18/19 [00:00<00:00, 18.36it/s, loss=1.08, v_num=htza, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 19/19 [00:01<00:00, 18.36it/s, loss=1.08, v_num=htza, BTC_val_a\u001b[A\n",
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 18.41it/s, loss=1.08, v_num=htza, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 1.081\n",
      "Epoch 6: 100%|█| 19/19 [00:01<00:00, 18.55it/s, loss=1.08, v_num=htza, BTC_val_a\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 18.11it/s, loss=1.07, v_num=htza, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.024 >= min_delta = 0.003. New best score: 1.057\n",
      "Epoch 7: 100%|█| 19/19 [00:01<00:00, 17.98it/s, loss=1.07, v_num=htza, BTC_val_a\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 18.11it/s, loss=1.05, v_num=htza, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 19/19 [00:01<00:00, 18.18it/s, loss=1.05, v_num=htza, BTC_val_a\u001b[A\n",
      "Epoch 9:  95%|▉| 18/19 [00:01<00:00, 17.76it/s, loss=1.04, v_num=htza, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:01<00:00, 17.93it/s, loss=1.04, v_num=htza, BTC_val_a\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 18.84it/s, loss=1.02, v_num=htza, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:01<00:00, 18.64it/s, loss=1.02, v_num=htza, BTC_val_\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 18.12it/s, loss=1, v_num=htza, BTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:01<00:00, 18.27it/s, loss=1, v_num=htza, BTC_val_acc\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 18.57it/s, loss=0.943, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:01<00:00, 18.67it/s, loss=0.943, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:01<00:00, 17.86it/s, loss=0.891, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.112 >= min_delta = 0.003. New best score: 0.944\n",
      "Epoch 13: 100%|█| 19/19 [00:01<00:00, 17.96it/s, loss=0.891, v_num=htza, BTC_val\n",
      "Epoch 14:  95%|▉| 18/19 [00:01<00:00, 17.98it/s, loss=0.88, v_num=htza, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:01<00:00, 18.06it/s, loss=0.88, v_num=htza, BTC_val_\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 18.08it/s, loss=0.852, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.044 >= min_delta = 0.003. New best score: 0.901\n",
      "Epoch 15: 100%|█| 19/19 [00:01<00:00, 18.12it/s, loss=0.852, v_num=htza, BTC_val\n",
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 18.77it/s, loss=0.804, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.897\n",
      "Epoch 16: 100%|█| 19/19 [00:01<00:00, 18.81it/s, loss=0.804, v_num=htza, BTC_val\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 18.12it/s, loss=0.793, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:01<00:00, 18.25it/s, loss=0.793, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 19.48it/s, loss=0.816, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.888\n",
      "Epoch 18: 100%|█| 19/19 [00:00<00:00, 19.50it/s, loss=0.816, v_num=htza, BTC_val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 18.54it/s, loss=0.782, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.030 >= min_delta = 0.003. New best score: 0.858\n",
      "Epoch 19: 100%|█| 19/19 [00:01<00:00, 18.62it/s, loss=0.782, v_num=htza, BTC_val\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 18.32it/s, loss=0.782, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:01<00:00, 18.25it/s, loss=0.782, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 18.32it/s, loss=0.751, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.024 >= min_delta = 0.003. New best score: 0.834\n",
      "Epoch 21: 100%|█| 19/19 [00:01<00:00, 18.42it/s, loss=0.751, v_num=htza, BTC_val\n",
      "Epoch 22:  95%|▉| 18/19 [00:00<00:00, 18.91it/s, loss=0.735, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:01<00:00, 18.98it/s, loss=0.735, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 18.67it/s, loss=0.727, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:01<00:00, 18.77it/s, loss=0.727, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 18.61it/s, loss=0.727, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:01<00:00, 18.61it/s, loss=0.727, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:01<00:00, 17.92it/s, loss=0.719, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:01<00:00, 17.92it/s, loss=0.719, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:01<00:00, 17.74it/s, loss=0.711, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.052 >= min_delta = 0.003. New best score: 0.781\n",
      "Epoch 26: 100%|█| 19/19 [00:01<00:00, 17.60it/s, loss=0.711, v_num=htza, BTC_val\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 18.31it/s, loss=0.708, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:01<00:00, 18.40it/s, loss=0.708, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 18.27it/s, loss=0.694, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:01<00:00, 18.29it/s, loss=0.694, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 18.86it/s, loss=0.697, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:01<00:00, 18.94it/s, loss=0.697, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 18.32it/s, loss=0.674, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:01<00:00, 18.35it/s, loss=0.674, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 18.55it/s, loss=0.652, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 0.771\n",
      "Epoch 31: 100%|█| 19/19 [00:01<00:00, 18.59it/s, loss=0.652, v_num=htza, BTC_val\n",
      "Epoch 32:  95%|▉| 18/19 [00:00<00:00, 18.58it/s, loss=0.673, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:01<00:00, 18.51it/s, loss=0.673, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 18/19 [00:00<00:00, 18.33it/s, loss=0.659, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:01<00:00, 18.36it/s, loss=0.659, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:00<00:00, 18.05it/s, loss=0.637, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:01<00:00, 18.09it/s, loss=0.637, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:00<00:00, 18.90it/s, loss=0.633, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:01<00:00, 19.00it/s, loss=0.633, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:01<00:00, 17.64it/s, loss=0.629, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 19/19 [00:01<00:00, 17.74it/s, loss=0.629, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 18/19 [00:01<00:00, 17.91it/s, loss=0.605, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:01<00:00, 17.77it/s, loss=0.605, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:00<00:00, 18.14it/s, loss=0.619, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 19/19 [00:01<00:00, 18.28it/s, loss=0.619, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 18/19 [00:00<00:00, 18.71it/s, loss=0.605, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 19/19 [00:01<00:00, 18.77it/s, loss=0.605, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 18/19 [00:01<00:00, 17.75it/s, loss=0.592, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.759\n",
      "Epoch 40: 100%|█| 19/19 [00:01<00:00, 17.88it/s, loss=0.592, v_num=htza, BTC_val\n",
      "Epoch 41:  95%|▉| 18/19 [00:00<00:00, 18.02it/s, loss=0.589, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 19/19 [00:01<00:00, 17.92it/s, loss=0.589, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 18/19 [00:00<00:00, 18.29it/s, loss=0.574, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 19/19 [00:01<00:00, 18.24it/s, loss=0.574, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 18/19 [00:00<00:00, 18.85it/s, loss=0.576, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 19/19 [00:01<00:00, 18.68it/s, loss=0.576, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 44:  95%|▉| 18/19 [00:00<00:00, 18.05it/s, loss=0.575, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 19/19 [00:01<00:00, 18.04it/s, loss=0.575, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 45:  95%|▉| 18/19 [00:00<00:00, 18.74it/s, loss=0.566, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 19/19 [00:01<00:00, 18.60it/s, loss=0.566, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 18/19 [00:00<00:00, 18.45it/s, loss=0.553, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 19/19 [00:01<00:00, 18.41it/s, loss=0.553, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 18/19 [00:00<00:00, 18.10it/s, loss=0.545, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 19/19 [00:01<00:00, 18.17it/s, loss=0.545, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 48:  95%|▉| 18/19 [00:01<00:00, 17.78it/s, loss=0.522, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 19/19 [00:01<00:00, 17.90it/s, loss=0.522, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 49:  95%|▉| 18/19 [00:00<00:00, 18.39it/s, loss=0.524, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 19/19 [00:01<00:00, 18.28it/s, loss=0.524, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 50:  95%|▉| 18/19 [00:00<00:00, 18.61it/s, loss=0.548, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 19/19 [00:01<00:00, 18.76it/s, loss=0.548, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 51:  95%|▉| 18/19 [00:01<00:00, 17.98it/s, loss=0.51, v_num=htza, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 19/19 [00:01<00:00, 17.94it/s, loss=0.51, v_num=htza, BTC_val_\u001b[A\n",
      "Epoch 52:  95%|▉| 18/19 [00:00<00:00, 18.98it/s, loss=0.51, v_num=htza, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 19/19 [00:00<00:00, 19.08it/s, loss=0.51, v_num=htza, BTC_val_\u001b[A\n",
      "Epoch 53:  95%|▉| 18/19 [00:01<00:00, 17.69it/s, loss=0.498, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 19/19 [00:01<00:00, 17.77it/s, loss=0.498, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 54:  95%|▉| 18/19 [00:01<00:00, 17.62it/s, loss=0.49, v_num=htza, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 19/19 [00:01<00:00, 17.61it/s, loss=0.49, v_num=htza, BTC_val_\u001b[A\n",
      "Epoch 55:  95%|▉| 18/19 [00:01<00:00, 17.89it/s, loss=0.493, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 19/19 [00:01<00:00, 17.90it/s, loss=0.493, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 56:  95%|▉| 18/19 [00:00<00:00, 18.71it/s, loss=0.495, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 19/19 [00:01<00:00, 18.50it/s, loss=0.495, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 57:  95%|▉| 18/19 [00:01<00:00, 17.44it/s, loss=0.466, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 19/19 [00:01<00:00, 17.54it/s, loss=0.466, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 58:  95%|▉| 18/19 [00:00<00:00, 18.18it/s, loss=0.453, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 19/19 [00:01<00:00, 18.19it/s, loss=0.453, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 59:  95%|▉| 18/19 [00:01<00:00, 17.62it/s, loss=0.443, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|█| 19/19 [00:01<00:00, 17.74it/s, loss=0.443, v_num=htza, BTC_val\u001b[A\n",
      "Epoch 60:  95%|▉| 18/19 [00:01<00:00, 17.86it/s, loss=0.441, v_num=htza, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.759. Signaling Trainer to stop.\n",
      "Epoch 60: 100%|█| 19/19 [00:01<00:00, 17.69it/s, loss=0.441, v_num=htza, BTC_val\n",
      "Epoch 60: 100%|█| 19/19 [00:01<00:00, 17.60it/s, loss=0.441, v_num=htza, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 29.48it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6000000238418579,\n",
      " 'BTC_test_f1': 0.5599206686019897,\n",
      " 'ETH_test_acc': 0.6666666865348816,\n",
      " 'ETH_test_f1': 0.650438666343689,\n",
      " 'LTC_test_acc': 0.6000000238418579,\n",
      " 'LTC_test_f1': 0.5315290093421936,\n",
      " 'test_loss': 0.9984196424484253}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 106737\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105254-324dhtza/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105254-324dhtza/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.80052\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.79897\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.81015\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.80465\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.82065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.81408\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.44469\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1098\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625903646\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.62576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.66484\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.75909\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.99383\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.78389\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.71875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.71955\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.78186\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.5338\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.55992\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.65044\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.53153\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.99842\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▂▂▂▃▃▃▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▂▂▃▃▃▄▅▅▅▅▆▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▁▁▂▂▂▃▃▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇█▇█▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▂▂▃▃▃▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇█▇█▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▂▃▂▂▃▃▃▄▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ██▇▇▇▇▇▇▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▂▃▁▃▆▆▃▁▁▃▃▅▂▃▆▃▅▇▃▅▆▇▅▇▅▆▇▇▆▅▆▃█▅██▃▅▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▂▂▁▂▅▅▂▁▁▂▃▄▃▃▆▃▅▇▃▅▆▇▅▇▄▆▇▇▆▅▆▃█▅██▃▅▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▄▃▄▄▅▁▁▁▅▆▆▆▅▇▆▆▇▆▆▇█▆▇▆▆▇▆▆▆▆▅▆▆▆▆▅▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▂▂▂▃▄▁▁▁▄▅▄▄▄▇▆▆▇▅▆▇█▆▇▅▆▇▆▅▅▅▄▆▆▅▅▄▅▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▃▁▃▄▄▄▃▅▆▅▅▆▆▅▇▆▅▇▆▇▇▇▇▇▇▇▇█▆▆▆▆▆▆▇▆▇█▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▂▁▃▂▂▂▂▅▅▅▅▆▆▄▇▇▅▇▇▇▇▇▇▇▇▇▇█▇▇▆▇▇▇▇▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▆▅▅▅▅▅▅▇█▃▃▃▃▃▂▃▃▁▂▂▂▁▁▂▂▃▁▂▃▃▄█▄▃▁▂▇▇▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▁▂▃▅▆▄▇▆▇▇▇█▆▆█▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▁▂▃▅▆▄▆▆▇▇▇█▆▆▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▃▄▄▄▅▅▆▆▅▆▆▇▆▇▆▆▇█▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▃▄▃▃▅▄▅▆▅▆▆▇▆▆▆▆▇█▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▃▂▃▃▄▃▆▆▄▆▆█▇▇▅█▆█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▂▂▂▂▄▂▆▆▄▅▆█▇▇▅█▆█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ███▇▆▅▅▄▃▄▃▃▁▃▃▃▂▂▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/324dhtza\u001b[0m\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:54:24.803545: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/2onkm2p4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105423-2onkm2p4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 109 K \n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.532     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 20.08it/s, loss=0.715, v_num=m2p4, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.699\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 19.60it/s, loss=0.715, v_num=m2p4, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 19.27it/s, loss=0.705, v_num=m2p4, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.038 >= min_delta = 0.003. New best score: 0.661\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 19.44it/s, loss=0.705, v_num=m2p4, BTC_val_\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 18.91it/s, loss=0.699, v_num=m2p4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 19/19 [00:00<00:00, 19.01it/s, loss=0.699, v_num=m2p4, BTC_val_\u001b[A\n",
      "Epoch 3:  95%|▉| 18/19 [00:01<00:00, 17.71it/s, loss=0.695, v_num=m2p4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.653\n",
      "Epoch 3: 100%|█| 19/19 [00:01<00:00, 17.75it/s, loss=0.695, v_num=m2p4, BTC_val_\n",
      "Epoch 4:  95%|▉| 18/19 [00:01<00:00, 16.81it/s, loss=0.694, v_num=m2p4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 19/19 [00:01<00:00, 16.93it/s, loss=0.694, v_num=m2p4, BTC_val_\u001b[A\n",
      "Epoch 5:  95%|▉| 18/19 [00:01<00:00, 17.68it/s, loss=0.691, v_num=m2p4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 19/19 [00:01<00:00, 17.80it/s, loss=0.691, v_num=m2p4, BTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 18/19 [00:01<00:00, 17.67it/s, loss=0.686, v_num=m2p4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.646\n",
      "Epoch 6: 100%|█| 19/19 [00:01<00:00, 17.61it/s, loss=0.686, v_num=m2p4, BTC_val_\n",
      "Epoch 7:  95%|▉| 18/19 [00:01<00:00, 17.40it/s, loss=0.681, v_num=m2p4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.022 >= min_delta = 0.003. New best score: 0.624\n",
      "Epoch 7: 100%|█| 19/19 [00:01<00:00, 17.44it/s, loss=0.681, v_num=m2p4, BTC_val_\n",
      "Epoch 8:  95%|▉| 18/19 [00:01<00:00, 17.31it/s, loss=0.684, v_num=m2p4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 19/19 [00:01<00:00, 17.39it/s, loss=0.684, v_num=m2p4, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 18.15it/s, loss=0.661, v_num=m2p4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:01<00:00, 18.17it/s, loss=0.661, v_num=m2p4, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:01<00:00, 17.54it/s, loss=0.646, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.031 >= min_delta = 0.003. New best score: 0.593\n",
      "Epoch 10: 100%|█| 19/19 [00:01<00:00, 17.56it/s, loss=0.646, v_num=m2p4, BTC_val\n",
      "Epoch 11:  95%|▉| 18/19 [00:01<00:00, 17.81it/s, loss=0.604, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:01<00:00, 17.86it/s, loss=0.604, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:01<00:00, 17.48it/s, loss=0.583, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:01<00:00, 17.60it/s, loss=0.583, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:01<00:00, 17.53it/s, loss=0.551, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.053 >= min_delta = 0.003. New best score: 0.539\n",
      "Epoch 13: 100%|█| 19/19 [00:01<00:00, 17.27it/s, loss=0.551, v_num=m2p4, BTC_val\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 18.28it/s, loss=0.513, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:01<00:00, 18.35it/s, loss=0.513, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:01<00:00, 17.38it/s, loss=0.505, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.042 >= min_delta = 0.003. New best score: 0.497\n",
      "Epoch 15: 100%|█| 19/19 [00:01<00:00, 17.52it/s, loss=0.505, v_num=m2p4, BTC_val\n",
      "Epoch 16:  95%|▉| 18/19 [00:01<00:00, 17.95it/s, loss=0.485, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:01<00:00, 17.95it/s, loss=0.485, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:01<00:00, 17.27it/s, loss=0.466, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.096 >= min_delta = 0.003. New best score: 0.401\n",
      "Epoch 17: 100%|█| 19/19 [00:01<00:00, 17.33it/s, loss=0.466, v_num=m2p4, BTC_val\n",
      "Epoch 18:  95%|▉| 18/19 [00:01<00:00, 17.81it/s, loss=0.46, v_num=m2p4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:01<00:00, 17.91it/s, loss=0.46, v_num=m2p4, BTC_val_\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:01<00:00, 17.94it/s, loss=0.461, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:01<00:00, 17.88it/s, loss=0.461, v_num=m2p4, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:  95%|▉| 18/19 [00:01<00:00, 17.92it/s, loss=0.463, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:01<00:00, 17.91it/s, loss=0.463, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:01<00:00, 17.47it/s, loss=0.45, v_num=m2p4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 0.398\n",
      "Epoch 21: 100%|█| 19/19 [00:01<00:00, 17.53it/s, loss=0.45, v_num=m2p4, BTC_val_\n",
      "Epoch 22:  95%|▉| 18/19 [00:01<00:00, 17.95it/s, loss=0.436, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:01<00:00, 18.00it/s, loss=0.436, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 18.24it/s, loss=0.434, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:01<00:00, 18.22it/s, loss=0.434, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 18.31it/s, loss=0.448, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:01<00:00, 18.37it/s, loss=0.448, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:01<00:00, 17.80it/s, loss=0.416, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:01<00:00, 17.86it/s, loss=0.416, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:01<00:00, 17.90it/s, loss=0.408, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:01<00:00, 18.05it/s, loss=0.408, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 18.09it/s, loss=0.403, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:01<00:00, 18.16it/s, loss=0.403, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:01<00:00, 17.84it/s, loss=0.405, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:01<00:00, 17.85it/s, loss=0.405, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:01<00:00, 17.65it/s, loss=0.398, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:01<00:00, 17.75it/s, loss=0.398, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 18.05it/s, loss=0.401, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:01<00:00, 18.10it/s, loss=0.401, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 18.51it/s, loss=0.388, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:01<00:00, 18.46it/s, loss=0.388, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:01<00:00, 17.68it/s, loss=0.364, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:01<00:00, 17.60it/s, loss=0.364, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 18/19 [00:01<00:00, 17.52it/s, loss=0.359, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:01<00:00, 17.58it/s, loss=0.359, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:01<00:00, 17.49it/s, loss=0.364, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:01<00:00, 17.56it/s, loss=0.364, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:01<00:00, 17.99it/s, loss=0.358, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:01<00:00, 17.93it/s, loss=0.358, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:01<00:00, 17.27it/s, loss=0.346, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 19/19 [00:01<00:00, 17.21it/s, loss=0.346, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 18/19 [00:01<00:00, 16.85it/s, loss=0.342, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:01<00:00, 16.93it/s, loss=0.342, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:01<00:00, 17.22it/s, loss=0.337, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 19/19 [00:01<00:00, 17.13it/s, loss=0.337, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 18/19 [00:01<00:00, 17.54it/s, loss=0.328, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 19/19 [00:01<00:00, 17.51it/s, loss=0.328, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 18/19 [00:00<00:00, 18.19it/s, loss=0.332, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 19/19 [00:01<00:00, 18.23it/s, loss=0.332, v_num=m2p4, BTC_val\u001b[A\n",
      "Epoch 41:  95%|▉| 18/19 [00:01<00:00, 17.84it/s, loss=0.321, v_num=m2p4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.398. Signaling Trainer to stop.\n",
      "Epoch 41: 100%|█| 19/19 [00:01<00:00, 18.02it/s, loss=0.321, v_num=m2p4, BTC_val\n",
      "Epoch 41: 100%|█| 19/19 [00:01<00:00, 17.96it/s, loss=0.321, v_num=m2p4, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 27.52it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.699999988079071,\n",
      " 'BTC_test_f1': 0.6914286017417908,\n",
      " 'ETH_test_acc': 0.6333333253860474,\n",
      " 'ETH_test_f1': 0.6296296119689941,\n",
      " 'LTC_test_acc': 0.800000011920929,\n",
      " 'LTC_test_f1': 0.7991071343421936,\n",
      " 'test_loss': 0.5401245355606079}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 107027\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105423-2onkm2p4/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105423-2onkm2p4/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.87752\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.87587\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.86439\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.86265\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.86177\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.86052\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.32197\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 41\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 756\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625903716\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.83333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.83333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.01565\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.89062\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.8906\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.84375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.84236\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.89062\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.8906\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.32433\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.69143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.63333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.62963\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.79911\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.54012\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▁▁▂▁▂▂▃▂▃▃▄▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▁▁▂▁▂▂▂▂▃▃▄▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▁▂▂▂▂▂▂▂▃▃▄▄▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▁▂▂▂▁▂▂▂▃▃▄▄▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇██▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▁▂▂▂▂▂▂▂▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▁▂▂▂▂▂▂▂▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ██████▇▇▇▇▇▆▆▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▂▄▄▄▁▄▄▄▄▄▄▄▄▄▇█▄█▅▅▇█▇▅▅▄▅▄▇▄▅▅▇▄▅▅▅▅▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▃▂▂▂▁▂▂▂▂▂▂▂▂▂▆█▄█▄▅▇█▇▅▅▄▄▄▇▄▄▄▇▂▄▄▄▄▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▆▆▆▄▅▆▆▆▁▆▆▆█▂█▄█▇▇██▅▇▇▇█▇▆▇█▇▇▇███▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▃▃▃▃▅▃▃▃▁▃▃▃█▂█▅█▆▇██▆▇▇▇█▇▆▇█▆▇▆███▆█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▄█▄████████████▄█▄█████▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▁▁▁▁▁▁▁▁▄▁▁▁▁▅▅▆█▅████████████▅█▅█████▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄▄▄▄▄▄▄▄▄▅▃▅▃▃▅▂▃▁▂▂▁▁▂▂▂▁▂▂▂▂▄▅▂▆▃▁▃▆▃█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▁▃▃▄▅▆▆▆▄▇█▇▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▁▁▃▄▅▆▆▆▄▇█▇▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▂▃▂▅▆▆▇▆▆▇█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▂▃▂▅▆▆▇▆▆▇█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▁▁▅▆▆▅█▆▆███▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▁▁▅▆▆▅█▆▆███▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ██▇▇▅▅▄▂▄▄▃▁▁▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/2onkm2p4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:55:34.265762: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/3k5l2m63\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105532-3k5l2m63\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 109 K \n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "382 K     Trainable params\n",
      "0         Non-trainable params\n",
      "382 K     Total params\n",
      "1.532     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 19.37it/s, loss=1.15, v_num=2m63, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.118\n",
      "Epoch 0: 100%|█| 19/19 [00:01<00:00, 18.89it/s, loss=1.15, v_num=2m63, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:01<00:00, 17.50it/s, loss=1.11, v_num=2m63, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.037 >= min_delta = 0.003. New best score: 1.080\n",
      "Epoch 1: 100%|█| 19/19 [00:01<00:00, 17.62it/s, loss=1.11, v_num=2m63, BTC_val_a\n",
      "Epoch 2:  95%|▉| 18/19 [00:01<00:00, 17.05it/s, loss=1.1, v_num=2m63, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 1.071\n",
      "Epoch 2: 100%|█| 19/19 [00:01<00:00, 17.17it/s, loss=1.1, v_num=2m63, BTC_val_ac\n",
      "Epoch 3:  95%|▉| 18/19 [00:01<00:00, 17.43it/s, loss=1.09, v_num=2m63, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 19/19 [00:01<00:00, 17.26it/s, loss=1.09, v_num=2m63, BTC_val_a\u001b[A\n",
      "Epoch 4:  95%|▉| 18/19 [00:01<00:00, 16.43it/s, loss=1.08, v_num=2m63, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 19/19 [00:01<00:00, 16.58it/s, loss=1.08, v_num=2m63, BTC_val_a\u001b[A\n",
      "Epoch 5:  95%|▉| 18/19 [00:01<00:00, 17.69it/s, loss=1.08, v_num=2m63, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 1.067\n",
      "Epoch 5: 100%|█| 19/19 [00:01<00:00, 17.78it/s, loss=1.08, v_num=2m63, BTC_val_a\n",
      "Epoch 6:  95%|▉| 18/19 [00:01<00:00, 17.80it/s, loss=1.08, v_num=2m63, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 19/19 [00:01<00:00, 17.93it/s, loss=1.08, v_num=2m63, BTC_val_a\u001b[A\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 18.19it/s, loss=1.07, v_num=2m63, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 19/19 [00:01<00:00, 18.40it/s, loss=1.07, v_num=2m63, BTC_val_a\u001b[A\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 18.37it/s, loss=1.07, v_num=2m63, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 19/19 [00:01<00:00, 18.49it/s, loss=1.07, v_num=2m63, BTC_val_a\u001b[A\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 18.22it/s, loss=1.05, v_num=2m63, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:01<00:00, 18.27it/s, loss=1.05, v_num=2m63, BTC_val_a\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 18.19it/s, loss=1.04, v_num=2m63, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:01<00:00, 18.28it/s, loss=1.04, v_num=2m63, BTC_val_\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 18.20it/s, loss=1.02, v_num=2m63, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:01<00:00, 18.22it/s, loss=1.02, v_num=2m63, BTC_val_\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:01<00:00, 16.74it/s, loss=0.993, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:01<00:00, 16.69it/s, loss=0.993, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:01<00:00, 17.55it/s, loss=0.935, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:01<00:00, 17.67it/s, loss=0.935, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 18.29it/s, loss=0.895, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.175 >= min_delta = 0.003. New best score: 0.893\n",
      "Epoch 14: 100%|█| 19/19 [00:01<00:00, 18.24it/s, loss=0.895, v_num=2m63, BTC_val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 18.26it/s, loss=0.855, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:01<00:00, 18.33it/s, loss=0.855, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:01<00:00, 17.10it/s, loss=0.816, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:01<00:00, 17.12it/s, loss=0.816, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:01<00:00, 17.35it/s, loss=0.814, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:01<00:00, 17.44it/s, loss=0.814, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:01<00:00, 17.96it/s, loss=0.788, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:01<00:00, 17.94it/s, loss=0.788, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:01<00:00, 17.25it/s, loss=0.783, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.075 >= min_delta = 0.003. New best score: 0.817\n",
      "Epoch 19: 100%|█| 19/19 [00:01<00:00, 17.21it/s, loss=0.783, v_num=2m63, BTC_val\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 18.23it/s, loss=0.77, v_num=2m63, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:01<00:00, 18.23it/s, loss=0.77, v_num=2m63, BTC_val_\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:01<00:00, 17.82it/s, loss=0.767, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.025 >= min_delta = 0.003. New best score: 0.792\n",
      "Epoch 21: 100%|█| 19/19 [00:01<00:00, 17.93it/s, loss=0.767, v_num=2m63, BTC_val\n",
      "Epoch 22:  95%|▉| 18/19 [00:01<00:00, 17.69it/s, loss=0.745, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:01<00:00, 17.65it/s, loss=0.745, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:01<00:00, 17.49it/s, loss=0.725, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:01<00:00, 17.62it/s, loss=0.725, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 18.84it/s, loss=0.716, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:01<00:00, 18.84it/s, loss=0.716, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:01<00:00, 17.69it/s, loss=0.707, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.086 >= min_delta = 0.003. New best score: 0.706\n",
      "Epoch 25: 100%|█| 19/19 [00:01<00:00, 17.87it/s, loss=0.707, v_num=2m63, BTC_val\n",
      "Epoch 26:  95%|▉| 18/19 [00:00<00:00, 18.12it/s, loss=0.742, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:01<00:00, 18.21it/s, loss=0.742, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:01<00:00, 17.46it/s, loss=0.708, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 0.702\n",
      "Epoch 27: 100%|█| 19/19 [00:01<00:00, 17.56it/s, loss=0.708, v_num=2m63, BTC_val\n",
      "Epoch 28:  95%|▉| 18/19 [00:01<00:00, 17.41it/s, loss=0.687, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:01<00:00, 17.29it/s, loss=0.687, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 18.22it/s, loss=0.67, v_num=2m63, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:01<00:00, 18.19it/s, loss=0.67, v_num=2m63, BTC_val_\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:01<00:00, 17.80it/s, loss=0.673, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:01<00:00, 17.85it/s, loss=0.673, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:01<00:00, 17.49it/s, loss=0.663, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:01<00:00, 17.56it/s, loss=0.663, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:00<00:00, 18.20it/s, loss=0.677, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.029 >= min_delta = 0.003. New best score: 0.674\n",
      "Epoch 32: 100%|█| 19/19 [00:01<00:00, 18.28it/s, loss=0.677, v_num=2m63, BTC_val\n",
      "Epoch 33:  95%|▉| 18/19 [00:01<00:00, 17.17it/s, loss=0.642, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:01<00:00, 17.24it/s, loss=0.642, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:01<00:00, 17.88it/s, loss=0.641, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:01<00:00, 17.74it/s, loss=0.641, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:01<00:00, 17.56it/s, loss=0.623, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:01<00:00, 17.61it/s, loss=0.623, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:01<00:00, 16.83it/s, loss=0.637, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 19/19 [00:01<00:00, 16.80it/s, loss=0.637, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 18/19 [00:01<00:00, 17.52it/s, loss=0.617, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:01<00:00, 17.59it/s, loss=0.617, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:01<00:00, 17.41it/s, loss=0.612, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 19/19 [00:01<00:00, 17.55it/s, loss=0.612, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 18/19 [00:01<00:00, 17.48it/s, loss=0.585, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 19/19 [00:01<00:00, 17.55it/s, loss=0.585, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 18/19 [00:01<00:00, 17.19it/s, loss=0.58, v_num=2m63, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 19/19 [00:01<00:00, 17.07it/s, loss=0.58, v_num=2m63, BTC_val_\u001b[A\n",
      "Epoch 41:  95%|▉| 18/19 [00:01<00:00, 17.92it/s, loss=0.576, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 19/19 [00:01<00:00, 18.08it/s, loss=0.576, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 18/19 [00:01<00:00, 17.64it/s, loss=0.591, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 19/19 [00:01<00:00, 17.46it/s, loss=0.591, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 18/19 [00:01<00:00, 17.62it/s, loss=0.574, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 19/19 [00:01<00:00, 17.70it/s, loss=0.574, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 44:  95%|▉| 18/19 [00:01<00:00, 17.76it/s, loss=0.565, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 19/19 [00:01<00:00, 17.92it/s, loss=0.565, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 45:  95%|▉| 18/19 [00:01<00:00, 17.88it/s, loss=0.542, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 19/19 [00:01<00:00, 17.98it/s, loss=0.542, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 18/19 [00:00<00:00, 18.00it/s, loss=0.542, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 19/19 [00:01<00:00, 18.11it/s, loss=0.542, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 18/19 [00:01<00:00, 17.91it/s, loss=0.559, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 19/19 [00:01<00:00, 17.95it/s, loss=0.559, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 48:  95%|▉| 18/19 [00:01<00:00, 17.98it/s, loss=0.529, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 19/19 [00:01<00:00, 17.98it/s, loss=0.529, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 49:  95%|▉| 18/19 [00:01<00:00, 17.64it/s, loss=0.524, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 19/19 [00:01<00:00, 17.73it/s, loss=0.524, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 50:  95%|▉| 18/19 [00:01<00:00, 17.92it/s, loss=0.508, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 19/19 [00:01<00:00, 17.99it/s, loss=0.508, v_num=2m63, BTC_val\u001b[A\n",
      "Epoch 51:  95%|▉| 18/19 [00:01<00:00, 17.86it/s, loss=0.49, v_num=2m63, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 19/19 [00:01<00:00, 17.68it/s, loss=0.49, v_num=2m63, BTC_val_\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52:  95%|▉| 18/19 [00:00<00:00, 18.43it/s, loss=0.498, v_num=2m63, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.674. Signaling Trainer to stop.\n",
      "Epoch 52: 100%|█| 19/19 [00:01<00:00, 18.29it/s, loss=0.498, v_num=2m63, BTC_val\n",
      "Epoch 52: 100%|█| 19/19 [00:01<00:00, 18.17it/s, loss=0.498, v_num=2m63, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 29.01it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6333333253860474,\n",
      " 'BTC_test_f1': 0.6212454438209534,\n",
      " 'ETH_test_acc': 0.6000000238418579,\n",
      " 'ETH_test_f1': 0.5736842155456543,\n",
      " 'LTC_test_acc': 0.6000000238418579,\n",
      " 'LTC_test_f1': 0.5555555820465088,\n",
      " 'test_loss': 0.9131283760070801}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 107317\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105532-3k5l2m63/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105532-3k5l2m63/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.8049\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.80085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.78128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.77648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.78915\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.78293\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.50387\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 954\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625903797\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.41667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.34444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.58333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.61515\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.41043\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.89062\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.88501\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.76562\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.76035\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.79688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.77716\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.43096\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.63333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.62125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.57368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.91313\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▂▂▃▃▃▃▃▄▅▅▆▆▆▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇▇█▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▃▃▃▃▃▄▄▅▅▅▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇█▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▁▂▂▂▂▂▂▃▃▄▄▅▅▆▆▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▂▂▃▃▃▃▃▄▅▅▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇█▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▁▁▂▂▂▂▂▃▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▂▂▃▂▂▃▃▄▄▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch ██▇▇▇▇▇▇▇▆▆▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▄▄▅▄▅▂▃▂▂▂▅▁▂▃▄▅▄▅▇▇█▇▇▇▅▄▇▄▅▄█▄▄▇▅▇▇▇▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▂▂▄▂▄▁▃▁▁▁▄▁▂▃▂▅▄▅▇▇█▇▇▇▄▄▇▄▅▄█▄▄▇▅▇▇▇▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▄▄▄▁▄▃▁▄▃▁▃▅▄▄▅▅▆▆▆▇█▇▇▆▇▆▅▇▅▆▆▇▆▆▇▇▇▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▂▂▂▂▂▃▁▂▂▁▃▄▄▄▅▄▆▅▆▇█▇▇▅▇▅▄▇▄▅▅▇▅▅▇▇▇▆▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▃▃▄▄▄▄▄▄▁▅▄▅▆█▅▇▆▇▆█▇▇▆▇▆▄▆▅▆▆▅▅▅▅▅▅▄▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▂▂▂▂▂▂▂▂▁▅▃▄▇█▄▇▇▇▇█▇▇▇▇▇▃▇▅▆▇▆▅▅▆▅▆▄▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▅▅▅▅▅▅▅▅▅▅▃▄▄▃▅▂▂▂▁▂▁▂▂▁▃▅▁▇▃▄▃▄▆▂▄▃▄▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▁▂▂▃▅▇▆▅▅▆▇▆▇▇▆▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▂▁▂▁▃▅▇▆▅▅▆▇▆▇▇▆▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▃▂▃▃▄▄▇▇▆▇▅▅▇▇▇█▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▂▂▃▃▄▄▇▆▆▆▆▅▆▆▇█▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▂▃▂▃▆▇▆▇▆▇▆█▆▆▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▂▃▁▃▆▇▆▇▆▇▆█▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ██▇▇▆▅▄▄▄▄▃▄▃▃▂▂▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/3k5l2m63\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:56:54.204530: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/hfcqsepm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105652-hfcqsepm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 20.47it/s, loss=0.708, v_num=sepm, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.733\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 19.96it/s, loss=0.708, v_num=sepm, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 20.32it/s, loss=0.674, v_num=sepm, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.154 >= min_delta = 0.003. New best score: 0.579\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 20.39it/s, loss=0.674, v_num=sepm, BTC_val_\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 18.51it/s, loss=0.628, v_num=sepm, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.056 >= min_delta = 0.003. New best score: 0.523\n",
      "Epoch 2: 100%|█| 19/19 [00:01<00:00, 18.52it/s, loss=0.628, v_num=sepm, BTC_val_\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 18.55it/s, loss=0.586, v_num=sepm, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 19/19 [00:01<00:00, 18.67it/s, loss=0.586, v_num=sepm, BTC_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 18/19 [00:00<00:00, 18.63it/s, loss=0.564, v_num=sepm, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.025 >= min_delta = 0.003. New best score: 0.498\n",
      "Epoch 4: 100%|█| 19/19 [00:01<00:00, 18.58it/s, loss=0.564, v_num=sepm, BTC_val_\n",
      "Epoch 5:  95%|▉| 18/19 [00:01<00:00, 17.37it/s, loss=0.544, v_num=sepm, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.044 >= min_delta = 0.003. New best score: 0.453\n",
      "Epoch 5: 100%|█| 19/19 [00:01<00:00, 17.35it/s, loss=0.544, v_num=sepm, BTC_val_\n",
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 19.44it/s, loss=0.525, v_num=sepm, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.048 >= min_delta = 0.003. New best score: 0.405\n",
      "Epoch 6: 100%|█| 19/19 [00:00<00:00, 19.41it/s, loss=0.525, v_num=sepm, BTC_val_\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 19.11it/s, loss=0.504, v_num=sepm, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 19/19 [00:00<00:00, 19.04it/s, loss=0.504, v_num=sepm, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 18.11it/s, loss=0.514, v_num=sepm, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 19/19 [00:01<00:00, 18.15it/s, loss=0.514, v_num=sepm, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 18.19it/s, loss=0.493, v_num=sepm, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:01<00:00, 18.04it/s, loss=0.493, v_num=sepm, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 19.33it/s, loss=0.486, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:01<00:00, 18.98it/s, loss=0.486, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:01<00:00, 17.92it/s, loss=0.483, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:01<00:00, 17.87it/s, loss=0.483, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 19.12it/s, loss=0.476, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:01<00:00, 18.93it/s, loss=0.476, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 19.07it/s, loss=0.476, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:01<00:00, 18.89it/s, loss=0.476, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 19.12it/s, loss=0.484, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:01<00:00, 18.97it/s, loss=0.484, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 19.66it/s, loss=0.488, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:00<00:00, 19.62it/s, loss=0.488, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 18.95it/s, loss=0.474, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.401\n",
      "Epoch 16: 100%|█| 19/19 [00:00<00:00, 19.03it/s, loss=0.474, v_num=sepm, BTC_val\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 18.58it/s, loss=0.474, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:01<00:00, 18.43it/s, loss=0.474, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 19.44it/s, loss=0.474, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:00<00:00, 19.44it/s, loss=0.474, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 18.43it/s, loss=0.468, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:01<00:00, 18.54it/s, loss=0.468, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 18.50it/s, loss=0.475, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:01<00:00, 18.35it/s, loss=0.475, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 18.75it/s, loss=0.47, v_num=sepm, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:01<00:00, 18.57it/s, loss=0.47, v_num=sepm, BTC_val_\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:01<00:00, 17.86it/s, loss=0.469, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:01<00:00, 17.94it/s, loss=0.469, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 19.93it/s, loss=0.458, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:00<00:00, 19.63it/s, loss=0.458, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 19.13it/s, loss=0.456, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:01<00:00, 18.82it/s, loss=0.456, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 19.38it/s, loss=0.458, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:00<00:00, 19.09it/s, loss=0.458, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:00<00:00, 18.25it/s, loss=0.447, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:01<00:00, 18.35it/s, loss=0.447, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 19.85it/s, loss=0.451, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:00<00:00, 19.57it/s, loss=0.451, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 19.00it/s, loss=0.46, v_num=sepm, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:01<00:00, 18.93it/s, loss=0.46, v_num=sepm, BTC_val_\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 19.77it/s, loss=0.457, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:00<00:00, 19.83it/s, loss=0.457, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 18.33it/s, loss=0.464, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:01<00:00, 18.23it/s, loss=0.464, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 18.89it/s, loss=0.453, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:01<00:00, 18.78it/s, loss=0.453, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:00<00:00, 19.35it/s, loss=0.459, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:00<00:00, 19.14it/s, loss=0.459, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 18/19 [00:00<00:00, 18.99it/s, loss=0.465, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:00<00:00, 19.00it/s, loss=0.465, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:00<00:00, 19.66it/s, loss=0.455, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:00<00:00, 19.69it/s, loss=0.455, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:00<00:00, 19.07it/s, loss=0.456, v_num=sepm, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:00<00:00, 19.13it/s, loss=0.456, v_num=sepm, BTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:00<00:00, 18.67it/s, loss=0.46, v_num=sepm, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.401. Signaling Trainer to stop.\n",
      "Epoch 36: 100%|█| 19/19 [00:01<00:00, 18.62it/s, loss=0.46, v_num=sepm, BTC_val_\n",
      "Epoch 36: 100%|█| 19/19 [00:01<00:00, 18.55it/s, loss=0.46, v_num=sepm, BTC_val_\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 23.83it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6666666865348816,\n",
      " 'BTC_test_f1': 0.6606335043907166,\n",
      " 'ETH_test_acc': 0.7666666507720947,\n",
      " 'ETH_test_f1': 0.7532315254211426,\n",
      " 'LTC_test_acc': 0.8333333134651184,\n",
      " 'LTC_test_f1': 0.8316497802734375,\n",
      " 'test_loss': 0.48255348205566406}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 107529\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105652-hfcqsepm/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105652-hfcqsepm/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.77865\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.77464\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.79528\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.79252\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.78478\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.78223\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.45873\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 36\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 666\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 46\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625903858\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 87\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.74825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.91667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.87368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.91667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.87368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.43579\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.78039\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.82812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.82466\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.76562\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.76511\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.47136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.66063\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.76667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.75323\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.83333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.83165\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.48255\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▃▄▅▆▆▇▇▇▇▇▇███▇▇▇▇▇█▇███████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▃▄▆▆▆▇▇▇▇█████▇▇█▇██▇███████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▃▄▅▆▆▇▇▇▇███▇█▇█████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▃▄▅▆▆▇▇▇▇███▇█▇█████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▃▄▆▆▆▇▇▇▇▇▇█▇▇▇█▇▇█▇▇███████▇▇██▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▃▅▆▆▆▇▇▇▇▇▇█▇▇▇██▇█▇▇████████▇██▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▃█▆███▆████▆██▃█▆█▆██▆▆▆█▆▃██▆█▆█▃██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▅█▆███▆████▇██▅█▇█▇██▇▇▇█▇▅██▆█▇█▅██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▇▇▆▇██▅███████▇███████████▇██▇███▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▇▇▅▇██▅███████▇███████████▇██▇███▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁█▇▇▇▇▇▄▇▇▇█▇▇█▅▇▇▇▇▇█▇▇█▇▇▇██▄███▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁█▇▇▇▇▇▅▇▇▇█▇▇█▅▇▇▇▇▇█▇▇█▇▇▇██▅███▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▅▄▄▃▂▁▅▂▁▂▁▂▁▁▂▁▂▂▂▃▁▂▁▂▁▁▂▁▂▃▂▁▂▂▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▂▂▃▅▅▆▁▂▄█▅▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▂▂▃▅▅▆▁▂▄█▄▃▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▂▅▃▄▅█▁▃▄▄▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▂▅▄▄▅█▁▃▄▄▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▂▁▁▅▁▂▅▅▅▂▅█▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▂▁▁▇▁▂▅▅▆▂▅█▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▆▅▄▅▁▇▂▄▃▁▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/hfcqsepm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:57:54.091637: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/3cfj2fz4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105752-3cfj2fz4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 20.09it/s, loss=1.11, v_num=2fz4, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.085\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 19.57it/s, loss=1.11, v_num=2fz4, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 19.73it/s, loss=1.06, v_num=2fz4, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.095 >= min_delta = 0.003. New best score: 0.990\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 19.65it/s, loss=1.06, v_num=2fz4, BTC_val_a\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 18.42it/s, loss=1.01, v_num=2fz4, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.024 >= min_delta = 0.003. New best score: 0.966\n",
      "Epoch 2: 100%|█| 19/19 [00:01<00:00, 18.31it/s, loss=1.01, v_num=2fz4, BTC_val_a\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 18.45it/s, loss=0.97, v_num=2fz4, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 19/19 [00:01<00:00, 18.56it/s, loss=0.97, v_num=2fz4, BTC_val_a\u001b[A\n",
      "Epoch 4:  95%|▉| 18/19 [00:00<00:00, 18.18it/s, loss=0.913, v_num=2fz4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.079 >= min_delta = 0.003. New best score: 0.887\n",
      "Epoch 4: 100%|█| 19/19 [00:01<00:00, 18.11it/s, loss=0.913, v_num=2fz4, BTC_val_\n",
      "Epoch 5:  95%|▉| 18/19 [00:01<00:00, 16.62it/s, loss=0.882, v_num=2fz4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.047 >= min_delta = 0.003. New best score: 0.840\n",
      "Epoch 5: 100%|█| 19/19 [00:01<00:00, 16.77it/s, loss=0.882, v_num=2fz4, BTC_val_\n",
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 19.26it/s, loss=0.849, v_num=2fz4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.125 >= min_delta = 0.003. New best score: 0.715\n",
      "Epoch 6: 100%|█| 19/19 [00:00<00:00, 19.10it/s, loss=0.849, v_num=2fz4, BTC_val_\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 18.96it/s, loss=0.819, v_num=2fz4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 19/19 [00:00<00:00, 19.02it/s, loss=0.819, v_num=2fz4, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 19.37it/s, loss=0.827, v_num=2fz4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 19/19 [00:00<00:00, 19.32it/s, loss=0.827, v_num=2fz4, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 19.12it/s, loss=0.817, v_num=2fz4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:01<00:00, 18.95it/s, loss=0.817, v_num=2fz4, BTC_val_\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 19.08it/s, loss=0.809, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:00<00:00, 19.18it/s, loss=0.809, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 18.96it/s, loss=0.801, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:00<00:00, 19.01it/s, loss=0.801, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 19.70it/s, loss=0.782, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:00<00:00, 19.54it/s, loss=0.782, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 20.00it/s, loss=0.798, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:00<00:00, 20.07it/s, loss=0.798, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 18.89it/s, loss=0.805, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:01<00:00, 18.99it/s, loss=0.805, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 19.25it/s, loss=0.787, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:00<00:00, 19.22it/s, loss=0.787, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 20.00it/s, loss=0.794, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:00<00:00, 20.13it/s, loss=0.794, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 19.34it/s, loss=0.784, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:00<00:00, 19.35it/s, loss=0.784, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 18.95it/s, loss=0.792, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:00<00:00, 19.09it/s, loss=0.792, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 19.50it/s, loss=0.776, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:00<00:00, 19.37it/s, loss=0.776, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 19.84it/s, loss=0.76, v_num=2fz4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.020 >= min_delta = 0.003. New best score: 0.694\n",
      "Epoch 20: 100%|█| 19/19 [00:00<00:00, 19.88it/s, loss=0.76, v_num=2fz4, BTC_val_\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 19.48it/s, loss=0.765, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:00<00:00, 19.47it/s, loss=0.765, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:00<00:00, 18.70it/s, loss=0.758, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:01<00:00, 18.71it/s, loss=0.758, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 20.14it/s, loss=0.768, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:00<00:00, 20.11it/s, loss=0.768, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 19.74it/s, loss=0.756, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:00<00:00, 19.85it/s, loss=0.756, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 19.44it/s, loss=0.758, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:00<00:00, 19.48it/s, loss=0.758, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:00<00:00, 19.08it/s, loss=0.773, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:00<00:00, 19.04it/s, loss=0.773, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 19.54it/s, loss=0.757, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:00<00:00, 19.35it/s, loss=0.757, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 19.54it/s, loss=0.767, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:00<00:00, 19.33it/s, loss=0.767, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 19.89it/s, loss=0.755, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:00<00:00, 19.97it/s, loss=0.755, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 18.52it/s, loss=0.757, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:01<00:00, 18.38it/s, loss=0.757, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 18.88it/s, loss=0.761, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:01<00:00, 18.62it/s, loss=0.761, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:00<00:00, 18.22it/s, loss=0.757, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:01<00:00, 18.17it/s, loss=0.757, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 18/19 [00:00<00:00, 18.79it/s, loss=0.751, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:01<00:00, 18.89it/s, loss=0.751, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:00<00:00, 18.53it/s, loss=0.755, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:01<00:00, 18.58it/s, loss=0.755, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:00<00:00, 18.98it/s, loss=0.751, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 19/19 [00:01<00:00, 18.94it/s, loss=0.751, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 36:  95%|▉| 18/19 [00:00<00:00, 18.12it/s, loss=0.755, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 19/19 [00:01<00:00, 18.12it/s, loss=0.755, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 18/19 [00:00<00:00, 18.50it/s, loss=0.75, v_num=2fz4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:01<00:00, 18.52it/s, loss=0.75, v_num=2fz4, BTC_val_\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:00<00:00, 18.52it/s, loss=0.724, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 19/19 [00:01<00:00, 18.51it/s, loss=0.724, v_num=2fz4, BTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 18/19 [00:00<00:00, 18.19it/s, loss=0.74, v_num=2fz4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 19/19 [00:01<00:00, 18.29it/s, loss=0.74, v_num=2fz4, BTC_val_\u001b[A\n",
      "Epoch 40:  95%|▉| 18/19 [00:00<00:00, 18.92it/s, loss=0.735, v_num=2fz4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.694. Signaling Trainer to stop.\n",
      "Epoch 40: 100%|█| 19/19 [00:01<00:00, 18.95it/s, loss=0.735, v_num=2fz4, BTC_val\n",
      "Epoch 40: 100%|█| 19/19 [00:01<00:00, 18.87it/s, loss=0.735, v_num=2fz4, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 27.13it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6666666865348816,\n",
      " 'BTC_test_f1': 0.6566047668457031,\n",
      " 'ETH_test_acc': 0.7333333492279053,\n",
      " 'ETH_test_f1': 0.7246499061584473,\n",
      " 'LTC_test_acc': 0.6333333253860474,\n",
      " 'LTC_test_f1': 0.47083336114883423,\n",
      " 'test_loss': 0.7550081014633179}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 107723\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105752-3cfj2fz4/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105752-3cfj2fz4/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.67804\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.67257\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.65967\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.64952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.65792\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.64958\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.7364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 738\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625903922\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.41667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.41667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.83333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.84127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.68889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.78121\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.70312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.70424\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.76562\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.76667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.73438\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.71884\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.61036\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.6566\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.73333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.72465\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.63333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.47083\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.75501\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▄▅▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████▇█▇▇█▇▇█▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▃▄▅▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████▇█▇▇█▇▇█▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▃▄▅▆▅▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇█▇▇█▇▇▇████▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▃▄▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇█▇▇▇▇▇▇████▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▃▄▅▆▆▇▇▇▇▇▇█▇▇▇▇▇▇███▇███▇████▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▃▄▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇███▇▇███▇████▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▂▅▅▂▇█▇▅█▇▅▆▇▅▃▃▃▁▅▂▇▇▃▃▅▃▅▃▂▂▃▃▃▃▅▃▅▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▅▅▃▅██▅█▇▅▆▇▅▄▄▄▂▄▃█▇▄▄▄▄▅▄▃▃▄▄▄▄▅▄▅▃▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▄▂▂▃▄▇▅▅▅▆▅▅█▆▆▆▇▅▆▅▅▆▆▅▆▅▆▇▆█▅▆▅▅▇▄▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▅▃▃▄▄▇▆▅▅▆▅▅█▆▆▆▇▅▆▅▅▆▆▅▆▅▆▇▆█▅▆▅▅▇▄▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▂▄▅▂▂██▄▄▇▄▄▇▅▇▅▄▄▄▄▄██▄█▄▅█▄█▄█▄▄▇▄▄█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▅▆▆▃▃██▄▄▇▄▄▇▆▇▆▅▄▄▅▄██▄█▄▆█▅█▄█▄▄▇▄▄█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▆▆▆▄▄▁▃▂▂▂▂▃▂▂▂▂▄▃▂▁▃▃▂▂▃▂▂▄▁▃▂▃▁▂▂▂▁▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▂▂▃▄▁▄▅▅█▆▆▄▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▂▃▄▁▄▅▅█▆▆▄▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▂▄▂▃▁▆▅▃▆▆▃█▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▄▂▄▁▆▅▃▆▅▃█▄█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▅▃▅▄▃▅▅▆▆▆█▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▅▃▅▄▄▅▅▆▆▇█▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▅▇▆█▄▃▅▃▂▃▁▅▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/3cfj2fz4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 10:59:20.723800: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_binary_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/26vxq76q\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105855-26vxq76q\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 130   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:00<00:00, 19.05it/s, loss=0.702, v_num=q76q, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.649\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 19.17it/s, loss=0.702, v_num=q76q, BTC_val_\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 19.75it/s, loss=0.673, v_num=q76q, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.089 >= min_delta = 0.003. New best score: 0.560\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 19.80it/s, loss=0.673, v_num=q76q, BTC_val_\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 18.70it/s, loss=0.623, v_num=q76q, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.052 >= min_delta = 0.003. New best score: 0.508\n",
      "Epoch 2: 100%|█| 19/19 [00:01<00:00, 18.56it/s, loss=0.623, v_num=q76q, BTC_val_\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 19.10it/s, loss=0.59, v_num=q76q, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 19/19 [00:00<00:00, 19.11it/s, loss=0.59, v_num=q76q, BTC_val_a\u001b[A\n",
      "Epoch 4:  95%|▉| 18/19 [00:00<00:00, 18.99it/s, loss=0.56, v_num=q76q, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.060 >= min_delta = 0.003. New best score: 0.448\n",
      "Epoch 4: 100%|█| 19/19 [00:01<00:00, 18.93it/s, loss=0.56, v_num=q76q, BTC_val_a\n",
      "Epoch 5:  95%|▉| 18/19 [00:00<00:00, 18.10it/s, loss=0.539, v_num=q76q, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 19/19 [00:01<00:00, 18.19it/s, loss=0.539, v_num=q76q, BTC_val_\u001b[A\n",
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 19.04it/s, loss=0.518, v_num=q76q, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 0.442\n",
      "Epoch 6: 100%|█| 19/19 [00:01<00:00, 18.90it/s, loss=0.518, v_num=q76q, BTC_val_\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 19.48it/s, loss=0.517, v_num=q76q, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.016 >= min_delta = 0.003. New best score: 0.426\n",
      "Epoch 7: 100%|█| 19/19 [00:00<00:00, 19.36it/s, loss=0.517, v_num=q76q, BTC_val_\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 18.96it/s, loss=0.498, v_num=q76q, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 19/19 [00:01<00:00, 18.98it/s, loss=0.498, v_num=q76q, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 18.67it/s, loss=0.509, v_num=q76q, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.019 >= min_delta = 0.003. New best score: 0.407\n",
      "Epoch 9: 100%|█| 19/19 [00:01<00:00, 18.81it/s, loss=0.509, v_num=q76q, BTC_val_\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 18.31it/s, loss=0.487, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:01<00:00, 18.30it/s, loss=0.487, v_num=q76q, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 18.48it/s, loss=0.484, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:01<00:00, 18.54it/s, loss=0.484, v_num=q76q, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 18.68it/s, loss=0.484, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.022 >= min_delta = 0.003. New best score: 0.385\n",
      "Epoch 12: 100%|█| 19/19 [00:01<00:00, 18.61it/s, loss=0.484, v_num=q76q, BTC_val\n",
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 18.90it/s, loss=0.474, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:01<00:00, 18.69it/s, loss=0.474, v_num=q76q, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 18.11it/s, loss=0.478, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 19/19 [00:01<00:00, 18.26it/s, loss=0.478, v_num=q76q, BTC_val\u001b[A\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 18.81it/s, loss=0.477, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:01<00:00, 18.78it/s, loss=0.477, v_num=q76q, BTC_val\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 18.40it/s, loss=0.468, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:01<00:00, 18.58it/s, loss=0.468, v_num=q76q, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 18.71it/s, loss=0.474, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:01<00:00, 18.65it/s, loss=0.474, v_num=q76q, BTC_val\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 18.61it/s, loss=0.465, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:01<00:00, 18.69it/s, loss=0.465, v_num=q76q, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 18.80it/s, loss=0.486, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:01<00:00, 18.88it/s, loss=0.486, v_num=q76q, BTC_val\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 18.55it/s, loss=0.466, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:01<00:00, 18.63it/s, loss=0.466, v_num=q76q, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 19.05it/s, loss=0.455, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:00<00:00, 19.00it/s, loss=0.455, v_num=q76q, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:00<00:00, 18.85it/s, loss=0.468, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:01<00:00, 18.80it/s, loss=0.468, v_num=q76q, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 18.59it/s, loss=0.474, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:01<00:00, 18.59it/s, loss=0.474, v_num=q76q, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 19.21it/s, loss=0.469, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:00<00:00, 19.24it/s, loss=0.469, v_num=q76q, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 18.63it/s, loss=0.461, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 19/19 [00:01<00:00, 18.69it/s, loss=0.461, v_num=q76q, BTC_val\u001b[A\n",
      "Epoch 26:  95%|▉| 18/19 [00:00<00:00, 19.54it/s, loss=0.46, v_num=q76q, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:00<00:00, 19.48it/s, loss=0.46, v_num=q76q, BTC_val_\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 19.17it/s, loss=0.461, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:00<00:00, 19.01it/s, loss=0.461, v_num=q76q, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 18.42it/s, loss=0.461, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:01<00:00, 18.56it/s, loss=0.461, v_num=q76q, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 18.34it/s, loss=0.454, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:01<00:00, 18.25it/s, loss=0.454, v_num=q76q, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 18.71it/s, loss=0.454, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:01<00:00, 18.74it/s, loss=0.454, v_num=q76q, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 19.01it/s, loss=0.463, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:00<00:00, 19.01it/s, loss=0.463, v_num=q76q, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:00<00:00, 19.21it/s, loss=0.455, v_num=q76q, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.385. Signaling Trainer to stop.\n",
      "Epoch 32: 100%|█| 19/19 [00:00<00:00, 19.29it/s, loss=0.455, v_num=q76q, BTC_val\n",
      "Epoch 32: 100%|█| 19/19 [00:00<00:00, 19.22it/s, loss=0.455, v_num=q76q, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 28.57it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.699999988079071,\n",
      " 'BTC_test_f1': 0.6914286017417908,\n",
      " 'ETH_test_acc': 0.6666666865348816,\n",
      " 'ETH_test_f1': 0.6606335639953613,\n",
      " 'LTC_test_acc': 0.800000011920929,\n",
      " 'LTC_test_f1': 0.7991071343421936,\n",
      " 'test_loss': 0.48803091049194336}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 107928\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105855-26vxq76q/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_105855-26vxq76q/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.79003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.78552\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.8014\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.79942\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.78828\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.78495\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.45561\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 594\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 65\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625904000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 77\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.91667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.87368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.91667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.87368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.38826\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.76562\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.76511\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.71875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.71875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.76562\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.76511\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.48786\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.69143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.66063\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.79911\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.48803\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▄▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▄▅▆▆▇▇▇▇▇▇▇▇▇▇▇█████▇▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▃▅▆▆▇▇▆▇▇█▇▇████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▅▆▆▆▇▆▇▇▇▇▇█████████▇██████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▂▄▆▆▆▆▇▇▇▇▇▇██▇████▇█▇█▇███▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▂▄▆▆▇▇▇▇█▇▇▇█████████▇██████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▆▆▆████▆█▆▃█▆██▃█▆█████▆▃▆██▃▆█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▇▇▇████▇█▇▅█▇██▅█▇█████▇▅▇██▅▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▄▄▄██████▄▄████▄████████▄███▄███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▆▆▆██████▆▆████▆████████▆███▆███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▃▃▆▆▆▆▆▆▆▆▆▁▆██▆▆▆▆▆█▆██▆▆▆██▃▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▅▆▆▆▆▆▆▆▆▆▄▆██▆▆▆▆▆█▆██▆▆▆██▅▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▆▄▄▃▃▃▂▃▂▂▃▁▁▃▂▃▂▁▂▂▁▂▁▁▃▂▂▂▄▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▂▂▄▁▄▆▅▄█▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▂▂▃▁▄▆▅▄█▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▁▁▃▇█▄▅▅▄▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▁▂▂██▄▅▆▄▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▁▅▅▂▇█▄▄▅▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▁▅▄▂▆█▄▄▅▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▄▃▅▄▁▄▃▆▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_binary_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/26vxq76q\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-07-10 11:00:15.586660: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_multi_classification\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_v3/runs/2ma0mobt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_110014-2ma0mobt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `F1` was deprecated since v1.3.0 in favor of `torchmetrics.classification.f_beta.F1`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  95%|▉| 18/19 [00:00<00:00, 21.22it/s, loss=1.12, v_num=mobt, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.089\n",
      "Epoch 0: 100%|█| 19/19 [00:00<00:00, 21.16it/s, loss=1.12, v_num=mobt, BTC_val_a\n",
      "                                                                                \u001b[A/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "  warning_cache.deprecation(\n",
      "Epoch 1:  95%|▉| 18/19 [00:00<00:00, 19.50it/s, loss=1.08, v_num=mobt, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.081 >= min_delta = 0.003. New best score: 1.008\n",
      "Epoch 1: 100%|█| 19/19 [00:00<00:00, 19.63it/s, loss=1.08, v_num=mobt, BTC_val_a\n",
      "Epoch 2:  95%|▉| 18/19 [00:00<00:00, 20.40it/s, loss=1.03, v_num=mobt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.019 >= min_delta = 0.003. New best score: 0.989\n",
      "Epoch 2: 100%|█| 19/19 [00:00<00:00, 20.22it/s, loss=1.03, v_num=mobt, BTC_val_a\n",
      "Epoch 3:  95%|▉| 18/19 [00:00<00:00, 19.45it/s, loss=0.978, v_num=mobt, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 19/19 [00:00<00:00, 19.33it/s, loss=0.978, v_num=mobt, BTC_val_\u001b[A\n",
      "Epoch 4:  95%|▉| 18/19 [00:00<00:00, 19.03it/s, loss=0.941, v_num=mobt, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.981\n",
      "Epoch 4: 100%|█| 19/19 [00:01<00:00, 18.71it/s, loss=0.941, v_num=mobt, BTC_val_\n",
      "Epoch 5:  95%|▉| 18/19 [00:01<00:00, 17.64it/s, loss=0.911, v_num=mobt, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.147 >= min_delta = 0.003. New best score: 0.834\n",
      "Epoch 5: 100%|█| 19/19 [00:01<00:00, 17.72it/s, loss=0.911, v_num=mobt, BTC_val_\n",
      "Epoch 6:  95%|▉| 18/19 [00:00<00:00, 19.31it/s, loss=0.861, v_num=mobt, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.090 >= min_delta = 0.003. New best score: 0.745\n",
      "Epoch 6: 100%|█| 19/19 [00:00<00:00, 19.28it/s, loss=0.861, v_num=mobt, BTC_val_\n",
      "Epoch 7:  95%|▉| 18/19 [00:00<00:00, 18.98it/s, loss=0.836, v_num=mobt, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 19/19 [00:00<00:00, 19.11it/s, loss=0.836, v_num=mobt, BTC_val_\u001b[A\n",
      "Epoch 8:  95%|▉| 18/19 [00:00<00:00, 18.68it/s, loss=0.829, v_num=mobt, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 19/19 [00:01<00:00, 18.78it/s, loss=0.829, v_num=mobt, BTC_val_\u001b[A\n",
      "Epoch 9:  95%|▉| 18/19 [00:00<00:00, 18.79it/s, loss=0.82, v_num=mobt, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 19/19 [00:01<00:00, 18.85it/s, loss=0.82, v_num=mobt, BTC_val_a\u001b[A\n",
      "Epoch 10:  95%|▉| 18/19 [00:00<00:00, 20.10it/s, loss=0.814, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 19/19 [00:00<00:00, 20.02it/s, loss=0.814, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 11:  95%|▉| 18/19 [00:00<00:00, 18.97it/s, loss=0.812, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 19/19 [00:01<00:00, 18.76it/s, loss=0.812, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 12:  95%|▉| 18/19 [00:00<00:00, 19.28it/s, loss=0.793, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 19/19 [00:00<00:00, 19.16it/s, loss=0.793, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 13:  95%|▉| 18/19 [00:00<00:00, 19.62it/s, loss=0.786, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 19/19 [00:00<00:00, 19.67it/s, loss=0.786, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 14:  95%|▉| 18/19 [00:00<00:00, 19.06it/s, loss=0.781, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.732\n",
      "Epoch 14: 100%|█| 19/19 [00:00<00:00, 19.17it/s, loss=0.781, v_num=mobt, BTC_val\n",
      "Epoch 15:  95%|▉| 18/19 [00:00<00:00, 18.48it/s, loss=0.77, v_num=mobt, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 19/19 [00:01<00:00, 18.56it/s, loss=0.77, v_num=mobt, BTC_val_\u001b[A\n",
      "Epoch 16:  95%|▉| 18/19 [00:00<00:00, 18.98it/s, loss=0.799, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 19/19 [00:01<00:00, 18.93it/s, loss=0.799, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 17:  95%|▉| 18/19 [00:00<00:00, 19.86it/s, loss=0.77, v_num=mobt, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 19/19 [00:00<00:00, 19.88it/s, loss=0.77, v_num=mobt, BTC_val_\u001b[A\n",
      "Epoch 18:  95%|▉| 18/19 [00:00<00:00, 19.41it/s, loss=0.771, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 19/19 [00:00<00:00, 19.53it/s, loss=0.771, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 19:  95%|▉| 18/19 [00:00<00:00, 19.36it/s, loss=0.77, v_num=mobt, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 19/19 [00:00<00:00, 19.46it/s, loss=0.77, v_num=mobt, BTC_val_\u001b[A\n",
      "Epoch 20:  95%|▉| 18/19 [00:00<00:00, 18.77it/s, loss=0.767, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 19/19 [00:01<00:00, 18.74it/s, loss=0.767, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 21:  95%|▉| 18/19 [00:00<00:00, 19.33it/s, loss=0.773, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 19/19 [00:00<00:00, 19.23it/s, loss=0.773, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 22:  95%|▉| 18/19 [00:00<00:00, 19.54it/s, loss=0.764, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 19/19 [00:00<00:00, 19.67it/s, loss=0.764, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 23:  95%|▉| 18/19 [00:00<00:00, 19.22it/s, loss=0.755, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 19/19 [00:00<00:00, 19.24it/s, loss=0.755, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 24:  95%|▉| 18/19 [00:00<00:00, 19.33it/s, loss=0.761, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 19/19 [00:01<00:00, 18.96it/s, loss=0.761, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 25:  95%|▉| 18/19 [00:00<00:00, 19.42it/s, loss=0.755, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.016 >= min_delta = 0.003. New best score: 0.716\n",
      "Epoch 25: 100%|█| 19/19 [00:00<00:00, 19.48it/s, loss=0.755, v_num=mobt, BTC_val\n",
      "Epoch 26:  95%|▉| 18/19 [00:00<00:00, 19.13it/s, loss=0.756, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 19/19 [00:00<00:00, 19.25it/s, loss=0.756, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 27:  95%|▉| 18/19 [00:00<00:00, 19.70it/s, loss=0.777, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 19/19 [00:00<00:00, 19.71it/s, loss=0.777, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 28:  95%|▉| 18/19 [00:00<00:00, 19.14it/s, loss=0.767, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 19/19 [00:00<00:00, 19.22it/s, loss=0.767, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 29:  95%|▉| 18/19 [00:00<00:00, 19.30it/s, loss=0.755, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 19/19 [00:00<00:00, 19.24it/s, loss=0.755, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 30:  95%|▉| 18/19 [00:00<00:00, 19.89it/s, loss=0.768, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 19/19 [00:00<00:00, 19.97it/s, loss=0.768, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 31:  95%|▉| 18/19 [00:00<00:00, 19.85it/s, loss=0.755, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 19/19 [00:00<00:00, 19.66it/s, loss=0.755, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 32:  95%|▉| 18/19 [00:00<00:00, 18.71it/s, loss=0.754, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 19/19 [00:01<00:00, 18.67it/s, loss=0.754, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 33:  95%|▉| 18/19 [00:00<00:00, 19.19it/s, loss=0.756, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 19/19 [00:00<00:00, 19.15it/s, loss=0.756, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 34:  95%|▉| 18/19 [00:00<00:00, 19.26it/s, loss=0.739, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 19/19 [00:00<00:00, 19.11it/s, loss=0.739, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 35:  95%|▉| 18/19 [00:00<00:00, 19.72it/s, loss=0.744, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 0.713\n",
      "Epoch 35: 100%|█| 19/19 [00:00<00:00, 19.68it/s, loss=0.744, v_num=mobt, BTC_val\n",
      "Epoch 36:  95%|▉| 18/19 [00:00<00:00, 19.24it/s, loss=0.734, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 19/19 [00:00<00:00, 19.14it/s, loss=0.734, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 37:  95%|▉| 18/19 [00:00<00:00, 19.55it/s, loss=0.742, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 19/19 [00:00<00:00, 19.57it/s, loss=0.742, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 38:  95%|▉| 18/19 [00:00<00:00, 19.39it/s, loss=0.751, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 19/19 [00:00<00:00, 19.49it/s, loss=0.751, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 39:  95%|▉| 18/19 [00:00<00:00, 19.35it/s, loss=0.736, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 19/19 [00:00<00:00, 19.42it/s, loss=0.736, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 40:  95%|▉| 18/19 [00:00<00:00, 19.05it/s, loss=0.73, v_num=mobt, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 19/19 [00:01<00:00, 18.95it/s, loss=0.73, v_num=mobt, BTC_val_\u001b[A\n",
      "Epoch 41:  95%|▉| 18/19 [00:00<00:00, 19.63it/s, loss=0.727, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 19/19 [00:00<00:00, 19.62it/s, loss=0.727, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 42:  95%|▉| 18/19 [00:00<00:00, 18.61it/s, loss=0.731, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 19/19 [00:01<00:00, 18.72it/s, loss=0.731, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 43:  95%|▉| 18/19 [00:00<00:00, 19.88it/s, loss=0.74, v_num=mobt, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 19/19 [00:00<00:00, 19.96it/s, loss=0.74, v_num=mobt, BTC_val_\u001b[A\n",
      "Epoch 44:  95%|▉| 18/19 [00:00<00:00, 19.28it/s, loss=0.733, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 19/19 [00:00<00:00, 19.39it/s, loss=0.733, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 45:  95%|▉| 18/19 [00:00<00:00, 19.99it/s, loss=0.727, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 19/19 [00:00<00:00, 20.04it/s, loss=0.727, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 46:  95%|▉| 18/19 [00:00<00:00, 20.59it/s, loss=0.732, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 19/19 [00:00<00:00, 20.59it/s, loss=0.732, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 47:  95%|▉| 18/19 [00:00<00:00, 19.60it/s, loss=0.714, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 19/19 [00:00<00:00, 19.47it/s, loss=0.714, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 48:  95%|▉| 18/19 [00:00<00:00, 19.59it/s, loss=0.719, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 19/19 [00:00<00:00, 19.48it/s, loss=0.719, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 49:  95%|▉| 18/19 [00:00<00:00, 19.08it/s, loss=0.717, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 19/19 [00:01<00:00, 18.64it/s, loss=0.717, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 50:  95%|▉| 18/19 [00:00<00:00, 19.03it/s, loss=0.719, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 19/19 [00:01<00:00, 18.93it/s, loss=0.719, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 51:  95%|▉| 18/19 [00:00<00:00, 20.41it/s, loss=0.737, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 19/19 [00:00<00:00, 20.12it/s, loss=0.737, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 52:  95%|▉| 18/19 [00:00<00:00, 18.89it/s, loss=0.719, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 19/19 [00:01<00:00, 18.69it/s, loss=0.719, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 53:  95%|▉| 18/19 [00:00<00:00, 20.26it/s, loss=0.727, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 19/19 [00:00<00:00, 20.28it/s, loss=0.727, v_num=mobt, BTC_val\u001b[A\n",
      "Epoch 54:  95%|▉| 18/19 [00:00<00:00, 19.20it/s, loss=0.72, v_num=mobt, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 19/19 [00:00<00:00, 19.23it/s, loss=0.72, v_num=mobt, BTC_val_\u001b[A\n",
      "Epoch 55:  95%|▉| 18/19 [00:00<00:00, 18.79it/s, loss=0.721, v_num=mobt, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 20 records. Best score: 0.713. Signaling Trainer to stop.\n",
      "Epoch 55: 100%|█| 19/19 [00:01<00:00, 18.83it/s, loss=0.721, v_num=mobt, BTC_val\n",
      "Epoch 55: 100%|█| 19/19 [00:01<00:00, 18.76it/s, loss=0.721, v_num=mobt, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 27.33it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6666666865348816,\n",
      " 'BTC_test_f1': 0.6583006978034973,\n",
      " 'ETH_test_acc': 0.6333333253860474,\n",
      " 'ETH_test_f1': 0.6072077751159668,\n",
      " 'LTC_test_acc': 0.6333333253860474,\n",
      " 'LTC_test_f1': 0.47083336114883423,\n",
      " 'test_loss': 0.7688199877738953}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 108210\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_110014-2ma0mobt/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/multi_task_price_change_prediction/notebooks/wandb/run-20210710_110014-2ma0mobt/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch 0.67542\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch 0.67243\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch 0.67542\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch 0.66824\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch 0.66492\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch 0.65428\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.71441\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 55\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 63\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625904077\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 132\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.41667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.58333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.45714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.77428\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step 0.64062\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step 0.64111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step 0.70312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step 0.70446\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step 0.67188\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step 0.66727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.68923\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.6583\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.63333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.60721\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.63333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.47083\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.76882\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   BTC_train_acc_epoch ▁▂▃▅▆▆▇▇▇▇▇▇▇▇█▇▇▇▇█▇█▇█▇▇█▇██████▇████▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_f1_epoch ▁▂▃▅▅▆▇▇▇▇▇▇▇▇█▇▇▇▇█▇█▇█▇██▇██████▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   ETH_train_acc_epoch ▁▂▃▅▆▆▇▇▇▇▇▇▇▇█▇▇▇█▇▇▇▇▇▇▇██▇██▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_f1_epoch ▁▂▃▅▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   LTC_train_acc_epoch ▁▂▄▅▅▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇█▇▇▇██▇███▇█▇█▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_f1_epoch ▁▂▃▅▅▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇█▇█████▇█▇█▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▅▆▅▅▇█▃█▇▇▃▅▃▃▂▅▆▃▆▁▇▃▅▆▅▆▃▅▅▃▂▃▅▂▃▃▅▃▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▄▆▄▄██▃█▇▇▄▅▄▄▂▄▄▃▆▁▇▄▅▄▄▆▃▄▄▃▂▃▄▂▃▄▄▃▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▄▃▃▁▃▄▆▆▆▆███▆█▆▆█▆▆▆█▆▆▆▆▆▆▆▆▆▆▆█▆█▆▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▃▂▂▁▃▃▄▅▅▅███▅█▅▅█▅▄▅█▅▅▅▅▅▅▅▅▅▅▅█▅█▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▄▁▁▅▄▂▅▄▄▄▅▄▇▄█▅▄▇▇█▅▇▄▅▄▄▄▅▄▄▄▅▄▅▄█▅▄▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▃▁▁▅▄▂▅▃▃▃▅▃▇▃█▄▃▇▇█▄▇▃▄▃▃▃▄▃▃▃▄▃▅▃█▄▃▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▆▆▆▃▃▂▂▂▂▁▂▂▁▁▃▂▂▁▃▂▃▁▂▁▁▂▁▁▁▂▁▂▁▂▂▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    BTC_train_acc_step ▁▁▁▃▂▃▆▆▇▅▃▃▅▃▄▃▅▅█▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     BTC_train_f1_step ▁▂▁▄▂▃▆▅▇▅▃▃▄▃▄▃▅▅█▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    ETH_train_acc_step ▂▁▅▂▂▄▃▅▄▄▄▂▆▅▄▄▃█▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     ETH_train_f1_step ▂▁▅▂▂▄▂▅▃▄▄▂▅▅▃▃▃█▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    LTC_train_acc_step ▃▅▂▁▃▃▇▇▄▆▆▄▅▆█▆▆▄▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     LTC_train_f1_step ▃▅▃▁▃▃█▇▄▆▇▅▆▇█▅▇▄█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▇▅▆█▆▄▂▂▄▃▆▁▄▄▅▃▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_stack_lstm_multi_classification\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_v3/runs/2ma0mobt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for c in (ParameterGrid(param_grid)):\n",
    "    config = CONFIG.copy()\n",
    "    config.update(c)\n",
    "    script = \"--currency-list \" + \" \".join([i for i in config[\"currency_list\"]])\n",
    "    script += \" --lstm-list \" + \" \".join([str(i) for i in config[\"lstm_hidden_sizes\"]])\n",
    "    script += \" -trend \" + str(1 if config[\"remove_trend\"] else 0)\n",
    "    script += \" -indicators \" + str(1 if config[\"indicators\"] else 0)\n",
    "    script += \" -ohlv \" + str(1 if config[\"ohlv\"] else 0)\n",
    "    script += \" -imfs \" + str(1 if config[\"imfs\"] else 0)\n",
    "    script += \" -bidirectional \" + str(1 if config[\"bidirectional\"] else 0)\n",
    "    script += \" -classes \" + str(config[\"n_classes\"] )\n",
    "    script += \" -weight \" + str(1 if config[\"loss_weight_calculate\"] else 0) \n",
    "\n",
    "    experiment(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "09_ak_experimenting_lstm-2.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "04984682def58a97e4300fcfdea82226e95c772fd8b0b63e42875ad1781ae0ab"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "04984682def58a97e4300fcfdea82226e95c772fd8b0b63e42875ad1781ae0ab"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "001002aa7ae54d1f9eea85ec7c2d2461": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_030d6ea6da3b424a820dd988e2146a9c",
       "IPY_MODEL_65af5c35d8f742508f26d1cffc205628"
      ],
      "layout": "IPY_MODEL_cffb821306cc40578d57c4ca3abb7541"
     }
    },
    "004204cf73ec41b1a4523a23744ad9d0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "004b1c07e44d4d40810abaee5a99ad3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "01cc5f07613a4557ac7f2adf1baf1b8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1dc435eef2354b5390ab041dbde32292",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ad8dbb0dc017486cbc042a92f4738370",
      "value": 1
     }
    },
    "030d6ea6da3b424a820dd988e2146a9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ce76e0ea3694dc9b62989a4a8919ea9",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_298f0f714c4a461ea403d74794058d7d",
      "value": 1
     }
    },
    "0381cec4bcd34e61a456437f92b69a82": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "05b1609fd7b6435894a7118faa671bd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3ad5b57d31c24c9d8c5b8beb951a4df1",
       "IPY_MODEL_aab735b043674e279438d35b0c3829e4"
      ],
      "layout": "IPY_MODEL_ca40adc1088b456caca137c68c304b87"
     }
    },
    "073459ac986d4a6b9bbd0331864c2b9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0838e59beb634c9d9f51d37873cf6170": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a9b0b16eb0ed4afd811c2205ba32b38f",
       "IPY_MODEL_01cc5f07613a4557ac7f2adf1baf1b8d"
      ],
      "layout": "IPY_MODEL_a91d3b3c1f66413e80af6f91cdfa8333"
     }
    },
    "0898797b70fa4de09b6c876080fd0390": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0a77ff0cce9a4dd9805de3934eda4e40": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b4ae2f7e3f641e1826c84eeb56a0d23": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0eb5cf4f737549c688a77d4b4e5d95d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14fd68e87bfa4a06ae7ec08e179a3e8b",
      "placeholder": "​",
      "style": "IPY_MODEL_f650e7cf31e640df8c4c0bb0b24401d8",
      "value": " 1/1 [00:00&lt;00:00, 19.75it/s]"
     }
    },
    "101a0888d70b48dab438635e1ad2cea2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "11282acb50e34b0894baa996c9267157": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42c6237380fc4494b1caf12812754794",
      "placeholder": "​",
      "style": "IPY_MODEL_0898797b70fa4de09b6c876080fd0390",
      "value": " 1/1 [00:00&lt;00:00, 22.70it/s]"
     }
    },
    "13568ce2679846e883ddc002283913d9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1429d3b406c84a8da158a31c37e27093": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "149ea47b64eb4375ace1beb9871b6b98": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "14fd68e87bfa4a06ae7ec08e179a3e8b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15d5239c57004349961ec6245e722016": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_94d26342f8264f45964fab6ca14f52f2",
       "IPY_MODEL_86c2b5672d90460ca07dc230b2be58b2"
      ],
      "layout": "IPY_MODEL_863ff2d4a3ec4f18a9e4b274ad9ab09d"
     }
    },
    "173ba810845448a7aa1ce5bbc5faf973": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1afc725ba84f499db1a2c4f557ae43e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8b14122d94447c180536812afec72fc",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9b2b9db9d2d24f078fc0551ac0e2afac",
      "value": 1
     }
    },
    "1b272f78d2784f51be805adba62ec27b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1b4c71595f2345c592d3879ea8dc3fd5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "1be29621ae064cc2b4e0cedd4df48a90": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_669185de465a43ababdc5b34577e8700",
       "IPY_MODEL_2b71ea6f4e634dd8ac919ae7a60192c4"
      ],
      "layout": "IPY_MODEL_6644970fe5f0412197edf44f588c1b28"
     }
    },
    "1cc6501157cf4cb28e8a9cfad3317a1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "1ce03d4f3de6400689145312ed4a5d86": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "1ce76e0ea3694dc9b62989a4a8919ea9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1dc435eef2354b5390ab041dbde32292": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e616b4d2fbe42d4942b50f50ff3a19e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6c3c2e74f8849cdac743d5399977ec2",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_581c8413af05406b96af1975dc014eea",
      "value": 1
     }
    },
    "21daa732d2914ec6b0343684cae7d671": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2328c71b3fd74611b8d1b85e645c0508": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "232e3f40a26c4aa4b8934fd820687530": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f1c1bb057a894fef9bc5b10420a9048f",
       "IPY_MODEL_f185f66dd39c434ab7b164437c7e7fb8"
      ],
      "layout": "IPY_MODEL_84f9b7a06cc74d78a1e6021713da12c0"
     }
    },
    "234f859aa2744f28a9697787b80485f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2da8dade2d1a42e8b100e0288fb5188d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b44f04486bf143619b3fe171aa687f18",
      "value": 1
     }
    },
    "23bccc402fa24a1a83460204e010f918": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac138621be054e0ebe78ca44ebf9f0a5",
      "placeholder": "​",
      "style": "IPY_MODEL_45d0d84f45fa4ca08b24c490586319de",
      "value": " 1/1 [00:00&lt;00:00, 18.37it/s]"
     }
    },
    "298f0f714c4a461ea403d74794058d7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2b4479a31b1147149f1f7ef6bb771d1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e9b61655db94a04bdd4b6532e3ba7c5",
      "placeholder": "​",
      "style": "IPY_MODEL_c6f0c32efccd4368b4cdbf29e5eee033",
      "value": " 1/1 [00:00&lt;00:00, 22.08it/s]"
     }
    },
    "2b71ea6f4e634dd8ac919ae7a60192c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21daa732d2914ec6b0343684cae7d671",
      "placeholder": "​",
      "style": "IPY_MODEL_f089fefd584240cfb263909b0e4ff38e",
      "value": " 1/1 [00:00&lt;00:00, 18.77it/s]"
     }
    },
    "2c22af65a86945af921ad7056a54cc4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5bca4a2c9ac746778bd129ab5e0bcbf5",
      "placeholder": "​",
      "style": "IPY_MODEL_e93337ba1da74acbb3fa347d9d49e9ea",
      "value": " 1/1 [00:00&lt;00:00, 18.37it/s]"
     }
    },
    "2d39e2890b1f4506a6edd4a3b2c2d12c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2da8dade2d1a42e8b100e0288fb5188d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e2488dd08e646e48f0fdebbb431be1f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ffa58aa61814c9186924de42e6ce464": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "31c96cf4f6cf4625b2c7f42a9d9e4456": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7b364d9e7d934c4bbce500ea282e55a0",
       "IPY_MODEL_fb73a9e3ebaf466183729edd4fcb2ea1"
      ],
      "layout": "IPY_MODEL_1b4c71595f2345c592d3879ea8dc3fd5"
     }
    },
    "31edfbdb82af40e8b67ad9b6585a562a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33ce28d3c500425b867369920a55e987": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34608d61d2584ccda71aade54f21769a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34c39fd383404d35b2cf0737f3b66981": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34ddec300ac640909eefab973ebb11fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_880d5a5ac1a84fa1a9665171d63159b3",
      "placeholder": "​",
      "style": "IPY_MODEL_1b272f78d2784f51be805adba62ec27b",
      "value": " 1/1 [00:00&lt;00:00, 16.31it/s]"
     }
    },
    "38b42ebc7be545a2b668d961440ae5d3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "3ad5b57d31c24c9d8c5b8beb951a4df1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f612ca8d591f457381f3397523fc7bb3",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_df967d7bfa654ce9aa7bc1b93d9a48c7",
      "value": 1
     }
    },
    "3d8d4cd8a7d64997a6267449f06a7a45": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e2488dd08e646e48f0fdebbb431be1f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ffae011089d1468ea74b9b17acdec20f",
      "value": 1
     }
    },
    "3ea6355a982e446aa5298069e5f18308": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "3f57f1391b6e418785fbcf32221ee893": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3f9caad4e9b842c08bd91e4555b6969c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_85ab5ec85e4945949dc4e5a763099611",
      "placeholder": "​",
      "style": "IPY_MODEL_a4d9ce425e55436ca95ed398ac5eb356",
      "value": " 1/1 [00:00&lt;00:00, 19.35it/s]"
     }
    },
    "42c6237380fc4494b1caf12812754794": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "432b70435d2b47afb587e7cbf257754a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "435523d75063464382a528bc063a421c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "4564330337014a6d9e238095de3902ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "45d0d84f45fa4ca08b24c490586319de": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4847aed5b8c347398a13c30e10c0410b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_72b038cdda4f4a9e915cd524759ec8a5",
       "IPY_MODEL_2c22af65a86945af921ad7056a54cc4e"
      ],
      "layout": "IPY_MODEL_435523d75063464382a528bc063a421c"
     }
    },
    "4ba12041852b4200a23a5536a4339b93": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70075f5fd4c34ebba9497ec97eb8b806",
      "placeholder": "​",
      "style": "IPY_MODEL_c922125e6f6c47398ea956bbfee20531",
      "value": " 1/1 [00:00&lt;00:00, 20.29it/s]"
     }
    },
    "4d788c5265504d58b4de2cfab3e20b03": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "4e9b61655db94a04bdd4b6532e3ba7c5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f2c45f34d5a4a60aec3e11f17339bcd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "528684a138fe44abb63c7c84da2a9b39": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validation sanity check: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b99e03bd189a400bb3cd7e8543269891",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fee172130956407cbd4cef7772b5a60e",
      "value": 1
     }
    },
    "52b624236c7846dfbf56cf05585f2f30": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "52db72907a60408b9e1db1ed91e908b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97fbd8eeb4ef42f5a8cca961c015e74b",
      "placeholder": "​",
      "style": "IPY_MODEL_79048080a924456580e7193c912e270e",
      "value": " 1/1 [00:00&lt;00:00, 20.67it/s]"
     }
    },
    "5430acd7f77444d58eb9db16097aee0a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "567b80eb991c4af396635a68d2a719f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "56e9e8878a394cd8a639f95c76565722": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "57e8113a683641f69633d987556e26ff": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "581c8413af05406b96af1975dc014eea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "590e385531db4402a17779e8238b575c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1e616b4d2fbe42d4942b50f50ff3a19e",
       "IPY_MODEL_d84faa4176b342f6890e552a10ca58bc"
      ],
      "layout": "IPY_MODEL_56e9e8878a394cd8a639f95c76565722"
     }
    },
    "5acaf2b196554817b5dcb42a2e861e46": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "5b30843f46a14c6aa6114e13dc43ad06": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5bca4a2c9ac746778bd129ab5e0bcbf5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c7b1d54e5d849669399989a4f3bb638": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6d1c4884c9d1462abf9a9a98151be7a8",
       "IPY_MODEL_d7dba68c81284dd3a9d33a5326df1bcd"
      ],
      "layout": "IPY_MODEL_149ea47b64eb4375ace1beb9871b6b98"
     }
    },
    "5d53a958248242dc8bc4aece83e4348d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5da980c0bac14ebe9c7ac855d48204d3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5eb5dbf1dfd243729c4873b9ee4ff40e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_173ba810845448a7aa1ce5bbc5faf973",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ac9b08e03a954f089d8efaaffe38bc81",
      "value": 1
     }
    },
    "5f39212d17e74155855ec3d86b474810": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5430acd7f77444d58eb9db16097aee0a",
      "placeholder": "​",
      "style": "IPY_MODEL_7c56e5a600fb4f709e4fbbc640b19a65",
      "value": " 1/1 [00:00&lt;00:00, 22.19it/s]"
     }
    },
    "5f7c64cb893e40408670c6b2c8f064e8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61031c90cd484bd58a4db6fd3e90e746": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65af5c35d8f742508f26d1cffc205628": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9afb61e97ee84541af2cb9e14658dd9e",
      "placeholder": "​",
      "style": "IPY_MODEL_85c8d92483ca4033b3b47de761be3e7d",
      "value": " 1/1 [00:00&lt;00:00, 19.80it/s]"
     }
    },
    "6644970fe5f0412197edf44f588c1b28": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "669185de465a43ababdc5b34577e8700": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1429d3b406c84a8da158a31c37e27093",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c4453ab8b2504d7db690f1c92ae8f967",
      "value": 1
     }
    },
    "688c166e368e4391b99f88702cb2c751": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c5e35dae04eb43c9be1b206ff98e5a13",
      "placeholder": "​",
      "style": "IPY_MODEL_5d53a958248242dc8bc4aece83e4348d",
      "value": " 1/1 [00:00&lt;00:00, 21.67it/s]"
     }
    },
    "6a9d16cc8be64bf983af3ce976a892d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "6ac7c1fc95264bd7bae2ef5c6c828249": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b049434616c4b95924c4e9df8193f83": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_79070d53d7164ce5b3ee6b8d0001f178",
       "IPY_MODEL_34ddec300ac640909eefab973ebb11fa"
      ],
      "layout": "IPY_MODEL_38b42ebc7be545a2b668d961440ae5d3"
     }
    },
    "6c450f114fa74e8e8a8d00d58c35c2ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e5059c546bbd4529943fd6cbce34486b",
       "IPY_MODEL_dca1b6ab2cf34a09914ae43b0bf5df53"
      ],
      "layout": "IPY_MODEL_2328c71b3fd74611b8d1b85e645c0508"
     }
    },
    "6d1c4884c9d1462abf9a9a98151be7a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Testing: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de670cc3f13b471090155fddd9e9a798",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1ce03d4f3de6400689145312ed4a5d86",
      "value": 1
     }
    },
    "6ff89bf75a444ca1aba38fdcac7538ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34c39fd383404d35b2cf0737f3b66981",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_98ffafe96f134f4180cee6c75306bd0a",
      "value": 1
     }
    },
    "70075f5fd4c34ebba9497ec97eb8b806": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "712a1758ba8143f3b65523ac70cb3e91": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "72b038cdda4f4a9e915cd524759ec8a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_61031c90cd484bd58a4db6fd3e90e746",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f287119ddc4b408eb98273405c74026c",
      "value": 1
     }
    },
    "7699bf3f282847ee87bf9595ffb57854": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b1721b764e55419aa655f1734dc3f03c",
       "IPY_MODEL_78cd948fb8514a46a577ce8b52091df7"
      ],
      "layout": "IPY_MODEL_3ea6355a982e446aa5298069e5f18308"
     }
    },
    "782949ba5ff44cb9a9a37b3aa4456b32": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "789da7bf5aeb4206a3c5c8b1aabe2637": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "78cd948fb8514a46a577ce8b52091df7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_add1a0907fd14da9bb6b2757d44e75ae",
      "placeholder": "​",
      "style": "IPY_MODEL_d29691ca7c5b4d29b0810d24cc8837ac",
      "value": " 1/1 [00:00&lt;00:00, 22.80it/s]"
     }
    },
    "79048080a924456580e7193c912e270e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79070d53d7164ce5b3ee6b8d0001f178": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92d397dce25c4977bb454f50dcf4f119",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0b4ae2f7e3f641e1826c84eeb56a0d23",
      "value": 1
     }
    },
    "7b364d9e7d934c4bbce500ea282e55a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ef1656930f764faca37a51440d1321cc",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4564330337014a6d9e238095de3902ba",
      "value": 1
     }
    },
    "7c56e5a600fb4f709e4fbbc640b19a65": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7fca719d592749779e7700c0ef732a38": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81fd519c0cfd4a2f8307a918e37645d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e60302c23be24b328b148202f132179b",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c8a500ff80ad40e494aa67bb81824d47",
      "value": 1
     }
    },
    "83e710060a3644849abaa259d8f4f2e6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "8444c9620a964f629d0b687f2f476277": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "84f9b7a06cc74d78a1e6021713da12c0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "85ab5ec85e4945949dc4e5a763099611": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85c8d92483ca4033b3b47de761be3e7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "85dd78cb7a334dc5ac7a4c91d14bac53": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "863ff2d4a3ec4f18a9e4b274ad9ab09d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "86c2b5672d90460ca07dc230b2be58b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d39e2890b1f4506a6edd4a3b2c2d12c",
      "placeholder": "​",
      "style": "IPY_MODEL_789da7bf5aeb4206a3c5c8b1aabe2637",
      "value": " 1/1 [00:00&lt;00:00, 21.53it/s]"
     }
    },
    "877ccdc4a7b348f79ea3c39d605af725": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "880d5a5ac1a84fa1a9665171d63159b3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8895fc082ac5443184e4a600eba89b7d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8be94868429b4bcd96e4d4dce85278e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c5e68d2f7e8548fa811d18dec0989d34",
       "IPY_MODEL_0eb5cf4f737549c688a77d4b4e5d95d5"
      ],
      "layout": "IPY_MODEL_4d788c5265504d58b4de2cfab3e20b03"
     }
    },
    "911ccbab9ac4425680111cc78146b6e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "92d397dce25c4977bb454f50dcf4f119": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "937b9ab4d33448269ba3ff1b64923b0d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "939c8981feb7434d8777694b08a2670a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94d26342f8264f45964fab6ca14f52f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f2c45f34d5a4a60aec3e11f17339bcd",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d1e59c2672014bd8a536d0a223371195",
      "value": 1
     }
    },
    "97e6a48a97964de6bd3b0e00b28f9a54": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eb8f44b5cf7a410aabbcab2fa12a47a4",
       "IPY_MODEL_5f39212d17e74155855ec3d86b474810"
      ],
      "layout": "IPY_MODEL_83e710060a3644849abaa259d8f4f2e6"
     }
    },
    "97fbd8eeb4ef42f5a8cca961c015e74b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98ffafe96f134f4180cee6c75306bd0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9a8d479b302c46afaa7f3c0eff07c9f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bcc76021f1f64b0f98a601aeeb87274f",
       "IPY_MODEL_d9ce85bf66144c649b32d492b2d3b753"
      ],
      "layout": "IPY_MODEL_101a0888d70b48dab438635e1ad2cea2"
     }
    },
    "9aa4829013f0413a9c0c2b3943c7b2c6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "9afb61e97ee84541af2cb9e14658dd9e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b2b9db9d2d24f078fc0551ac0e2afac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9c8fa7cbc7814736a9258cd47b7426b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31edfbdb82af40e8b67ad9b6585a562a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1cc6501157cf4cb28e8a9cfad3317a1c",
      "value": 1
     }
    },
    "9db3a957f14b4cbb8647fde8b674ed8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1afc725ba84f499db1a2c4f557ae43e9",
       "IPY_MODEL_688c166e368e4391b99f88702cb2c751"
      ],
      "layout": "IPY_MODEL_8444c9620a964f629d0b687f2f476277"
     }
    },
    "a44c8953d967468bb48673652b144bfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a4d597b8f44249c3a71b041f4d789e6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9c8fa7cbc7814736a9258cd47b7426b3",
       "IPY_MODEL_11282acb50e34b0894baa996c9267157"
      ],
      "layout": "IPY_MODEL_bd064493e1bf4538baa1bcde3001f2db"
     }
    },
    "a4d9ce425e55436ca95ed398ac5eb356": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8d1591e402d48b8b9614466879ed770": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13568ce2679846e883ddc002283913d9",
      "placeholder": "​",
      "style": "IPY_MODEL_af64f25c469d4d52a57eff3420bcbfbe",
      "value": " 1/1 [00:00&lt;00:00, 21.04it/s]"
     }
    },
    "a91d3b3c1f66413e80af6f91cdfa8333": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9b0b16eb0ed4afd811c2205ba32b38f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7a968e61fd94cacba24309b55ece42f",
      "placeholder": "​",
      "style": "IPY_MODEL_c32af4c22cf84d43a04042f93f07458f",
      "value": " 4.43MB of 4.43MB uploaded (0.00MB deduped)\r"
     }
    },
    "aab735b043674e279438d35b0c3829e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cabcfe06cca64b4291a17f2591fac81b",
      "placeholder": "​",
      "style": "IPY_MODEL_fa5aae008f6d41e2a9a7428b7a615c3b",
      "value": " 1/1 [00:00&lt;00:00, 20.57it/s]"
     }
    },
    "ac138621be054e0ebe78ca44ebf9f0a5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac9b08e03a954f089d8efaaffe38bc81": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ad62407911e7478fb5f33515c9e64dcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad8dbb0dc017486cbc042a92f4738370": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "add1a0907fd14da9bb6b2757d44e75ae": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af64f25c469d4d52a57eff3420bcbfbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1721b764e55419aa655f1734dc3f03c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_939c8981feb7434d8777694b08a2670a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_712a1758ba8143f3b65523ac70cb3e91",
      "value": 1
     }
    },
    "b2a7e67f3bdc4f7782ed8374ac799376": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3ff1c933b34436ab5ed83645664c7ae",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d37a9c1baa894d729817d01d18fec45e",
      "value": 1
     }
    },
    "b2f19ead2d744899b6d4cc450768be48": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "b44f04486bf143619b3fe171aa687f18": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b4568a28ca3c41aaa27b365079ad1bec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6ff89bf75a444ca1aba38fdcac7538ec",
       "IPY_MODEL_2b4479a31b1147149f1f7ef6bb771d1e"
      ],
      "layout": "IPY_MODEL_937b9ab4d33448269ba3ff1b64923b0d"
     }
    },
    "b56b6868a11c4a19b0fb8c28b8e925ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_432b70435d2b47afb587e7cbf257754a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d75b360658f8471e94028db9b7c220f9",
      "value": 1
     }
    },
    "b6404efd23814cafbc6e1331683e7727": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b2a7e67f3bdc4f7782ed8374ac799376",
       "IPY_MODEL_4ba12041852b4200a23a5536a4339b93"
      ],
      "layout": "IPY_MODEL_57e8113a683641f69633d987556e26ff"
     }
    },
    "b99e03bd189a400bb3cd7e8543269891": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bcc76021f1f64b0f98a601aeeb87274f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f7c64cb893e40408670c6b2c8f064e8",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_073459ac986d4a6b9bbd0331864c2b9c",
      "value": 1
     }
    },
    "bd064493e1bf4538baa1bcde3001f2db": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "be540232d5c64e87aae655338dc050ad": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c32af4c22cf84d43a04042f93f07458f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3ff1c933b34436ab5ed83645664c7ae": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4453ab8b2504d7db690f1c92ae8f967": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c5e35dae04eb43c9be1b206ff98e5a13": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5e68d2f7e8548fa811d18dec0989d34": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a77ff0cce9a4dd9805de3934eda4e40",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_52b624236c7846dfbf56cf05585f2f30",
      "value": 1
     }
    },
    "c6f0c32efccd4368b4cdbf29e5eee033": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8a500ff80ad40e494aa67bb81824d47": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c8b14122d94447c180536812afec72fc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c922125e6f6c47398ea956bbfee20531": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c9519776aaa54bc6a717ec251748bc0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d14fd2cdefb94e81914fe4de68ce12cb",
       "IPY_MODEL_ff7dd27010bc44feb5ff2f1eaa77a52e"
      ],
      "layout": "IPY_MODEL_e6b49f2082f348fa843a25085c0d80e8"
     }
    },
    "ca40adc1088b456caca137c68c304b87": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "cabcfe06cca64b4291a17f2591fac81b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccff8c1550624a28bf89b5a4a07304ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5eb5dbf1dfd243729c4873b9ee4ff40e",
       "IPY_MODEL_52db72907a60408b9e1db1ed91e908b4"
      ],
      "layout": "IPY_MODEL_004204cf73ec41b1a4523a23744ad9d0"
     }
    },
    "cf9b4e2a5b284e94ae3c4048bb0b6f43": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34608d61d2584ccda71aade54f21769a",
      "placeholder": "​",
      "style": "IPY_MODEL_911ccbab9ac4425680111cc78146b6e0",
      "value": " 1/1 [00:00&lt;00:00, 16.60it/s]"
     }
    },
    "cffb821306cc40578d57c4ca3abb7541": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "d14fd2cdefb94e81914fe4de68ce12cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be540232d5c64e87aae655338dc050ad",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fba760627f07473cbc527bf0bc811e7d",
      "value": 1
     }
    },
    "d1e59c2672014bd8a536d0a223371195": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d29691ca7c5b4d29b0810d24cc8837ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d37a9c1baa894d729817d01d18fec45e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d75b360658f8471e94028db9b7c220f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d7dba68c81284dd3a9d33a5326df1bcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ac7c1fc95264bd7bae2ef5c6c828249",
      "placeholder": "​",
      "style": "IPY_MODEL_a44c8953d967468bb48673652b144bfe",
      "value": " 2/2 [00:00&lt;00:00, 15.68it/s]"
     }
    },
    "d84faa4176b342f6890e552a10ca58bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33ce28d3c500425b867369920a55e987",
      "placeholder": "​",
      "style": "IPY_MODEL_004b1c07e44d4d40810abaee5a99ad3f",
      "value": " 1/1 [00:00&lt;00:00, 18.99it/s]"
     }
    },
    "d9ce85bf66144c649b32d492b2d3b753": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b30843f46a14c6aa6114e13dc43ad06",
      "placeholder": "​",
      "style": "IPY_MODEL_dadef3f82f654698830226a05311267b",
      "value": " 1/1 [00:00&lt;00:00, 22.65it/s]"
     }
    },
    "dadef3f82f654698830226a05311267b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc75341527d848ce83a822613b081c91": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_234f859aa2744f28a9697787b80485f2",
       "IPY_MODEL_3f9caad4e9b842c08bd91e4555b6969c"
      ],
      "layout": "IPY_MODEL_5acaf2b196554817b5dcb42a2e861e46"
     }
    },
    "dca1b6ab2cf34a09914ae43b0bf5df53": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8895fc082ac5443184e4a600eba89b7d",
      "placeholder": "​",
      "style": "IPY_MODEL_0381cec4bcd34e61a456437f92b69a82",
      "value": " 1/1 [00:00&lt;00:00, 17.93it/s]"
     }
    },
    "de670cc3f13b471090155fddd9e9a798": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ded409ddc4814342b6a9dfb2bea2d97f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "df967d7bfa654ce9aa7bc1b93d9a48c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e5059c546bbd4529943fd6cbce34486b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_85dd78cb7a334dc5ac7a4c91d14bac53",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6a9d16cc8be64bf983af3ce976a892d3",
      "value": 1
     }
    },
    "e60302c23be24b328b148202f132179b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6a2a6c6678243dcb9f57715eca1586b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6b49f2082f348fa843a25085c0d80e8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "e6c3c2e74f8849cdac743d5399977ec2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7a968e61fd94cacba24309b55ece42f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7bb8595244b4407b4215b7d52ae3839": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_81fd519c0cfd4a2f8307a918e37645d3",
       "IPY_MODEL_f1b87bbda1f94e32b827751fd7ef8dbc"
      ],
      "layout": "IPY_MODEL_b2f19ead2d744899b6d4cc450768be48"
     }
    },
    "e93337ba1da74acbb3fa347d9d49e9ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb8f44b5cf7a410aabbcab2fa12a47a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validating: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6a2a6c6678243dcb9f57715eca1586b",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3f57f1391b6e418785fbcf32221ee893",
      "value": 1
     }
    },
    "ebf0421749e0432bade49a54fa66eca8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_528684a138fe44abb63c7c84da2a9b39",
       "IPY_MODEL_cf9b4e2a5b284e94ae3c4048bb0b6f43"
      ],
      "layout": "IPY_MODEL_782949ba5ff44cb9a9a37b3aa4456b32"
     }
    },
    "ec7da0f02286452486398a75cb131430": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee1ba2d34fab42b5aac4fceb7159ed3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b56b6868a11c4a19b0fb8c28b8e925ba",
       "IPY_MODEL_a8d1591e402d48b8b9614466879ed770"
      ],
      "layout": "IPY_MODEL_ded409ddc4814342b6a9dfb2bea2d97f"
     }
    },
    "ee3ad82f1523475a898afdfdbe88ac05": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ee4acb2bb6454af99a8597ffb4b3ad52": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3d8d4cd8a7d64997a6267449f06a7a45",
       "IPY_MODEL_23bccc402fa24a1a83460204e010f918"
      ],
      "layout": "IPY_MODEL_9aa4829013f0413a9c0c2b3943c7b2c6"
     }
    },
    "ef1656930f764faca37a51440d1321cc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f089fefd584240cfb263909b0e4ff38e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f185f66dd39c434ab7b164437c7e7fb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fca719d592749779e7700c0ef732a38",
      "placeholder": "​",
      "style": "IPY_MODEL_2ffa58aa61814c9186924de42e6ce464",
      "value": " 73/73 [00:03&lt;00:00, 20.50it/s, loss=0.698, v_num=q613, BTC_val_acc=0.500, BTC_val_f1=0.333, ETH_val_acc=0.875, ETH_val_f1=0.467, LTC_val_acc=0.875, LTC_val_f1=0.467, val_loss=0.670, BTC_train_acc_step=0.643, BTC_train_f1_step=0.524, ETH_train_acc_step=0.571, ETH_train_f1_step=0.571, LTC_train_acc_step=0.643, LTC_train_f1_step=0.626, train_loss_step=0.676, BTC_train_acc_epoch=0.547, BTC_train_f1_epoch=0.444, ETH_train_acc_epoch=0.516, ETH_train_f1_epoch=0.494, LTC_train_acc_epoch=0.533, LTC_train_f1_epoch=0.512, train_loss_epoch=0.691]"
     }
    },
    "f1b87bbda1f94e32b827751fd7ef8dbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2be62d0639b4cdb8488c50afb034119",
      "placeholder": "​",
      "style": "IPY_MODEL_567b80eb991c4af396635a68d2a719f1",
      "value": " 1/1 [00:00&lt;00:00, 19.01it/s]"
     }
    },
    "f1c1bb057a894fef9bc5b10420a9048f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Epoch 22: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5da980c0bac14ebe9c7ac855d48204d3",
      "max": 73,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_877ccdc4a7b348f79ea3c39d605af725",
      "value": 73
     }
    },
    "f287119ddc4b408eb98273405c74026c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f2be62d0639b4cdb8488c50afb034119": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f612ca8d591f457381f3397523fc7bb3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f650e7cf31e640df8c4c0bb0b24401d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa5aae008f6d41e2a9a7428b7a615c3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "facf644d5e9842f5b8fe5e7598eaa7cb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb73a9e3ebaf466183729edd4fcb2ea1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec7da0f02286452486398a75cb131430",
      "placeholder": "​",
      "style": "IPY_MODEL_ad62407911e7478fb5f33515c9e64dcd",
      "value": " 1/1 [00:00&lt;00:00, 16.61it/s]"
     }
    },
    "fba760627f07473cbc527bf0bc811e7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "fee172130956407cbd4cef7772b5a60e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ff7dd27010bc44feb5ff2f1eaa77a52e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_facf644d5e9842f5b8fe5e7598eaa7cb",
      "placeholder": "​",
      "style": "IPY_MODEL_ee3ad82f1523475a898afdfdbe88ac05",
      "value": " 1/1 [00:00&lt;00:00, 17.28it/s]"
     }
    },
    "ffae011089d1468ea74b9b17acdec20f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
