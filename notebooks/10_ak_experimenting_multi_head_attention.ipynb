{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aysenur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataPreparation import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 currency_list,\n",
    "                 x: np.ndarray, \n",
    "                 y: np.ndarray,\n",
    "                 data_use_type,\n",
    "                 train_percentage,\n",
    "                 val_percentage,\n",
    "                 test_percentage,\n",
    "                 seq_len, \n",
    "                 ):\n",
    "        self.currencies = currency_list\n",
    "        self.n_currencies = len(self.currencies)\n",
    "        self.x = torch.tensor(x[:self.n_currencies]).float()\n",
    "        self.y = torch.tensor(y[:self.n_currencies]).long()\n",
    "        self.seq_len = seq_len\n",
    "        self.data_use_type = data_use_type\n",
    "        \n",
    "        \n",
    "        #self.train_size = int(len(self.x[0]) * train_percentage)\n",
    "        self.val_size = int(len(self.x[0]) * val_percentage)\n",
    "        self.test_size = int(len(self.x[0]) * test_percentage)\n",
    "        self.train_size = len(self.x[0]) - self.val_size - self.test_size \n",
    "        \n",
    "        self.train_mean = [self.x[i][:self.train_size].mean() for i in range(self.n_currencies)]\n",
    "        self.train_std = [self.x[i][:self.train_size].std() for i in range(self.n_currencies)]\n",
    "        \n",
    "#         self.train_min = [self.x[i][:self.train_size].min() for i in range(n_currencies)]\n",
    "#         self.train_max = [self.x[i][:self.train_size].max() for i in range(n_currencies)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        if self.data_use_type == \"train\":\n",
    "            return self.train_size - ( self.seq_len)\n",
    "\n",
    "        elif self.data_use_type == \"val\":\n",
    "            return self.val_size\n",
    "  \n",
    "        else:\n",
    "            return self.test_size\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        item = dict()\n",
    "        \n",
    "        if self.data_use_type ==\"val\":\n",
    "            index = self.train_size + index - self.seq_len\n",
    "            \n",
    "        elif self.data_use_type ==\"test\":\n",
    "            index = self.train_size + self.val_size + index - self.seq_len\n",
    "        \n",
    "        for i in range(self.n_currencies):\n",
    "            window = self.x[i][index:index+self.seq_len]\n",
    "            window = (window -self.train_mean[i]) / self.train_std[i]\n",
    "            \n",
    "            item[self.currencies[i] + \"_window\"] = window\n",
    "            item[self.currencies[i] + \"_label\"]  = self.y[i][index+self.seq_len]\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Stack all weight matrices 1...h together for efficiency\n",
    "        # Note that in many implementations you see \"bias=False\" which is optional\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self._reset_parameters()\n",
    "        \n",
    "        \n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization, see PyTorch documentation\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "        \n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        # Determine value outputs\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "        values = values.reshape(batch_size, seq_length, embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "        \n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o\n",
    "        \n",
    "class EncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Dimensionality of the input\n",
    "            num_heads - Number of heads to use in the attention block\n",
    "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
    "            dropout - Dropout probability to use in the dropout layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Attention layer\n",
    "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
    "        \n",
    "        # Two-layer MLP\n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim)\n",
    "        )\n",
    "        \n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Attention part\n",
    "        attn_out = self.self_attn(x, mask=mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # MLP part\n",
    "        linear_out = self.linear_net(x)\n",
    "        x = x + self.dropout(linear_out)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers, **block_args):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
    "        \n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        for l in self.layers:\n",
    "            x = l(x, mask=mask)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def get_attention_maps(self, x, mask=None):\n",
    "        attention_maps = []\n",
    "        for l in self.layers:\n",
    "            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n",
    "            attention_maps.append(attn_map)\n",
    "            x = l(x)\n",
    "        return attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            d_model - Hidden dimensionality of the input.\n",
    "            max_len - Maximum length of a sequence to expect.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
    "        # Used for tensors that need to be on the same device as the module.\n",
    "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model) \n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "        \n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "    \n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradePredictor(pl.LightningModule): \n",
    "    def __init__(self, \n",
    "                 train_dataset,\n",
    "                 val_dataset,\n",
    "                 test_dataset,\n",
    "                 calculate_loss_weights,\n",
    "                 currencies,\n",
    "                 window_size,\n",
    "                 batch_size,\n",
    "                 input_dim, model_dim, num_classes, num_heads, num_layers, \n",
    "                 lr,dropout=0.0, input_dropout=0.0,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Hidden dimensionality of the input\n",
    "            model_dim - Hidden dimensionality to use inside the Transformer\n",
    "            num_classes - Number of classes to predict per sequence element\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention blocks\n",
    "            num_layers - Number of encoder blocks to use.\n",
    "            lr - Learning rate in the optimizer\n",
    "            warmup - Number of warmup steps. Usually between 50 and 500\n",
    "            max_iters - Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n",
    "            dropout - Dropout to apply inside the model\n",
    "            input_dropout - Dropout to apply on the input features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()    \n",
    "        self.num_tasks = len(currencies)\n",
    "\n",
    "        self._create_model()\n",
    "        \n",
    "        self.f1_score = pl.metrics.F1(num_classes=self.hparams.num_classes, average=\"macro\")\n",
    "        self.accuracy_score = pl.metrics.Accuracy()\n",
    "        \n",
    "        self.train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle = True)\n",
    "        self.val_dl = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        self.test_dl = DataLoader(test_dataset, batch_size=batch_size)\n",
    "        \n",
    "        if self.hparams.calculate_loss_weights:\n",
    "            loss_weights = []\n",
    "            for i in range(self.num_tasks):\n",
    "                train_labels = [int(train_dataset[n][self.hparams.currencies[i] +\"_label\"] )for n in range(train_dataset.__len__())]\n",
    "                samples_size = pd.DataFrame({\"label\": train_labels}).groupby(\"label\").size().to_numpy()\n",
    "                loss_weights.append((1 / samples_size) * sum(samples_size)/2)\n",
    "            self.weights = loss_weights\n",
    "        else:\n",
    "            self.weights = None\n",
    "            \n",
    "        if self.weights != None:\n",
    "            self.cross_entropy_loss = [nn.CrossEntropyLoss(weight= torch.tensor(weights).float()) for weights in self.weights]\n",
    "        else:\n",
    "            self.cross_entropy_loss = [nn.CrossEntropyLoss() for _ in range(self.num_tasks)]\n",
    "        \n",
    "        self.cross_entropy_loss = torch.nn.ModuleList(self.cross_entropy_loss)\n",
    "        \n",
    "    def _create_model(self):\n",
    "        # Input dim -> Model dim\n",
    "        self.input_net = nn.Linear(self.hparams.input_dim, self.hparams.model_dim)\n",
    "        \n",
    "        # Positional encoding for sequences\n",
    "        self.positional_encoding = PositionalEncoding(d_model=self.hparams.model_dim)\n",
    "        # Transformer\n",
    "        self.transformer = TransformerEncoder(num_layers=self.hparams.num_layers,\n",
    "                                              input_dim=self.hparams.model_dim,\n",
    "                                              dim_feedforward=2*self.hparams.model_dim,\n",
    "                                              num_heads=self.hparams.num_heads,\n",
    "                                              dropout=self.hparams.dropout)\n",
    "        # Output classifier per sequence lement\n",
    "        self.output_net = [nn.Sequential(\n",
    "            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n",
    "            nn.LayerNorm(self.hparams.model_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.hparams.dropout),\n",
    "            nn.Linear(self.hparams.model_dim, self.hparams.num_classes)\n",
    "        ) ]* self.num_tasks\n",
    "        \n",
    "        self.output_net = torch.nn.ModuleList(self.output_net)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, i, mask=None, add_positional_encoding=True):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features of shape [Batch, SeqLen, input_dim]\n",
    "            mask - Mask to apply on the attention outputs (optional)\n",
    "            add_positional_encoding - If True, we add the positional encoding to the input.\n",
    "                                      Might not be desired for some tasks.\n",
    "        \"\"\"\n",
    "        x = self.input_net(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        x = self.transformer(x, mask=mask)\n",
    "        x = x[:,-1,:]\n",
    "        x = self.output_net[i](x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n",
    "        \"\"\"\n",
    "        Function for extracting the attention matrices of the whole Transformer for a single batch.\n",
    "        Input arguments same as the forward pass.\n",
    "        \"\"\"\n",
    "        x = self.input_net(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n",
    "        return attention_maps\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        \n",
    "        # We don't return the lr scheduler because we need to apply it per iteration, not per epoch\n",
    "        self.lr_scheduler = CosineWarmupScheduler(optimizer, \n",
    "                                                  warmup=50, \n",
    "                                                  max_iters = 80* self.train_dl.__len__())\n",
    "        return optimizer\n",
    "    \n",
    "    \n",
    "    def optimizer_step(self, *args, **kwargs):\n",
    "        super().optimizer_step(*args, **kwargs)\n",
    "        self.lr_scheduler.step() # Step per iteration\n",
    "\n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        \n",
    "        loss = (torch.tensor(0.0, device=\"cuda:0\", requires_grad=True) + \\\n",
    "                torch.tensor(0.0, device=\"cuda:0\", requires_grad=True)) \n",
    "        for i in range(self.num_tasks):\n",
    "            x, y = batch[self.hparams.currencies[i] + \"_window\"], batch[self.hparams.currencies[i] + \"_label\"]\n",
    "  \n",
    "            preds = self.forward(x, i, add_positional_encoding=True) # No positional encodings as it is a set, not a sequence!\n",
    "\n",
    "            loss += self.cross_entropy_loss[i](preds, y) # Softmax/CE over set dimension\n",
    "            \n",
    "            acc = self.accuracy_score(torch.max(preds, dim=1)[1], y)\n",
    "            self.log(self.hparams.currencies[i] + \"_%s_acc\" % mode, acc, on_step=False,  prog_bar=True, on_epoch=True)\n",
    "            \n",
    "            f1 = self.f1_score(torch.max(preds, dim=1)[1], y)\n",
    "            self.log(self.hparams.currencies[i] + \"_%s_f1\" % mode, f1, on_step=False,  prog_bar=True, on_epoch=True)\n",
    "        \n",
    "        loss = loss / torch.tensor(self.num_tasks)\n",
    "        self.log(\"%s_loss\" % mode, loss, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"val\")\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"test\") \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_dl\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_dl\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_model(config):\n",
    "    task = \"multi_task_\" + \"_\".join(config[\"currency_list\"]) if len(config[\"currency_list\"]) > 1 else \"single_task_\" + config[\"currency_list\"][0]\n",
    "    classification = \"multi_classification\" if config[\"n_classes\"] > 2 else \"binary_classification\"\n",
    "    trend_removed = \"trend_removed\" if config[\"remove_trend\"] else \"\"\n",
    "    loss_weighted = \"loss_weighted\" if config[\"loss_weight_calculate\"] else \"\"\n",
    "\n",
    "    return \"_\".join([task, \"multi_head_attention\", loss_weighted, classification, trend_removed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deneme 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {#fix for this project\n",
    "          \"window_size\": 50, \n",
    "          \"dataset_percentages\": [0.97, 0.007, 0.023],\n",
    "          \"frenquency\": \"D\", \n",
    "          \"neutral_quantile\": 0.33,\n",
    "          \"batch_size\": 16,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CONFIG.copy()\n",
    "config.update({\"n_classes\": 2,\n",
    "               \"currency_list\": ['BTC'],\n",
    "               \"remove_trend\": True,\n",
    "              \"loss_weight_calculate\": False,}\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = name_model(config)\n",
    "####\n",
    "CURRENCY_LST = config[\"currency_list\"]\n",
    "N_CLASSES = config[\"n_classes\"]\n",
    "REMOVE_TREND =config[\"remove_trend\"]\n",
    "LOSS_WEIGHT_CALCULATE = config[\"loss_weight_calculate\"]\n",
    "###\n",
    "#FIXED\n",
    "TRAIN_PERCENTAGE, VAL_PERCENTAGE, TEST_PERCENTAGE = config[\"dataset_percentages\"] \n",
    "WINDOW_SIZE = config[\"window_size\"]\n",
    "FREQUENCY = config[\"frenquency\"]\n",
    "NEUTRAL_QUANTILE = config[\"neutral_quantile\"] if N_CLASSES > 2 else 0 \n",
    "BATCH_SIZE= config[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, features, dfs = get_data(CURRENCY_LST,\n",
    "                                N_CLASSES,\n",
    "                                 FREQUENCY, \n",
    "                                 WINDOW_SIZE,\n",
    "                                 neutral_quantile = NEUTRAL_QUANTILE,\n",
    "                                 log_price=True,\n",
    "                                 remove_trend=True,\n",
    "                                 include_indicators = False,\n",
    "                                 include_imfs = False\n",
    "                                )\n",
    "INPUT_FEATURE_SIZE = X.shape[-1]\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = [TimeSeriesDataset(CURRENCY_LST, \n",
    "                                                              X, \n",
    "                                                              y, \n",
    "                                                              dtype, \n",
    "                                                              TRAIN_PERCENTAGE, \n",
    "                                                              VAL_PERCENTAGE, \n",
    "                                                              TEST_PERCENTAGE, \n",
    "                                                              WINDOW_SIZE) for dtype in ['train', 'val', 'test']]\n",
    "config[\"dataset_sizes\"] = [len(train_dataset), len(val_dataset), len(test_dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:kp2pw8ko) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 134627<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_100009-kp2pw8ko/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_100009-kp2pw8ko/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">single_task_BTC_multi_head_attention__binary_classification_trend_removed</strong>: <a href=\"https://wandb.ai/aysenurk/deneme_2/runs/kp2pw8ko\" target=\"_blank\">https://wandb.ai/aysenurk/deneme_2/runs/kp2pw8ko</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:kp2pw8ko). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">single_task_BTC_multi_head_attention__binary_classification_trend_removed</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/aysenurk/deneme_2\" target=\"_blank\">https://wandb.ai/aysenurk/deneme_2</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/aysenurk/deneme_2/runs/3rmv7295\" target=\"_blank\">https://wandb.ai/aysenurk/deneme_2/runs/3rmv7295</a><br/>\n",
       "                Run data is saved locally in <code>/home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_100035-3rmv7295</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"deneme_2\",\n",
    "           config=config,\n",
    "           name = MODEL_NAME)\n",
    "logger = WandbLogger()\n",
    "# logger = TensorBoardLogger(\"../output/models/lstm_model_logs\", name=\"self_attention\")\n",
    "model = TradePredictor( \n",
    "                    input_dim=INPUT_FEATURE_SIZE,\n",
    "                    model_dim=64,\n",
    "                    num_heads=8,\n",
    "                    num_classes=N_CLASSES,\n",
    "                    num_layers=4,\n",
    "                    dropout=0.5,\n",
    "                    lr=5e-4,\n",
    "                    train_dataset = train_dataset,\n",
    "                    val_dataset = val_dataset,\n",
    "                    test_dataset = test_dataset,\n",
    "                    calculate_loss_weights = LOSS_WEIGHT_CALCULATE,\n",
    "                    currencies = CURRENCY_LST,\n",
    "                    window_size = WINDOW_SIZE,\n",
    "                    batch_size=BATCH_SIZE,)\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='val_loss',\n",
    "   min_delta=0.003,\n",
    "   patience=25,\n",
    "   verbose=True,\n",
    "   mode='min'\n",
    ")\n",
    "trainer = pl.Trainer(#default_root_dir=root_dir, \n",
    "                     #checkpoint_callback=ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
    "                     gpus=-1 , \n",
    "                     max_epochs=80,\n",
    "                     gradient_clip_val=2,\n",
    "                     progress_bar_refresh_rate=1, \n",
    "                     logger = logger, \n",
    "                     callbacks=[early_stop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff2c6161d2d440eb340b38f54296d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.617\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.071 >= min_delta = 0.003. New best score: 0.545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 0.535\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.016 >= min_delta = 0.003. New best score: 0.519\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537ec3d74cbd4cbcbe7af8fdf72e2fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.7096773982048035,\n",
      " 'BTC_test_f1': 0.7012333869934082,\n",
      " 'test_loss': 0.62295001745224}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 134747<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_100035-3rmv7295/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_100035-3rmv7295/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train_loss_step</td><td>0.45758</td></tr><tr><td>epoch</td><td>31</td></tr><tr><td>trainer/global_step</td><td>2467</td></tr><tr><td>_runtime</td><td>174</td></tr><tr><td>_timestamp</td><td>1621494212</td></tr><tr><td>_step</td><td>111</td></tr><tr><td>BTC_train_acc</td><td>0.73159</td></tr><tr><td>BTC_train_f1</td><td>0.71692</td></tr><tr><td>train_loss_epoch</td><td>0.57576</td></tr><tr><td>BTC_val_acc</td><td>0.77778</td></tr><tr><td>BTC_val_f1</td><td>0.775</td></tr><tr><td>val_loss</td><td>0.5789</td></tr><tr><td>BTC_test_acc</td><td>0.70968</td></tr><tr><td>BTC_test_f1</td><td>0.70123</td></tr><tr><td>test_loss</td><td>0.62295</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train_loss_step</td><td>▅▄▇▂▅▃▃▂▄▃▄▃▃▄▅▃▄▃▄▄▅▅▃▅▁▃▃▃▃▃▂▅▄▄▅▃▅▁█▂</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>BTC_train_acc</td><td>▁▅▆▆▆▇▆▆▇▇▆▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>BTC_train_f1</td><td>▁▅▆▆▆▇▆▇▇▇▆▇▇▇▇▇▇▆▇▇▇▇█▇▇▇▇█▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▅▃▄▄▃▂▃▂▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>BTC_val_acc</td><td>▁▅▁▅▅▅▅▅▅▅▅▁▁▅▁▅█████▅▅███▅████</td></tr><tr><td>BTC_val_f1</td><td>▁▆▃▆▆▆▆▆▆▆▆▃▃▆▃▆█████▆▆███▆████</td></tr><tr><td>val_loss</td><td>▆▆▇▂▅▇▂▄▅▂▆█▇▅█▂▃▃▂▃▂▂▁▂▂▃▂▄▂▂▄</td></tr><tr><td>BTC_test_acc</td><td>▁</td></tr><tr><td>BTC_test_f1</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">single_task_BTC_multi_head_attention__binary_classification_trend_removed</strong>: <a href=\"https://wandb.ai/aysenurk/deneme_2/runs/3rmv7295\" target=\"_blank\">https://wandb.ai/aysenurk/deneme_2/runs/3rmv7295</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.test()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#denemeleri tamamlandı experiment_mha.py dosyasına yazıldı işlemler\n",
    "def experiment(script):\n",
    "    !python ../pipelines/multi_task_price_change_prediction/experiment_mha.py $script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {#fix for this project\n",
    "          \"window_size\": 50, \n",
    "          \"dataset_percentages\": [0.97, 0.007, 0.023],\n",
    "          \"frenquency\": \"D\", \n",
    "          \"neutral_quantile\": 0.33,\n",
    "          \"batch_size\": 16,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "param_grid = {\n",
    "          \"n_classes\": [2,3],\n",
    "          \"currency_list\": [['BTC'], ['ETH'], ['LTC'], ['BTC', 'ETH'],  ['BTC', 'ETH', 'LTC']],\n",
    "          \"remove_trend\": [True, False],\n",
    "          \"loss_weight_calculate\": [True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 10:34:00.390855: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_multi_head_attention_loss_weighted_binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/n519psyv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_103358-n519psyv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|█| 80/80 [00:04<00:00, 18.48it/s, loss=0.694, v_num=psyv, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.642\n",
      "Epoch 0: 100%|█| 80/80 [00:04<00:00, 18.39it/s, loss=0.694, v_num=psyv, BTC_val_\n",
      "Epoch 1:  99%|▉| 79/80 [00:05<00:00, 15.18it/s, loss=0.666, v_num=psyv, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.077 >= min_delta = 0.003. New best score: 0.565\n",
      "Epoch 1: 100%|█| 80/80 [00:05<00:00, 15.20it/s, loss=0.666, v_num=psyv, BTC_val_\n",
      "Epoch 2: 100%|█| 80/80 [00:03<00:00, 20.69it/s, loss=0.615, v_num=psyv, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:03<00:00, 20.61it/s, loss=0.615, v_num=psyv, BTC_val_\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:03<00:00, 25.85it/s, loss=0.613, v_num=psyv, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.040 >= min_delta = 0.003. New best score: 0.525\n",
      "Epoch 3: 100%|█| 80/80 [00:03<00:00, 25.76it/s, loss=0.613, v_num=psyv, BTC_val_\n",
      "Epoch 4: 100%|█| 80/80 [00:03<00:00, 20.59it/s, loss=0.6, v_num=psyv, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:03<00:00, 20.49it/s, loss=0.6, v_num=psyv, BTC_val_ac\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:03<00:00, 24.56it/s, loss=0.635, v_num=psyv, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:03<00:00, 24.47it/s, loss=0.635, v_num=psyv, BTC_val_\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:03<00:00, 22.26it/s, loss=0.606, v_num=psyv, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:03<00:00, 22.19it/s, loss=0.606, v_num=psyv, BTC_val_\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:03<00:00, 20.40it/s, loss=0.637, v_num=psyv, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:03<00:00, 20.34it/s, loss=0.637, v_num=psyv, BTC_val_\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:04<00:00, 18.19it/s, loss=0.609, v_num=psyv, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:04<00:00, 18.14it/s, loss=0.609, v_num=psyv, BTC_val_\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:03<00:00, 22.78it/s, loss=0.597, v_num=psyv, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:03<00:00, 22.70it/s, loss=0.597, v_num=psyv, BTC_val_\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:03<00:00, 22.47it/s, loss=0.615, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:03<00:00, 22.39it/s, loss=0.615, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 29.72it/s, loss=0.594, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 29.52it/s, loss=0.594, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:04<00:00, 18.44it/s, loss=0.634, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:04<00:00, 18.38it/s, loss=0.634, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:04<00:00, 18.84it/s, loss=0.568, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:04<00:00, 18.76it/s, loss=0.568, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 28.02it/s, loss=0.587, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 27.86it/s, loss=0.587, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:03<00:00, 21.26it/s, loss=0.613, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:03<00:00, 21.17it/s, loss=0.613, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 29.14it/s, loss=0.597, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 29.02it/s, loss=0.597, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 29.28it/s, loss=0.565, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 0.505\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 29.15it/s, loss=0.565, v_num=psyv, BTC_val\n",
      "Epoch 18: 100%|█| 80/80 [00:04<00:00, 18.39it/s, loss=0.585, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:04<00:00, 18.35it/s, loss=0.585, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:03<00:00, 21.39it/s, loss=0.604, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:03<00:00, 21.31it/s, loss=0.604, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:03<00:00, 24.81it/s, loss=0.582, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:03<00:00, 24.68it/s, loss=0.582, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:03<00:00, 23.71it/s, loss=0.593, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:03<00:00, 23.60it/s, loss=0.593, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 28.84it/s, loss=0.571, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 28.68it/s, loss=0.571, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:05<00:00, 15.67it/s, loss=0.601, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:05<00:00, 15.62it/s, loss=0.601, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:03<00:00, 26.48it/s, loss=0.632, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:03<00:00, 26.34it/s, loss=0.632, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 27.42it/s, loss=0.595, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 27.06it/s, loss=0.595, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:04<00:00, 17.91it/s, loss=0.615, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:04<00:00, 17.86it/s, loss=0.615, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:03<00:00, 23.37it/s, loss=0.598, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:03<00:00, 23.24it/s, loss=0.598, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:04<00:00, 19.19it/s, loss=0.551, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:04<00:00, 18.96it/s, loss=0.551, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:05<00:00, 14.39it/s, loss=0.596, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:05<00:00, 14.36it/s, loss=0.596, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:06<00:00, 13.22it/s, loss=0.599, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:06<00:00, 13.20it/s, loss=0.599, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 31: 100%|█| 80/80 [00:05<00:00, 15.22it/s, loss=0.552, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.042 >= min_delta = 0.003. New best score: 0.463\n",
      "Epoch 31: 100%|█| 80/80 [00:05<00:00, 15.18it/s, loss=0.552, v_num=psyv, BTC_val\n",
      "Epoch 32: 100%|█| 80/80 [00:07<00:00, 10.97it/s, loss=0.55, v_num=psyv, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 80/80 [00:07<00:00, 10.94it/s, loss=0.55, v_num=psyv, BTC_val_\u001b[A\n",
      "Epoch 33: 100%|█| 80/80 [00:06<00:00, 13.27it/s, loss=0.642, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 80/80 [00:06<00:00, 13.24it/s, loss=0.642, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 34: 100%|█| 80/80 [00:03<00:00, 22.55it/s, loss=0.583, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 80/80 [00:03<00:00, 22.49it/s, loss=0.583, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 35: 100%|█| 80/80 [00:02<00:00, 30.57it/s, loss=0.568, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 80/80 [00:02<00:00, 30.45it/s, loss=0.568, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 36: 100%|█| 80/80 [00:03<00:00, 26.64it/s, loss=0.586, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 80/80 [00:03<00:00, 26.55it/s, loss=0.586, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 37: 100%|█| 80/80 [00:02<00:00, 37.43it/s, loss=0.606, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 80/80 [00:02<00:00, 37.25it/s, loss=0.606, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 38: 100%|█| 80/80 [00:02<00:00, 36.98it/s, loss=0.583, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 80/80 [00:02<00:00, 36.81it/s, loss=0.583, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 39: 100%|█| 80/80 [00:02<00:00, 37.04it/s, loss=0.614, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 80/80 [00:02<00:00, 36.87it/s, loss=0.614, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 40: 100%|█| 80/80 [00:02<00:00, 32.67it/s, loss=0.55, v_num=psyv, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 80/80 [00:02<00:00, 32.54it/s, loss=0.55, v_num=psyv, BTC_val_\u001b[A\n",
      "Epoch 41: 100%|█| 80/80 [00:02<00:00, 32.67it/s, loss=0.628, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 80/80 [00:02<00:00, 32.53it/s, loss=0.628, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 42: 100%|█| 80/80 [00:02<00:00, 32.94it/s, loss=0.559, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 80/80 [00:02<00:00, 32.60it/s, loss=0.559, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 43: 100%|█| 80/80 [00:02<00:00, 31.87it/s, loss=0.561, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 80/80 [00:02<00:00, 31.70it/s, loss=0.561, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 44: 100%|█| 80/80 [00:02<00:00, 29.97it/s, loss=0.549, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 80/80 [00:02<00:00, 29.86it/s, loss=0.549, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 45: 100%|█| 80/80 [00:03<00:00, 26.42it/s, loss=0.569, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 80/80 [00:03<00:00, 26.20it/s, loss=0.569, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 46: 100%|█| 80/80 [00:03<00:00, 23.92it/s, loss=0.603, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 80/80 [00:03<00:00, 23.85it/s, loss=0.603, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 47: 100%|█| 80/80 [00:02<00:00, 33.06it/s, loss=0.547, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 80/80 [00:02<00:00, 32.92it/s, loss=0.547, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 48: 100%|█| 80/80 [00:02<00:00, 29.71it/s, loss=0.566, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 80/80 [00:02<00:00, 29.60it/s, loss=0.566, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 49: 100%|█| 80/80 [00:02<00:00, 33.17it/s, loss=0.545, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 80/80 [00:02<00:00, 33.04it/s, loss=0.545, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 50: 100%|█| 80/80 [00:02<00:00, 36.96it/s, loss=0.521, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 80/80 [00:02<00:00, 36.79it/s, loss=0.521, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 51: 100%|█| 80/80 [00:02<00:00, 36.55it/s, loss=0.522, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 80/80 [00:02<00:00, 36.37it/s, loss=0.522, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 52: 100%|█| 80/80 [00:02<00:00, 36.87it/s, loss=0.528, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 80/80 [00:02<00:00, 36.69it/s, loss=0.528, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 53: 100%|█| 80/80 [00:02<00:00, 37.20it/s, loss=0.568, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 80/80 [00:02<00:00, 37.04it/s, loss=0.568, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 54: 100%|█| 80/80 [00:02<00:00, 36.87it/s, loss=0.588, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 80/80 [00:02<00:00, 36.70it/s, loss=0.588, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 55: 100%|█| 80/80 [00:02<00:00, 36.74it/s, loss=0.529, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 80/80 [00:02<00:00, 36.55it/s, loss=0.529, v_num=psyv, BTC_val\u001b[A\n",
      "Epoch 56: 100%|█| 80/80 [00:02<00:00, 36.96it/s, loss=0.533, v_num=psyv, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.463. Signaling Trainer to stop.\n",
      "Epoch 56: 100%|█| 80/80 [00:02<00:00, 36.77it/s, loss=0.533, v_num=psyv, BTC_val\n",
      "Epoch 56: 100%|█| 80/80 [00:02<00:00, 36.73it/s, loss=0.533, v_num=psyv, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 100.09it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6774193644523621,\n",
      " 'BTC_test_f1': 0.6693548560142517,\n",
      " 'test_loss': 0.6569545865058899}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 140042\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_103358-n519psyv/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_103358-n519psyv/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.49306\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 56\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4503\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 207\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621496245\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 204\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.73555\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.72153\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.54985\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.77778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.53219\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.67742\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.66935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.65695\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅▆█▆▄▃▇▃▅▂▄▅▂▃▆▆▅▃▄▆▂▄▃▄▂▅▅▅▂▂▆▆▃▁▂▅▅▅▅▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▁▄▅▆▆▇▆▆▆▆▆▆▆▇▇▇▆▇▇▇▇▇▇▇▇█▇▇█▇▇█▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▁▄▅▆▆▇▆▆▆▆▆▆▆▆▇▆▆▆▇▇▇▇▇▇▇█▇▇▇▇▇▇▇██▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▅▄▃▃▃▄▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁███████▁████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁█▁▁▁▁▁▁▁▁▁▁▂▁▁▁█▇█████▂████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▆▄▄▆▅▅▇█▇▃▅▅▂▅▃▅▃▃▃▃▃▃▁▂▂▃▄▄▃▄▃▂▃▄▄▄▄▄▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_multi_head_attention_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/n519psyv\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 10:37:36.816977: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_multi_head_attention_loss_weighted_binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2yhkc4db\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_103735-2yhkc4db\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|█| 80/80 [00:02<00:00, 37.03it/s, loss=0.667, v_num=c4db, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 80/80 [00:02<00:00, 36.83it/s, loss=0.667, v_num=c4db, BTC_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved. New best score: 0.610\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 35.83it/s, loss=0.601, v_num=c4db, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.042 >= min_delta = 0.003. New best score: 0.568\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 35.68it/s, loss=0.601, v_num=c4db, BTC_val_\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 34.37it/s, loss=0.594, v_num=c4db, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.106 >= min_delta = 0.003. New best score: 0.462\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 34.20it/s, loss=0.594, v_num=c4db, BTC_val_\n",
      "Epoch 3: 100%|█| 80/80 [00:02<00:00, 32.72it/s, loss=0.616, v_num=c4db, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:02<00:00, 32.53it/s, loss=0.616, v_num=c4db, BTC_val_\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 31.90it/s, loss=0.551, v_num=c4db, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 31.77it/s, loss=0.551, v_num=c4db, BTC_val_\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 34.19it/s, loss=0.611, v_num=c4db, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 34.05it/s, loss=0.611, v_num=c4db, BTC_val_\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 31.90it/s, loss=0.634, v_num=c4db, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 31.77it/s, loss=0.634, v_num=c4db, BTC_val_\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:02<00:00, 32.28it/s, loss=0.548, v_num=c4db, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:02<00:00, 32.15it/s, loss=0.548, v_num=c4db, BTC_val_\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 32.20it/s, loss=0.6, v_num=c4db, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 32.06it/s, loss=0.6, v_num=c4db, BTC_val_ac\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 29.75it/s, loss=0.622, v_num=c4db, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 29.64it/s, loss=0.622, v_num=c4db, BTC_val_\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:02<00:00, 31.41it/s, loss=0.614, v_num=c4db, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:02<00:00, 31.27it/s, loss=0.614, v_num=c4db, BTC_val\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 37.34it/s, loss=0.604, v_num=c4db, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 37.18it/s, loss=0.604, v_num=c4db, BTC_val\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 36.99it/s, loss=0.621, v_num=c4db, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 36.82it/s, loss=0.621, v_num=c4db, BTC_val\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 36.86it/s, loss=0.602, v_num=c4db, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 36.69it/s, loss=0.602, v_num=c4db, BTC_val\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 37.01it/s, loss=0.573, v_num=c4db, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 36.83it/s, loss=0.573, v_num=c4db, BTC_val\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 37.38it/s, loss=0.597, v_num=c4db, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 37.20it/s, loss=0.597, v_num=c4db, BTC_val\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 36.99it/s, loss=0.641, v_num=c4db, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 36.81it/s, loss=0.641, v_num=c4db, BTC_val\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 34.82it/s, loss=0.579, v_num=c4db, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 34.68it/s, loss=0.579, v_num=c4db, BTC_val\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 36.77it/s, loss=0.606, v_num=c4db, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 36.58it/s, loss=0.606, v_num=c4db, BTC_val\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 36.44it/s, loss=0.621, v_num=c4db, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 36.27it/s, loss=0.621, v_num=c4db, BTC_val\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 37.08it/s, loss=0.553, v_num=c4db, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 36.91it/s, loss=0.553, v_num=c4db, BTC_val\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 36.99it/s, loss=0.609, v_num=c4db, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 36.82it/s, loss=0.609, v_num=c4db, BTC_val\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 37.07it/s, loss=0.584, v_num=c4db, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 36.89it/s, loss=0.584, v_num=c4db, BTC_val\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:02<00:00, 37.27it/s, loss=0.564, v_num=c4db, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:02<00:00, 37.09it/s, loss=0.564, v_num=c4db, BTC_val\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 36.70it/s, loss=0.557, v_num=c4db, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 36.53it/s, loss=0.557, v_num=c4db, BTC_val\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 36.47it/s, loss=0.608, v_num=c4db, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 36.30it/s, loss=0.608, v_num=c4db, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|█| 80/80 [00:02<00:00, 36.90it/s, loss=0.579, v_num=c4db, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:02<00:00, 36.73it/s, loss=0.579, v_num=c4db, BTC_val\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 36.74it/s, loss=0.596, v_num=c4db, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 36.57it/s, loss=0.596, v_num=c4db, BTC_val\u001b[A\n",
      "                                                                                \u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.462. Signaling Trainer to stop.\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 36.52it/s, loss=0.596, v_num=c4db, BTC_val\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 106.69it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5806451439857483,\n",
      " 'BTC_test_f1': 0.5788606405258179,\n",
      " 'test_loss': 0.6972178220748901}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 140614\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_103735-2yhkc4db/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_103735-2yhkc4db/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.38692\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 27\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2212\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 73\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621496328\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.70309\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.6918\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.58217\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.88889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.88312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.50106\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.58065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.57886\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.69722\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▄▄▆▅▅▃▅▂▅▅▄▆▆▆▄▄▅▆▂▄▂▃▄▃▇▃▁▅█▃▅▃▄▁▅▄▂▅▄▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▁▄▇▆▇█▇▇█▇█▇▇█▇██████▇████▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▁▃▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇██▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▃▄▃▃▃▂▃▂▂▃▃▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▅▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁█▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▅▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁█▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▆▄▁▄▄▃▄█▂▄▃▄▇▂▂▂▇▄▃▅▄▂▃▄▄▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_multi_head_attention_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2yhkc4db\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 10:39:00.019067: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_multi_head_attention_loss_weighted_multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3cygvmko\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_103858-3cygvmko\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.5 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 79/80 [00:02<00:00, 36.43it/s, loss=1.13, v_num=vmko, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.046\n",
      "Epoch 0: 100%|█| 80/80 [00:02<00:00, 36.25it/s, loss=1.13, v_num=vmko, BTC_val_a\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 36.82it/s, loss=1.13, v_num=vmko, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 36.66it/s, loss=1.13, v_num=vmko, BTC_val_a\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 27.78it/s, loss=1.11, v_num=vmko, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 27.67it/s, loss=1.11, v_num=vmko, BTC_val_a\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:02<00:00, 28.20it/s, loss=1.11, v_num=vmko, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:02<00:00, 27.95it/s, loss=1.11, v_num=vmko, BTC_val_a\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 33.12it/s, loss=1.1, v_num=vmko, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 32.98it/s, loss=1.1, v_num=vmko, BTC_val_ac\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 33.69it/s, loss=1.11, v_num=vmko, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 33.55it/s, loss=1.11, v_num=vmko, BTC_val_a\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 33.16it/s, loss=1.1, v_num=vmko, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 33.02it/s, loss=1.1, v_num=vmko, BTC_val_ac\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|█| 80/80 [00:02<00:00, 33.94it/s, loss=1.1, v_num=vmko, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:02<00:00, 33.78it/s, loss=1.1, v_num=vmko, BTC_val_ac\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 37.58it/s, loss=1.1, v_num=vmko, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 37.42it/s, loss=1.1, v_num=vmko, BTC_val_ac\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 37.46it/s, loss=1.09, v_num=vmko, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 37.28it/s, loss=1.09, v_num=vmko, BTC_val_a\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:02<00:00, 37.40it/s, loss=1, v_num=vmko, BTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:02<00:00, 37.22it/s, loss=1, v_num=vmko, BTC_val_acc\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 37.43it/s, loss=0.982, v_num=vmko, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 37.25it/s, loss=0.982, v_num=vmko, BTC_val\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 37.30it/s, loss=0.964, v_num=vmko, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 37.04it/s, loss=0.964, v_num=vmko, BTC_val\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 30.79it/s, loss=1.03, v_num=vmko, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 30.67it/s, loss=1.03, v_num=vmko, BTC_val_\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 35.57it/s, loss=1.02, v_num=vmko, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 35.41it/s, loss=1.02, v_num=vmko, BTC_val_\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 28.89it/s, loss=0.956, v_num=vmko, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 28.63it/s, loss=0.956, v_num=vmko, BTC_val\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 30.91it/s, loss=0.954, v_num=vmko, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 30.78it/s, loss=0.954, v_num=vmko, BTC_val\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 36.16it/s, loss=1.08, v_num=vmko, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 35.98it/s, loss=1.08, v_num=vmko, BTC_val_\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 36.56it/s, loss=1.02, v_num=vmko, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 36.38it/s, loss=1.02, v_num=vmko, BTC_val_\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 32.03it/s, loss=0.994, v_num=vmko, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 31.91it/s, loss=0.994, v_num=vmko, BTC_val\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 31.00it/s, loss=1, v_num=vmko, BTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 30.87it/s, loss=1, v_num=vmko, BTC_val_acc\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 32.11it/s, loss=1.03, v_num=vmko, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 31.98it/s, loss=1.03, v_num=vmko, BTC_val_\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 30.83it/s, loss=0.966, v_num=vmko, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 30.70it/s, loss=0.966, v_num=vmko, BTC_val\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:02<00:00, 36.95it/s, loss=1.01, v_num=vmko, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:02<00:00, 36.77it/s, loss=1.01, v_num=vmko, BTC_val_\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 37.37it/s, loss=0.927, v_num=vmko, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 37.20it/s, loss=0.927, v_num=vmko, BTC_val\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 32.98it/s, loss=0.997, v_num=vmko, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 1.046. Signaling Trainer to stop.\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 32.85it/s, loss=0.997, v_num=vmko, BTC_val\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 32.82it/s, loss=0.997, v_num=vmko, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 101.86it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.35483869910240173,\n",
      " 'BTC_test_f1': 0.17448680102825165,\n",
      " 'test_loss': 1.0732976198196411}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 140873\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_103858-3cygvmko/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_103858-3cygvmko/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.93707\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2054\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 71\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621496409\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 93\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.40301\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.35709\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.99612\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.22222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20635\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.29529\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.35484\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.17449\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.0733\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ██▇▅▇▇▅▇▆▆▆▆▅▆▆▆▁▇▂▁▄▁█▄▄▇▂▇▁▄▄▆▃▆▂▅▄▇▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▃▄▄▁▆▄▂▅▂▇▆▇██▇▇▆▇▇▆█▇▇▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▂▄▃▁▃▃▂▃▂▅▆▇█▇▆▇▆▆▇▆▇▇▇▇▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▇▆▆▆▆▆▆▆▄▃▂▂▂▁▂▂▁▂▂▂▂▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc █▆███▁█▁█▃▁▁▃▁▁▆▁▃▁▁█▆▆▆▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▄▄▄▄▄▁▄▁▄▃▁▁▃▁▁▅▁▃▁▁█▅▅▅▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▂▂▂▂▂▂▂▂▂▆▆▇▄▅▃█▄▆▆▂▃▂▃▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_multi_head_attention_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3cygvmko\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 10:40:21.231836: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_multi_head_attention_loss_weighted_multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2z9rjrou\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_104019-2z9rjrou\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.5 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 79/80 [00:02<00:00, 35.20it/s, loss=1.15, v_num=jrou, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.097\n",
      "Epoch 0: 100%|█| 80/80 [00:02<00:00, 35.06it/s, loss=1.15, v_num=jrou, BTC_val_a\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 30.68it/s, loss=1.13, v_num=jrou, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 30.55it/s, loss=1.13, v_num=jrou, BTC_val_a\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 30.42it/s, loss=1.12, v_num=jrou, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.016 >= min_delta = 0.003. New best score: 1.081\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 30.30it/s, loss=1.12, v_num=jrou, BTC_val_a\n",
      "Epoch 3: 100%|█| 80/80 [00:02<00:00, 27.19it/s, loss=1.12, v_num=jrou, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:02<00:00, 26.96it/s, loss=1.12, v_num=jrou, BTC_val_a\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 33.54it/s, loss=1.12, v_num=jrou, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 33.40it/s, loss=1.12, v_num=jrou, BTC_val_a\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 34.38it/s, loss=1.08, v_num=jrou, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 34.23it/s, loss=1.08, v_num=jrou, BTC_val_a\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 30.98it/s, loss=1.08, v_num=jrou, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 30.86it/s, loss=1.08, v_num=jrou, BTC_val_a\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:02<00:00, 31.33it/s, loss=1.09, v_num=jrou, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.071 >= min_delta = 0.003. New best score: 1.010\n",
      "Epoch 7: 100%|█| 80/80 [00:02<00:00, 31.21it/s, loss=1.09, v_num=jrou, BTC_val_a\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 28.71it/s, loss=1.1, v_num=jrou, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 28.48it/s, loss=1.1, v_num=jrou, BTC_val_ac\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 30.16it/s, loss=1.06, v_num=jrou, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 30.05it/s, loss=1.06, v_num=jrou, BTC_val_a\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:02<00:00, 33.37it/s, loss=1.06, v_num=jrou, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:02<00:00, 33.24it/s, loss=1.06, v_num=jrou, BTC_val_\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 37.76it/s, loss=1.01, v_num=jrou, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 37.57it/s, loss=1.01, v_num=jrou, BTC_val_\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 27.73it/s, loss=1.07, v_num=jrou, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 27.63it/s, loss=1.07, v_num=jrou, BTC_val_\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 31.16it/s, loss=0.918, v_num=jrou, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 31.05it/s, loss=0.918, v_num=jrou, BTC_val\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 37.08it/s, loss=0.975, v_num=jrou, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 36.91it/s, loss=0.975, v_num=jrou, BTC_val\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 31.26it/s, loss=1, v_num=jrou, BTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 31.14it/s, loss=1, v_num=jrou, BTC_val_acc\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 33.47it/s, loss=0.978, v_num=jrou, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 33.33it/s, loss=0.978, v_num=jrou, BTC_val\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 33.12it/s, loss=0.998, v_num=jrou, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 32.97it/s, loss=0.998, v_num=jrou, BTC_val\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 37.49it/s, loss=1.01, v_num=jrou, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 37.31it/s, loss=1.01, v_num=jrou, BTC_val_\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 34.00it/s, loss=1.01, v_num=jrou, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 33.85it/s, loss=1.01, v_num=jrou, BTC_val_\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 35.85it/s, loss=1.01, v_num=jrou, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 35.70it/s, loss=1.01, v_num=jrou, BTC_val_\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 36.96it/s, loss=0.978, v_num=jrou, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 36.79it/s, loss=0.978, v_num=jrou, BTC_val\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 37.23it/s, loss=0.958, v_num=jrou, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 37.02it/s, loss=0.958, v_num=jrou, BTC_val\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:02<00:00, 36.59it/s, loss=0.977, v_num=jrou, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:02<00:00, 36.42it/s, loss=0.977, v_num=jrou, BTC_val\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 29.12it/s, loss=0.987, v_num=jrou, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 29.01it/s, loss=0.987, v_num=jrou, BTC_val\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 28.97it/s, loss=0.981, v_num=jrou, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 28.71it/s, loss=0.981, v_num=jrou, BTC_val\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:02<00:00, 29.55it/s, loss=0.996, v_num=jrou, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:02<00:00, 29.43it/s, loss=0.996, v_num=jrou, BTC_val\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 33.53it/s, loss=1.04, v_num=jrou, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 33.40it/s, loss=1.04, v_num=jrou, BTC_val_\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:02<00:00, 33.89it/s, loss=1.03, v_num=jrou, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:02<00:00, 33.53it/s, loss=1.03, v_num=jrou, BTC_val_\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:03<00:00, 20.90it/s, loss=0.999, v_num=jrou, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:03<00:00, 20.85it/s, loss=0.999, v_num=jrou, BTC_val\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:02<00:00, 30.90it/s, loss=0.961, v_num=jrou, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:02<00:00, 30.78it/s, loss=0.961, v_num=jrou, BTC_val\u001b[A\n",
      "Epoch 31: 100%|█| 80/80 [00:02<00:00, 32.27it/s, loss=0.945, v_num=jrou, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 80/80 [00:02<00:00, 32.11it/s, loss=0.945, v_num=jrou, BTC_val\u001b[A\n",
      "Epoch 32: 100%|█| 80/80 [00:02<00:00, 32.41it/s, loss=1.01, v_num=jrou, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 1.010. Signaling Trainer to stop.\n",
      "Epoch 32: 100%|█| 80/80 [00:02<00:00, 32.27it/s, loss=1.01, v_num=jrou, BTC_val_\n",
      "Epoch 32: 100%|█| 80/80 [00:02<00:00, 32.23it/s, loss=1.01, v_num=jrou, BTC_val_\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 28.24it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.35483869910240173,\n",
      " 'BTC_test_f1': 0.3497339189052582,\n",
      " 'test_loss': 0.9946774244308472}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 141112\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_104019-2z9rjrou/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_104019-2z9rjrou/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.87213\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2607\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 93\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621496512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 118\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.42359\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.37673\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.96084\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.22222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20635\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.22546\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.35484\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.34973\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.99468\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▄▅▆▅▅▅▅▅▄▅▅▂▄▂▃▅▅▄▄▂▃▇▅▄▇▄▁▂▃▂▄▄▇▇▃▄▅▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▃▁▄▁▄▃▅▁▂▄▆▆▆▆▆▇▇▇▇▇▇▇▇▆▇▆▇█▇█▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▂▁▁▁▃▃▄▃▄▆▇▇▇▇▆█▇▇▇▇▇▆▆▆▆▆▇▇▆█▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▆▆▆▅▅▅▅▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▂▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▅▁▅█▅▂▁▄▁▁▁▂▁▂▅▁▁▂▄▁▄▂▄▅▂▁▂▂▂▄▄▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▃▁▃█▃▂▁▅▁▁▁▃▁▃▇▁▁▃▅▁▅▃▅▇▃▁▃▃▃▅▅▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▂▂▂▂▂▂▂▁▃▄▄▆▅▄▃▇█▅▁▆▂▆▄▃▅▅▄▅▃▃▄▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_multi_head_attention_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2z9rjrou\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 10:42:03.263277: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_multi_head_attention__binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1zdit00v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_104201-1zdit00v\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/80 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 79/80 [00:02<00:00, 38.15it/s, loss=0.662, v_num=t00v, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.583\n",
      "Epoch 0: 100%|█| 80/80 [00:02<00:00, 36.83it/s, loss=0.662, v_num=t00v, BTC_val_\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 37.67it/s, loss=0.636, v_num=t00v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 37.50it/s, loss=0.636, v_num=t00v, BTC_val_\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 27.43it/s, loss=0.606, v_num=t00v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 27.34it/s, loss=0.606, v_num=t00v, BTC_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.057 >= min_delta = 0.003. New best score: 0.526\n",
      "Epoch 3: 100%|█| 80/80 [00:02<00:00, 29.22it/s, loss=0.638, v_num=t00v, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:02<00:00, 29.12it/s, loss=0.638, v_num=t00v, BTC_val_\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 38.35it/s, loss=0.608, v_num=t00v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 38.14it/s, loss=0.608, v_num=t00v, BTC_val_\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 39.00it/s, loss=0.598, v_num=t00v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 38.82it/s, loss=0.598, v_num=t00v, BTC_val_\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 34.43it/s, loss=0.596, v_num=t00v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 34.27it/s, loss=0.596, v_num=t00v, BTC_val_\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:02<00:00, 33.27it/s, loss=0.601, v_num=t00v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:02<00:00, 33.12it/s, loss=0.601, v_num=t00v, BTC_val_\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 31.83it/s, loss=0.589, v_num=t00v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 31.63it/s, loss=0.589, v_num=t00v, BTC_val_\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 30.23it/s, loss=0.607, v_num=t00v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 30.12it/s, loss=0.607, v_num=t00v, BTC_val_\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:02<00:00, 27.61it/s, loss=0.601, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:02<00:00, 27.52it/s, loss=0.601, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 31.39it/s, loss=0.595, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 31.26it/s, loss=0.595, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 27.51it/s, loss=0.564, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 27.42it/s, loss=0.564, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 36.55it/s, loss=0.619, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 36.39it/s, loss=0.619, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 37.00it/s, loss=0.578, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 36.82it/s, loss=0.578, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 37.34it/s, loss=0.626, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 37.17it/s, loss=0.626, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 36.70it/s, loss=0.599, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 0.516\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 36.53it/s, loss=0.599, v_num=t00v, BTC_val\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 34.33it/s, loss=0.571, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 34.16it/s, loss=0.571, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 29.23it/s, loss=0.578, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 29.12it/s, loss=0.578, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 33.03it/s, loss=0.59, v_num=t00v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 32.88it/s, loss=0.59, v_num=t00v, BTC_val_\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 32.99it/s, loss=0.599, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 32.84it/s, loss=0.599, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 32.84it/s, loss=0.623, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 32.71it/s, loss=0.623, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 34.48it/s, loss=0.573, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 34.33it/s, loss=0.573, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:02<00:00, 36.76it/s, loss=0.612, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:02<00:00, 36.59it/s, loss=0.612, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 36.28it/s, loss=0.625, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 36.09it/s, loss=0.625, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 27.58it/s, loss=0.564, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 27.49it/s, loss=0.564, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:02<00:00, 27.90it/s, loss=0.589, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:02<00:00, 27.66it/s, loss=0.589, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 30.40it/s, loss=0.57, v_num=t00v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 30.28it/s, loss=0.57, v_num=t00v, BTC_val_\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:02<00:00, 34.99it/s, loss=0.567, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:02<00:00, 34.83it/s, loss=0.567, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:02<00:00, 33.12it/s, loss=0.577, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:02<00:00, 32.98it/s, loss=0.577, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:02<00:00, 27.62it/s, loss=0.527, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:02<00:00, 27.53it/s, loss=0.527, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 31: 100%|█| 80/80 [00:02<00:00, 31.33it/s, loss=0.578, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 80/80 [00:02<00:00, 31.20it/s, loss=0.578, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 32: 100%|█| 80/80 [00:02<00:00, 33.44it/s, loss=0.579, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.510\n",
      "Epoch 32: 100%|█| 80/80 [00:02<00:00, 33.29it/s, loss=0.579, v_num=t00v, BTC_val\n",
      "Epoch 33: 100%|█| 80/80 [00:03<00:00, 25.15it/s, loss=0.584, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 80/80 [00:03<00:00, 24.95it/s, loss=0.584, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 34: 100%|█| 80/80 [00:02<00:00, 31.58it/s, loss=0.574, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 80/80 [00:02<00:00, 31.46it/s, loss=0.574, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 35: 100%|█| 80/80 [00:02<00:00, 28.07it/s, loss=0.593, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 80/80 [00:02<00:00, 27.88it/s, loss=0.593, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 36: 100%|█| 80/80 [00:02<00:00, 28.76it/s, loss=0.597, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 80/80 [00:02<00:00, 28.65it/s, loss=0.597, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 37: 100%|█| 80/80 [00:02<00:00, 26.86it/s, loss=0.609, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 80/80 [00:02<00:00, 26.75it/s, loss=0.609, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 38: 100%|█| 80/80 [00:02<00:00, 29.17it/s, loss=0.555, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 80/80 [00:02<00:00, 29.06it/s, loss=0.555, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 39: 100%|█| 80/80 [00:03<00:00, 26.46it/s, loss=0.571, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 0.496\n",
      "Epoch 39: 100%|█| 80/80 [00:03<00:00, 26.38it/s, loss=0.571, v_num=t00v, BTC_val\n",
      "Epoch 40: 100%|█| 80/80 [00:02<00:00, 27.80it/s, loss=0.612, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 80/80 [00:02<00:00, 27.56it/s, loss=0.612, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 41: 100%|█| 80/80 [00:02<00:00, 26.82it/s, loss=0.601, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 80/80 [00:02<00:00, 26.74it/s, loss=0.601, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 42: 100%|█| 80/80 [00:03<00:00, 23.31it/s, loss=0.558, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 80/80 [00:03<00:00, 23.24it/s, loss=0.558, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 43: 100%|█| 80/80 [00:02<00:00, 37.97it/s, loss=0.607, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 80/80 [00:02<00:00, 37.79it/s, loss=0.607, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 44: 100%|█| 80/80 [00:02<00:00, 34.88it/s, loss=0.579, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 80/80 [00:02<00:00, 34.72it/s, loss=0.579, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 45: 100%|█| 80/80 [00:02<00:00, 30.01it/s, loss=0.55, v_num=t00v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 80/80 [00:02<00:00, 29.90it/s, loss=0.55, v_num=t00v, BTC_val_\u001b[A\n",
      "Epoch 46: 100%|█| 80/80 [00:02<00:00, 33.56it/s, loss=0.583, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 80/80 [00:02<00:00, 33.29it/s, loss=0.583, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 47: 100%|█| 80/80 [00:02<00:00, 31.69it/s, loss=0.55, v_num=t00v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 80/80 [00:02<00:00, 31.57it/s, loss=0.55, v_num=t00v, BTC_val_\u001b[A\n",
      "Epoch 48: 100%|█| 80/80 [00:02<00:00, 37.77it/s, loss=0.565, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 80/80 [00:02<00:00, 37.59it/s, loss=0.565, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 49: 100%|█| 80/80 [00:02<00:00, 33.77it/s, loss=0.553, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 80/80 [00:02<00:00, 33.62it/s, loss=0.553, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 50: 100%|█| 80/80 [00:02<00:00, 34.09it/s, loss=0.563, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 80/80 [00:02<00:00, 33.76it/s, loss=0.563, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 51: 100%|█| 80/80 [00:02<00:00, 33.20it/s, loss=0.567, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 80/80 [00:02<00:00, 33.07it/s, loss=0.567, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 52: 100%|█| 80/80 [00:02<00:00, 33.91it/s, loss=0.517, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 80/80 [00:02<00:00, 33.77it/s, loss=0.517, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 53: 100%|█| 80/80 [00:03<00:00, 25.09it/s, loss=0.57, v_num=t00v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 80/80 [00:03<00:00, 25.02it/s, loss=0.57, v_num=t00v, BTC_val_\u001b[A\n",
      "Epoch 54: 100%|█| 80/80 [00:02<00:00, 36.10it/s, loss=0.619, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 80/80 [00:02<00:00, 35.94it/s, loss=0.619, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 55: 100%|█| 80/80 [00:02<00:00, 30.22it/s, loss=0.563, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 80/80 [00:02<00:00, 30.11it/s, loss=0.563, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 56: 100%|█| 80/80 [00:02<00:00, 30.01it/s, loss=0.54, v_num=t00v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 80/80 [00:02<00:00, 29.87it/s, loss=0.54, v_num=t00v, BTC_val_\u001b[A\n",
      "Epoch 57: 100%|█| 80/80 [00:02<00:00, 31.15it/s, loss=0.542, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 80/80 [00:02<00:00, 31.04it/s, loss=0.542, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 58: 100%|█| 80/80 [00:02<00:00, 32.99it/s, loss=0.59, v_num=t00v, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 80/80 [00:02<00:00, 32.85it/s, loss=0.59, v_num=t00v, BTC_val_\u001b[A\n",
      "Epoch 59: 100%|█| 80/80 [00:02<00:00, 34.26it/s, loss=0.549, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|█| 80/80 [00:02<00:00, 34.11it/s, loss=0.549, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 60: 100%|█| 80/80 [00:02<00:00, 36.90it/s, loss=0.616, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 60: 100%|█| 80/80 [00:02<00:00, 36.72it/s, loss=0.616, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 61: 100%|█| 80/80 [00:02<00:00, 36.82it/s, loss=0.576, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|█| 80/80 [00:02<00:00, 36.64it/s, loss=0.576, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 62: 100%|█| 80/80 [00:02<00:00, 36.33it/s, loss=0.547, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 62: 100%|█| 80/80 [00:02<00:00, 36.10it/s, loss=0.547, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 63: 100%|█| 80/80 [00:02<00:00, 29.67it/s, loss=0.557, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 63: 100%|█| 80/80 [00:02<00:00, 29.57it/s, loss=0.557, v_num=t00v, BTC_val\u001b[A\n",
      "Epoch 64: 100%|█| 80/80 [00:02<00:00, 32.92it/s, loss=0.547, v_num=t00v, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 64: 100%|█| 80/80 [00:02<00:00, 32.79it/s, loss=0.547, v_num=t00v, BTC_val\u001b[A\n",
      "                                                                                \u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.496. Signaling Trainer to stop.\n",
      "Epoch 64: 100%|█| 80/80 [00:02<00:00, 32.74it/s, loss=0.547, v_num=t00v, BTC_val\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 104.39it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6451612710952759,\n",
      " 'BTC_test_f1': 0.6364643573760986,\n",
      " 'test_loss': 0.645943284034729}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 141376\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_104201-1zdit00v/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_104201-1zdit00v/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.4853\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 5135\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 174\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621496695\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 232\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.73159\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.71683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.5558\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.77778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.56116\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.64516\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.63646\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.64594\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▄▅▂█▄▅▄▁▃▂▄▄▄▁▄▁▃▂▂▃▄▄▃▄▃▅▂▁▂▃▅▃▅▄▅▅▁▄▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▁▄▆▆▆▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▁▄▆▆▆▆▆▆▆▆▆▆▇▇▆▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▅▄▄▄▄▄▃▃▄▃▃▃▃▄▃▃▃▂▃▂▂▂▃▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅█████▅█████████▅██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅█████▅█████████▅██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄█▆▄▂▆▅▇▄▅▂▅▂▃▇▅▆▂▂▃▁▃▁▂▁▂▂▃▁▃▄▁▃▂▃▂▂▃▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_multi_head_attention__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1zdit00v\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 10:45:06.235903: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_multi_head_attention__binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2p3wgkuj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_104504-2p3wgkuj\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 79/80 [00:02<00:00, 37.32it/s, loss=0.717, v_num=gkuj, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.658\n",
      "Epoch 0: 100%|█| 80/80 [00:02<00:00, 36.12it/s, loss=0.717, v_num=gkuj, BTC_val_\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 39.13it/s, loss=0.642, v_num=gkuj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 38.95it/s, loss=0.642, v_num=gkuj, BTC_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.085 >= min_delta = 0.003. New best score: 0.573\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 35.43it/s, loss=0.631, v_num=gkuj, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.025 >= min_delta = 0.003. New best score: 0.548\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 35.26it/s, loss=0.631, v_num=gkuj, BTC_val_\n",
      "Epoch 3: 100%|█| 80/80 [00:02<00:00, 34.52it/s, loss=0.629, v_num=gkuj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:02<00:00, 34.35it/s, loss=0.629, v_num=gkuj, BTC_val_\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 35.45it/s, loss=0.602, v_num=gkuj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 35.28it/s, loss=0.602, v_num=gkuj, BTC_val_\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 36.63it/s, loss=0.608, v_num=gkuj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 36.44it/s, loss=0.608, v_num=gkuj, BTC_val_\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 37.73it/s, loss=0.602, v_num=gkuj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 37.56it/s, loss=0.602, v_num=gkuj, BTC_val_\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:02<00:00, 37.65it/s, loss=0.601, v_num=gkuj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:02<00:00, 37.46it/s, loss=0.601, v_num=gkuj, BTC_val_\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 34.08it/s, loss=0.619, v_num=gkuj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 33.93it/s, loss=0.619, v_num=gkuj, BTC_val_\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 36.84it/s, loss=0.622, v_num=gkuj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 36.67it/s, loss=0.622, v_num=gkuj, BTC_val_\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:02<00:00, 35.91it/s, loss=0.634, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:02<00:00, 35.75it/s, loss=0.634, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 29.36it/s, loss=0.611, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 29.25it/s, loss=0.611, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 31.68it/s, loss=0.602, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 31.55it/s, loss=0.602, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 37.00it/s, loss=0.607, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 36.82it/s, loss=0.607, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 37.02it/s, loss=0.625, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 36.85it/s, loss=0.625, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 32.98it/s, loss=0.608, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 32.85it/s, loss=0.608, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 27.53it/s, loss=0.599, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 0.527\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 27.42it/s, loss=0.599, v_num=gkuj, BTC_val\n",
      "Epoch 17: 100%|█| 80/80 [00:03<00:00, 25.63it/s, loss=0.573, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:03<00:00, 25.43it/s, loss=0.573, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 28.99it/s, loss=0.585, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 28.73it/s, loss=0.585, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 28.20it/s, loss=0.605, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 28.10it/s, loss=0.605, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 31.44it/s, loss=0.549, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.019 >= min_delta = 0.003. New best score: 0.508\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 31.31it/s, loss=0.549, v_num=gkuj, BTC_val\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 29.11it/s, loss=0.56, v_num=gkuj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 29.00it/s, loss=0.56, v_num=gkuj, BTC_val_\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:04<00:00, 18.91it/s, loss=0.586, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:04<00:00, 18.81it/s, loss=0.586, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:03<00:00, 26.28it/s, loss=0.606, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:03<00:00, 26.20it/s, loss=0.606, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 36.89it/s, loss=0.619, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 36.72it/s, loss=0.619, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 36.69it/s, loss=0.576, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 36.51it/s, loss=0.576, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:02<00:00, 36.14it/s, loss=0.583, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:02<00:00, 35.97it/s, loss=0.583, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 36.93it/s, loss=0.585, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 36.75it/s, loss=0.585, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:02<00:00, 36.61it/s, loss=0.562, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:02<00:00, 36.44it/s, loss=0.562, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:02<00:00, 36.70it/s, loss=0.595, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:02<00:00, 36.54it/s, loss=0.595, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:02<00:00, 36.75it/s, loss=0.577, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:02<00:00, 36.58it/s, loss=0.577, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 31: 100%|█| 80/80 [00:02<00:00, 36.35it/s, loss=0.62, v_num=gkuj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 80/80 [00:02<00:00, 36.18it/s, loss=0.62, v_num=gkuj, BTC_val_\u001b[A\n",
      "Epoch 32: 100%|█| 80/80 [00:02<00:00, 36.26it/s, loss=0.578, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 80/80 [00:02<00:00, 36.10it/s, loss=0.578, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 33: 100%|█| 80/80 [00:02<00:00, 36.78it/s, loss=0.554, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 80/80 [00:02<00:00, 36.61it/s, loss=0.554, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 34: 100%|█| 80/80 [00:02<00:00, 36.32it/s, loss=0.545, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 80/80 [00:02<00:00, 36.15it/s, loss=0.545, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 35: 100%|█| 80/80 [00:02<00:00, 36.41it/s, loss=0.573, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 80/80 [00:02<00:00, 36.24it/s, loss=0.573, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 36: 100%|█| 80/80 [00:02<00:00, 36.54it/s, loss=0.604, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 80/80 [00:02<00:00, 36.37it/s, loss=0.604, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 37: 100%|█| 80/80 [00:02<00:00, 36.27it/s, loss=0.576, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 80/80 [00:02<00:00, 36.10it/s, loss=0.576, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 38: 100%|█| 80/80 [00:02<00:00, 36.53it/s, loss=0.57, v_num=gkuj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 80/80 [00:02<00:00, 36.33it/s, loss=0.57, v_num=gkuj, BTC_val_\u001b[A\n",
      "Epoch 39: 100%|█| 80/80 [00:02<00:00, 36.23it/s, loss=0.541, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 80/80 [00:02<00:00, 36.06it/s, loss=0.541, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 40: 100%|█| 80/80 [00:02<00:00, 36.57it/s, loss=0.584, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 80/80 [00:02<00:00, 36.40it/s, loss=0.584, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 41: 100%|█| 80/80 [00:02<00:00, 36.69it/s, loss=0.551, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 80/80 [00:02<00:00, 36.52it/s, loss=0.551, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 42: 100%|█| 80/80 [00:02<00:00, 36.82it/s, loss=0.541, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 80/80 [00:02<00:00, 36.65it/s, loss=0.541, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 43: 100%|█| 80/80 [00:02<00:00, 35.92it/s, loss=0.59, v_num=gkuj, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 80/80 [00:02<00:00, 35.75it/s, loss=0.59, v_num=gkuj, BTC_val_\u001b[A\n",
      "Epoch 44: 100%|█| 80/80 [00:02<00:00, 36.28it/s, loss=0.563, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 80/80 [00:02<00:00, 36.11it/s, loss=0.563, v_num=gkuj, BTC_val\u001b[A\n",
      "Epoch 45: 100%|█| 80/80 [00:02<00:00, 36.05it/s, loss=0.589, v_num=gkuj, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.508. Signaling Trainer to stop.\n",
      "Epoch 45: 100%|█| 80/80 [00:02<00:00, 35.88it/s, loss=0.589, v_num=gkuj, BTC_val\n",
      "Epoch 45: 100%|█| 80/80 [00:02<00:00, 35.83it/s, loss=0.589, v_num=gkuj, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 90.36it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6129032373428345,\n",
      " 'BTC_test_f1': 0.6117511987686157,\n",
      " 'test_loss': 0.6672828197479248}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 141816\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_104504-2p3wgkuj/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_104504-2p3wgkuj/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.55597\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 45\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3634\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 119\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621496823\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 164\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.70942\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.69924\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.58111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.58462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.57811\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.6129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.61175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.66728\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▅▆▇▃▇▇▄▄▄▅▅▄█▅▄▆█▄▅▂▂▃▄▅▁▆▅▇▄▃▄▅▅▅▆▃▃▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▁▃▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇█▇██▇█████▇█▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▁▃▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇██▇█████▇█▇█▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▄▃▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁██████████████████████████████████████▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁████████████████████████████████▆███▆█▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▇▄▃▅█▅▄█▅▃▄▄▅▅▂▂▂▃▁▂▄▃▂▃▂▄▄▃▅▅▄▃▄▄▂▃▃▃▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_multi_head_attention__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2p3wgkuj\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 10:47:14.291992: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_multi_head_attention__multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1now3sy4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_104712-1now3sy4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.5 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 79/80 [00:02<00:00, 30.70it/s, loss=1.1, v_num=3sy4, BTC_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.016\n",
      "Epoch 0: 100%|█| 80/80 [00:02<00:00, 30.56it/s, loss=1.1, v_num=3sy4, BTC_val_ac\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 34.64it/s, loss=1.05, v_num=3sy4, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 34.49it/s, loss=1.05, v_num=3sy4, BTC_val_a\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 37.71it/s, loss=1.02, v_num=3sy4, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 37.40it/s, loss=1.02, v_num=3sy4, BTC_val_a\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.019 >= min_delta = 0.003. New best score: 0.997\n",
      "Epoch 3: 100%|█| 80/80 [00:02<00:00, 30.44it/s, loss=1.07, v_num=3sy4, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:02<00:00, 30.30it/s, loss=1.07, v_num=3sy4, BTC_val_a\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 35.87it/s, loss=1.02, v_num=3sy4, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 35.70it/s, loss=1.02, v_num=3sy4, BTC_val_a\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 32.74it/s, loss=0.99, v_num=3sy4, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.992\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 32.60it/s, loss=0.99, v_num=3sy4, BTC_val_a\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 29.75it/s, loss=1.05, v_num=3sy4, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 29.64it/s, loss=1.05, v_num=3sy4, BTC_val_a\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:02<00:00, 37.55it/s, loss=1.01, v_num=3sy4, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:02<00:00, 37.37it/s, loss=1.01, v_num=3sy4, BTC_val_a\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 30.05it/s, loss=1.02, v_num=3sy4, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 29.94it/s, loss=1.02, v_num=3sy4, BTC_val_a\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 35.20it/s, loss=1, v_num=3sy4, BTC_val_acc=\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 35.03it/s, loss=1, v_num=3sy4, BTC_val_acc=\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:02<00:00, 31.14it/s, loss=1.02, v_num=3sy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:02<00:00, 30.94it/s, loss=1.02, v_num=3sy4, BTC_val_\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 28.98it/s, loss=1.03, v_num=3sy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 28.87it/s, loss=1.03, v_num=3sy4, BTC_val_\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 31.03it/s, loss=1, v_num=3sy4, BTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 30.90it/s, loss=1, v_num=3sy4, BTC_val_acc\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 36.68it/s, loss=1.02, v_num=3sy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 36.52it/s, loss=1.02, v_num=3sy4, BTC_val_\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 36.46it/s, loss=1.02, v_num=3sy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 36.30it/s, loss=1.02, v_num=3sy4, BTC_val_\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 36.80it/s, loss=0.986, v_num=3sy4, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 36.62it/s, loss=0.986, v_num=3sy4, BTC_val\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 36.53it/s, loss=1.05, v_num=3sy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 36.38it/s, loss=1.05, v_num=3sy4, BTC_val_\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 27.53it/s, loss=1.02, v_num=3sy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 27.41it/s, loss=1.02, v_num=3sy4, BTC_val_\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 33.98it/s, loss=1.03, v_num=3sy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 33.83it/s, loss=1.03, v_num=3sy4, BTC_val_\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 32.54it/s, loss=1.03, v_num=3sy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 32.41it/s, loss=1.03, v_num=3sy4, BTC_val_\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 29.23it/s, loss=1.03, v_num=3sy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 29.12it/s, loss=1.03, v_num=3sy4, BTC_val_\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 30.63it/s, loss=1.01, v_num=3sy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 30.51it/s, loss=1.01, v_num=3sy4, BTC_val_\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 36.24it/s, loss=1.03, v_num=3sy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 36.07it/s, loss=1.03, v_num=3sy4, BTC_val_\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:02<00:00, 36.62it/s, loss=1.02, v_num=3sy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:02<00:00, 36.46it/s, loss=1.02, v_num=3sy4, BTC_val_\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 36.50it/s, loss=1.03, v_num=3sy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 36.32it/s, loss=1.03, v_num=3sy4, BTC_val_\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 35.86it/s, loss=1.02, v_num=3sy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 35.70it/s, loss=1.02, v_num=3sy4, BTC_val_\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:02<00:00, 36.46it/s, loss=1.04, v_num=3sy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:02<00:00, 36.29it/s, loss=1.04, v_num=3sy4, BTC_val_\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 36.76it/s, loss=1.03, v_num=3sy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 36.59it/s, loss=1.03, v_num=3sy4, BTC_val_\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:02<00:00, 36.27it/s, loss=1.02, v_num=3sy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:02<00:00, 36.10it/s, loss=1.02, v_num=3sy4, BTC_val_\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:02<00:00, 36.23it/s, loss=1.05, v_num=3sy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:02<00:00, 36.04it/s, loss=1.05, v_num=3sy4, BTC_val_\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:02<00:00, 35.85it/s, loss=1.04, v_num=3sy4, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:02<00:00, 35.69it/s, loss=1.04, v_num=3sy4, BTC_val_\u001b[A\n",
      "Monitored metric val_loss did not improve in the last 25 records. Best score: 0.992. Signaling Trainer to stop.\n",
      "Epoch 30: 100%|█| 80/80 [00:02<00:00, 35.64it/s, loss=1.04, v_num=3sy4, BTC_val_\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 102.10it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5483871102333069,\n",
      " 'BTC_test_f1': 0.23566308617591858,\n",
      " 'test_loss': 0.9630898237228394}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 142130\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_104712-1now3sy4/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_104712-1now3sy4/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.01079\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2449\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 83\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621496915\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 110\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.49089\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.22993\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.0168\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.00501\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.54839\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.23566\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.96309\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅▅▂▄▄█▆▄▆▅▆▅▃▆▇▃▄▃█▄▆██▅▄▄▃▁▃▄▃▄▂▄▆▃▃▄▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▁▂▅▅▆▇▇█▆█▇██████▆████████████▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 █▆▄▄▂▃▂▂▂▂▁▂▁▂▁▁▂▃▁▁▁▁▁▁▁▁▁▁▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▄▄▂▂▂▁▂▂▃▂▂▂▂▂▁▂▂▂▂▁▂▂▂▂▁▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ██▃▆▄▂▄▄▃▅▄▆▅▄▃▅▁▆▄▄▄▄▄▃▂▅▅▄▅▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_multi_head_attention__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1now3sy4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 10:48:47.439364: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_BTC_multi_head_attention__multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1b3ysuze\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_104845-1b3ysuze\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.5 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/80 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 79/80 [00:02<00:00, 30.85it/s, loss=1.09, v_num=suze, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.988\n",
      "Epoch 0: 100%|█| 80/80 [00:02<00:00, 30.73it/s, loss=1.09, v_num=suze, BTC_val_a\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 38.59it/s, loss=0.995, v_num=suze, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 38.39it/s, loss=0.995, v_num=suze, BTC_val_\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 26.81it/s, loss=1.04, v_num=suze, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 26.73it/s, loss=1.04, v_num=suze, BTC_val_a\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:02<00:00, 32.75it/s, loss=1.05, v_num=suze, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:02<00:00, 32.62it/s, loss=1.05, v_num=suze, BTC_val_a\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 34.59it/s, loss=1.03, v_num=suze, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 34.43it/s, loss=1.03, v_num=suze, BTC_val_a\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 32.00it/s, loss=1.08, v_num=suze, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 31.88it/s, loss=1.08, v_num=suze, BTC_val_a\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 37.46it/s, loss=1.03, v_num=suze, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 37.28it/s, loss=1.03, v_num=suze, BTC_val_a\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:02<00:00, 34.31it/s, loss=1, v_num=suze, BTC_val_acc=\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:02<00:00, 34.06it/s, loss=1, v_num=suze, BTC_val_acc=\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 30.45it/s, loss=1.03, v_num=suze, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 30.32it/s, loss=1.03, v_num=suze, BTC_val_a\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 33.38it/s, loss=1.04, v_num=suze, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 33.25it/s, loss=1.04, v_num=suze, BTC_val_a\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:02<00:00, 29.23it/s, loss=1.01, v_num=suze, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.032 >= min_delta = 0.003. New best score: 0.956\n",
      "Epoch 10: 100%|█| 80/80 [00:02<00:00, 29.13it/s, loss=1.01, v_num=suze, BTC_val_\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 29.41it/s, loss=0.991, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 29.30it/s, loss=0.991, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 33.99it/s, loss=0.996, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 33.84it/s, loss=0.996, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 37.01it/s, loss=0.974, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 36.84it/s, loss=0.974, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 36.95it/s, loss=0.957, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 36.77it/s, loss=0.957, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 37.16it/s, loss=0.984, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 36.97it/s, loss=0.984, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 31.20it/s, loss=0.966, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 31.08it/s, loss=0.966, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 36.01it/s, loss=0.971, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 35.84it/s, loss=0.971, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 29.03it/s, loss=0.941, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 28.92it/s, loss=0.941, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 29.23it/s, loss=0.952, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 29.11it/s, loss=0.952, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 31.74it/s, loss=0.933, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 31.61it/s, loss=0.933, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 33.30it/s, loss=0.963, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 33.17it/s, loss=0.963, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 33.04it/s, loss=0.962, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 32.89it/s, loss=0.962, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:02<00:00, 35.06it/s, loss=0.944, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:02<00:00, 34.86it/s, loss=0.944, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 33.25it/s, loss=0.938, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 33.11it/s, loss=0.938, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 30.82it/s, loss=0.969, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 30.70it/s, loss=0.969, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:02<00:00, 32.50it/s, loss=0.97, v_num=suze, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:02<00:00, 32.33it/s, loss=0.97, v_num=suze, BTC_val_\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 37.52it/s, loss=0.956, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 37.34it/s, loss=0.956, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:02<00:00, 33.82it/s, loss=0.957, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:02<00:00, 33.67it/s, loss=0.957, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:02<00:00, 35.70it/s, loss=0.957, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:02<00:00, 35.52it/s, loss=0.957, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:02<00:00, 31.23it/s, loss=0.917, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:02<00:00, 31.07it/s, loss=0.917, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 31: 100%|█| 80/80 [00:02<00:00, 29.66it/s, loss=0.941, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 80/80 [00:02<00:00, 29.54it/s, loss=0.941, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 32: 100%|█| 80/80 [00:02<00:00, 29.59it/s, loss=0.957, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 80/80 [00:02<00:00, 29.49it/s, loss=0.957, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 33: 100%|█| 80/80 [00:02<00:00, 35.22it/s, loss=0.899, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 80/80 [00:02<00:00, 35.06it/s, loss=0.899, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 34: 100%|█| 80/80 [00:02<00:00, 34.31it/s, loss=0.911, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 80/80 [00:02<00:00, 34.15it/s, loss=0.911, v_num=suze, BTC_val\u001b[A\n",
      "Epoch 35: 100%|█| 80/80 [00:02<00:00, 29.88it/s, loss=0.916, v_num=suze, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 80/80 [00:02<00:00, 29.76it/s, loss=0.916, v_num=suze, BTC_val\u001b[A\n",
      "                                                                                \u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.956. Signaling Trainer to stop.\n",
      "Epoch 35: 100%|█| 80/80 [00:02<00:00, 29.73it/s, loss=0.916, v_num=suze, BTC_val\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 101.35it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5161290168762207,\n",
      " 'BTC_test_f1': 0.22689731419086456,\n",
      " 'test_loss': 0.8969842195510864}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 142396\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_104845-1b3ysuze/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_104845-1b3ysuze/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.85563\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 35\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2844\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 98\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621497023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.48298\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.26411\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.93176\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.17221\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.51613\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.2269\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.89698\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▅▂▄▃█▃▄▄▃▄▅▁▆▃▅▂▁▃▃▂▃▄▄▄▄▂▅▅▂▆▄▅▃▁▅█▆▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▁▅▅▆▆▆▇▇▇█▇█▇▅▇▆▇▅▆▆▇▆▆▆▆▅▇▇█▇▆▇▇▇▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▅▄▂▂▁▁▂▂▁▃▃▂▅▆▄▅▇▇▄▇▆▄▃█▃▆▃▅▇▇▆▅▂▆▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▅▅▅▅▅▅▅▄▄▄▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁█▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁█▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▂▂▂▂▂▂▂▂▂▃▁▁▃▃▅▂▅▆█▂▅▄▂▅▆▄▅▅▃▃▅▄▄▄▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_BTC_multi_head_attention__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1b3ysuze\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 10:50:37.564975: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_multi_head_attention_loss_weighted_binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/nvff0fti\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_105035-nvff0fti\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Training: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 79/80 [00:02<00:00, 27.94it/s, loss=0.684, v_num=0fti, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.537\n",
      "Epoch 0: 100%|█| 80/80 [00:02<00:00, 27.86it/s, loss=0.684, v_num=0fti, ETH_val_\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 37.89it/s, loss=0.644, v_num=0fti, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 37.69it/s, loss=0.644, v_num=0fti, ETH_val_\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 36.12it/s, loss=0.611, v_num=0fti, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.142 >= min_delta = 0.003. New best score: 0.395\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 35.96it/s, loss=0.611, v_num=0fti, ETH_val_\n",
      "Epoch 3: 100%|█| 80/80 [00:02<00:00, 30.31it/s, loss=0.605, v_num=0fti, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:02<00:00, 30.20it/s, loss=0.605, v_num=0fti, ETH_val_\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 31.95it/s, loss=0.601, v_num=0fti, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 31.83it/s, loss=0.601, v_num=0fti, ETH_val_\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 30.66it/s, loss=0.609, v_num=0fti, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 30.40it/s, loss=0.609, v_num=0fti, ETH_val_\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 29.99it/s, loss=0.593, v_num=0fti, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 29.88it/s, loss=0.593, v_num=0fti, ETH_val_\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:03<00:00, 25.50it/s, loss=0.593, v_num=0fti, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.019 >= min_delta = 0.003. New best score: 0.376\n",
      "Epoch 7: 100%|█| 80/80 [00:03<00:00, 25.41it/s, loss=0.593, v_num=0fti, ETH_val_\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 30.08it/s, loss=0.592, v_num=0fti, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 29.94it/s, loss=0.592, v_num=0fti, ETH_val_\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 29.44it/s, loss=0.62, v_num=0fti, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 29.17it/s, loss=0.62, v_num=0fti, ETH_val_a\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:02<00:00, 29.34it/s, loss=0.594, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:02<00:00, 29.21it/s, loss=0.594, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 34.86it/s, loss=0.597, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 34.69it/s, loss=0.597, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 32.83it/s, loss=0.621, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 32.70it/s, loss=0.621, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 32.92it/s, loss=0.634, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 32.78it/s, loss=0.634, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:03<00:00, 25.57it/s, loss=0.627, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:03<00:00, 25.47it/s, loss=0.627, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 31.60it/s, loss=0.571, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 31.47it/s, loss=0.571, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 34.93it/s, loss=0.584, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 34.79it/s, loss=0.584, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 37.32it/s, loss=0.569, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 37.14it/s, loss=0.569, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 34.14it/s, loss=0.607, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 33.99it/s, loss=0.607, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 33.62it/s, loss=0.593, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 33.48it/s, loss=0.593, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 30.73it/s, loss=0.567, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 30.59it/s, loss=0.567, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 28.83it/s, loss=0.613, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 28.72it/s, loss=0.613, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 33.43it/s, loss=0.566, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 33.29it/s, loss=0.566, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:02<00:00, 33.70it/s, loss=0.544, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:02<00:00, 33.56it/s, loss=0.544, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 30.77it/s, loss=0.536, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 30.63it/s, loss=0.536, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 32.90it/s, loss=0.551, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 32.76it/s, loss=0.551, v_num=0fti, ETH_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|█| 80/80 [00:02<00:00, 27.97it/s, loss=0.581, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:02<00:00, 27.84it/s, loss=0.581, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 27.82it/s, loss=0.578, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 27.71it/s, loss=0.578, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:02<00:00, 35.48it/s, loss=0.566, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:02<00:00, 35.22it/s, loss=0.566, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:02<00:00, 34.06it/s, loss=0.593, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:02<00:00, 33.90it/s, loss=0.593, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:02<00:00, 36.70it/s, loss=0.548, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:02<00:00, 36.52it/s, loss=0.548, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 31: 100%|█| 80/80 [00:02<00:00, 36.83it/s, loss=0.535, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 80/80 [00:02<00:00, 36.66it/s, loss=0.535, v_num=0fti, ETH_val\u001b[A\n",
      "Epoch 32: 100%|█| 80/80 [00:02<00:00, 36.34it/s, loss=0.561, v_num=0fti, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 80/80 [00:02<00:00, 36.17it/s, loss=0.561, v_num=0fti, ETH_valMonitored metric val_loss did not improve in the last 25 records. Best score: 0.376. Signaling Trainer to stop.\n",
      "\n",
      "Epoch 32: 100%|█| 80/80 [00:02<00:00, 36.11it/s, loss=0.561, v_num=0fti, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 100.87it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.5806451439857483,\n",
      " 'ETH_test_f1': 0.5806451439857483,\n",
      " 'test_loss': 0.6567022800445557}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 142684\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_105035-nvff0fti/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_105035-nvff0fti/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.52805\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2607\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 93\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621497129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 118\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.71338\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.70071\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.56001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.49077\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.58065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.58065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.6567\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▇▇▂▄▃▂▇▅█▅▃▅▅▃▄▃▂▄▂▆▂▄▂▇▇▅▂▅▄█▁▅▄▂▂▃▁█▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▆▆▆▆▇▇▇█▇▇▇▆▇▇▇█▇█▇▇██▇▇██████▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▁▅▆▆▆▇▇▇█▇▇▇▆▇▇▇█▇██▇██▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▄▄▃▃▃▂▂▃▂▂▂▃▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▃▁▆▆▆▆▆▆▆█▅▅▅▆▆▆▆▆▅▁▆▃▃▆▅▃▅▃▁▁▃▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▃▁▆▆▆▆▆▆▆█▅▅▅▆▆▆▆▆▅▁▆▃▃▆▅▃▅▃▁▁▃▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅█▁▂▁▂▃▁▂▄▄▃▄▂▃▂▃▃▃▅▃▄▃▄▃▃▄▄▅▄▄▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_multi_head_attention_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/nvff0fti\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 10:52:20.997804: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_multi_head_attention_loss_weighted_binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3dsopgb3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_105219-3dsopgb3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 79/80 [00:02<00:00, 30.72it/s, loss=0.655, v_num=pgb3, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.576\n",
      "Epoch 0: 100%|█| 80/80 [00:02<00:00, 30.79it/s, loss=0.655, v_num=pgb3, ETH_val_\n",
      "Epoch 1:  99%|▉| 79/80 [00:02<00:00, 29.04it/s, loss=0.608, v_num=pgb3, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.181 >= min_delta = 0.003. New best score: 0.395\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 29.10it/s, loss=0.608, v_num=pgb3, ETH_val_\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 30.09it/s, loss=0.641, v_num=pgb3, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 29.96it/s, loss=0.641, v_num=pgb3, ETH_val_\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:02<00:00, 33.30it/s, loss=0.611, v_num=pgb3, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.020 >= min_delta = 0.003. New best score: 0.375\n",
      "Epoch 3: 100%|█| 80/80 [00:02<00:00, 33.03it/s, loss=0.611, v_num=pgb3, ETH_val_\n",
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 27.83it/s, loss=0.601, v_num=pgb3, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 27.58it/s, loss=0.601, v_num=pgb3, ETH_val_\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 30.51it/s, loss=0.594, v_num=pgb3, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 30.40it/s, loss=0.594, v_num=pgb3, ETH_val_\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 31.68it/s, loss=0.602, v_num=pgb3, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 31.56it/s, loss=0.602, v_num=pgb3, ETH_val_\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:02<00:00, 32.88it/s, loss=0.594, v_num=pgb3, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:02<00:00, 32.74it/s, loss=0.594, v_num=pgb3, ETH_val_\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 27.75it/s, loss=0.596, v_num=pgb3, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 27.51it/s, loss=0.596, v_num=pgb3, ETH_val_\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 31.61it/s, loss=0.587, v_num=pgb3, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 31.40it/s, loss=0.587, v_num=pgb3, ETH_val_\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:02<00:00, 29.80it/s, loss=0.613, v_num=pgb3, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:02<00:00, 29.69it/s, loss=0.613, v_num=pgb3, ETH_val\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 31.60it/s, loss=0.597, v_num=pgb3, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 31.47it/s, loss=0.597, v_num=pgb3, ETH_val\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 31.31it/s, loss=0.587, v_num=pgb3, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 31.20it/s, loss=0.587, v_num=pgb3, ETH_val\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 31.02it/s, loss=0.612, v_num=pgb3, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 30.89it/s, loss=0.612, v_num=pgb3, ETH_val\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 36.96it/s, loss=0.582, v_num=pgb3, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 36.79it/s, loss=0.582, v_num=pgb3, ETH_val\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 36.31it/s, loss=0.57, v_num=pgb3, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 36.10it/s, loss=0.57, v_num=pgb3, ETH_val_\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 36.56it/s, loss=0.564, v_num=pgb3, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 36.39it/s, loss=0.564, v_num=pgb3, ETH_val\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 36.46it/s, loss=0.583, v_num=pgb3, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 36.29it/s, loss=0.583, v_num=pgb3, ETH_val\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 36.35it/s, loss=0.555, v_num=pgb3, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 36.19it/s, loss=0.555, v_num=pgb3, ETH_val\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 36.46it/s, loss=0.575, v_num=pgb3, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 36.29it/s, loss=0.575, v_num=pgb3, ETH_val\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 34.98it/s, loss=0.576, v_num=pgb3, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 34.83it/s, loss=0.576, v_num=pgb3, ETH_val\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 32.85it/s, loss=0.587, v_num=pgb3, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 32.71it/s, loss=0.587, v_num=pgb3, ETH_val\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 34.32it/s, loss=0.574, v_num=pgb3, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 34.16it/s, loss=0.574, v_num=pgb3, ETH_val\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:02<00:00, 35.15it/s, loss=0.618, v_num=pgb3, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:02<00:00, 34.99it/s, loss=0.618, v_num=pgb3, ETH_val\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 29.50it/s, loss=0.57, v_num=pgb3, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 29.40it/s, loss=0.57, v_num=pgb3, ETH_val_\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 34.51it/s, loss=0.554, v_num=pgb3, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 34.36it/s, loss=0.554, v_num=pgb3, ETH_val\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:02<00:00, 31.19it/s, loss=0.595, v_num=pgb3, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:02<00:00, 31.06it/s, loss=0.595, v_num=pgb3, ETH_val\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 30.77it/s, loss=0.557, v_num=pgb3, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 30.66it/s, loss=0.557, v_num=pgb3, ETH_val\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:02<00:00, 29.97it/s, loss=0.533, v_num=pgb3, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.375. Signaling Trainer to stop.\n",
      "Epoch 28: 100%|█| 80/80 [00:02<00:00, 29.86it/s, loss=0.533, v_num=pgb3, ETH_val\n",
      "Epoch 28: 100%|█| 80/80 [00:02<00:00, 29.82it/s, loss=0.533, v_num=pgb3, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 104.97it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.6451612710952759,\n",
      " 'ETH_test_f1': 0.6436997652053833,\n",
      " 'test_loss': 0.6356152892112732}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 143008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_105219-3dsopgb3/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_105219-3dsopgb3/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.56444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 28\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2291\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 81\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621497220\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.71496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.69872\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.56728\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.51532\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.64516\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.6437\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.63562\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▅▃█▄▄▃█▆▅▆▄▆▄▅▃▃▅▃▄▄▄█▅▃▄▄▃▅▄▄▄▁▄▃▃▄▃▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▆▆▇▆▇▇▆▇▇▇▆▇▇▇▇▇▇▇▇▆████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▁▅▆▇▆▇▇▇▇▇▇▆▇▇▇█▇▇▇▇▆████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▄▃▃▃▃▃▂▃▂▃▂▂▂▂▂▂▂▁▂▂▁▁▂▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁██▆▆▆▆▆▆▆▆▆▆█▆▆▆▆▆▆▁▁▅▁▅▁▅▅▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁██▆▆▆▆▆▆▆▆▆▆█▆▆▆▆▆▆▁▁▅▁▅▁▅▅▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▂▄▁▁▂▄▃▄▃▃▃▅▃▃▂▁▄▂▃▄▅▅▆▄▅▅▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_multi_head_attention_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3dsopgb3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 10:53:51.905311: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_multi_head_attention_loss_weighted_multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/18mze7so\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_105350-18mze7so\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.5 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/80 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|█| 80/80 [00:02<00:00, 37.56it/s, loss=1.16, v_num=e7so, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.147\n",
      "Epoch 0: 100%|█| 80/80 [00:02<00:00, 37.34it/s, loss=1.16, v_num=e7so, ETH_val_a\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 36.54it/s, loss=1.12, v_num=e7so, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.028 >= min_delta = 0.003. New best score: 1.118\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 36.25it/s, loss=1.12, v_num=e7so, ETH_val_a\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 30.90it/s, loss=1.11, v_num=e7so, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 30.58it/s, loss=1.11, v_num=e7so, ETH_val_a\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:02<00:00, 32.02it/s, loss=1.12, v_num=e7so, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:02<00:00, 31.87it/s, loss=1.12, v_num=e7so, ETH_val_a\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 30.44it/s, loss=1.13, v_num=e7so, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 30.32it/s, loss=1.13, v_num=e7so, ETH_val_a\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 35.47it/s, loss=1.11, v_num=e7so, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 35.32it/s, loss=1.11, v_num=e7so, ETH_val_a\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.022 >= min_delta = 0.003. New best score: 1.096\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 33.35it/s, loss=1.12, v_num=e7so, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 33.17it/s, loss=1.12, v_num=e7so, ETH_val_a\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:02<00:00, 30.23it/s, loss=1.1, v_num=e7so, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:02<00:00, 30.11it/s, loss=1.1, v_num=e7so, ETH_val_ac\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 33.48it/s, loss=1.1, v_num=e7so, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 33.34it/s, loss=1.1, v_num=e7so, ETH_val_ac\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 34.81it/s, loss=1.1, v_num=e7so, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 34.66it/s, loss=1.1, v_num=e7so, ETH_val_ac\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:02<00:00, 33.17it/s, loss=1.09, v_num=e7so, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:02<00:00, 33.04it/s, loss=1.09, v_num=e7so, ETH_val_\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 34.76it/s, loss=1.06, v_num=e7so, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 34.61it/s, loss=1.06, v_num=e7so, ETH_val_\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 36.93it/s, loss=1.07, v_num=e7so, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 36.76it/s, loss=1.07, v_num=e7so, ETH_val_\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 30.21it/s, loss=1.05, v_num=e7so, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 30.10it/s, loss=1.05, v_num=e7so, ETH_val_\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 31.97it/s, loss=1.03, v_num=e7so, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.174 >= min_delta = 0.003. New best score: 0.922\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 31.78it/s, loss=1.03, v_num=e7so, ETH_val_\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 33.77it/s, loss=1.01, v_num=e7so, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 33.59it/s, loss=1.01, v_num=e7so, ETH_val_\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 26.90it/s, loss=0.977, v_num=e7so, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 26.81it/s, loss=0.977, v_num=e7so, ETH_val\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 33.21it/s, loss=0.991, v_num=e7so, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 32.99it/s, loss=0.991, v_num=e7so, ETH_val\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 29.02it/s, loss=0.948, v_num=e7so, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 28.90it/s, loss=0.948, v_num=e7so, ETH_val\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 28.77it/s, loss=0.961, v_num=e7so, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 28.67it/s, loss=0.961, v_num=e7so, ETH_val\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 29.47it/s, loss=0.939, v_num=e7so, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 29.36it/s, loss=0.939, v_num=e7so, ETH_val\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 37.69it/s, loss=0.958, v_num=e7so, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 37.51it/s, loss=0.958, v_num=e7so, ETH_val\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 33.84it/s, loss=0.976, v_num=e7so, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 33.71it/s, loss=0.976, v_num=e7so, ETH_val\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:02<00:00, 29.25it/s, loss=0.94, v_num=e7so, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:02<00:00, 29.15it/s, loss=0.94, v_num=e7so, ETH_val_\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:03<00:00, 20.54it/s, loss=0.967, v_num=e7so, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:03<00:00, 20.49it/s, loss=0.967, v_num=e7so, ETH_val\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 34.97it/s, loss=0.939, v_num=e7so, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 34.82it/s, loss=0.939, v_num=e7so, ETH_val\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:02<00:00, 32.80it/s, loss=0.952, v_num=e7so, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:02<00:00, 32.56it/s, loss=0.952, v_num=e7so, ETH_val\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 30.20it/s, loss=0.951, v_num=e7so, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 29.91it/s, loss=0.951, v_num=e7so, ETH_val\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:02<00:00, 30.90it/s, loss=0.93, v_num=e7so, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:02<00:00, 30.75it/s, loss=0.93, v_num=e7so, ETH_val_\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:02<00:00, 30.22it/s, loss=0.921, v_num=e7so, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:02<00:00, 30.09it/s, loss=0.921, v_num=e7so, ETH_val\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:02<00:00, 35.28it/s, loss=0.938, v_num=e7so, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:02<00:00, 34.86it/s, loss=0.938, v_num=e7so, ETH_val\u001b[A\n",
      "Epoch 31: 100%|█| 80/80 [00:02<00:00, 33.30it/s, loss=0.973, v_num=e7so, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 80/80 [00:02<00:00, 33.16it/s, loss=0.973, v_num=e7so, ETH_val\u001b[A\n",
      "Epoch 32: 100%|█| 80/80 [00:02<00:00, 32.58it/s, loss=1, v_num=e7so, ETH_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 80/80 [00:02<00:00, 32.37it/s, loss=1, v_num=e7so, ETH_val_acc\u001b[A\n",
      "Epoch 33: 100%|█| 80/80 [00:02<00:00, 29.32it/s, loss=0.925, v_num=e7so, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 80/80 [00:02<00:00, 29.19it/s, loss=0.925, v_num=e7so, ETH_val\u001b[A\n",
      "Epoch 34: 100%|█| 80/80 [00:03<00:00, 24.57it/s, loss=0.976, v_num=e7so, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 80/80 [00:03<00:00, 24.48it/s, loss=0.976, v_num=e7so, ETH_val\u001b[A\n",
      "Epoch 35: 100%|█| 80/80 [00:03<00:00, 23.41it/s, loss=0.9, v_num=e7so, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 80/80 [00:03<00:00, 23.33it/s, loss=0.9, v_num=e7so, ETH_val_a\u001b[A\n",
      "Epoch 36: 100%|█| 80/80 [00:03<00:00, 23.44it/s, loss=0.962, v_num=e7so, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 80/80 [00:03<00:00, 23.33it/s, loss=0.962, v_num=e7so, ETH_val\u001b[A\n",
      "Epoch 37: 100%|█| 80/80 [00:03<00:00, 25.40it/s, loss=0.958, v_num=e7so, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 80/80 [00:03<00:00, 25.31it/s, loss=0.958, v_num=e7so, ETH_val\u001b[A\n",
      "Epoch 38: 100%|█| 80/80 [00:04<00:00, 19.37it/s, loss=0.978, v_num=e7so, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 80/80 [00:04<00:00, 19.03it/s, loss=0.978, v_num=e7so, ETH_val\u001b[A\n",
      "Epoch 39: 100%|█| 80/80 [00:02<00:00, 29.50it/s, loss=0.962, v_num=e7so, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.922. Signaling Trainer to stop.\n",
      "Epoch 39: 100%|█| 80/80 [00:02<00:00, 29.32it/s, loss=0.962, v_num=e7so, ETH_val\n",
      "Epoch 39: 100%|█| 80/80 [00:02<00:00, 29.27it/s, loss=0.962, v_num=e7so, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 69.45it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.4193548262119293,\n",
      " 'ETH_test_f1': 0.39972689747810364,\n",
      " 'test_loss': 0.9228352904319763}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 143289\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_105350-18mze7so/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_105350-18mze7so/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.98611\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 39\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 115\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621497345\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.4323\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.3762\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.94286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.11111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.09524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.17488\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.41935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.39973\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.92284\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▆▆███▆▆▇▆▅▅▅▆▂▅▁▁▅▁▂▃▂▄▄▂▆▅▃▇▃▄█▄▅▅▁▂▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▂▂▄▁▂▁▃▄▄▄▁▅▅▅▇▇▆▆▇▇█▇▇▇▇▇▇█▇▇▇▇▇█▇█▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▂▂▃▁▁▁▂▄▃▃▂▆▆▆█▇▆▇▇▇█▇▇▇▇▇████▇▇███▇█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▇▆▆▆▆▆▆▆▅▅▄▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▁▁▁▃▃▁▁▃▁▁█▁▆▃▁▃▁▃▁▁▁▃▃▃▃▃▃▃▃▃▁▃▃▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▁▁▂▂▁▁▂▁▁█▂▇▅▂▅▂▅▂▂▂▅▅▅▅▅▅▅▅▅▂▅▅▂▂▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▅▅▅▅▄▄▄▄▄▄▆▆▄▁▇▄▇▄▅▇█▅▅▄▅▄▄▄▅▃▅▅▅▇▆▇▅▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_multi_head_attention_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/18mze7so\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 10:56:02.491293: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_multi_head_attention_loss_weighted_multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2ku8knwg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_105600-2ku8knwg\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.5 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Training: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|▉| 79/80 [00:03<00:00, 24.49it/s, loss=1.15, v_num=knwg, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.159\n",
      "Epoch 0: 100%|█| 80/80 [00:03<00:00, 24.51it/s, loss=1.15, v_num=knwg, ETH_val_a\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 30.71it/s, loss=1.1, v_num=knwg, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.044 >= min_delta = 0.003. New best score: 1.116\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 30.55it/s, loss=1.1, v_num=knwg, ETH_val_ac\n",
      "Epoch 2: 100%|█| 80/80 [00:03<00:00, 23.19it/s, loss=1.12, v_num=knwg, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.046 >= min_delta = 0.003. New best score: 1.070\n",
      "Epoch 2: 100%|█| 80/80 [00:03<00:00, 23.09it/s, loss=1.12, v_num=knwg, ETH_val_a\n",
      "Epoch 3: 100%|█| 80/80 [00:03<00:00, 26.00it/s, loss=1.11, v_num=knwg, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:03<00:00, 25.86it/s, loss=1.11, v_num=knwg, ETH_val_a\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 28.52it/s, loss=1.1, v_num=knwg, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:02<00:00, 28.37it/s, loss=1.1, v_num=knwg, ETH_val_ac\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 28.54it/s, loss=1.11, v_num=knwg, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:02<00:00, 28.41it/s, loss=1.11, v_num=knwg, ETH_val_a\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 28.47it/s, loss=1.1, v_num=knwg, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:02<00:00, 28.31it/s, loss=1.1, v_num=knwg, ETH_val_ac\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:04<00:00, 18.11it/s, loss=1.1, v_num=knwg, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:04<00:00, 17.83it/s, loss=1.1, v_num=knwg, ETH_val_ac\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:05<00:00, 15.07it/s, loss=1.1, v_num=knwg, ETH_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:05<00:00, 15.03it/s, loss=1.1, v_num=knwg, ETH_val_ac\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:04<00:00, 18.27it/s, loss=1.09, v_num=knwg, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:04<00:00, 18.21it/s, loss=1.09, v_num=knwg, ETH_val_a\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:03<00:00, 22.36it/s, loss=1.09, v_num=knwg, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:03<00:00, 22.29it/s, loss=1.09, v_num=knwg, ETH_val_\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:05<00:00, 15.81it/s, loss=1.08, v_num=knwg, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:05<00:00, 15.77it/s, loss=1.08, v_num=knwg, ETH_val_\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:04<00:00, 18.66it/s, loss=1.05, v_num=knwg, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:04<00:00, 18.60it/s, loss=1.05, v_num=knwg, ETH_val_\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 28.10it/s, loss=1.02, v_num=knwg, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 1.060\n",
      "Epoch 13: 100%|█| 80/80 [00:02<00:00, 27.97it/s, loss=1.02, v_num=knwg, ETH_val_\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 26.78it/s, loss=1.04, v_num=knwg, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:03<00:00, 26.64it/s, loss=1.04, v_num=knwg, ETH_val_\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 28.13it/s, loss=1.06, v_num=knwg, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 27.98it/s, loss=1.06, v_num=knwg, ETH_val_\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 28.71it/s, loss=0.99, v_num=knwg, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 28.51it/s, loss=0.99, v_num=knwg, ETH_val_\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 28.53it/s, loss=1.01, v_num=knwg, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 28.39it/s, loss=1.01, v_num=knwg, ETH_val_\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 28.18it/s, loss=0.964, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:02<00:00, 28.05it/s, loss=0.964, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 28.30it/s, loss=1.01, v_num=knwg, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 28.16it/s, loss=1.01, v_num=knwg, ETH_val_\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 28.90it/s, loss=1.03, v_num=knwg, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 28.73it/s, loss=1.03, v_num=knwg, ETH_val_\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 28.24it/s, loss=0.99, v_num=knwg, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 28.10it/s, loss=0.99, v_num=knwg, ETH_val_\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 28.46it/s, loss=0.99, v_num=knwg, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 28.32it/s, loss=0.99, v_num=knwg, ETH_val_\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:02<00:00, 27.99it/s, loss=0.936, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:02<00:00, 27.81it/s, loss=0.936, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 28.16it/s, loss=0.952, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 28.04it/s, loss=0.952, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:03<00:00, 25.17it/s, loss=0.941, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:03<00:00, 25.04it/s, loss=0.941, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:02<00:00, 27.98it/s, loss=1.01, v_num=knwg, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:02<00:00, 27.87it/s, loss=1.01, v_num=knwg, ETH_val_\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 28.29it/s, loss=0.998, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:02<00:00, 28.15it/s, loss=0.998, v_num=knwg, ETH_val\u001b[A\n",
      "                                                                                Metric val_loss improved by 0.114 >= min_delta = 0.003. New best score: 0.946\n",
      "Epoch 28: 100%|█| 80/80 [00:04<00:00, 17.96it/s, loss=1.05, v_num=knwg, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:04<00:00, 17.90it/s, loss=1.05, v_num=knwg, ETH_val_\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:04<00:00, 16.29it/s, loss=1.04, v_num=knwg, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:04<00:00, 16.23it/s, loss=1.04, v_num=knwg, ETH_val_\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:04<00:00, 17.72it/s, loss=0.979, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:04<00:00, 17.67it/s, loss=0.979, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 31: 100%|█| 80/80 [00:03<00:00, 23.15it/s, loss=1.01, v_num=knwg, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 80/80 [00:03<00:00, 23.08it/s, loss=1.01, v_num=knwg, ETH_val_\u001b[A\n",
      "Epoch 32: 100%|█| 80/80 [00:04<00:00, 18.36it/s, loss=0.997, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 80/80 [00:04<00:00, 18.26it/s, loss=0.997, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 33: 100%|█| 80/80 [00:02<00:00, 27.33it/s, loss=0.919, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 80/80 [00:02<00:00, 27.20it/s, loss=0.919, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 34: 100%|█| 80/80 [00:02<00:00, 27.39it/s, loss=0.99, v_num=knwg, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 80/80 [00:02<00:00, 27.27it/s, loss=0.99, v_num=knwg, ETH_val_\u001b[A\n",
      "Epoch 35: 100%|█| 80/80 [00:02<00:00, 26.96it/s, loss=0.945, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 80/80 [00:02<00:00, 26.85it/s, loss=0.945, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 36: 100%|█| 80/80 [00:02<00:00, 27.25it/s, loss=0.957, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 80/80 [00:02<00:00, 27.13it/s, loss=0.957, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 37: 100%|█| 80/80 [00:02<00:00, 27.32it/s, loss=0.996, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 80/80 [00:02<00:00, 27.20it/s, loss=0.996, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 38: 100%|█| 80/80 [00:02<00:00, 27.70it/s, loss=0.95, v_num=knwg, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 80/80 [00:02<00:00, 27.56it/s, loss=0.95, v_num=knwg, ETH_val_\u001b[A\n",
      "Epoch 39: 100%|█| 80/80 [00:02<00:00, 27.11it/s, loss=0.977, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 80/80 [00:02<00:00, 26.99it/s, loss=0.977, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 40: 100%|█| 80/80 [00:02<00:00, 27.09it/s, loss=0.971, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 80/80 [00:02<00:00, 26.99it/s, loss=0.971, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 41: 100%|█| 80/80 [00:02<00:00, 27.42it/s, loss=0.958, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 80/80 [00:02<00:00, 27.31it/s, loss=0.958, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 42: 100%|█| 80/80 [00:02<00:00, 27.19it/s, loss=0.965, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 80/80 [00:02<00:00, 27.04it/s, loss=0.965, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 43: 100%|█| 80/80 [00:02<00:00, 27.38it/s, loss=0.946, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 80/80 [00:02<00:00, 27.28it/s, loss=0.946, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 44: 100%|█| 80/80 [00:02<00:00, 27.35it/s, loss=0.971, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 80/80 [00:02<00:00, 27.22it/s, loss=0.971, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 45: 100%|█| 80/80 [00:02<00:00, 27.35it/s, loss=0.916, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 80/80 [00:02<00:00, 27.20it/s, loss=0.916, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 46: 100%|█| 80/80 [00:04<00:00, 17.98it/s, loss=0.933, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 80/80 [00:04<00:00, 17.71it/s, loss=0.933, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 47: 100%|█| 80/80 [00:03<00:00, 20.01it/s, loss=0.943, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 80/80 [00:04<00:00, 19.91it/s, loss=0.943, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 48: 100%|█| 80/80 [00:05<00:00, 14.55it/s, loss=0.937, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 80/80 [00:05<00:00, 14.50it/s, loss=0.937, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 49: 100%|█| 80/80 [00:06<00:00, 12.11it/s, loss=0.968, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 80/80 [00:06<00:00, 12.08it/s, loss=0.968, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 50: 100%|█| 80/80 [00:03<00:00, 22.09it/s, loss=0.893, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 80/80 [00:03<00:00, 22.00it/s, loss=0.893, v_num=knwg, ETH_val\u001b[A\n",
      "Epoch 51: 100%|█| 80/80 [00:03<00:00, 24.80it/s, loss=0.91, v_num=knwg, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 80/80 [00:03<00:00, 24.70it/s, loss=0.91, v_num=knwg, ETH_val_\u001b[A\n",
      "Epoch 52: 100%|█| 80/80 [00:04<00:00, 19.29it/s, loss=0.939, v_num=knwg, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.946. Signaling Trainer to stop.\n",
      "Epoch 52: 100%|█| 80/80 [00:04<00:00, 19.22it/s, loss=0.939, v_num=knwg, ETH_val\n",
      "Epoch 52: 100%|█| 80/80 [00:04<00:00, 19.20it/s, loss=0.939, v_num=knwg, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 79.58it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.4516128897666931,\n",
      " 'ETH_test_f1': 0.40469205379486084,\n",
      " 'test_loss': 0.8125471472740173}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 143608\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_105600-2ku8knwg/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_105600-2ku8knwg/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.85964\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4187\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 193\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621497554\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 189\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.4323\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.3852\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.9298\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.11111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.11111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.15094\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.45161\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.40469\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.81255\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▆▆▆▅▅▅▅▅▄▄▃▄▆▄▅▃▃▅▄▂▃▄▆▇▅▅▄▃▁█▄▂▃▁▄▃▄▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▃▃▃▁▅▁▄▄▃▅▆▆▇▇▇▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▃▃▃▁▃▂▂▃▃▆▇▇▇▇▇▆▇▇▆▆▇▇▇█▇▆▆▆▆▆▇▇▇▇██▆▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▇▇▆▆▆▆▆▅▄▃▄▃▃▄▃▃▂▃▃▃▃▃▃▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▁█▂▁▁▂▂▁▁▂▁▁▁▁▁▁▁▁▂▂▁▁▁▂▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁█▃▁▁▃▃▂▂▆▂▃▂▃▂▂▂▃▆▆▂▂▂▆▂▂▂▂▂▃▆▆▂▂▃▃▃▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄▃▁▂▃▂▂▂▂▄▁▇██▃▇▆▆▃▂▂▄▂▂▁▇▆▅▃▄▄▃▂▄▄▂▄▃▆▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_multi_head_attention_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2ku8knwg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 10:59:25.998961: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_multi_head_attention__binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/6ok8az42\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_105924-6ok8az42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Training: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 79/80 [00:08<00:00,  9.22it/s, loss=0.655, v_num=az42, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validating: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  2.80it/s]\u001b[AMetric val_loss improved. New best score: 0.427\n",
      "Epoch 0: 100%|█| 80/80 [00:09<00:00,  8.81it/s, loss=0.655, v_num=az42, ETH_val_\n",
      "Epoch 1:  99%|▉| 79/80 [00:07<00:00, 10.95it/s, loss=0.631, v_num=az42, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.016 >= min_delta = 0.003. New best score: 0.411\n",
      "Epoch 1: 100%|█| 80/80 [00:07<00:00, 10.99it/s, loss=0.631, v_num=az42, ETH_val_\n",
      "Epoch 2:  99%|▉| 79/80 [00:08<00:00,  9.67it/s, loss=0.691, v_num=az42, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:08<00:00,  9.63it/s, loss=0.691, v_num=az42, ETH_val_\u001b[A\n",
      "Epoch 3:  99%|▉| 79/80 [00:08<00:00,  9.65it/s, loss=0.591, v_num=az42, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.029 >= min_delta = 0.003. New best score: 0.383\n",
      "Epoch 3: 100%|█| 80/80 [00:08<00:00,  9.60it/s, loss=0.591, v_num=az42, ETH_val_\n",
      "Epoch 4: 100%|█| 80/80 [00:13<00:00,  5.76it/s, loss=0.546, v_num=az42, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.044 >= min_delta = 0.003. New best score: 0.339\n",
      "Epoch 4: 100%|█| 80/80 [00:13<00:00,  5.75it/s, loss=0.546, v_num=az42, ETH_val_\n",
      "Epoch 5: 100%|█| 80/80 [00:09<00:00,  8.44it/s, loss=0.627, v_num=az42, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:09<00:00,  8.42it/s, loss=0.627, v_num=az42, ETH_val_\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:09<00:00,  8.73it/s, loss=0.576, v_num=az42, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:09<00:00,  8.72it/s, loss=0.576, v_num=az42, ETH_val_\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:08<00:00,  9.47it/s, loss=0.588, v_num=az42, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:08<00:00,  9.46it/s, loss=0.588, v_num=az42, ETH_val_\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:08<00:00,  9.38it/s, loss=0.601, v_num=az42, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:08<00:00,  9.34it/s, loss=0.601, v_num=az42, ETH_val_\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:08<00:00,  9.93it/s, loss=0.607, v_num=az42, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:08<00:00,  9.89it/s, loss=0.607, v_num=az42, ETH_val_\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:08<00:00,  9.96it/s, loss=0.606, v_num=az42, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:08<00:00,  9.94it/s, loss=0.606, v_num=az42, ETH_val\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:07<00:00, 10.56it/s, loss=0.595, v_num=az42, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:07<00:00, 10.52it/s, loss=0.595, v_num=az42, ETH_val\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:07<00:00, 10.78it/s, loss=0.601, v_num=az42, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:07<00:00, 10.33it/s, loss=0.601, v_num=az42, ETH_val\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:08<00:00,  9.97it/s, loss=0.601, v_num=az42, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:08<00:00,  9.93it/s, loss=0.601, v_num=az42, ETH_val\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:07<00:00, 10.06it/s, loss=0.577, v_num=az42, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:08<00:00,  9.98it/s, loss=0.577, v_num=az42, ETH_val\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:07<00:00, 10.19it/s, loss=0.61, v_num=az42, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:07<00:00, 10.17it/s, loss=0.61, v_num=az42, ETH_val_\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:07<00:00, 10.01it/s, loss=0.611, v_num=az42, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:08<00:00,  9.98it/s, loss=0.611, v_num=az42, ETH_val\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:07<00:00, 10.35it/s, loss=0.6, v_num=az42, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:08<00:00,  9.82it/s, loss=0.6, v_num=az42, ETH_val_a\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:07<00:00, 11.28it/s, loss=0.562, v_num=az42, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:07<00:00, 11.23it/s, loss=0.562, v_num=az42, ETH_val\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:07<00:00, 11.40it/s, loss=0.611, v_num=az42, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:07<00:00, 11.36it/s, loss=0.611, v_num=az42, ETH_val\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:07<00:00, 10.01it/s, loss=0.555, v_num=az42, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:08<00:00,  9.99it/s, loss=0.555, v_num=az42, ETH_val\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:08<00:00,  9.89it/s, loss=0.551, v_num=az42, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:08<00:00,  9.86it/s, loss=0.551, v_num=az42, ETH_val\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:06<00:00, 11.81it/s, loss=0.567, v_num=az42, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:06<00:00, 11.77it/s, loss=0.567, v_num=az42, ETH_val\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:08<00:00,  9.38it/s, loss=0.579, v_num=az42, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:08<00:00,  9.37it/s, loss=0.579, v_num=az42, ETH_val\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:09<00:00,  8.56it/s, loss=0.563, v_num=az42, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:09<00:00,  8.54it/s, loss=0.563, v_num=az42, ETH_val\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:09<00:00,  8.01it/s, loss=0.568, v_num=az42, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:10<00:00,  8.00it/s, loss=0.568, v_num=az42, ETH_val\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:09<00:00,  8.26it/s, loss=0.562, v_num=az42, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:10<00:00,  7.91it/s, loss=0.562, v_num=az42, ETH_val\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:08<00:00,  9.13it/s, loss=0.597, v_num=az42, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:08<00:00,  9.08it/s, loss=0.597, v_num=az42, ETH_val\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:08<00:00,  9.23it/s, loss=0.587, v_num=az42, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:08<00:00,  9.21it/s, loss=0.587, v_num=az42, ETH_val\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:09<00:00,  8.65it/s, loss=0.538, v_num=az42, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.339. Signaling Trainer to stop.\n",
      "Epoch 29: 100%|█| 80/80 [00:09<00:00,  8.63it/s, loss=0.538, v_num=az42, ETH_val\n",
      "Epoch 29: 100%|█| 80/80 [00:09<00:00,  8.63it/s, loss=0.538, v_num=az42, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 68.09it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.5806451439857483,\n",
      " 'ETH_test_f1': 0.5806451439857483,\n",
      " 'test_loss': 0.6977804899215698}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 144175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_105924-6ok8az42/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_105924-6ok8az42/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.59673\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2370\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 270\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621497834\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 107\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.70467\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.69092\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.57072\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.46968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.58065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.58065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.69778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆█▅▆▅▅▅▅▇▃▆▅▇▅▆▂▇▆▇▇▆▇▅▇▅▆▁▄▆▂▆█▄▇▄▃▄▅▃▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▆▇▆▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇████▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▁▆▇▇▇▇▇█▇██▇█▇███▇▇▇▇▇████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▁▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▅▆█▆▆▆█▆▆▆▆▆▆▆▆▆▆▃▁▆▆▆▁▆▁▅▅▅▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▅▆█▆▆▆█▆▆▆▆▆▆▆▆▆▆▃▁▆▆▆▁▆▁▅▅▅▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄▃▅▂▁▄▄▃▄▄▄▄▅▅▄▅▅▅▅▅▄▅█▅▆▆▅▆▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_multi_head_attention__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/6ok8az42\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 11:04:05.636461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_multi_head_attention__binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3uelduvp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_110403-3uelduvp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|█| 80/80 [00:06<00:00, 13.16it/s, loss=0.686, v_num=duvp, ETH_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.595\n",
      "Epoch 0: 100%|█| 80/80 [00:06<00:00, 13.13it/s, loss=0.686, v_num=duvp, ETH_val_\n",
      "Epoch 1:  99%|▉| 79/80 [00:06<00:00, 12.68it/s, loss=0.599, v_num=duvp, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.094 >= min_delta = 0.003. New best score: 0.501\n",
      "Epoch 1: 100%|█| 80/80 [00:06<00:00, 12.76it/s, loss=0.599, v_num=duvp, ETH_val_\n",
      "Epoch 2:  99%|▉| 79/80 [00:06<00:00, 11.62it/s, loss=0.636, v_num=duvp, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:06<00:00, 11.71it/s, loss=0.636, v_num=duvp, ETH_val_Metric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.492\n",
      "\n",
      "Epoch 3:  99%|▉| 79/80 [00:07<00:00, 10.40it/s, loss=0.606, v_num=duvp, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.109 >= min_delta = 0.003. New best score: 0.382\n",
      "Epoch 3: 100%|█| 80/80 [00:07<00:00, 10.47it/s, loss=0.606, v_num=duvp, ETH_val_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  99%|▉| 79/80 [00:06<00:00, 11.34it/s, loss=0.635, v_num=duvp, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:07<00:00, 11.36it/s, loss=0.635, v_num=duvp, ETH_val_\u001b[A\n",
      "Epoch 5:  99%|▉| 79/80 [00:08<00:00,  9.09it/s, loss=0.623, v_num=duvp, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:08<00:00,  9.14it/s, loss=0.623, v_num=duvp, ETH_val_\u001b[A\n",
      "Epoch 6:  99%|▉| 79/80 [00:10<00:00,  7.66it/s, loss=0.639, v_num=duvp, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:10<00:00,  7.72it/s, loss=0.639, v_num=duvp, ETH_val_\u001b[A\n",
      "Epoch 7:  99%|▉| 79/80 [00:15<00:00,  5.22it/s, loss=0.558, v_num=duvp, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:15<00:00,  5.21it/s, loss=0.558, v_num=duvp, ETH_val_\u001b[A\n",
      "Epoch 8:  99%|▉| 79/80 [00:13<00:00,  6.05it/s, loss=0.595, v_num=duvp, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:13<00:00,  6.11it/s, loss=0.595, v_num=duvp, ETH_val_\u001b[A\n",
      "Epoch 9:  99%|▉| 79/80 [00:20<00:00,  3.84it/s, loss=0.606, v_num=duvp, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:21<00:00,  3.78it/s, loss=0.606, v_num=duvp, ETH_val_\u001b[A\n",
      "Epoch 10:  99%|▉| 79/80 [00:08<00:00,  9.12it/s, loss=0.622, v_num=duvp, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:08<00:00,  9.19it/s, loss=0.622, v_num=duvp, ETH_val\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:08<00:00,  9.15it/s, loss=0.574, v_num=duvp, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:09<00:00,  8.81it/s, loss=0.574, v_num=duvp, ETH_val\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:10<00:00,  7.83it/s, loss=0.595, v_num=duvp, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:10<00:00,  7.81it/s, loss=0.595, v_num=duvp, ETH_val\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:10<00:00,  7.98it/s, loss=0.595, v_num=duvp, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:10<00:00,  7.77it/s, loss=0.595, v_num=duvp, ETH_val\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:08<00:00,  9.15it/s, loss=0.57, v_num=duvp, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:08<00:00,  9.13it/s, loss=0.57, v_num=duvp, ETH_val_\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:10<00:00,  8.00it/s, loss=0.575, v_num=duvp, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:10<00:00,  7.96it/s, loss=0.575, v_num=duvp, ETH_val\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:11<00:00,  6.92it/s, loss=0.561, v_num=duvp, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:11<00:00,  6.90it/s, loss=0.561, v_num=duvp, ETH_val\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:11<00:00,  7.13it/s, loss=0.56, v_num=duvp, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:11<00:00,  7.09it/s, loss=0.56, v_num=duvp, ETH_val_\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:11<00:00,  6.85it/s, loss=0.55, v_num=duvp, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:11<00:00,  6.84it/s, loss=0.55, v_num=duvp, ETH_val_\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:07<00:00, 11.03it/s, loss=0.618, v_num=duvp, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:07<00:00, 10.99it/s, loss=0.618, v_num=duvp, ETH_val\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:05<00:00, 13.75it/s, loss=0.588, v_num=duvp, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:05<00:00, 13.72it/s, loss=0.588, v_num=duvp, ETH_val\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:04<00:00, 16.51it/s, loss=0.611, v_num=duvp, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:04<00:00, 16.45it/s, loss=0.611, v_num=duvp, ETH_val\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:04<00:00, 16.46it/s, loss=0.567, v_num=duvp, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:04<00:00, 16.42it/s, loss=0.567, v_num=duvp, ETH_val\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:04<00:00, 16.41it/s, loss=0.579, v_num=duvp, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:04<00:00, 16.27it/s, loss=0.579, v_num=duvp, ETH_val\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:03<00:00, 20.44it/s, loss=0.555, v_num=duvp, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:03<00:00, 20.37it/s, loss=0.555, v_num=duvp, ETH_val\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:04<00:00, 17.97it/s, loss=0.581, v_num=duvp, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:04<00:00, 17.91it/s, loss=0.581, v_num=duvp, ETH_val\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:08<00:00,  9.86it/s, loss=0.541, v_num=duvp, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:08<00:00,  9.84it/s, loss=0.541, v_num=duvp, ETH_val\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:07<00:00, 10.26it/s, loss=0.627, v_num=duvp, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:07<00:00, 10.23it/s, loss=0.627, v_num=duvp, ETH_val\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:07<00:00, 10.05it/s, loss=0.525, v_num=duvp, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.382. Signaling Trainer to stop.\n",
      "Epoch 28: 100%|█| 80/80 [00:07<00:00, 10.02it/s, loss=0.525, v_num=duvp, ETH_val\n",
      "Epoch 28: 100%|█| 80/80 [00:07<00:00, 10.02it/s, loss=0.525, v_num=duvp, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 54.76it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.6451612710952759,\n",
      " 'ETH_test_f1': 0.6436997652053833,\n",
      " 'test_loss': 0.6194477081298828}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 145187\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_110403-3uelduvp/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_110403-3uelduvp/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.57313\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 28\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2291\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 269\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621498112\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.71496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.69706\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.56138\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.55\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.49903\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.64516\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.6437\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.61945\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▃█▄▃▇▆▆▄▃▃▇▇▄▃▅▄▆▂▇▂▅▂▂▃▃▅▁▆▄▁▄▅▂█▇▂▄▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▆▇▇▇▇▇▇▇▇█▇█▇▇▇▇█▇██████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▁▆▇▇▇▇▇▇▇▇█▇█▇▇▇▇▇▇██████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▃▃▃██████████▃▁▁▁▃▁█▁▁▁█▃▁█▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▄▄▄██████████▄▁▁▂▄▁█▂▁▂█▄▂█▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▅▅▁▂▃▅▁▁▃▄▂▃▄▆▅▅▄▆▄▅▆▅▄▅▅▄▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_multi_head_attention__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3uelduvp\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 11:08:44.341725: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_multi_head_attention__multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/343wxu9q\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_110842-343wxu9q\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.5 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|█| 80/80 [00:04<00:00, 18.27it/s, loss=1.11, v_num=xu9q, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 80/80 [00:04<00:00, 18.17it/s, loss=1.11, v_num=xu9q, ETH_val_aMetric val_loss improved. New best score: 0.884\n",
      "\n",
      "Epoch 1: 100%|█| 80/80 [00:05<00:00, 13.68it/s, loss=1.08, v_num=xu9q, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 80/80 [00:05<00:00, 13.65it/s, loss=1.08, v_num=xu9q, ETH_val_a\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:03<00:00, 21.36it/s, loss=1.01, v_num=xu9q, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:03<00:00, 21.26it/s, loss=1.01, v_num=xu9q, ETH_val_a\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:03<00:00, 20.78it/s, loss=1.08, v_num=xu9q, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:03<00:00, 20.66it/s, loss=1.08, v_num=xu9q, ETH_val_a\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:05<00:00, 14.69it/s, loss=1.05, v_num=xu9q, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:05<00:00, 14.65it/s, loss=1.05, v_num=xu9q, ETH_val_a\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:05<00:00, 14.34it/s, loss=1.05, v_num=xu9q, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:05<00:00, 14.29it/s, loss=1.05, v_num=xu9q, ETH_val_a\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:05<00:00, 14.20it/s, loss=1.06, v_num=xu9q, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:05<00:00, 14.17it/s, loss=1.06, v_num=xu9q, ETH_val_a\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:04<00:00, 17.06it/s, loss=1.04, v_num=xu9q, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:04<00:00, 17.02it/s, loss=1.04, v_num=xu9q, ETH_val_a\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:03<00:00, 23.00it/s, loss=1.03, v_num=xu9q, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:03<00:00, 22.93it/s, loss=1.03, v_num=xu9q, ETH_val_a\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 31.94it/s, loss=1.01, v_num=xu9q, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:02<00:00, 31.81it/s, loss=1.01, v_num=xu9q, ETH_val_a\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:03<00:00, 22.38it/s, loss=1.04, v_num=xu9q, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:03<00:00, 22.19it/s, loss=1.04, v_num=xu9q, ETH_val_\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 27.70it/s, loss=1.02, v_num=xu9q, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:02<00:00, 27.59it/s, loss=1.02, v_num=xu9q, ETH_val_\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 32.02it/s, loss=1.01, v_num=xu9q, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:02<00:00, 31.88it/s, loss=1.01, v_num=xu9q, ETH_val_\u001b[A\n",
      "Epoch 13:  99%|▉| 79/80 [00:03<00:00, 23.86it/s, loss=0.993, v_num=xu9q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:03<00:00, 23.94it/s, loss=0.993, v_num=xu9q, ETH_val\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:03<00:00, 24.88it/s, loss=0.956, v_num=xu9q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:03<00:00, 24.79it/s, loss=0.956, v_num=xu9q, ETH_val\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 29.11it/s, loss=0.996, v_num=xu9q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 28.94it/s, loss=0.996, v_num=xu9q, ETH_val\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:03<00:00, 23.21it/s, loss=0.898, v_num=xu9q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:03<00:00, 23.09it/s, loss=0.898, v_num=xu9q, ETH_val\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 26.86it/s, loss=0.97, v_num=xu9q, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:02<00:00, 26.75it/s, loss=0.97, v_num=xu9q, ETH_val_\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:03<00:00, 26.07it/s, loss=1.01, v_num=xu9q, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:03<00:00, 25.93it/s, loss=1.01, v_num=xu9q, ETH_val_\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 29.76it/s, loss=0.962, v_num=xu9q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 29.61it/s, loss=0.962, v_num=xu9q, ETH_val\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 33.11it/s, loss=0.975, v_num=xu9q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 32.95it/s, loss=0.975, v_num=xu9q, ETH_val\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 33.90it/s, loss=0.929, v_num=xu9q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:02<00:00, 33.54it/s, loss=0.929, v_num=xu9q, ETH_val\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:03<00:00, 22.85it/s, loss=0.919, v_num=xu9q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:03<00:00, 22.78it/s, loss=0.919, v_num=xu9q, ETH_val\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:03<00:00, 24.10it/s, loss=0.972, v_num=xu9q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:03<00:00, 24.03it/s, loss=0.972, v_num=xu9q, ETH_val\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:03<00:00, 26.12it/s, loss=0.927, v_num=xu9q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:03<00:00, 26.04it/s, loss=0.927, v_num=xu9q, ETH_val\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:03<00:00, 22.55it/s, loss=0.936, v_num=xu9q, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.884. Signaling Trainer to stop.\n",
      "Epoch 25: 100%|█| 80/80 [00:03<00:00, 22.48it/s, loss=0.936, v_num=xu9q, ETH_val\n",
      "Epoch 25: 100%|█| 80/80 [00:03<00:00, 22.46it/s, loss=0.936, v_num=xu9q, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 101.28it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.4838709533214569,\n",
      " 'ETH_test_f1': 0.2169238030910492,\n",
      " 'test_loss': 1.063524603843689}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 146048\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_110842-343wxu9q/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_110842-343wxu9q/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.84005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2054\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621498227\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 93\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.46239\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.30361\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.94809\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.37576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.91444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.48387\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.21692\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.06352\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▄▆▇▇▅▇▅▄▅▇▆▆▆▄▇▃▃▅▄▁▃▆▁▄▂▆▂▂▇▃▂▂▄▄▃▂▅█▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▃▄▆▃▆▆▆▆▇▇▆▅▅▇▅▅▅▆▇▅▆█▅▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▄▃▃▂▂▁▃▁▂▃▃▂▃▇█▅▅▅▄▅▅▅▆▄▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▆▅▅▅▅▅▅▄▄▄▃▂▂▂▂▂▂▂▁▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▆▆▆▆▆▆▆▆▃▆▆▆▃▆▆▆▃█▆▃▆▁▃▆▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▅▁█▂▄▂▂▄▂▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▂▃▄▄▄▃▃▄▃▃▂█▅▃▂▂▃▇██▄▄▃█▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_multi_head_attention__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/343wxu9q\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 11:10:38.224338: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_ETH_multi_head_attention__multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/pnnjxwaj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_111036-pnnjxwaj\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.5 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 79/80 [00:03<00:00, 24.28it/s, loss=1.09, v_num=xwaj, ETH_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.893\n",
      "Epoch 0: 100%|█| 80/80 [00:03<00:00, 24.37it/s, loss=1.09, v_num=xwaj, ETH_val_a\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 33.95it/s, loss=1.06, v_num=xwaj, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 80/80 [00:02<00:00, 33.81it/s, loss=1.06, v_num=xwaj, ETH_val_a\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 32.43it/s, loss=1.05, v_num=xwaj, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:02<00:00, 32.22it/s, loss=1.05, v_num=xwaj, ETH_val_a\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:03<00:00, 25.48it/s, loss=1.06, v_num=xwaj, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:03<00:00, 25.40it/s, loss=1.06, v_num=xwaj, ETH_val_a\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:03<00:00, 24.75it/s, loss=1.05, v_num=xwaj, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:03<00:00, 23.40it/s, loss=1.05, v_num=xwaj, ETH_val_a\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:03<00:00, 25.52it/s, loss=1.04, v_num=xwaj, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:03<00:00, 25.44it/s, loss=1.04, v_num=xwaj, ETH_val_a\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:03<00:00, 23.48it/s, loss=1.02, v_num=xwaj, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:03<00:00, 23.41it/s, loss=1.02, v_num=xwaj, ETH_val_a\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:03<00:00, 26.53it/s, loss=1.04, v_num=xwaj, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:03<00:00, 26.42it/s, loss=1.04, v_num=xwaj, ETH_val_a\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 29.46it/s, loss=1.07, v_num=xwaj, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:02<00:00, 29.34it/s, loss=1.07, v_num=xwaj, ETH_val_a\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:03<00:00, 24.81it/s, loss=1.03, v_num=xwaj, ETH_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:03<00:00, 24.73it/s, loss=1.03, v_num=xwaj, ETH_val_a\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:03<00:00, 24.48it/s, loss=1.02, v_num=xwaj, ETH_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:03<00:00, 24.41it/s, loss=1.02, v_num=xwaj, ETH_val_\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:03<00:00, 25.68it/s, loss=0.992, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:03<00:00, 25.57it/s, loss=0.992, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:03<00:00, 24.79it/s, loss=0.955, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:03<00:00, 24.71it/s, loss=0.955, v_num=xwaj, ETH_val\u001b[A\n",
      "Metric val_loss improved by 0.116 >= min_delta = 0.003. New best score: 0.777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|█| 80/80 [00:03<00:00, 24.06it/s, loss=0.924, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:03<00:00, 23.99it/s, loss=0.924, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 31.82it/s, loss=0.945, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:02<00:00, 31.69it/s, loss=0.945, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 34.28it/s, loss=0.958, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:02<00:00, 34.12it/s, loss=0.958, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 34.30it/s, loss=0.978, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:02<00:00, 34.09it/s, loss=0.978, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:03<00:00, 25.70it/s, loss=0.951, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:03<00:00, 25.62it/s, loss=0.951, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:03<00:00, 24.98it/s, loss=0.954, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:03<00:00, 24.90it/s, loss=0.954, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 32.56it/s, loss=1, v_num=xwaj, ETH_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:02<00:00, 32.43it/s, loss=1, v_num=xwaj, ETH_val_acc\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 32.19it/s, loss=0.925, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:02<00:00, 32.04it/s, loss=0.925, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:03<00:00, 25.92it/s, loss=0.988, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:03<00:00, 25.60it/s, loss=0.988, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 28.89it/s, loss=0.974, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:02<00:00, 28.77it/s, loss=0.974, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:03<00:00, 23.07it/s, loss=0.937, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:03<00:00, 22.99it/s, loss=0.937, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 30.99it/s, loss=0.922, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:02<00:00, 30.84it/s, loss=0.922, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 30.51it/s, loss=0.961, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:02<00:00, 30.18it/s, loss=0.961, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:03<00:00, 22.09it/s, loss=0.973, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:03<00:00, 21.99it/s, loss=0.973, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:03<00:00, 22.07it/s, loss=0.966, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:03<00:00, 22.01it/s, loss=0.966, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:02<00:00, 34.29it/s, loss=0.949, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:02<00:00, 34.14it/s, loss=0.949, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:02<00:00, 32.04it/s, loss=0.951, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:02<00:00, 31.91it/s, loss=0.951, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:03<00:00, 20.61it/s, loss=0.955, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:04<00:00, 19.73it/s, loss=0.955, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 31: 100%|█| 80/80 [00:04<00:00, 18.55it/s, loss=0.948, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 80/80 [00:04<00:00, 18.48it/s, loss=0.948, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 32: 100%|█| 80/80 [00:03<00:00, 20.21it/s, loss=0.942, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 80/80 [00:03<00:00, 20.16it/s, loss=0.942, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 33: 100%|█| 80/80 [00:04<00:00, 16.30it/s, loss=0.885, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 80/80 [00:04<00:00, 16.25it/s, loss=0.885, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 34: 100%|█| 80/80 [00:07<00:00, 11.13it/s, loss=0.945, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 80/80 [00:07<00:00, 11.03it/s, loss=0.945, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 35: 100%|█| 80/80 [00:02<00:00, 28.22it/s, loss=0.975, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 80/80 [00:02<00:00, 28.12it/s, loss=0.975, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 36: 100%|█| 80/80 [00:02<00:00, 35.07it/s, loss=0.909, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 80/80 [00:02<00:00, 34.91it/s, loss=0.909, v_num=xwaj, ETH_val\u001b[A\n",
      "Epoch 37: 100%|█| 80/80 [00:02<00:00, 36.29it/s, loss=0.965, v_num=xwaj, ETH_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.777. Signaling Trainer to stop.\n",
      "Epoch 37: 100%|█| 80/80 [00:02<00:00, 36.12it/s, loss=0.965, v_num=xwaj, ETH_val\n",
      "Epoch 37: 100%|█| 80/80 [00:02<00:00, 36.06it/s, loss=0.965, v_num=xwaj, ETH_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 106.74it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ETH_test_acc': 0.6129032373428345,\n",
      " 'ETH_test_f1': 0.46925094723701477,\n",
      " 'test_loss': 0.8984501361846924}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 146361\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_111036-pnnjxwaj/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_111036-pnnjxwaj/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.80358\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 37\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 130\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621498366\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.4901\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.35312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.93937\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.37576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.98788\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.46925\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.89845\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▆▆▃▅▆▆▇▇█▆▅▇▅▃▁▁▃▄▃▄▄▃▃▄▄▅▅▂▆▄▄▆▄▂▄▃▅█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▅▆▆▆▆▅▇▆▆▇▆▆▆▄▆▅▆▇▆▇▆▇██▆▆▇▇▆▆▆▇▆▆▅▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▃▂▃▂▂▁▁▁▁▁▁▃▅▃▄▃▄▄▆▅▅▆▆▆█▇▇▅▄▄▅▃▅▅▅▃▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▅▅▅▅▅▅▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▂▁▂▂▂▂▂▁▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▇▇▇▇▇▇▇▇▇▇▇▇▇▄▇▇▇▄▇▇▇▇▇▁▄▅██▅▅▅█▅▄▇▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▂▂▂▂▂▂▂▂▂▂▂▂▆▃▂▂▂▃▂▂▆▂▂▁▃▄██▄▄▄█▄▃▂▄▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▃▄▃▄▄▄▄▄▄▄▄▄▁▆▃▃▅▇▅▄▄▅▅▅▆█▄▄▆█▇▄▅█▆▆▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_ETH_multi_head_attention__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/pnnjxwaj\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 11:12:57.113399: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_multi_head_attention_loss_weighted_binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2z56pn3c\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_111255-2z56pn3c\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 72/73 [00:02<00:00, 24.60it/s, loss=0.669, v_num=pn3c, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 73/73 [00:03<00:00, 24.12it/s, loss=0.669, v_num=pn3c, LTC_val_Metric val_loss improved. New best score: 0.584\n",
      "\n",
      "Epoch 1:  99%|▉| 72/73 [00:02<00:00, 30.58it/s, loss=0.635, v_num=pn3c, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 73/73 [00:02<00:00, 30.66it/s, loss=0.635, v_num=pn3c, LTC_val_\u001b[A\n",
      "Epoch 2:  99%|▉| 72/73 [00:02<00:00, 28.55it/s, loss=0.616, v_num=pn3c, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 73/73 [00:02<00:00, 28.63it/s, loss=0.616, v_num=pn3c, LTC_val_\u001b[A\n",
      "Epoch 3:  99%|▉| 72/73 [00:02<00:00, 26.77it/s, loss=0.585, v_num=pn3c, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 73/73 [00:02<00:00, 26.86it/s, loss=0.585, v_num=pn3c, LTC_val_\u001b[A\n",
      "Epoch 4:  99%|▉| 72/73 [00:03<00:00, 21.54it/s, loss=0.622, v_num=pn3c, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 73/73 [00:03<00:00, 21.57it/s, loss=0.622, v_num=pn3c, LTC_val_\u001b[A\n",
      "Epoch 5:  99%|▉| 72/73 [00:02<00:00, 27.68it/s, loss=0.579, v_num=pn3c, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 73/73 [00:02<00:00, 27.73it/s, loss=0.579, v_num=pn3c, LTC_val_\u001b[A\n",
      "Epoch 6:  99%|▉| 72/73 [00:02<00:00, 26.19it/s, loss=0.557, v_num=pn3c, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 73/73 [00:02<00:00, 26.24it/s, loss=0.557, v_num=pn3c, LTC_val_\u001b[A\n",
      "Epoch 7:  99%|▉| 72/73 [00:02<00:00, 25.51it/s, loss=0.605, v_num=pn3c, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 73/73 [00:02<00:00, 25.52it/s, loss=0.605, v_num=pn3c, LTC_val_\u001b[A\n",
      "Epoch 8:  99%|▉| 72/73 [00:03<00:00, 20.97it/s, loss=0.585, v_num=pn3c, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 73/73 [00:03<00:00, 21.03it/s, loss=0.585, v_num=pn3c, LTC_val_\u001b[A\n",
      "Epoch 9:  99%|▉| 72/73 [00:05<00:00, 14.19it/s, loss=0.583, v_num=pn3c, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 73/73 [00:05<00:00, 14.00it/s, loss=0.583, v_num=pn3c, LTC_val_\u001b[A\n",
      "Epoch 10:  99%|▉| 72/73 [00:03<00:00, 18.94it/s, loss=0.561, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 73/73 [00:03<00:00, 19.04it/s, loss=0.561, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 11:  99%|▉| 72/73 [00:04<00:00, 15.89it/s, loss=0.561, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 73/73 [00:04<00:00, 15.99it/s, loss=0.561, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 12:  99%|▉| 72/73 [00:03<00:00, 23.24it/s, loss=0.568, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 73/73 [00:03<00:00, 23.31it/s, loss=0.568, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 13:  99%|▉| 72/73 [00:02<00:00, 33.02it/s, loss=0.624, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.580\n",
      "Epoch 13: 100%|█| 73/73 [00:02<00:00, 33.01it/s, loss=0.624, v_num=pn3c, LTC_val\n",
      "Epoch 14:  99%|▉| 72/73 [00:02<00:00, 25.23it/s, loss=0.571, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 73/73 [00:02<00:00, 25.08it/s, loss=0.571, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 15:  99%|▉| 72/73 [00:02<00:00, 34.84it/s, loss=0.58, v_num=pn3c, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 73/73 [00:02<00:00, 34.82it/s, loss=0.58, v_num=pn3c, LTC_val_\u001b[A\n",
      "Epoch 16:  99%|▉| 72/73 [00:02<00:00, 34.92it/s, loss=0.583, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 73/73 [00:02<00:00, 34.88it/s, loss=0.583, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 17:  99%|▉| 72/73 [00:02<00:00, 34.77it/s, loss=0.64, v_num=pn3c, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.084 >= min_delta = 0.003. New best score: 0.495\n",
      "Epoch 17: 100%|█| 73/73 [00:02<00:00, 34.72it/s, loss=0.64, v_num=pn3c, LTC_val_\n",
      "Epoch 18:  99%|▉| 72/73 [00:02<00:00, 32.20it/s, loss=0.547, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 73/73 [00:02<00:00, 32.10it/s, loss=0.547, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 19:  99%|▉| 72/73 [00:02<00:00, 34.46it/s, loss=0.577, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 73/73 [00:02<00:00, 34.21it/s, loss=0.577, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 20:  99%|▉| 72/73 [00:02<00:00, 35.11it/s, loss=0.606, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 73/73 [00:02<00:00, 35.09it/s, loss=0.606, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 21:  99%|▉| 72/73 [00:02<00:00, 35.21it/s, loss=0.604, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 73/73 [00:02<00:00, 35.18it/s, loss=0.604, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 22:  99%|▉| 72/73 [00:02<00:00, 35.26it/s, loss=0.626, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 73/73 [00:02<00:00, 35.25it/s, loss=0.626, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 23:  99%|▉| 72/73 [00:02<00:00, 35.09it/s, loss=0.567, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 73/73 [00:02<00:00, 35.02it/s, loss=0.567, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 24:  99%|▉| 72/73 [00:02<00:00, 35.21it/s, loss=0.565, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 73/73 [00:02<00:00, 35.18it/s, loss=0.565, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 25:  99%|▉| 72/73 [00:02<00:00, 35.28it/s, loss=0.585, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 73/73 [00:02<00:00, 35.25it/s, loss=0.585, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 26:  99%|▉| 72/73 [00:02<00:00, 31.33it/s, loss=0.576, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 73/73 [00:02<00:00, 31.23it/s, loss=0.576, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 27:  99%|▉| 72/73 [00:03<00:00, 21.86it/s, loss=0.574, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 73/73 [00:03<00:00, 21.98it/s, loss=0.574, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 28:  99%|▉| 72/73 [00:02<00:00, 31.92it/s, loss=0.561, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 73/73 [00:02<00:00, 31.94it/s, loss=0.561, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 29:  99%|▉| 72/73 [00:02<00:00, 33.85it/s, loss=0.584, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 73/73 [00:02<00:00, 33.84it/s, loss=0.584, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 30:  99%|▉| 72/73 [00:03<00:00, 23.34it/s, loss=0.571, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 73/73 [00:03<00:00, 23.29it/s, loss=0.571, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 31:  99%|▉| 72/73 [00:02<00:00, 30.92it/s, loss=0.568, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 73/73 [00:02<00:00, 30.95it/s, loss=0.568, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 32:  99%|▉| 72/73 [00:04<00:00, 15.77it/s, loss=0.544, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 73/73 [00:04<00:00, 15.88it/s, loss=0.544, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 33:  99%|▉| 72/73 [00:02<00:00, 25.88it/s, loss=0.549, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 73/73 [00:02<00:00, 25.89it/s, loss=0.549, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 34:  99%|▉| 72/73 [00:04<00:00, 14.68it/s, loss=0.585, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 73/73 [00:04<00:00, 14.71it/s, loss=0.585, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 35:  99%|▉| 72/73 [00:07<00:00,  9.19it/s, loss=0.536, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 73/73 [00:07<00:00,  9.27it/s, loss=0.536, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 36:  99%|▉| 72/73 [00:03<00:00, 23.11it/s, loss=0.533, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 73/73 [00:03<00:00, 23.01it/s, loss=0.533, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 37:  99%|▉| 72/73 [00:02<00:00, 26.56it/s, loss=0.579, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 73/73 [00:02<00:00, 26.61it/s, loss=0.579, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 38:  99%|▉| 72/73 [00:02<00:00, 24.31it/s, loss=0.559, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 73/73 [00:02<00:00, 24.42it/s, loss=0.559, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 39:  99%|▉| 72/73 [00:02<00:00, 25.27it/s, loss=0.527, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 73/73 [00:02<00:00, 25.22it/s, loss=0.527, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 40:  99%|▉| 72/73 [00:03<00:00, 19.19it/s, loss=0.574, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 73/73 [00:03<00:00, 19.25it/s, loss=0.574, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 41:  99%|▉| 72/73 [00:04<00:00, 16.39it/s, loss=0.567, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 73/73 [00:04<00:00, 16.47it/s, loss=0.567, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 42:  99%|▉| 72/73 [00:03<00:00, 21.10it/s, loss=0.535, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.064 >= min_delta = 0.003. New best score: 0.431\n",
      "Epoch 42: 100%|█| 73/73 [00:03<00:00, 21.11it/s, loss=0.535, v_num=pn3c, LTC_val\n",
      "Epoch 43:  99%|▉| 72/73 [00:04<00:00, 16.66it/s, loss=0.584, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 73/73 [00:04<00:00, 16.78it/s, loss=0.584, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 44:  99%|▉| 72/73 [00:02<00:00, 25.28it/s, loss=0.536, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 73/73 [00:02<00:00, 25.35it/s, loss=0.536, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 45:  99%|▉| 72/73 [00:03<00:00, 20.63it/s, loss=0.526, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 73/73 [00:03<00:00, 20.64it/s, loss=0.526, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 46:  99%|▉| 72/73 [00:02<00:00, 26.43it/s, loss=0.559, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 73/73 [00:02<00:00, 26.34it/s, loss=0.559, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 47:  99%|▉| 72/73 [00:03<00:00, 18.98it/s, loss=0.552, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 73/73 [00:03<00:00, 19.07it/s, loss=0.552, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 48:  99%|▉| 72/73 [00:03<00:00, 23.13it/s, loss=0.541, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 73/73 [00:03<00:00, 23.21it/s, loss=0.541, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 49:  99%|▉| 72/73 [00:03<00:00, 23.03it/s, loss=0.528, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 73/73 [00:03<00:00, 23.08it/s, loss=0.528, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 50:  99%|▉| 72/73 [00:02<00:00, 28.89it/s, loss=0.565, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 73/73 [00:02<00:00, 28.93it/s, loss=0.565, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 51:  99%|▉| 72/73 [00:02<00:00, 28.78it/s, loss=0.551, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 73/73 [00:02<00:00, 28.84it/s, loss=0.551, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 52:  99%|▉| 72/73 [00:02<00:00, 32.54it/s, loss=0.557, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 73/73 [00:02<00:00, 32.54it/s, loss=0.557, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 53:  99%|▉| 72/73 [00:02<00:00, 29.88it/s, loss=0.535, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 73/73 [00:02<00:00, 29.91it/s, loss=0.535, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 54:  99%|▉| 72/73 [00:02<00:00, 30.07it/s, loss=0.541, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 73/73 [00:02<00:00, 30.01it/s, loss=0.541, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 55:  99%|▉| 72/73 [00:02<00:00, 29.17it/s, loss=0.555, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 73/73 [00:02<00:00, 29.23it/s, loss=0.555, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 56:  99%|▉| 72/73 [00:02<00:00, 28.69it/s, loss=0.562, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 73/73 [00:02<00:00, 28.74it/s, loss=0.562, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 57:  99%|▉| 72/73 [00:02<00:00, 33.62it/s, loss=0.565, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 73/73 [00:02<00:00, 33.61it/s, loss=0.565, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 58:  99%|▉| 72/73 [00:02<00:00, 31.63it/s, loss=0.525, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 73/73 [00:02<00:00, 31.48it/s, loss=0.525, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 59:  99%|▉| 72/73 [00:03<00:00, 21.25it/s, loss=0.566, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|█| 73/73 [00:03<00:00, 21.27it/s, loss=0.566, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 60:  99%|▉| 72/73 [00:03<00:00, 19.88it/s, loss=0.53, v_num=pn3c, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 60: 100%|█| 73/73 [00:03<00:00, 19.86it/s, loss=0.53, v_num=pn3c, LTC_val_\u001b[A\n",
      "Epoch 61:  99%|▉| 72/73 [00:03<00:00, 22.91it/s, loss=0.532, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|█| 73/73 [00:03<00:00, 23.01it/s, loss=0.532, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 62:  99%|▉| 72/73 [00:02<00:00, 25.54it/s, loss=0.49, v_num=pn3c, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 62: 100%|█| 73/73 [00:02<00:00, 25.54it/s, loss=0.49, v_num=pn3c, LTC_val_\u001b[A\n",
      "Epoch 63:  99%|▉| 72/73 [00:03<00:00, 21.56it/s, loss=0.507, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 63: 100%|█| 73/73 [00:03<00:00, 21.67it/s, loss=0.507, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 64:  99%|▉| 72/73 [00:02<00:00, 28.54it/s, loss=0.55, v_num=pn3c, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 64: 100%|█| 73/73 [00:02<00:00, 28.58it/s, loss=0.55, v_num=pn3c, LTC_val_\u001b[A\n",
      "Epoch 65:  99%|▉| 72/73 [00:02<00:00, 25.74it/s, loss=0.566, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|█| 73/73 [00:02<00:00, 25.83it/s, loss=0.566, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 66:  99%|▉| 72/73 [00:02<00:00, 29.59it/s, loss=0.506, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 66: 100%|█| 73/73 [00:02<00:00, 29.52it/s, loss=0.506, v_num=pn3c, LTC_val\u001b[A\n",
      "Epoch 67:  99%|▉| 72/73 [00:02<00:00, 28.22it/s, loss=0.567, v_num=pn3c, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.431. Signaling Trainer to stop.\n",
      "Epoch 67: 100%|█| 73/73 [00:02<00:00, 28.28it/s, loss=0.567, v_num=pn3c, LTC_val\n",
      "Epoch 67: 100%|█| 73/73 [00:02<00:00, 28.24it/s, loss=0.567, v_num=pn3c, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 95.10it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.75,\n",
      " 'LTC_test_f1': 0.7428570985794067,\n",
      " 'test_loss': 0.4379305839538574}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 146706\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_111255-2z56pn3c/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_111255-2z56pn3c/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.70961\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 67\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4896\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 212\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621498587\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 233\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc 0.73438\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 0.72076\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.53603\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.61905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.51151\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.74286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.43793\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▄▄▅▆▃▄▄▄▇▄▄▃▄▇▆▅▂▃▃▆█▅▃▅▂▁▅▄▂▄▅▃▄▂▅▃▃▃▃▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc ▁▅▅▆▆▆▆▆▆▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇██▇██▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 ▁▅▅▆▆▆▆▆▆▅▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇██▇██▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▅▄▄▄▄▄▄▄▃▄▃▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▅▅▁▁▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅▅▅▅▅▁▁▅▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄▆▆▇█▆▅▅▄▆▂▅▂▃▄▄▅▄▃▄▄▄▅▄▄▁▃▃▄▃▃▄▄▃▃▃▃▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_multi_head_attention_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2z56pn3c\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 11:16:40.051769: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_multi_head_attention_loss_weighted_binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/bvv82luo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_111638-bvv82luo\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|█| 73/73 [00:02<00:00, 27.60it/s, loss=0.681, v_num=2luo, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.593\n",
      "Epoch 0: 100%|█| 73/73 [00:02<00:00, 27.47it/s, loss=0.681, v_num=2luo, LTC_val_\n",
      "Epoch 1:  99%|▉| 72/73 [00:02<00:00, 24.55it/s, loss=0.674, v_num=2luo, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.579\n",
      "Epoch 1: 100%|█| 73/73 [00:02<00:00, 24.60it/s, loss=0.674, v_num=2luo, LTC_val_\n",
      "Epoch 2:  99%|▉| 72/73 [00:03<00:00, 21.85it/s, loss=0.616, v_num=2luo, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 73/73 [00:03<00:00, 21.94it/s, loss=0.616, v_num=2luo, LTC_val_\u001b[A\n",
      "Epoch 3:  99%|▉| 72/73 [00:03<00:00, 23.64it/s, loss=0.624, v_num=2luo, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 73/73 [00:03<00:00, 23.69it/s, loss=0.624, v_num=2luo, LTC_val_\u001b[A\n",
      "Epoch 4:  99%|▉| 72/73 [00:03<00:00, 22.97it/s, loss=0.567, v_num=2luo, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 73/73 [00:03<00:00, 23.02it/s, loss=0.567, v_num=2luo, LTC_val_\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  99%|▉| 72/73 [00:02<00:00, 24.16it/s, loss=0.629, v_num=2luo, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 73/73 [00:03<00:00, 24.25it/s, loss=0.629, v_num=2luo, LTC_val_\u001b[A\n",
      "Epoch 6:  99%|▉| 72/73 [00:02<00:00, 29.66it/s, loss=0.594, v_num=2luo, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 73/73 [00:02<00:00, 29.71it/s, loss=0.594, v_num=2luo, LTC_val_\u001b[A\n",
      "Epoch 7:  99%|▉| 72/73 [00:02<00:00, 29.80it/s, loss=0.642, v_num=2luo, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 73/73 [00:02<00:00, 29.72it/s, loss=0.642, v_num=2luo, LTC_val_\u001b[A\n",
      "Epoch 8:  99%|▉| 72/73 [00:02<00:00, 25.94it/s, loss=0.627, v_num=2luo, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 73/73 [00:02<00:00, 25.93it/s, loss=0.627, v_num=2luo, LTC_val_\u001b[A\n",
      "Epoch 9:  99%|▉| 72/73 [00:02<00:00, 32.85it/s, loss=0.596, v_num=2luo, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 73/73 [00:02<00:00, 32.89it/s, loss=0.596, v_num=2luo, LTC_val_\u001b[A\n",
      "Epoch 10:  99%|▉| 72/73 [00:02<00:00, 32.47it/s, loss=0.603, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 73/73 [00:02<00:00, 32.46it/s, loss=0.603, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 11:  99%|▉| 72/73 [00:02<00:00, 33.58it/s, loss=0.646, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 73/73 [00:02<00:00, 33.55it/s, loss=0.646, v_num=2luo, LTC_val\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.022 >= min_delta = 0.003. New best score: 0.557\n",
      "Epoch 12:  99%|▉| 72/73 [00:02<00:00, 32.12it/s, loss=0.588, v_num=2luo, LTC_val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 73/73 [00:02<00:00, 32.11it/s, loss=0.588, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 13:  99%|▉| 72/73 [00:02<00:00, 32.43it/s, loss=0.613, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 73/73 [00:02<00:00, 32.44it/s, loss=0.613, v_num=2luo, LTC_val\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.026 >= min_delta = 0.003. New best score: 0.531\n",
      "Epoch 14:  99%|▉| 72/73 [00:03<00:00, 23.06it/s, loss=0.598, v_num=2luo, LTC_val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 0.517\n",
      "Epoch 14: 100%|█| 73/73 [00:03<00:00, 23.09it/s, loss=0.598, v_num=2luo, LTC_val\n",
      "Epoch 15:  99%|▉| 72/73 [00:02<00:00, 24.07it/s, loss=0.602, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 73/73 [00:03<00:00, 24.16it/s, loss=0.602, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 16:  99%|▉| 72/73 [00:02<00:00, 29.58it/s, loss=0.583, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 73/73 [00:02<00:00, 29.60it/s, loss=0.583, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 17:  99%|▉| 72/73 [00:02<00:00, 32.90it/s, loss=0.596, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 73/73 [00:02<00:00, 32.81it/s, loss=0.596, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 18:  99%|▉| 72/73 [00:02<00:00, 32.36it/s, loss=0.584, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 73/73 [00:02<00:00, 32.38it/s, loss=0.584, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 19:  99%|▉| 72/73 [00:02<00:00, 31.43it/s, loss=0.553, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 73/73 [00:02<00:00, 31.37it/s, loss=0.553, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 20:  99%|▉| 72/73 [00:02<00:00, 32.53it/s, loss=0.593, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 73/73 [00:02<00:00, 32.55it/s, loss=0.593, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 21:  99%|▉| 72/73 [00:02<00:00, 31.38it/s, loss=0.627, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 73/73 [00:02<00:00, 31.12it/s, loss=0.627, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 22:  99%|▉| 72/73 [00:03<00:00, 21.49it/s, loss=0.602, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 73/73 [00:03<00:00, 21.55it/s, loss=0.602, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 23:  99%|▉| 72/73 [00:03<00:00, 20.71it/s, loss=0.591, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.505\n",
      "Epoch 23: 100%|█| 73/73 [00:03<00:00, 20.79it/s, loss=0.591, v_num=2luo, LTC_val\n",
      "Epoch 24:  99%|▉| 72/73 [00:04<00:00, 17.20it/s, loss=0.585, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 73/73 [00:04<00:00, 17.28it/s, loss=0.585, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 25:  99%|▉| 72/73 [00:02<00:00, 24.75it/s, loss=0.575, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.499\n",
      "Epoch 25: 100%|█| 73/73 [00:02<00:00, 24.75it/s, loss=0.575, v_num=2luo, LTC_val\n",
      "Epoch 26:  99%|▉| 72/73 [00:04<00:00, 15.68it/s, loss=0.594, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.041 >= min_delta = 0.003. New best score: 0.458\n",
      "Epoch 26: 100%|█| 73/73 [00:04<00:00, 15.78it/s, loss=0.594, v_num=2luo, LTC_val\n",
      "Epoch 27:  99%|▉| 72/73 [00:03<00:00, 22.12it/s, loss=0.596, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 73/73 [00:03<00:00, 22.19it/s, loss=0.596, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 28:  99%|▉| 72/73 [00:03<00:00, 19.45it/s, loss=0.542, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 73/73 [00:03<00:00, 19.51it/s, loss=0.542, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 29:  99%|▉| 72/73 [00:03<00:00, 21.73it/s, loss=0.548, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 73/73 [00:03<00:00, 21.82it/s, loss=0.548, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 30:  99%|▉| 72/73 [00:02<00:00, 28.20it/s, loss=0.59, v_num=2luo, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 73/73 [00:02<00:00, 28.26it/s, loss=0.59, v_num=2luo, LTC_val_\u001b[A\n",
      "Epoch 31:  99%|▉| 72/73 [00:02<00:00, 34.04it/s, loss=0.536, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 73/73 [00:02<00:00, 33.98it/s, loss=0.536, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 32:  99%|▉| 72/73 [00:02<00:00, 25.92it/s, loss=0.529, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 73/73 [00:02<00:00, 25.99it/s, loss=0.529, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 33:  99%|▉| 72/73 [00:03<00:00, 20.14it/s, loss=0.563, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 73/73 [00:03<00:00, 19.87it/s, loss=0.563, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 34:  99%|▉| 72/73 [00:02<00:00, 24.12it/s, loss=0.571, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 73/73 [00:03<00:00, 24.18it/s, loss=0.571, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 35:  99%|▉| 72/73 [00:03<00:00, 18.27it/s, loss=0.562, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 0.445\n",
      "Epoch 35: 100%|█| 73/73 [00:03<00:00, 18.38it/s, loss=0.562, v_num=2luo, LTC_val\n",
      "Epoch 36:  99%|▉| 72/73 [00:03<00:00, 19.82it/s, loss=0.606, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 73/73 [00:03<00:00, 19.90it/s, loss=0.606, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 37:  99%|▉| 72/73 [00:02<00:00, 31.17it/s, loss=0.533, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 73/73 [00:02<00:00, 31.18it/s, loss=0.533, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 38:  99%|▉| 72/73 [00:02<00:00, 26.97it/s, loss=0.55, v_num=2luo, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 73/73 [00:02<00:00, 27.05it/s, loss=0.55, v_num=2luo, LTC_val_\u001b[A\n",
      "Epoch 39:  99%|▉| 72/73 [00:02<00:00, 31.09it/s, loss=0.554, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 73/73 [00:02<00:00, 30.95it/s, loss=0.554, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 40:  99%|▉| 72/73 [00:02<00:00, 28.12it/s, loss=0.565, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 73/73 [00:02<00:00, 28.16it/s, loss=0.565, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 41:  99%|▉| 72/73 [00:02<00:00, 28.81it/s, loss=0.55, v_num=2luo, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 73/73 [00:02<00:00, 28.88it/s, loss=0.55, v_num=2luo, LTC_val_\u001b[A\n",
      "Epoch 42:  99%|▉| 72/73 [00:02<00:00, 26.40it/s, loss=0.52, v_num=2luo, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 73/73 [00:02<00:00, 26.43it/s, loss=0.52, v_num=2luo, LTC_val_\u001b[A\n",
      "Epoch 43:  99%|▉| 72/73 [00:02<00:00, 30.31it/s, loss=0.541, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 73/73 [00:02<00:00, 30.34it/s, loss=0.541, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 44:  99%|▉| 72/73 [00:02<00:00, 33.91it/s, loss=0.527, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 73/73 [00:02<00:00, 33.88it/s, loss=0.527, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 45:  99%|▉| 72/73 [00:02<00:00, 33.20it/s, loss=0.558, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 73/73 [00:02<00:00, 33.21it/s, loss=0.558, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 46:  99%|▉| 72/73 [00:02<00:00, 28.92it/s, loss=0.553, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 73/73 [00:02<00:00, 28.98it/s, loss=0.553, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 47:  99%|▉| 72/73 [00:02<00:00, 29.15it/s, loss=0.537, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 73/73 [00:02<00:00, 29.15it/s, loss=0.537, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 48:  99%|▉| 72/73 [00:02<00:00, 30.60it/s, loss=0.519, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 73/73 [00:02<00:00, 30.64it/s, loss=0.519, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 49:  99%|▉| 72/73 [00:02<00:00, 31.46it/s, loss=0.521, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 73/73 [00:02<00:00, 31.51it/s, loss=0.521, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 50:  99%|▉| 72/73 [00:02<00:00, 29.40it/s, loss=0.527, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 73/73 [00:02<00:00, 29.43it/s, loss=0.527, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 51:  99%|▉| 72/73 [00:02<00:00, 30.07it/s, loss=0.526, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 73/73 [00:02<00:00, 30.02it/s, loss=0.526, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 52:  99%|▉| 72/73 [00:02<00:00, 31.81it/s, loss=0.531, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 73/73 [00:02<00:00, 31.82it/s, loss=0.531, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 53:  99%|▉| 72/73 [00:02<00:00, 32.91it/s, loss=0.583, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 73/73 [00:02<00:00, 32.78it/s, loss=0.583, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 54:  99%|▉| 72/73 [00:02<00:00, 31.71it/s, loss=0.538, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 73/73 [00:02<00:00, 31.74it/s, loss=0.538, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 55:  99%|▉| 72/73 [00:02<00:00, 32.47it/s, loss=0.513, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 73/73 [00:02<00:00, 32.48it/s, loss=0.513, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 56:  99%|▉| 72/73 [00:02<00:00, 32.37it/s, loss=0.559, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 73/73 [00:02<00:00, 32.35it/s, loss=0.559, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 57:  99%|▉| 72/73 [00:02<00:00, 31.44it/s, loss=0.573, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 73/73 [00:02<00:00, 31.44it/s, loss=0.573, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 58:  99%|▉| 72/73 [00:02<00:00, 29.23it/s, loss=0.538, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 73/73 [00:02<00:00, 29.13it/s, loss=0.538, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 59:  99%|▉| 72/73 [00:02<00:00, 28.24it/s, loss=0.538, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|█| 73/73 [00:02<00:00, 28.29it/s, loss=0.538, v_num=2luo, LTC_val\u001b[A\n",
      "Epoch 60:  99%|▉| 72/73 [00:02<00:00, 30.30it/s, loss=0.506, v_num=2luo, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.445. Signaling Trainer to stop.\n",
      "Epoch 60: 100%|█| 73/73 [00:02<00:00, 30.32it/s, loss=0.506, v_num=2luo, LTC_val\n",
      "Epoch 60: 100%|█| 73/73 [00:02<00:00, 30.27it/s, loss=0.506, v_num=2luo, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 100.43it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.8214285969734192,\n",
      " 'LTC_test_f1': 0.8189676403999329,\n",
      " 'test_loss': 0.42044439911842346}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 147619\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_111638-bvv82luo/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_111638-bvv82luo/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.43156\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4392\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621498773\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 209\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc 0.72656\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 0.70607\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.53034\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.61905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.47554\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.82143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.81897\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.42044\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▄▅▅▅▃▄▇▅█▂▃▂▃▇▇▄▇▇▃▄▂▄▂▆▃▆▃▃▆▅▁▃▂▆▃▆█▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc ▁▅▆▆▆▇▆▇▆▇▆▆▇▇▇▇▇▇▆▇▇▇▆▇▇▇██▇██▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 ▁▅▆▆▆▇▆▇▆▇▇▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇██▇██▇▇▇█████▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▃▃▃▃▃▃▃▃▃▃▃▃▃▁▃▁▃█▃▃▁▆▁▆▁▃▃▃▆▃▁▆▃▆▃▃▆▆▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▃▃▃▃▃▃▃▃▃▃▃▃▃▁▃▁▃█▃▃▁▆▁▆▁▃▃▃▆▃▁▆▃▆▃▃▆▆▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄▄▅█▆▄▅▅▄▃▄▃▄▃▃▂▂▁▂▂▂▁▃▁▁▂▂▃▁▃▂▁▁▂▂▃▂▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_multi_head_attention_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/bvv82luo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-20 11:19:45.485211: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_multi_head_attention_loss_weighted_multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3sx3kgvt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_111943-3sx3kgvt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.5 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 72/73 [00:02<00:00, 29.31it/s, loss=1.12, v_num=kgvt, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 73/73 [00:02<00:00, 29.20it/s, loss=1.12, v_num=kgvt, LTC_val_a\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved. New best score: 1.093\n",
      "Epoch 1:  99%|▉| 72/73 [00:02<00:00, 32.22it/s, loss=1.1, v_num=kgvt, LTC_val_ac\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 73/73 [00:02<00:00, 32.24it/s, loss=1.1, v_num=kgvt, LTC_val_ac\u001b[A\n",
      "Epoch 2:  99%|▉| 72/73 [00:02<00:00, 33.05it/s, loss=1.13, v_num=kgvt, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 1.081\n",
      "Epoch 2: 100%|█| 73/73 [00:02<00:00, 33.05it/s, loss=1.13, v_num=kgvt, LTC_val_a\n",
      "Epoch 3:  99%|▉| 72/73 [00:02<00:00, 28.11it/s, loss=1.11, v_num=kgvt, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 73/73 [00:02<00:00, 28.06it/s, loss=1.11, v_num=kgvt, LTC_val_a\u001b[A\n",
      "Epoch 4:  99%|▉| 72/73 [00:02<00:00, 26.08it/s, loss=1.12, v_num=kgvt, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 73/73 [00:02<00:00, 26.08it/s, loss=1.12, v_num=kgvt, LTC_val_a\u001b[A\n",
      "Epoch 5:  99%|▉| 72/73 [00:02<00:00, 26.60it/s, loss=1.1, v_num=kgvt, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 73/73 [00:02<00:00, 26.68it/s, loss=1.1, v_num=kgvt, LTC_val_ac\u001b[A\n",
      "Epoch 6:  99%|▉| 72/73 [00:02<00:00, 26.08it/s, loss=1.1, v_num=kgvt, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 73/73 [00:02<00:00, 26.01it/s, loss=1.1, v_num=kgvt, LTC_val_ac\u001b[A\n",
      "Epoch 7:  99%|▉| 72/73 [00:03<00:00, 21.63it/s, loss=1.11, v_num=kgvt, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 73/73 [00:03<00:00, 21.74it/s, loss=1.11, v_num=kgvt, LTC_val_a\u001b[A\n",
      "Epoch 8:  99%|▉| 72/73 [00:03<00:00, 22.24it/s, loss=1.1, v_num=kgvt, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 73/73 [00:03<00:00, 22.31it/s, loss=1.1, v_num=kgvt, LTC_val_ac\u001b[A\n",
      "Epoch 9:  99%|▉| 72/73 [00:02<00:00, 28.90it/s, loss=1.1, v_num=kgvt, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 73/73 [00:02<00:00, 28.94it/s, loss=1.1, v_num=kgvt, LTC_val_ac\u001b[A\n",
      "Epoch 10:  99%|▉| 72/73 [00:02<00:00, 31.90it/s, loss=1.09, v_num=kgvt, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 73/73 [00:02<00:00, 31.80it/s, loss=1.09, v_num=kgvt, LTC_val_\u001b[A\n",
      "Epoch 11:  99%|▉| 72/73 [00:03<00:00, 22.28it/s, loss=1.07, v_num=kgvt, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 73/73 [00:03<00:00, 22.37it/s, loss=1.07, v_num=kgvt, LTC_val_\u001b[A\n",
      "Epoch 12:  99%|▉| 72/73 [00:03<00:00, 21.39it/s, loss=1.07, v_num=kgvt, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 73/73 [00:03<00:00, 21.47it/s, loss=1.07, v_num=kgvt, LTC_val_\u001b[A\n",
      "Epoch 13:  99%|▉| 72/73 [00:02<00:00, 28.78it/s, loss=1.03, v_num=kgvt, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.074 >= min_delta = 0.003. New best score: 1.006\n",
      "Epoch 13: 100%|█| 73/73 [00:02<00:00, 28.85it/s, loss=1.03, v_num=kgvt, LTC_val_\n",
      "Epoch 14:  99%|▉| 72/73 [00:02<00:00, 24.26it/s, loss=1.06, v_num=kgvt, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 73/73 [00:02<00:00, 24.36it/s, loss=1.06, v_num=kgvt, LTC_val_\u001b[A\n",
      "Epoch 15:  99%|▉| 72/73 [00:03<00:00, 22.65it/s, loss=1.03, v_num=kgvt, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 73/73 [00:03<00:00, 22.69it/s, loss=1.03, v_num=kgvt, LTC_val_\u001b[A\n",
      "Epoch 16:  99%|▉| 72/73 [00:03<00:00, 20.78it/s, loss=1.01, v_num=kgvt, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 73/73 [00:03<00:00, 20.83it/s, loss=1.01, v_num=kgvt, LTC_val_\u001b[A\n",
      "Epoch 17:  99%|▉| 72/73 [00:03<00:00, 21.66it/s, loss=0.997, v_num=kgvt, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 73/73 [00:03<00:00, 21.77it/s, loss=0.997, v_num=kgvt, LTC_val\u001b[A\n",
      "Epoch 18:  99%|▉| 72/73 [00:03<00:00, 22.65it/s, loss=1.05, v_num=kgvt, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 73/73 [00:03<00:00, 22.53it/s, loss=1.05, v_num=kgvt, LTC_val_\u001b[A\n",
      "Epoch 19:  99%|▉| 72/73 [00:03<00:00, 22.89it/s, loss=1.02, v_num=kgvt, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 73/73 [00:03<00:00, 22.98it/s, loss=1.02, v_num=kgvt, LTC_val_\u001b[A\n",
      "Epoch 20:  99%|▉| 72/73 [00:02<00:00, 25.42it/s, loss=1.03, v_num=kgvt, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 73/73 [00:02<00:00, 25.40it/s, loss=1.03, v_num=kgvt, LTC_val_\u001b[A\n",
      "Epoch 21:  99%|▉| 72/73 [00:02<00:00, 29.74it/s, loss=1.06, v_num=kgvt, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 73/73 [00:02<00:00, 29.75it/s, loss=1.06, v_num=kgvt, LTC_val_\u001b[A\n",
      "Epoch 22:  99%|▉| 72/73 [00:02<00:00, 24.83it/s, loss=1.01, v_num=kgvt, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 73/73 [00:02<00:00, 24.93it/s, loss=1.01, v_num=kgvt, LTC_val_\u001b[A\n",
      "Epoch 23:  99%|▉| 72/73 [00:02<00:00, 29.13it/s, loss=1.02, v_num=kgvt, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 73/73 [00:02<00:00, 29.20it/s, loss=1.02, v_num=kgvt, LTC_val_\u001b[A\n",
      "Epoch 24:  99%|▉| 72/73 [00:02<00:00, 30.66it/s, loss=1.04, v_num=kgvt, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 73/73 [00:02<00:00, 30.67it/s, loss=1.04, v_num=kgvt, LTC_val_\u001b[A\n",
      "Epoch 25:  99%|▉| 72/73 [00:02<00:00, 32.54it/s, loss=0.975, v_num=kgvt, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 73/73 [00:02<00:00, 32.52it/s, loss=0.975, v_num=kgvt, LTC_val\u001b[A\n",
      "Epoch 26:  99%|▉| 72/73 [00:02<00:00, 33.98it/s, loss=1.01, v_num=kgvt, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 73/73 [00:02<00:00, 33.91it/s, loss=1.01, v_num=kgvt, LTC_val_\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27:  99%|▉| 72/73 [00:02<00:00, 33.70it/s, loss=0.985, v_num=kgvt, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 73/73 [00:02<00:00, 33.70it/s, loss=0.985, v_num=kgvt, LTC_val\u001b[A\n",
      "Epoch 28:  99%|▉| 72/73 [00:02<00:00, 31.44it/s, loss=0.986, v_num=kgvt, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 73/73 [00:02<00:00, 31.37it/s, loss=0.986, v_num=kgvt, LTC_val\u001b[A\n",
      "Epoch 29:  99%|▉| 72/73 [00:02<00:00, 31.43it/s, loss=1.01, v_num=kgvt, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 73/73 [00:02<00:00, 31.42it/s, loss=1.01, v_num=kgvt, LTC_val_\u001b[A\n",
      "Epoch 30:  99%|▉| 72/73 [00:02<00:00, 32.28it/s, loss=1, v_num=kgvt, LTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 73/73 [00:02<00:00, 32.23it/s, loss=1, v_num=kgvt, LTC_val_acc\u001b[A\n",
      "Epoch 31:  99%|▉| 72/73 [00:02<00:00, 33.05it/s, loss=0.999, v_num=kgvt, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 73/73 [00:02<00:00, 33.07it/s, loss=0.999, v_num=kgvt, LTC_val\u001b[A\n",
      "Epoch 32:  99%|▉| 72/73 [00:02<00:00, 28.50it/s, loss=0.975, v_num=kgvt, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 73/73 [00:02<00:00, 28.52it/s, loss=0.975, v_num=kgvt, LTC_val\u001b[A\n",
      "Epoch 33:  99%|▉| 72/73 [00:02<00:00, 30.80it/s, loss=1.01, v_num=kgvt, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 73/73 [00:02<00:00, 30.82it/s, loss=1.01, v_num=kgvt, LTC_val_\u001b[A\n",
      "Epoch 34:  99%|▉| 72/73 [00:02<00:00, 34.74it/s, loss=1.01, v_num=kgvt, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 73/73 [00:02<00:00, 34.74it/s, loss=1.01, v_num=kgvt, LTC_val_\u001b[A\n",
      "Epoch 35:  99%|▉| 72/73 [00:04<00:00, 17.55it/s, loss=0.993, v_num=kgvt, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 73/73 [00:04<00:00, 17.66it/s, loss=0.993, v_num=kgvt, LTC_val\u001b[A\n",
      "Epoch 36:  99%|▉| 72/73 [00:03<00:00, 21.08it/s, loss=0.95, v_num=kgvt, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 73/73 [00:03<00:00, 21.10it/s, loss=0.95, v_num=kgvt, LTC_val_\u001b[A\n",
      "Epoch 37:  99%|▉| 72/73 [00:02<00:00, 27.49it/s, loss=0.985, v_num=kgvt, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 73/73 [00:02<00:00, 27.53it/s, loss=0.985, v_num=kgvt, LTC_val\u001b[A\n",
      "Epoch 38:  99%|▉| 72/73 [00:03<00:00, 21.95it/s, loss=0.977, v_num=kgvt, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 1.006. Signaling Trainer to stop.\n",
      "Epoch 38: 100%|█| 73/73 [00:03<00:00, 22.05it/s, loss=0.977, v_num=kgvt, LTC_val\n",
      "Epoch 38: 100%|█| 73/73 [00:03<00:00, 22.03it/s, loss=0.977, v_num=kgvt, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 108.38it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.4642857015132904,\n",
      " 'LTC_test_f1': 0.42032572627067566,\n",
      " 'test_loss': 0.9090121984481812}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 148105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_111943-3sx3kgvt/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_111943-3sx3kgvt/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.89747\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 38\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2808\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 118\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621498901\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 134\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc 0.40625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 0.35695\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.98188\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.31481\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.30933\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.46429\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.42033\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.90901\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▅▆▅▄▅▅▅▅▅▅▅▅▄▂▅▃▃▃▄▅▅▅▃▄▄▄▄▂▄▄▄▃▅▂▅▄▁▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc ▁▃▁▇▂▁▂▃▂▄▂▆▄▅▄▆▄▆█▅▆▅▇▆▆▇▇█▇▆▆▅▇██▇▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 ▃▃▁▃▂▃▂▁▂▄▃▄▅▆▅▇▄▆█▅▆▅▇▆▆▇█▇▆▆▅▅▇▆▇▅▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▇▆▆▅▆▆▅▅▅▅▄▃▃▃▃▂▃▃▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▅▅▅▁▅▅▅▅▁▅█▁▅██▁▅▅▅▅▅▁▁▁▅▁▁▅▅▅▁▅▅▅▅▁▁▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▄▂▂▁▂▂▂▂▁▂█▄▅▇▇▁▅▅▅▅▅▁▁▁▅▁▁▅▅▅▁▅▅▅▅▁▁▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▂▃▂▂▂▃▃▂▂▂▂▃▄▁▁▆▃▅▄▄▃▇▆▇▅▇▇▂▄▄█▄▅▄▅▆▇▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_multi_head_attention_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3sx3kgvt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 11:21:52.616299: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_multi_head_attention_loss_weighted_multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3kanzipz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_112151-3kanzipz\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.5 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/73 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|▉| 72/73 [00:02<00:00, 25.66it/s, loss=1.15, v_num=zipz, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.109\n",
      "Epoch 0: 100%|█| 73/73 [00:02<00:00, 25.74it/s, loss=1.15, v_num=zipz, LTC_val_a\n",
      "Epoch 1:  99%|▉| 72/73 [00:02<00:00, 30.42it/s, loss=1.13, v_num=zipz, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 1.101\n",
      "Epoch 1: 100%|█| 73/73 [00:02<00:00, 30.45it/s, loss=1.13, v_num=zipz, LTC_val_a\n",
      "Epoch 2:  99%|▉| 72/73 [00:02<00:00, 26.28it/s, loss=1.1, v_num=zipz, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.007 >= min_delta = 0.003. New best score: 1.094\n",
      "Epoch 2: 100%|█| 73/73 [00:02<00:00, 26.22it/s, loss=1.1, v_num=zipz, LTC_val_ac\n",
      "Epoch 3:  99%|▉| 72/73 [00:02<00:00, 24.42it/s, loss=1.14, v_num=zipz, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.037 >= min_delta = 0.003. New best score: 1.057\n",
      "Epoch 3: 100%|█| 73/73 [00:02<00:00, 24.51it/s, loss=1.14, v_num=zipz, LTC_val_a\n",
      "Epoch 4:  99%|▉| 72/73 [00:02<00:00, 27.41it/s, loss=1.11, v_num=zipz, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 73/73 [00:02<00:00, 27.47it/s, loss=1.11, v_num=zipz, LTC_val_a\u001b[A\n",
      "Epoch 5:  99%|▉| 72/73 [00:02<00:00, 26.53it/s, loss=1.11, v_num=zipz, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 73/73 [00:02<00:00, 26.46it/s, loss=1.11, v_num=zipz, LTC_val_a\u001b[A\n",
      "Epoch 6:  99%|▉| 72/73 [00:02<00:00, 24.75it/s, loss=1.11, v_num=zipz, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 73/73 [00:02<00:00, 24.75it/s, loss=1.11, v_num=zipz, LTC_val_a\u001b[A\n",
      "Epoch 7:  99%|▉| 72/73 [00:02<00:00, 26.96it/s, loss=1.11, v_num=zipz, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 73/73 [00:02<00:00, 26.96it/s, loss=1.11, v_num=zipz, LTC_val_a\u001b[A\n",
      "Epoch 8:  99%|▉| 72/73 [00:03<00:00, 23.89it/s, loss=1.1, v_num=zipz, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 73/73 [00:03<00:00, 23.98it/s, loss=1.1, v_num=zipz, LTC_val_ac\u001b[A\n",
      "Epoch 9:  99%|▉| 72/73 [00:02<00:00, 25.39it/s, loss=1.1, v_num=zipz, LTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 73/73 [00:02<00:00, 25.49it/s, loss=1.1, v_num=zipz, LTC_val_ac\u001b[A\n",
      "Epoch 10:  99%|▉| 72/73 [00:02<00:00, 31.92it/s, loss=1.11, v_num=zipz, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 73/73 [00:02<00:00, 31.90it/s, loss=1.11, v_num=zipz, LTC_val_\u001b[A\n",
      "Epoch 11:  99%|▉| 72/73 [00:02<00:00, 29.63it/s, loss=1.1, v_num=zipz, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 73/73 [00:02<00:00, 29.66it/s, loss=1.1, v_num=zipz, LTC_val_a\u001b[A\n",
      "Epoch 12:  99%|▉| 72/73 [00:02<00:00, 32.52it/s, loss=1.11, v_num=zipz, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 73/73 [00:02<00:00, 32.47it/s, loss=1.11, v_num=zipz, LTC_val_\u001b[A\n",
      "Epoch 13:  99%|▉| 72/73 [00:02<00:00, 34.13it/s, loss=1.1, v_num=zipz, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 73/73 [00:02<00:00, 34.10it/s, loss=1.1, v_num=zipz, LTC_val_a\u001b[A\n",
      "Epoch 14:  99%|▉| 72/73 [00:02<00:00, 32.96it/s, loss=1.1, v_num=zipz, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 73/73 [00:02<00:00, 32.89it/s, loss=1.1, v_num=zipz, LTC_val_a\u001b[A\n",
      "Epoch 15:  99%|▉| 72/73 [00:03<00:00, 21.93it/s, loss=1.1, v_num=zipz, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 73/73 [00:03<00:00, 22.02it/s, loss=1.1, v_num=zipz, LTC_val_a\u001b[A\n",
      "Epoch 16:  99%|▉| 72/73 [00:02<00:00, 24.41it/s, loss=1.1, v_num=zipz, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 73/73 [00:02<00:00, 24.36it/s, loss=1.1, v_num=zipz, LTC_val_a\u001b[A\n",
      "Epoch 17:  99%|▉| 72/73 [00:02<00:00, 29.03it/s, loss=1.1, v_num=zipz, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 73/73 [00:02<00:00, 29.09it/s, loss=1.1, v_num=zipz, LTC_val_a\u001b[A\n",
      "Epoch 18:  99%|▉| 72/73 [00:02<00:00, 31.78it/s, loss=1.1, v_num=zipz, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 73/73 [00:02<00:00, 31.79it/s, loss=1.1, v_num=zipz, LTC_val_a\u001b[A\n",
      "Epoch 19:  99%|▉| 72/73 [00:02<00:00, 32.23it/s, loss=1.1, v_num=zipz, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 73/73 [00:02<00:00, 32.10it/s, loss=1.1, v_num=zipz, LTC_val_a\u001b[A\n",
      "Epoch 20:  99%|▉| 72/73 [00:02<00:00, 32.32it/s, loss=1.05, v_num=zipz, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 73/73 [00:02<00:00, 32.27it/s, loss=1.05, v_num=zipz, LTC_val_\u001b[A\n",
      "Epoch 21:  99%|▉| 72/73 [00:02<00:00, 31.23it/s, loss=1.03, v_num=zipz, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 73/73 [00:02<00:00, 31.25it/s, loss=1.03, v_num=zipz, LTC_val_\u001b[A\n",
      "Epoch 22:  99%|▉| 72/73 [00:02<00:00, 32.13it/s, loss=1.05, v_num=zipz, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 73/73 [00:02<00:00, 32.14it/s, loss=1.05, v_num=zipz, LTC_val_\u001b[A\n",
      "Epoch 23:  99%|▉| 72/73 [00:02<00:00, 32.06it/s, loss=1.01, v_num=zipz, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 73/73 [00:02<00:00, 32.04it/s, loss=1.01, v_num=zipz, LTC_val_\u001b[A\n",
      "Epoch 24:  99%|▉| 72/73 [00:02<00:00, 32.05it/s, loss=1.04, v_num=zipz, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 73/73 [00:02<00:00, 32.03it/s, loss=1.04, v_num=zipz, LTC_val_\u001b[A\n",
      "Epoch 25:  99%|▉| 72/73 [00:02<00:00, 31.06it/s, loss=0.994, v_num=zipz, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 73/73 [00:02<00:00, 31.09it/s, loss=0.994, v_num=zipz, LTC_val\u001b[A\n",
      "Epoch 26:  99%|▉| 72/73 [00:02<00:00, 31.34it/s, loss=1.06, v_num=zipz, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 73/73 [00:02<00:00, 31.37it/s, loss=1.06, v_num=zipz, LTC_val_\u001b[A\n",
      "Epoch 27:  99%|▉| 72/73 [00:02<00:00, 29.25it/s, loss=1.01, v_num=zipz, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 73/73 [00:02<00:00, 29.29it/s, loss=1.01, v_num=zipz, LTC_val_\u001b[A\n",
      "Epoch 28:  99%|▉| 72/73 [00:02<00:00, 31.53it/s, loss=0.981, v_num=zipz, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 1.057. Signaling Trainer to stop.\n",
      "Epoch 28: 100%|█| 73/73 [00:02<00:00, 31.56it/s, loss=0.981, v_num=zipz, LTC_val\n",
      "Epoch 28: 100%|█| 73/73 [00:02<00:00, 31.52it/s, loss=0.981, v_num=zipz, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 101.87it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.1785714328289032,\n",
      " 'LTC_test_f1': 0.0981685072183609,\n",
      " 'test_loss': 1.1079226732254028}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 148464\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_112151-3kanzipz/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_112151-3kanzipz/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.06611\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 28\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2088\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 83\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621498994\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc 0.40104\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 0.35041\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 1.00656\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.31481\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.28084\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.17857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.09817\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.10792\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▃▂▄▄▅▄▃▃▄▄▄▄▄▃▄▃▃▃▄▃▄▄▃▃▄▃▄▃▃▄▃▁▃▇▃▃▁█▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc ▁▃▄▄▆▄▅▃▇▃▃▅▇▅▆▄█▄▆▆▅▆█▆▇█▇▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 ▁▃▃▃▅▃▄▃▅▃▂▄▅▃▄▃▅▂▃▅▅▆█▅▇▇▇▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▅▅▄▅▅▅▄▅▅▅▄▅▄▅▄▅▄▄▄▃▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▁▃▃▃▃▆▃█▃▃▃▃▃▃▁▃▁▃▃▆▃▃▃▁▁▃▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▁▂▂▂▂▇▂█▂▂▂▂▂▂▁▂▂▂▅▇▅▅▅▁▁▅▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▂▂▂▁▂▂▂▁▂▂▂▂▁▂▁▂▂▂▂▁▂▆▃▆▆█▄▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_multi_head_attention_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3kanzipz\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 11:23:25.587652: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_multi_head_attention__binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/xfjkqio2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_112324-xfjkqio2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 72/73 [00:02<00:00, 30.71it/s, loss=0.709, v_num=qio2, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.634\n",
      "Epoch 0: 100%|█| 73/73 [00:02<00:00, 29.96it/s, loss=0.709, v_num=qio2, LTC_val_\n",
      "Epoch 1:  99%|▉| 72/73 [00:02<00:00, 27.98it/s, loss=0.602, v_num=qio2, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 73/73 [00:02<00:00, 28.06it/s, loss=0.602, v_num=qio2, LTC_val_\u001b[A\n",
      "Epoch 2:  99%|▉| 72/73 [00:02<00:00, 29.27it/s, loss=0.631, v_num=qio2, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 73/73 [00:02<00:00, 29.09it/s, loss=0.631, v_num=qio2, LTC_val_\u001b[A\n",
      "Epoch 3:  99%|▉| 72/73 [00:02<00:00, 27.39it/s, loss=0.622, v_num=qio2, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 73/73 [00:02<00:00, 27.46it/s, loss=0.622, v_num=qio2, LTC_val_\u001b[A\n",
      "Epoch 4:  99%|▉| 72/73 [00:03<00:00, 23.77it/s, loss=0.627, v_num=qio2, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.052 >= min_delta = 0.003. New best score: 0.581\n",
      "Epoch 4: 100%|█| 73/73 [00:03<00:00, 23.82it/s, loss=0.627, v_num=qio2, LTC_val_\n",
      "Epoch 5:  99%|▉| 72/73 [00:03<00:00, 21.69it/s, loss=0.611, v_num=qio2, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 73/73 [00:03<00:00, 21.78it/s, loss=0.611, v_num=qio2, LTC_val_\u001b[A\n",
      "Epoch 6:  99%|▉| 72/73 [00:02<00:00, 30.99it/s, loss=0.571, v_num=qio2, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 73/73 [00:02<00:00, 31.00it/s, loss=0.571, v_num=qio2, LTC_val_\u001b[A\n",
      "Epoch 7:  99%|▉| 72/73 [00:02<00:00, 24.77it/s, loss=0.601, v_num=qio2, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 73/73 [00:02<00:00, 24.88it/s, loss=0.601, v_num=qio2, LTC_val_\u001b[A\n",
      "Epoch 8:  99%|▉| 72/73 [00:02<00:00, 33.32it/s, loss=0.621, v_num=qio2, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 73/73 [00:02<00:00, 33.32it/s, loss=0.621, v_num=qio2, LTC_val_\u001b[A\n",
      "Epoch 9:  99%|▉| 72/73 [00:03<00:00, 20.47it/s, loss=0.585, v_num=qio2, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 73/73 [00:03<00:00, 20.52it/s, loss=0.585, v_num=qio2, LTC_val_\u001b[A\n",
      "Epoch 10:  99%|▉| 72/73 [00:03<00:00, 23.61it/s, loss=0.603, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.027 >= min_delta = 0.003. New best score: 0.554\n",
      "Epoch 10: 100%|█| 73/73 [00:03<00:00, 23.60it/s, loss=0.603, v_num=qio2, LTC_val\n",
      "Epoch 11:  99%|▉| 72/73 [00:03<00:00, 23.52it/s, loss=0.576, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 73/73 [00:03<00:00, 23.60it/s, loss=0.576, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 12:  99%|▉| 72/73 [00:02<00:00, 24.21it/s, loss=0.556, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 73/73 [00:03<00:00, 24.25it/s, loss=0.556, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 13:  99%|▉| 72/73 [00:02<00:00, 27.14it/s, loss=0.578, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 73/73 [00:02<00:00, 27.19it/s, loss=0.578, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 14:  99%|▉| 72/73 [00:02<00:00, 31.97it/s, loss=0.583, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 73/73 [00:02<00:00, 31.91it/s, loss=0.583, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 15:  99%|▉| 72/73 [00:02<00:00, 27.28it/s, loss=0.593, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 73/73 [00:02<00:00, 27.23it/s, loss=0.593, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 16:  99%|▉| 72/73 [00:03<00:00, 23.87it/s, loss=0.611, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.067 >= min_delta = 0.003. New best score: 0.487\n",
      "Epoch 16: 100%|█| 73/73 [00:03<00:00, 23.96it/s, loss=0.611, v_num=qio2, LTC_val\n",
      "Epoch 17:  99%|▉| 72/73 [00:03<00:00, 20.07it/s, loss=0.615, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 73/73 [00:03<00:00, 20.04it/s, loss=0.615, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 18:  99%|▉| 72/73 [00:02<00:00, 25.97it/s, loss=0.579, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.038 >= min_delta = 0.003. New best score: 0.450\n",
      "Epoch 18: 100%|█| 73/73 [00:02<00:00, 26.03it/s, loss=0.579, v_num=qio2, LTC_val\n",
      "Epoch 19:  99%|▉| 72/73 [00:03<00:00, 21.43it/s, loss=0.576, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 73/73 [00:03<00:00, 21.54it/s, loss=0.576, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 20:  99%|▉| 72/73 [00:03<00:00, 22.68it/s, loss=0.611, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 73/73 [00:03<00:00, 22.63it/s, loss=0.611, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 21:  99%|▉| 72/73 [00:03<00:00, 19.62it/s, loss=0.532, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 73/73 [00:03<00:00, 19.76it/s, loss=0.532, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 22:  99%|▉| 72/73 [00:02<00:00, 27.78it/s, loss=0.568, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 73/73 [00:02<00:00, 27.78it/s, loss=0.568, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 23:  99%|▉| 72/73 [00:02<00:00, 32.03it/s, loss=0.609, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.015 >= min_delta = 0.003. New best score: 0.434\n",
      "Epoch 23: 100%|█| 73/73 [00:02<00:00, 32.03it/s, loss=0.609, v_num=qio2, LTC_val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24:  99%|▉| 72/73 [00:02<00:00, 26.38it/s, loss=0.57, v_num=qio2, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 73/73 [00:02<00:00, 26.44it/s, loss=0.57, v_num=qio2, LTC_val_\u001b[A\n",
      "Epoch 25:  99%|▉| 72/73 [00:03<00:00, 23.05it/s, loss=0.554, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 73/73 [00:03<00:00, 23.14it/s, loss=0.554, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 26:  99%|▉| 72/73 [00:03<00:00, 23.61it/s, loss=0.591, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 73/73 [00:03<00:00, 23.71it/s, loss=0.591, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 27:  99%|▉| 72/73 [00:02<00:00, 25.33it/s, loss=0.583, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 73/73 [00:02<00:00, 25.42it/s, loss=0.583, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 28:  99%|▉| 72/73 [00:03<00:00, 23.52it/s, loss=0.589, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 73/73 [00:03<00:00, 23.60it/s, loss=0.589, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 29:  99%|▉| 72/73 [00:02<00:00, 25.18it/s, loss=0.56, v_num=qio2, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 73/73 [00:02<00:00, 25.27it/s, loss=0.56, v_num=qio2, LTC_val_\u001b[A\n",
      "Epoch 30:  99%|▉| 72/73 [00:03<00:00, 20.32it/s, loss=0.56, v_num=qio2, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 73/73 [00:03<00:00, 20.42it/s, loss=0.56, v_num=qio2, LTC_val_\u001b[A\n",
      "Epoch 31:  99%|▉| 72/73 [00:03<00:00, 19.63it/s, loss=0.575, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 73/73 [00:03<00:00, 19.70it/s, loss=0.575, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 32:  99%|▉| 72/73 [00:03<00:00, 22.03it/s, loss=0.554, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 73/73 [00:03<00:00, 22.13it/s, loss=0.554, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 33:  99%|▉| 72/73 [00:02<00:00, 27.45it/s, loss=0.549, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 0.423\n",
      "Epoch 33: 100%|█| 73/73 [00:02<00:00, 27.25it/s, loss=0.549, v_num=qio2, LTC_val\n",
      "Epoch 34:  99%|▉| 72/73 [00:03<00:00, 23.35it/s, loss=0.549, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 73/73 [00:03<00:00, 23.41it/s, loss=0.549, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 35:  99%|▉| 72/73 [00:03<00:00, 23.21it/s, loss=0.586, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 73/73 [00:03<00:00, 23.32it/s, loss=0.586, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 36:  99%|▉| 72/73 [00:02<00:00, 31.01it/s, loss=0.543, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 73/73 [00:02<00:00, 31.05it/s, loss=0.543, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 37:  99%|▉| 72/73 [00:02<00:00, 32.97it/s, loss=0.548, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 73/73 [00:02<00:00, 32.96it/s, loss=0.548, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 38:  99%|▉| 72/73 [00:02<00:00, 33.30it/s, loss=0.52, v_num=qio2, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 73/73 [00:02<00:00, 33.28it/s, loss=0.52, v_num=qio2, LTC_val_\u001b[A\n",
      "Epoch 39:  99%|▉| 72/73 [00:02<00:00, 33.58it/s, loss=0.52, v_num=qio2, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 73/73 [00:02<00:00, 33.58it/s, loss=0.52, v_num=qio2, LTC_val_\u001b[A\n",
      "Epoch 40:  99%|▉| 72/73 [00:02<00:00, 30.57it/s, loss=0.526, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 73/73 [00:02<00:00, 30.59it/s, loss=0.526, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 41:  99%|▉| 72/73 [00:02<00:00, 31.00it/s, loss=0.557, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 73/73 [00:02<00:00, 30.88it/s, loss=0.557, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 42:  99%|▉| 72/73 [00:02<00:00, 29.52it/s, loss=0.548, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 73/73 [00:02<00:00, 29.32it/s, loss=0.548, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 43:  99%|▉| 72/73 [00:02<00:00, 26.43it/s, loss=0.545, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 73/73 [00:02<00:00, 26.49it/s, loss=0.545, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 44:  99%|▉| 72/73 [00:02<00:00, 30.23it/s, loss=0.525, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 73/73 [00:02<00:00, 30.23it/s, loss=0.525, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 45:  99%|▉| 72/73 [00:02<00:00, 30.15it/s, loss=0.56, v_num=qio2, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 73/73 [00:02<00:00, 30.18it/s, loss=0.56, v_num=qio2, LTC_val_\u001b[A\n",
      "Epoch 46:  99%|▉| 72/73 [00:02<00:00, 31.04it/s, loss=0.57, v_num=qio2, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 73/73 [00:02<00:00, 30.99it/s, loss=0.57, v_num=qio2, LTC_val_\u001b[A\n",
      "Epoch 47:  99%|▉| 72/73 [00:02<00:00, 31.25it/s, loss=0.53, v_num=qio2, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 73/73 [00:02<00:00, 31.27it/s, loss=0.53, v_num=qio2, LTC_val_\u001b[A\n",
      "Epoch 48:  99%|▉| 72/73 [00:02<00:00, 30.47it/s, loss=0.56, v_num=qio2, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 73/73 [00:02<00:00, 30.46it/s, loss=0.56, v_num=qio2, LTC_val_\u001b[A\n",
      "Epoch 49:  99%|▉| 72/73 [00:02<00:00, 25.33it/s, loss=0.523, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 73/73 [00:02<00:00, 25.40it/s, loss=0.523, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 50:  99%|▉| 72/73 [00:02<00:00, 31.41it/s, loss=0.54, v_num=qio2, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 73/73 [00:02<00:00, 31.36it/s, loss=0.54, v_num=qio2, LTC_val_\u001b[A\n",
      "Epoch 51:  99%|▉| 72/73 [00:02<00:00, 32.54it/s, loss=0.522, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 73/73 [00:02<00:00, 32.53it/s, loss=0.522, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 52:  99%|▉| 72/73 [00:02<00:00, 32.89it/s, loss=0.551, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 73/73 [00:02<00:00, 32.89it/s, loss=0.551, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 53:  99%|▉| 72/73 [00:02<00:00, 33.15it/s, loss=0.517, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 73/73 [00:02<00:00, 33.15it/s, loss=0.517, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 54:  99%|▉| 72/73 [00:02<00:00, 33.08it/s, loss=0.506, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 73/73 [00:02<00:00, 33.09it/s, loss=0.506, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 55:  99%|▉| 72/73 [00:02<00:00, 32.57it/s, loss=0.494, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 73/73 [00:02<00:00, 32.59it/s, loss=0.494, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 56:  99%|▉| 72/73 [00:02<00:00, 32.13it/s, loss=0.538, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 73/73 [00:02<00:00, 32.15it/s, loss=0.538, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 57:  99%|▉| 72/73 [00:02<00:00, 32.75it/s, loss=0.539, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 73/73 [00:02<00:00, 32.70it/s, loss=0.539, v_num=qio2, LTC_val\u001b[A\n",
      "Epoch 58:  99%|▉| 72/73 [00:02<00:00, 32.99it/s, loss=0.524, v_num=qio2, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.423. Signaling Trainer to stop.\n",
      "Epoch 58: 100%|█| 73/73 [00:02<00:00, 32.96it/s, loss=0.524, v_num=qio2, LTC_val\n",
      "Epoch 58: 100%|█| 73/73 [00:02<00:00, 32.92it/s, loss=0.524, v_num=qio2, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 78.20it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.8571428656578064,\n",
      " 'LTC_test_f1': 0.8564729690551758,\n",
      " 'test_loss': 0.43036583065986633}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 148742\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_112324-xfjkqio2/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_112324-xfjkqio2/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.54339\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 58\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4248\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 169\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621499173\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 202\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc 0.73785\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 0.72618\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.54388\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.73333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.49591\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.85714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.85647\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.43037\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▆▅▁▆▃▆▄▃▃▅▃▄▆▅▅▄▂▃▄▃▄▅▁▄▃▇▆▅▃▃▂▃▃▄▃█▃▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc ▁▅▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇█████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 ▁▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇█████▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▅▄▄▄▄▃▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▃▃▃▃▃▃▃▃▃▃▃▃▃▃▆▃█▃▁▃▆▃▃█▁▁▁▆█▃▁▆▁▆▁▁▁▆▃▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▄▄▄▄▄▄▄▄▄▄▄▄▄▄▆▄█▄▂▄▆▄▄█▂▂▂▆█▄▂▆▂▆▁▂▂▆▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▆██▅▇▇▆▄▆▅▅▂▃▂▁▂▁▃▂▃▂▂▂▁▂▂▄▁▃▂▂▂▂▃▄▃▂▁▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_multi_head_attention__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/xfjkqio2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 11:26:24.914949: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_multi_head_attention__binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3pe2a10k\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_112623-3pe2a10k\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 72/73 [00:01<00:00, 37.22it/s, loss=0.674, v_num=a10k, LTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.605\n",
      "Epoch 0: 100%|█| 73/73 [00:02<00:00, 36.01it/s, loss=0.674, v_num=a10k, LTC_val_\n",
      "Epoch 1:  99%|▉| 72/73 [00:02<00:00, 31.78it/s, loss=0.629, v_num=a10k, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 73/73 [00:02<00:00, 31.81it/s, loss=0.629, v_num=a10k, LTC_val_\u001b[A\n",
      "Epoch 2:  99%|▉| 72/73 [00:02<00:00, 33.52it/s, loss=0.616, v_num=a10k, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 73/73 [00:02<00:00, 33.50it/s, loss=0.616, v_num=a10k, LTC_val_\u001b[A\n",
      "Epoch 3:  99%|▉| 72/73 [00:02<00:00, 31.81it/s, loss=0.614, v_num=a10k, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 73/73 [00:02<00:00, 31.85it/s, loss=0.614, v_num=a10k, LTC_val_\u001b[A\n",
      "Epoch 4:  99%|▉| 72/73 [00:03<00:00, 22.84it/s, loss=0.615, v_num=a10k, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 73/73 [00:03<00:00, 22.96it/s, loss=0.615, v_num=a10k, LTC_val_\u001b[A\n",
      "Epoch 5:  99%|▉| 72/73 [00:02<00:00, 34.96it/s, loss=0.609, v_num=a10k, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 73/73 [00:02<00:00, 34.95it/s, loss=0.609, v_num=a10k, LTC_val_\u001b[A\n",
      "Epoch 6:  99%|▉| 72/73 [00:02<00:00, 27.87it/s, loss=0.579, v_num=a10k, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 73/73 [00:02<00:00, 27.95it/s, loss=0.579, v_num=a10k, LTC_val_\u001b[A\n",
      "Epoch 7:  99%|▉| 72/73 [00:02<00:00, 27.56it/s, loss=0.571, v_num=a10k, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 73/73 [00:02<00:00, 27.66it/s, loss=0.571, v_num=a10k, LTC_val_\u001b[A\n",
      "Epoch 8:  99%|▉| 72/73 [00:02<00:00, 26.83it/s, loss=0.613, v_num=a10k, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 73/73 [00:02<00:00, 26.88it/s, loss=0.613, v_num=a10k, LTC_val_\u001b[A\n",
      "Epoch 9:  99%|▉| 72/73 [00:02<00:00, 26.08it/s, loss=0.608, v_num=a10k, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 73/73 [00:02<00:00, 26.15it/s, loss=0.608, v_num=a10k, LTC_val_\u001b[A\n",
      "Epoch 10:  99%|▉| 72/73 [00:02<00:00, 26.66it/s, loss=0.573, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 0.593\n",
      "Epoch 10: 100%|█| 73/73 [00:02<00:00, 26.68it/s, loss=0.573, v_num=a10k, LTC_val\n",
      "Epoch 11:  99%|▉| 72/73 [00:02<00:00, 25.56it/s, loss=0.613, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.589\n",
      "Epoch 11: 100%|█| 73/73 [00:02<00:00, 25.63it/s, loss=0.613, v_num=a10k, LTC_val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  99%|▉| 72/73 [00:02<00:00, 25.53it/s, loss=0.619, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 73/73 [00:02<00:00, 25.62it/s, loss=0.619, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 13:  99%|▉| 72/73 [00:02<00:00, 24.46it/s, loss=0.616, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 73/73 [00:02<00:00, 24.53it/s, loss=0.616, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 14:  99%|▉| 72/73 [00:02<00:00, 25.53it/s, loss=0.572, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 73/73 [00:02<00:00, 25.60it/s, loss=0.572, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 15:  99%|▉| 72/73 [00:03<00:00, 23.50it/s, loss=0.563, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 73/73 [00:03<00:00, 23.58it/s, loss=0.563, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 16:  99%|▉| 72/73 [00:02<00:00, 33.81it/s, loss=0.57, v_num=a10k, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 73/73 [00:02<00:00, 33.80it/s, loss=0.57, v_num=a10k, LTC_val_\u001b[A\n",
      "Epoch 17:  99%|▉| 72/73 [00:02<00:00, 33.70it/s, loss=0.609, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 73/73 [00:02<00:00, 33.72it/s, loss=0.609, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 18:  99%|▉| 72/73 [00:02<00:00, 34.21it/s, loss=0.581, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 73/73 [00:02<00:00, 34.22it/s, loss=0.581, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 19:  99%|▉| 72/73 [00:02<00:00, 33.78it/s, loss=0.604, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 0.580\n",
      "Epoch 19: 100%|█| 73/73 [00:02<00:00, 33.79it/s, loss=0.604, v_num=a10k, LTC_val\n",
      "Epoch 20:  99%|▉| 72/73 [00:02<00:00, 24.15it/s, loss=0.581, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.029 >= min_delta = 0.003. New best score: 0.551\n",
      "Epoch 20: 100%|█| 73/73 [00:03<00:00, 24.23it/s, loss=0.581, v_num=a10k, LTC_val\n",
      "Epoch 21:  99%|▉| 72/73 [00:02<00:00, 27.55it/s, loss=0.59, v_num=a10k, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.075 >= min_delta = 0.003. New best score: 0.476\n",
      "Epoch 21: 100%|█| 73/73 [00:02<00:00, 27.61it/s, loss=0.59, v_num=a10k, LTC_val_\n",
      "Epoch 22:  99%|▉| 72/73 [00:02<00:00, 31.10it/s, loss=0.572, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 73/73 [00:02<00:00, 31.13it/s, loss=0.572, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 23:  99%|▉| 72/73 [00:02<00:00, 31.94it/s, loss=0.565, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 73/73 [00:02<00:00, 31.96it/s, loss=0.565, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 24:  99%|▉| 72/73 [00:02<00:00, 30.39it/s, loss=0.57, v_num=a10k, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.017 >= min_delta = 0.003. New best score: 0.459\n",
      "Epoch 24: 100%|█| 73/73 [00:02<00:00, 30.41it/s, loss=0.57, v_num=a10k, LTC_val_\n",
      "Epoch 25:  99%|▉| 72/73 [00:02<00:00, 29.03it/s, loss=0.57, v_num=a10k, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 73/73 [00:02<00:00, 29.11it/s, loss=0.57, v_num=a10k, LTC_val_\u001b[A\n",
      "Epoch 26:  99%|▉| 72/73 [00:02<00:00, 27.21it/s, loss=0.571, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 73/73 [00:02<00:00, 27.28it/s, loss=0.571, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 27:  99%|▉| 72/73 [00:02<00:00, 27.52it/s, loss=0.608, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 73/73 [00:02<00:00, 27.31it/s, loss=0.608, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 28:  99%|▉| 72/73 [00:02<00:00, 29.52it/s, loss=0.533, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 73/73 [00:02<00:00, 29.52it/s, loss=0.533, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 29:  99%|▉| 72/73 [00:02<00:00, 29.88it/s, loss=0.567, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 73/73 [00:02<00:00, 29.88it/s, loss=0.567, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 30:  99%|▉| 72/73 [00:02<00:00, 31.82it/s, loss=0.573, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.453\n",
      "Epoch 30: 100%|█| 73/73 [00:02<00:00, 31.83it/s, loss=0.573, v_num=a10k, LTC_val\n",
      "Epoch 31:  99%|▉| 72/73 [00:02<00:00, 25.17it/s, loss=0.561, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 73/73 [00:02<00:00, 25.11it/s, loss=0.561, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 32:  99%|▉| 72/73 [00:02<00:00, 29.01it/s, loss=0.589, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 73/73 [00:02<00:00, 29.08it/s, loss=0.589, v_num=a10k, LTC_val\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.449\n",
      "Epoch 33:  99%|▉| 72/73 [00:02<00:00, 25.09it/s, loss=0.573, v_num=a10k, LTC_val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.047 >= min_delta = 0.003. New best score: 0.402\n",
      "Epoch 33: 100%|█| 73/73 [00:02<00:00, 25.19it/s, loss=0.573, v_num=a10k, LTC_val\n",
      "Epoch 34:  99%|▉| 72/73 [00:02<00:00, 24.28it/s, loss=0.583, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 73/73 [00:02<00:00, 24.38it/s, loss=0.583, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 35:  99%|▉| 72/73 [00:02<00:00, 30.01it/s, loss=0.553, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 73/73 [00:02<00:00, 30.05it/s, loss=0.553, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 36:  99%|▉| 72/73 [00:02<00:00, 25.25it/s, loss=0.522, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 73/73 [00:02<00:00, 25.35it/s, loss=0.522, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 37:  99%|▉| 72/73 [00:02<00:00, 29.40it/s, loss=0.587, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 73/73 [00:02<00:00, 29.41it/s, loss=0.587, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 38:  99%|▉| 72/73 [00:02<00:00, 25.84it/s, loss=0.536, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 73/73 [00:02<00:00, 25.81it/s, loss=0.536, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 39:  99%|▉| 72/73 [00:02<00:00, 29.53it/s, loss=0.533, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 73/73 [00:02<00:00, 29.61it/s, loss=0.533, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 40:  99%|▉| 72/73 [00:02<00:00, 28.32it/s, loss=0.569, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 73/73 [00:02<00:00, 28.38it/s, loss=0.569, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 41:  99%|▉| 72/73 [00:02<00:00, 26.95it/s, loss=0.54, v_num=a10k, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 73/73 [00:02<00:00, 27.01it/s, loss=0.54, v_num=a10k, LTC_val_\u001b[A\n",
      "Epoch 42:  99%|▉| 72/73 [00:02<00:00, 27.23it/s, loss=0.518, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 73/73 [00:02<00:00, 27.29it/s, loss=0.518, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 43:  99%|▉| 72/73 [00:02<00:00, 32.35it/s, loss=0.517, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 73/73 [00:02<00:00, 32.36it/s, loss=0.517, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 44:  99%|▉| 72/73 [00:02<00:00, 26.55it/s, loss=0.53, v_num=a10k, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 73/73 [00:02<00:00, 26.61it/s, loss=0.53, v_num=a10k, LTC_val_\u001b[A\n",
      "Epoch 45:  99%|▉| 72/73 [00:02<00:00, 25.47it/s, loss=0.548, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 73/73 [00:02<00:00, 25.55it/s, loss=0.548, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 46:  99%|▉| 72/73 [00:02<00:00, 31.61it/s, loss=0.571, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 73/73 [00:02<00:00, 31.64it/s, loss=0.571, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 47:  99%|▉| 72/73 [00:02<00:00, 34.36it/s, loss=0.546, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 73/73 [00:02<00:00, 34.36it/s, loss=0.546, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 48:  99%|▉| 72/73 [00:02<00:00, 34.08it/s, loss=0.527, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 73/73 [00:02<00:00, 34.05it/s, loss=0.527, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 49:  99%|▉| 72/73 [00:02<00:00, 33.58it/s, loss=0.548, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 73/73 [00:02<00:00, 33.47it/s, loss=0.548, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 50:  99%|▉| 72/73 [00:02<00:00, 33.85it/s, loss=0.585, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 73/73 [00:02<00:00, 33.79it/s, loss=0.585, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 51:  99%|▉| 72/73 [00:02<00:00, 31.82it/s, loss=0.578, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 73/73 [00:02<00:00, 31.79it/s, loss=0.578, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 52:  99%|▉| 72/73 [00:02<00:00, 34.21it/s, loss=0.548, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 73/73 [00:02<00:00, 34.23it/s, loss=0.548, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 53:  99%|▉| 72/73 [00:02<00:00, 33.67it/s, loss=0.524, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 73/73 [00:02<00:00, 33.63it/s, loss=0.524, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 54:  99%|▉| 72/73 [00:02<00:00, 34.44it/s, loss=0.553, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 73/73 [00:02<00:00, 34.41it/s, loss=0.553, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 55:  99%|▉| 72/73 [00:02<00:00, 34.40it/s, loss=0.56, v_num=a10k, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 73/73 [00:02<00:00, 34.35it/s, loss=0.56, v_num=a10k, LTC_val_\u001b[A\n",
      "Epoch 56:  99%|▉| 72/73 [00:02<00:00, 34.17it/s, loss=0.587, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 73/73 [00:02<00:00, 34.17it/s, loss=0.587, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 57:  99%|▉| 72/73 [00:02<00:00, 33.66it/s, loss=0.546, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 73/73 [00:02<00:00, 33.64it/s, loss=0.546, v_num=a10k, LTC_val\u001b[A\n",
      "Epoch 58:  99%|▉| 72/73 [00:02<00:00, 29.34it/s, loss=0.553, v_num=a10k, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.402. Signaling Trainer to stop.\n",
      "Epoch 58: 100%|█| 73/73 [00:02<00:00, 29.35it/s, loss=0.553, v_num=a10k, LTC_val\n",
      "Epoch 58: 100%|█| 73/73 [00:02<00:00, 29.32it/s, loss=0.553, v_num=a10k, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 102.53it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.8214285969734192,\n",
      " 'LTC_test_f1': 0.8189676403999329,\n",
      " 'test_loss': 0.4337545931339264}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 149165\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_112623-3pe2a10k/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_112623-3pe2a10k/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.75961\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 58\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4248\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 157\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621499340\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 202\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc 0.72656\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 0.71364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.53866\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.61905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.45414\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.82143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.81897\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.43375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆█▃▄▅▃▅▅▄▅▅▃▄▃▃▄▃▃▄▅▄▄▄▂▆▂▃▃▆▄▃▂▄▃▄▁▄▃█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc ▁▄▅▆▆▇▆▆▆▇▆▇▇▇▇▇▇█▇▇▆███▇█▇███▇▇███▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 ▁▄▅▅▅▇▆▆▆▆▅▇▆▇▇▇▆▇▇▇▆█▇▇▇█▇█▇█▇▇███▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▅▄▄▃▄▃▃▃▃▃▃▂▂▂▂▂▃▂▃▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▃▃▃▃▃▃▃▃▃▃▃▁▃▁▁▁▁▁▃▆▃▃▆▁▆█▆▆▃▃▃▃▃▆▃▆▃▆▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▃▃▃▃▃▃▃▃▃▃▃▁▃▁▁▁▁▁▃▆▃▃▆▁▆█▆▆▃▃▃▃▃▆▃▆▃▆▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▇▇▇▆█▆▅▅▅▇▆▇▅▄▄▃▄▄▂▂▄▂▂▂▁▁▁▃▂▂▃▃▁▂▁▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_multi_head_attention__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3pe2a10k\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 11:29:11.049271: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_multi_head_attention__multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/11wu1sok\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_112909-11wu1sok\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.5 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█| 73/73 [00:02<00:00, 25.32it/s, loss=1.08, v_num=1sok, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.139\n",
      "Epoch 0: 100%|█| 73/73 [00:02<00:00, 25.09it/s, loss=1.08, v_num=1sok, LTC_val_a\n",
      "Epoch 1:  99%|▉| 72/73 [00:02<00:00, 32.63it/s, loss=1.02, v_num=1sok, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 73/73 [00:02<00:00, 32.67it/s, loss=1.02, v_num=1sok, LTC_val_a\u001b[A\n",
      "Epoch 2:  99%|▉| 72/73 [00:02<00:00, 35.38it/s, loss=1.03, v_num=1sok, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 73/73 [00:02<00:00, 34.97it/s, loss=1.03, v_num=1sok, LTC_val_a\u001b[A\n",
      "Epoch 3:  99%|▉| 72/73 [00:03<00:00, 21.90it/s, loss=1.06, v_num=1sok, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 73/73 [00:03<00:00, 22.01it/s, loss=1.06, v_num=1sok, LTC_val_a\u001b[A\n",
      "Epoch 4:  99%|▉| 72/73 [00:02<00:00, 30.30it/s, loss=1.06, v_num=1sok, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 73/73 [00:02<00:00, 30.33it/s, loss=1.06, v_num=1sok, LTC_val_a\u001b[A\n",
      "Epoch 5:  99%|▉| 72/73 [00:03<00:00, 22.19it/s, loss=1.08, v_num=1sok, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 73/73 [00:03<00:00, 22.27it/s, loss=1.08, v_num=1sok, LTC_val_a\u001b[A\n",
      "Epoch 6:  99%|▉| 72/73 [00:02<00:00, 35.41it/s, loss=1.05, v_num=1sok, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 73/73 [00:02<00:00, 35.26it/s, loss=1.05, v_num=1sok, LTC_val_a\u001b[A\n",
      "Epoch 7:  99%|▉| 72/73 [00:02<00:00, 29.21it/s, loss=1.05, v_num=1sok, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 73/73 [00:02<00:00, 29.26it/s, loss=1.05, v_num=1sok, LTC_val_a\u001b[A\n",
      "Epoch 8:  99%|▉| 72/73 [00:02<00:00, 34.63it/s, loss=1.02, v_num=1sok, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 73/73 [00:02<00:00, 34.56it/s, loss=1.02, v_num=1sok, LTC_val_a\u001b[A\n",
      "Epoch 9:  99%|▉| 72/73 [00:02<00:00, 34.40it/s, loss=1, v_num=1sok, LTC_val_acc=\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 73/73 [00:02<00:00, 34.37it/s, loss=1, v_num=1sok, LTC_val_acc=\u001b[A\n",
      "Epoch 10:  99%|▉| 72/73 [00:02<00:00, 34.39it/s, loss=1.05, v_num=1sok, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 73/73 [00:02<00:00, 34.40it/s, loss=1.05, v_num=1sok, LTC_val_\u001b[A\n",
      "Epoch 11:  99%|▉| 72/73 [00:02<00:00, 28.34it/s, loss=1.06, v_num=1sok, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 73/73 [00:02<00:00, 28.38it/s, loss=1.06, v_num=1sok, LTC_val_\u001b[A\n",
      "Epoch 12:  99%|▉| 72/73 [00:03<00:00, 20.41it/s, loss=1.02, v_num=1sok, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 73/73 [00:03<00:00, 20.48it/s, loss=1.02, v_num=1sok, LTC_val_\u001b[A\n",
      "Epoch 13:  99%|▉| 72/73 [00:02<00:00, 32.56it/s, loss=1.03, v_num=1sok, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 73/73 [00:02<00:00, 32.56it/s, loss=1.03, v_num=1sok, LTC_val_\u001b[A\n",
      "Epoch 14:  99%|▉| 72/73 [00:02<00:00, 30.78it/s, loss=0.997, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 73/73 [00:02<00:00, 30.83it/s, loss=0.997, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 15:  99%|▉| 72/73 [00:02<00:00, 32.90it/s, loss=1, v_num=1sok, LTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 73/73 [00:02<00:00, 32.93it/s, loss=1, v_num=1sok, LTC_val_acc\u001b[A\n",
      "Epoch 16:  99%|▉| 72/73 [00:02<00:00, 31.87it/s, loss=0.981, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 73/73 [00:02<00:00, 31.92it/s, loss=0.981, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 17:  99%|▉| 72/73 [00:02<00:00, 33.12it/s, loss=0.973, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.057 >= min_delta = 0.003. New best score: 1.082\n",
      "Epoch 17: 100%|█| 73/73 [00:02<00:00, 33.15it/s, loss=0.973, v_num=1sok, LTC_val\n",
      "Epoch 18:  99%|▉| 72/73 [00:02<00:00, 29.31it/s, loss=1.01, v_num=1sok, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 73/73 [00:02<00:00, 29.37it/s, loss=1.01, v_num=1sok, LTC_val_\u001b[A\n",
      "Epoch 19:  99%|▉| 72/73 [00:02<00:00, 29.65it/s, loss=1, v_num=1sok, LTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 73/73 [00:02<00:00, 29.69it/s, loss=1, v_num=1sok, LTC_val_acc\u001b[A\n",
      "Epoch 20:  99%|▉| 72/73 [00:02<00:00, 25.61it/s, loss=0.975, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 73/73 [00:02<00:00, 25.65it/s, loss=0.975, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 21:  99%|▉| 72/73 [00:02<00:00, 27.35it/s, loss=1.02, v_num=1sok, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 73/73 [00:02<00:00, 27.39it/s, loss=1.02, v_num=1sok, LTC_val_\u001b[A\n",
      "Epoch 22:  99%|▉| 72/73 [00:02<00:00, 30.45it/s, loss=0.965, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 73/73 [00:02<00:00, 30.34it/s, loss=0.965, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 23:  99%|▉| 72/73 [00:02<00:00, 28.89it/s, loss=0.977, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 73/73 [00:02<00:00, 28.95it/s, loss=0.977, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 24:  99%|▉| 72/73 [00:02<00:00, 27.92it/s, loss=0.945, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 73/73 [00:02<00:00, 27.97it/s, loss=0.945, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 25:  99%|▉| 72/73 [00:02<00:00, 32.04it/s, loss=0.965, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 73/73 [00:02<00:00, 32.06it/s, loss=0.965, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 26:  99%|▉| 72/73 [00:02<00:00, 31.34it/s, loss=0.956, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 73/73 [00:02<00:00, 31.38it/s, loss=0.956, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 27:  99%|▉| 72/73 [00:02<00:00, 27.96it/s, loss=0.96, v_num=1sok, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 73/73 [00:02<00:00, 28.02it/s, loss=0.96, v_num=1sok, LTC_val_\u001b[A\n",
      "Epoch 28:  99%|▉| 72/73 [00:02<00:00, 31.87it/s, loss=0.951, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 73/73 [00:02<00:00, 31.61it/s, loss=0.951, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 29:  99%|▉| 72/73 [00:02<00:00, 28.57it/s, loss=0.952, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 73/73 [00:02<00:00, 28.51it/s, loss=0.952, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 30:  99%|▉| 72/73 [00:03<00:00, 19.36it/s, loss=0.955, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 73/73 [00:03<00:00, 18.94it/s, loss=0.955, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 31:  99%|▉| 72/73 [00:04<00:00, 15.28it/s, loss=0.985, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 73/73 [00:04<00:00, 15.39it/s, loss=0.985, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 32:  99%|▉| 72/73 [00:04<00:00, 17.82it/s, loss=0.978, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 73/73 [00:04<00:00, 17.92it/s, loss=0.978, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 33:  99%|▉| 72/73 [00:03<00:00, 20.68it/s, loss=0.976, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 73/73 [00:03<00:00, 20.78it/s, loss=0.976, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 34:  99%|▉| 72/73 [00:02<00:00, 25.27it/s, loss=0.961, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 73/73 [00:02<00:00, 25.35it/s, loss=0.961, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 35:  99%|▉| 72/73 [00:02<00:00, 28.89it/s, loss=0.952, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 73/73 [00:02<00:00, 28.94it/s, loss=0.952, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 36:  99%|▉| 72/73 [00:02<00:00, 32.76it/s, loss=0.987, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 73/73 [00:02<00:00, 32.76it/s, loss=0.987, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 37:  99%|▉| 72/73 [00:03<00:00, 23.14it/s, loss=0.978, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 73/73 [00:03<00:00, 23.22it/s, loss=0.978, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 38:  99%|▉| 72/73 [00:03<00:00, 22.82it/s, loss=0.933, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 73/73 [00:03<00:00, 22.82it/s, loss=0.933, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 39:  99%|▉| 72/73 [00:02<00:00, 31.60it/s, loss=0.952, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 73/73 [00:02<00:00, 31.64it/s, loss=0.952, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 40:  99%|▉| 72/73 [00:03<00:00, 23.25it/s, loss=0.973, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 73/73 [00:03<00:00, 23.24it/s, loss=0.973, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 41:  99%|▉| 72/73 [00:02<00:00, 26.88it/s, loss=0.959, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 73/73 [00:02<00:00, 26.94it/s, loss=0.959, v_num=1sok, LTC_val\u001b[A\n",
      "Epoch 42:  99%|▉| 72/73 [00:02<00:00, 35.02it/s, loss=0.943, v_num=1sok, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 1.082. Signaling Trainer to stop.\n",
      "Epoch 42: 100%|█| 73/73 [00:02<00:00, 35.01it/s, loss=0.943, v_num=1sok, LTC_val\n",
      "Epoch 42: 100%|█| 73/73 [00:02<00:00, 34.97it/s, loss=0.943, v_num=1sok, LTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 95.70it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.4642857015132904,\n",
      " 'LTC_test_f1': 0.21118013560771942,\n",
      " 'test_loss': 0.9001368284225464}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 149567\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_112909-11wu1sok/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_112909-11wu1sok/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.86497\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3096\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 124\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621499473\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 147\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc 0.48524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 0.28087\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.94466\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.08333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.34109\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.46429\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.21118\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.90014\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▄▇▂▆█▄▃▅▅▅▃▄▃▅▆▁▂▂▄▃▁▆▃▃▃▃▇▃▄▂▂▁▃▂▂▂▄▄▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc ▁▄▅▅▆▆▆▇▇▆▇▆▇▅▅▅▆▆▇▇▅▆▄▆▅▅▆▇▇▅▆▇▇▆█▆▇▄▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 ▅▄▃▂▂▂▂▂▂▁▁▁▂▄▃▇▅▃▅▄█▅▅▅▂▄▃▄▆▆▄▄▅▃▄▄▃▂▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▆▆▆▆▆▆▆▅▆▅▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ███████████████▅████▁▁██████▅██████████▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▁▁▇▇▇▇▇▇▅▇▇█▇▇▇▇▇▇▇▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▂▄▄▃▃▃▄▃▃▄▃▃▃▃▃▇▁▅▇▆▇▆▆▇▆▇▇▇▇▇▇▆▆▇▅▇█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_multi_head_attention__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/11wu1sok\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 11:31:24.956075: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msingle_task_LTC_multi_head_attention__multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3i0nkg76\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_113123-3i0nkg76\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.5 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|█| 73/73 [00:02<00:00, 30.23it/s, loss=1.09, v_num=kg76, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 73/73 [00:02<00:00, 29.99it/s, loss=1.09, v_num=kg76, LTC_val_a\u001b[A\n",
      "                                                                                Metric val_loss improved. New best score: 1.189\n",
      "Epoch 1:  99%|▉| 72/73 [00:01<00:00, 36.24it/s, loss=1.07, v_num=kg76, LTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 73/73 [00:02<00:00, 36.27it/s, loss=1.07, v_num=kg76, LTC_val_a\u001b[A\n",
      "Epoch 2:  99%|▉| 72/73 [00:02<00:00, 34.62it/s, loss=1.03, v_num=kg76, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 73/73 [00:02<00:00, 34.62it/s, loss=1.03, v_num=kg76, LTC_val_a\u001b[A\n",
      "Epoch 3:  99%|▉| 72/73 [00:02<00:00, 27.88it/s, loss=1.03, v_num=kg76, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 1.179\n",
      "Epoch 3: 100%|█| 73/73 [00:02<00:00, 27.95it/s, loss=1.03, v_num=kg76, LTC_val_a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  99%|▉| 72/73 [00:02<00:00, 28.45it/s, loss=1.05, v_num=kg76, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 73/73 [00:02<00:00, 28.51it/s, loss=1.05, v_num=kg76, LTC_val_a\u001b[A\n",
      "Epoch 5:  99%|▉| 72/73 [00:02<00:00, 32.06it/s, loss=1.05, v_num=kg76, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 73/73 [00:02<00:00, 32.06it/s, loss=1.05, v_num=kg76, LTC_val_a\u001b[A\n",
      "Epoch 6:  99%|▉| 72/73 [00:03<00:00, 21.59it/s, loss=1.04, v_num=kg76, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.015 >= min_delta = 0.003. New best score: 1.164\n",
      "Epoch 6: 100%|█| 73/73 [00:03<00:00, 21.69it/s, loss=1.04, v_num=kg76, LTC_val_a\n",
      "Epoch 7:  99%|▉| 72/73 [00:02<00:00, 29.17it/s, loss=1.06, v_num=kg76, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 73/73 [00:02<00:00, 29.22it/s, loss=1.06, v_num=kg76, LTC_val_a\u001b[A\n",
      "Epoch 8:  99%|▉| 72/73 [00:02<00:00, 27.21it/s, loss=1.05, v_num=kg76, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 73/73 [00:02<00:00, 27.29it/s, loss=1.05, v_num=kg76, LTC_val_a\u001b[A\n",
      "Epoch 9:  99%|▉| 72/73 [00:02<00:00, 27.53it/s, loss=1.03, v_num=kg76, LTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 73/73 [00:02<00:00, 27.58it/s, loss=1.03, v_num=kg76, LTC_val_a\u001b[A\n",
      "Epoch 10:  99%|▉| 72/73 [00:02<00:00, 26.96it/s, loss=1.05, v_num=kg76, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 73/73 [00:02<00:00, 26.97it/s, loss=1.05, v_num=kg76, LTC_val_\u001b[A\n",
      "Epoch 11:  99%|▉| 72/73 [00:02<00:00, 32.12it/s, loss=1.03, v_num=kg76, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 73/73 [00:02<00:00, 32.14it/s, loss=1.03, v_num=kg76, LTC_val_\u001b[A\n",
      "Epoch 12:  99%|▉| 72/73 [00:02<00:00, 34.82it/s, loss=1.01, v_num=kg76, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 73/73 [00:02<00:00, 34.81it/s, loss=1.01, v_num=kg76, LTC_val_\u001b[A\n",
      "Epoch 13:  99%|▉| 72/73 [00:02<00:00, 33.85it/s, loss=0.973, v_num=kg76, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 73/73 [00:02<00:00, 33.85it/s, loss=0.973, v_num=kg76, LTC_val\u001b[A\n",
      "Epoch 14:  99%|▉| 72/73 [00:02<00:00, 33.36it/s, loss=0.991, v_num=kg76, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.028 >= min_delta = 0.003. New best score: 1.135\n",
      "Epoch 14: 100%|█| 73/73 [00:02<00:00, 33.34it/s, loss=0.991, v_num=kg76, LTC_val\n",
      "Epoch 15:  99%|▉| 72/73 [00:02<00:00, 34.76it/s, loss=0.967, v_num=kg76, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 73/73 [00:02<00:00, 34.71it/s, loss=0.967, v_num=kg76, LTC_val\u001b[A\n",
      "Epoch 16:  99%|▉| 72/73 [00:02<00:00, 35.14it/s, loss=1.02, v_num=kg76, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 73/73 [00:02<00:00, 35.11it/s, loss=1.02, v_num=kg76, LTC_val_\u001b[A\n",
      "Epoch 17:  99%|▉| 72/73 [00:02<00:00, 33.47it/s, loss=0.988, v_num=kg76, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 73/73 [00:02<00:00, 33.29it/s, loss=0.988, v_num=kg76, LTC_val\u001b[A\n",
      "Epoch 18:  99%|▉| 72/73 [00:02<00:00, 32.65it/s, loss=1, v_num=kg76, LTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 73/73 [00:02<00:00, 32.62it/s, loss=1, v_num=kg76, LTC_val_acc\u001b[A\n",
      "Epoch 19:  99%|▉| 72/73 [00:03<00:00, 19.92it/s, loss=0.948, v_num=kg76, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 73/73 [00:03<00:00, 20.03it/s, loss=0.948, v_num=kg76, LTC_val\u001b[A\n",
      "Epoch 20:  99%|▉| 72/73 [00:02<00:00, 27.79it/s, loss=1.01, v_num=kg76, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 73/73 [00:02<00:00, 27.84it/s, loss=1.01, v_num=kg76, LTC_val_\u001b[A\n",
      "Epoch 21:  99%|▉| 72/73 [00:02<00:00, 31.30it/s, loss=0.969, v_num=kg76, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 73/73 [00:02<00:00, 31.22it/s, loss=0.969, v_num=kg76, LTC_val\u001b[A\n",
      "Epoch 22:  99%|▉| 72/73 [00:02<00:00, 30.49it/s, loss=0.935, v_num=kg76, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 73/73 [00:02<00:00, 30.54it/s, loss=0.935, v_num=kg76, LTC_val\u001b[A\n",
      "Epoch 23:  99%|▉| 72/73 [00:02<00:00, 31.77it/s, loss=0.96, v_num=kg76, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 73/73 [00:02<00:00, 31.73it/s, loss=0.96, v_num=kg76, LTC_val_\u001b[A\n",
      "Epoch 24:  99%|▉| 72/73 [00:03<00:00, 22.88it/s, loss=0.952, v_num=kg76, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 73/73 [00:03<00:00, 22.97it/s, loss=0.952, v_num=kg76, LTC_val\u001b[A\n",
      "Epoch 25:  99%|▉| 72/73 [00:02<00:00, 26.26it/s, loss=0.994, v_num=kg76, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 73/73 [00:02<00:00, 26.34it/s, loss=0.994, v_num=kg76, LTC_val\u001b[A\n",
      "Epoch 26:  99%|▉| 72/73 [00:02<00:00, 30.45it/s, loss=0.951, v_num=kg76, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 73/73 [00:02<00:00, 30.50it/s, loss=0.951, v_num=kg76, LTC_val\u001b[A\n",
      "Epoch 27:  99%|▉| 72/73 [00:02<00:00, 34.38it/s, loss=1, v_num=kg76, LTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 73/73 [00:02<00:00, 34.30it/s, loss=1, v_num=kg76, LTC_val_acc\u001b[A\n",
      "Epoch 28:  99%|▉| 72/73 [00:02<00:00, 33.01it/s, loss=0.964, v_num=kg76, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 73/73 [00:02<00:00, 32.94it/s, loss=0.964, v_num=kg76, LTC_val\u001b[A\n",
      "Epoch 29:  99%|▉| 72/73 [00:02<00:00, 33.31it/s, loss=0.983, v_num=kg76, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 73/73 [00:02<00:00, 33.21it/s, loss=0.983, v_num=kg76, LTC_val\u001b[A\n",
      "Epoch 30:  99%|▉| 72/73 [00:02<00:00, 32.70it/s, loss=0.925, v_num=kg76, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 73/73 [00:02<00:00, 32.44it/s, loss=0.925, v_num=kg76, LTC_val\u001b[A\n",
      "Epoch 31:  99%|▉| 72/73 [00:02<00:00, 32.96it/s, loss=0.975, v_num=kg76, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 73/73 [00:02<00:00, 32.95it/s, loss=0.975, v_num=kg76, LTC_val\u001b[A\n",
      "Epoch 32:  99%|▉| 72/73 [00:02<00:00, 32.00it/s, loss=0.982, v_num=kg76, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 73/73 [00:02<00:00, 32.01it/s, loss=0.982, v_num=kg76, LTC_val\u001b[A\n",
      "Epoch 33:  99%|▉| 72/73 [00:02<00:00, 31.77it/s, loss=0.982, v_num=kg76, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 73/73 [00:02<00:00, 31.78it/s, loss=0.982, v_num=kg76, LTC_val\u001b[A\n",
      "Epoch 34:  99%|▉| 72/73 [00:02<00:00, 30.91it/s, loss=0.994, v_num=kg76, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 73/73 [00:02<00:00, 30.85it/s, loss=0.994, v_num=kg76, LTC_val\u001b[A\n",
      "Epoch 35:  99%|▉| 72/73 [00:02<00:00, 32.75it/s, loss=0.96, v_num=kg76, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 73/73 [00:02<00:00, 32.72it/s, loss=0.96, v_num=kg76, LTC_val_\u001b[A\n",
      "Epoch 36:  99%|▉| 72/73 [00:02<00:00, 34.35it/s, loss=0.933, v_num=kg76, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 73/73 [00:02<00:00, 34.32it/s, loss=0.933, v_num=kg76, LTC_val\u001b[A\n",
      "Epoch 37:  99%|▉| 72/73 [00:02<00:00, 33.41it/s, loss=0.967, v_num=kg76, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 73/73 [00:02<00:00, 33.42it/s, loss=0.967, v_num=kg76, LTC_val\u001b[A\n",
      "Epoch 38:  99%|▉| 72/73 [00:02<00:00, 34.40it/s, loss=0.979, v_num=kg76, LTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 73/73 [00:02<00:00, 34.37it/s, loss=0.979, v_num=kg76, LTC_val\u001b[A\n",
      "Epoch 39:  99%|▉| 72/73 [00:02<00:00, 32.77it/s, loss=1.01, v_num=kg76, LTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 1.135. Signaling Trainer to stop.\n",
      "Epoch 39: 100%|█| 73/73 [00:02<00:00, 32.77it/s, loss=1.01, v_num=kg76, LTC_val_\n",
      "Epoch 39: 100%|█| 73/73 [00:02<00:00, 32.72it/s, loss=1.01, v_num=kg76, LTC_val_\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|███████████████████████████████████| 2/2 [00:00<00:00, 103.82it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'LTC_test_acc': 0.4642857015132904,\n",
      " 'LTC_test_f1': 0.21118013560771942,\n",
      " 'test_loss': 0.9317809343338013}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 150348\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_113123-3i0nkg76/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_113123-3i0nkg76/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.87113\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 39\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2880\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621499588\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 137\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc 0.48524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 0.27466\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.95518\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.08333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.47615\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.46429\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.21118\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.93178\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▇▇▆▅▅▅▅▆█▆▆▆▇▂▃▅▄█▃▁▂▅▆▅▃▅█▂▃▅▄▅▃▃▃▁▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc ▁▄▅▅▅▆▇▆▇▇▇▇▆▆▅▆▆▅▅▆▆▆▅▅▆█▆▆▇▇▆▇▅▇▇▆▇▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 ▅▄▃▄▃▃▂▁▁▂▂▁▁▃▇▄▆▄▄▅▄▇▆▅▂█▃▇▆▆▇▆▅▄▃▄▄▇▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▆▅▆▆▆▅▅▅▅▅▅▄▄▃▃▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc █████████████████▃██████▆█▃▃▃▃▆▃██▃█▁▃█▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▄▇▇▇▇█▇▆▇▄▄▄▄▆▄▇▇▄▇▁▄▇▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▂▃▂▂▂▂▂▂▂▂▂▂▂▃▁▇▆▇███▃▅▃▄▃▇▅▄▃▅▅▄▅▇▅▅▇▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msingle_task_LTC_multi_head_attention__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3i0nkg76\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 11:33:19.963231: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_multi_head_attention_loss_weighted_binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2t4o27ce\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_113318-2t4o27ce\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 79/80 [00:04<00:00, 17.44it/s, loss=0.602, v_num=27ce, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.460\n",
      "Epoch 0: 100%|█| 80/80 [00:04<00:00, 17.40it/s, loss=0.602, v_num=27ce, BTC_val_\n",
      "Epoch 1:  99%|▉| 79/80 [00:04<00:00, 18.77it/s, loss=0.579, v_num=27ce, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 80/80 [00:04<00:00, 18.79it/s, loss=0.579, v_num=27ce, BTC_val_\u001b[A\n",
      "Epoch 2:  99%|▉| 79/80 [00:04<00:00, 17.73it/s, loss=0.648, v_num=27ce, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:04<00:00, 17.74it/s, loss=0.648, v_num=27ce, BTC_val_\u001b[A\n",
      "Epoch 3:  99%|▉| 79/80 [00:04<00:00, 17.68it/s, loss=0.627, v_num=27ce, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:04<00:00, 17.72it/s, loss=0.627, v_num=27ce, BTC_val_\u001b[A\n",
      "Epoch 4:  99%|▉| 79/80 [00:03<00:00, 19.81it/s, loss=0.583, v_num=27ce, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:04<00:00, 19.81it/s, loss=0.583, v_num=27ce, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|▉| 79/80 [00:03<00:00, 20.71it/s, loss=0.594, v_num=27ce, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:03<00:00, 20.72it/s, loss=0.594, v_num=27ce, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|▉| 79/80 [00:04<00:00, 17.31it/s, loss=0.577, v_num=27ce, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:04<00:00, 17.23it/s, loss=0.577, v_num=27ce, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|▉| 79/80 [00:05<00:00, 15.15it/s, loss=0.573, v_num=27ce, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:05<00:00, 15.20it/s, loss=0.573, v_num=27ce, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|▉| 79/80 [00:05<00:00, 13.88it/s, loss=0.582, v_num=27ce, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:05<00:00, 13.94it/s, loss=0.582, v_num=27ce, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|▉| 79/80 [00:05<00:00, 15.69it/s, loss=0.6, v_num=27ce, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:05<00:00, 15.70it/s, loss=0.6, v_num=27ce, BTC_val_ac\u001b[A\n",
      "Epoch 10:  99%|▉| 79/80 [00:05<00:00, 15.18it/s, loss=0.592, v_num=27ce, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:05<00:00, 15.23it/s, loss=0.592, v_num=27ce, BTC_val\u001b[A\n",
      "Epoch 11:  99%|▉| 79/80 [00:04<00:00, 18.33it/s, loss=0.607, v_num=27ce, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:04<00:00, 18.37it/s, loss=0.607, v_num=27ce, BTC_val\u001b[A\n",
      "Epoch 12:  99%|▉| 79/80 [00:04<00:00, 19.74it/s, loss=0.59, v_num=27ce, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:04<00:00, 19.75it/s, loss=0.59, v_num=27ce, BTC_val_\u001b[A\n",
      "Epoch 13:  99%|▉| 79/80 [00:04<00:00, 17.85it/s, loss=0.624, v_num=27ce, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:04<00:00, 17.87it/s, loss=0.624, v_num=27ce, BTC_val\u001b[A\n",
      "Epoch 14:  99%|▉| 79/80 [00:04<00:00, 18.99it/s, loss=0.565, v_num=27ce, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:04<00:00, 19.01it/s, loss=0.565, v_num=27ce, BTC_val\u001b[A\n",
      "Epoch 15:  99%|▉| 79/80 [00:04<00:00, 18.98it/s, loss=0.591, v_num=27ce, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:04<00:00, 19.02it/s, loss=0.591, v_num=27ce, BTC_val\u001b[A\n",
      "Epoch 16:  99%|▉| 79/80 [00:04<00:00, 18.82it/s, loss=0.571, v_num=27ce, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:04<00:00, 18.85it/s, loss=0.571, v_num=27ce, BTC_val\u001b[A\n",
      "Epoch 17:  99%|▉| 79/80 [00:04<00:00, 18.82it/s, loss=0.571, v_num=27ce, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:04<00:00, 18.85it/s, loss=0.571, v_num=27ce, BTC_val\u001b[A\n",
      "Epoch 18:  99%|▉| 79/80 [00:04<00:00, 18.98it/s, loss=0.596, v_num=27ce, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:04<00:00, 19.01it/s, loss=0.596, v_num=27ce, BTC_val\u001b[A\n",
      "Epoch 19:  99%|▉| 79/80 [00:04<00:00, 18.63it/s, loss=0.628, v_num=27ce, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:04<00:00, 18.63it/s, loss=0.628, v_num=27ce, BTC_val\u001b[A\n",
      "Epoch 20:  99%|▉| 79/80 [00:05<00:00, 14.72it/s, loss=0.597, v_num=27ce, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:05<00:00, 14.77it/s, loss=0.597, v_num=27ce, BTC_val\u001b[A\n",
      "Epoch 21:  99%|▉| 79/80 [00:04<00:00, 18.28it/s, loss=0.572, v_num=27ce, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:04<00:00, 18.32it/s, loss=0.572, v_num=27ce, BTC_val\u001b[A\n",
      "Epoch 22:  99%|▉| 79/80 [00:03<00:00, 19.93it/s, loss=0.593, v_num=27ce, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:04<00:00, 19.94it/s, loss=0.593, v_num=27ce, BTC_val\u001b[A\n",
      "Epoch 23:  99%|▉| 79/80 [00:05<00:00, 15.21it/s, loss=0.561, v_num=27ce, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:05<00:00, 15.28it/s, loss=0.561, v_num=27ce, BTC_val\u001b[A\n",
      "Epoch 24:  99%|▉| 79/80 [00:05<00:00, 14.98it/s, loss=0.587, v_num=27ce, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:05<00:00, 15.00it/s, loss=0.587, v_num=27ce, BTC_val\u001b[A\n",
      "Epoch 25:  99%|▉| 79/80 [00:04<00:00, 19.01it/s, loss=0.571, v_num=27ce, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.460. Signaling Trainer to stop.\n",
      "Epoch 25: 100%|█| 80/80 [00:04<00:00, 19.04it/s, loss=0.571, v_num=27ce, BTC_val\n",
      "Epoch 25: 100%|█| 80/80 [00:04<00:00, 19.02it/s, loss=0.571, v_num=27ce, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 54.04it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6451612710952759,\n",
      " 'BTC_test_f1': 0.6410742998123169,\n",
      " 'ETH_test_acc': 0.5483871102333069,\n",
      " 'ETH_test_f1': 0.5434267520904541,\n",
      " 'test_loss': 0.6840435862541199}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 150666\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_113318-2t4o27ce/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_113318-2t4o27ce/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.61527\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2054\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621499726\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 93\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.71259\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.69492\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.71734\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.70565\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.57197\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.77778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.64935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.51998\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.64516\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.64107\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.54839\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.54343\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.68404\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆█▆▇▄▆▅▆▄▅▆▄█▇▅▅▅▆▆█▃▇▇▄▅▆▆▇▃▅▃▃▅▄▃█▄▁▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▁▅▆▅▆▇▇▆▇▇▇▇▇▆▆▇▇▇▇█▇█▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▁▅▆▆▆▇▇▆▆▇▇▇▇▇▆▇▇▇██▇███▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▆▅▆▆▆▆▆▆▇▆▆▇▇▆▇▇▇██▇██▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▁▆▅▆▆▆▆▆▆▆▆▆▆▇▆▇▇▇██▆█▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▅▄▄▃▃▃▃▃▃▂▃▂▃▂▂▁▁▂▂▁▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▃▃▃▃▃▃▃▃▃▁▃▁▃▃█▆▆▃▆▆▆▁▆▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▄▄▄▄▄▄▄▄▄▁▄▁▄▄█▆▆▄▆▆▆▁▆▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▃▆▆▆▆▆█▆█▆█▆█▃▁▁▅▁▅▃▁▆▁▁▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▃▆▆▆▆▆█▆█▆█▆█▃▁▁▅▁▅▃▁▆▁▁▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▄▆▄▄▃▃▄▃▆▄▇▄▇▅▅▄▆▇▆█▅▆▆▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_multi_head_attention_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2t4o27ce\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 11:35:38.595598: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_multi_head_attention_loss_weighted_binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1oqj8a20\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_113536-1oqj8a20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 79/80 [00:03<00:00, 20.27it/s, loss=0.665, v_num=8a20, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.530\n",
      "Epoch 0: 100%|█| 80/80 [00:03<00:00, 20.26it/s, loss=0.665, v_num=8a20, BTC_val_\n",
      "Epoch 1:  99%|▉| 79/80 [00:04<00:00, 17.75it/s, loss=0.636, v_num=8a20, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 0.510\n",
      "Epoch 1: 100%|█| 80/80 [00:04<00:00, 17.79it/s, loss=0.636, v_num=8a20, BTC_val_\n",
      "Epoch 2:  99%|▉| 79/80 [00:04<00:00, 17.07it/s, loss=0.628, v_num=8a20, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:04<00:00, 17.13it/s, loss=0.628, v_num=8a20, BTC_val_\u001b[A\n",
      "Epoch 3:  99%|▉| 79/80 [00:05<00:00, 13.41it/s, loss=0.614, v_num=8a20, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.022 >= min_delta = 0.003. New best score: 0.488\n",
      "Epoch 3: 100%|█| 80/80 [00:05<00:00, 13.47it/s, loss=0.614, v_num=8a20, BTC_val_\n",
      "Epoch 4:  99%|▉| 79/80 [00:05<00:00, 15.73it/s, loss=0.597, v_num=8a20, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:05<00:00, 15.76it/s, loss=0.597, v_num=8a20, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|▉| 79/80 [00:04<00:00, 17.33it/s, loss=0.586, v_num=8a20, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:04<00:00, 17.38it/s, loss=0.586, v_num=8a20, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|▉| 79/80 [00:04<00:00, 17.08it/s, loss=0.629, v_num=8a20, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:04<00:00, 17.11it/s, loss=0.629, v_num=8a20, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|▉| 79/80 [00:04<00:00, 16.44it/s, loss=0.61, v_num=8a20, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:04<00:00, 16.32it/s, loss=0.61, v_num=8a20, BTC_val_a\u001b[A\n",
      "Epoch 8:  99%|▉| 79/80 [00:04<00:00, 16.07it/s, loss=0.583, v_num=8a20, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:04<00:00, 16.12it/s, loss=0.583, v_num=8a20, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|▉| 79/80 [00:04<00:00, 19.25it/s, loss=0.598, v_num=8a20, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:04<00:00, 19.28it/s, loss=0.598, v_num=8a20, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|▉| 79/80 [00:04<00:00, 18.45it/s, loss=0.58, v_num=8a20, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:04<00:00, 18.48it/s, loss=0.58, v_num=8a20, BTC_val_\u001b[A\n",
      "Epoch 11:  99%|▉| 79/80 [00:04<00:00, 18.75it/s, loss=0.581, v_num=8a20, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:04<00:00, 18.78it/s, loss=0.581, v_num=8a20, BTC_val\u001b[A\n",
      "Epoch 12:  99%|▉| 79/80 [00:04<00:00, 18.79it/s, loss=0.559, v_num=8a20, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:04<00:00, 18.83it/s, loss=0.559, v_num=8a20, BTC_val\u001b[A\n",
      "Epoch 13:  99%|▉| 79/80 [00:05<00:00, 15.10it/s, loss=0.593, v_num=8a20, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:05<00:00, 15.15it/s, loss=0.593, v_num=8a20, BTC_val\u001b[A\n",
      "Epoch 14:  99%|▉| 79/80 [00:04<00:00, 19.02it/s, loss=0.571, v_num=8a20, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:04<00:00, 19.04it/s, loss=0.571, v_num=8a20, BTC_val\u001b[A\n",
      "Epoch 15:  99%|▉| 79/80 [00:04<00:00, 19.18it/s, loss=0.591, v_num=8a20, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:04<00:00, 19.14it/s, loss=0.591, v_num=8a20, BTC_val\u001b[A\n",
      "Epoch 16:  99%|▉| 79/80 [00:05<00:00, 15.05it/s, loss=0.592, v_num=8a20, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:05<00:00, 15.08it/s, loss=0.592, v_num=8a20, BTC_val\u001b[A\n",
      "Epoch 17:  99%|▉| 79/80 [00:05<00:00, 13.88it/s, loss=0.605, v_num=8a20, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:05<00:00, 13.93it/s, loss=0.605, v_num=8a20, BTC_val\u001b[A\n",
      "Epoch 18:  99%|▉| 79/80 [00:05<00:00, 14.27it/s, loss=0.618, v_num=8a20, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:05<00:00, 14.33it/s, loss=0.618, v_num=8a20, BTC_val\u001b[A\n",
      "Epoch 19:  99%|▉| 79/80 [00:04<00:00, 19.67it/s, loss=0.546, v_num=8a20, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:04<00:00, 19.68it/s, loss=0.546, v_num=8a20, BTC_val\u001b[A\n",
      "Epoch 20:  99%|▉| 79/80 [00:04<00:00, 16.57it/s, loss=0.622, v_num=8a20, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:04<00:00, 16.61it/s, loss=0.622, v_num=8a20, BTC_val\u001b[A\n",
      "Epoch 21:  99%|▉| 79/80 [00:04<00:00, 19.11it/s, loss=0.578, v_num=8a20, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:04<00:00, 19.14it/s, loss=0.578, v_num=8a20, BTC_val\u001b[A\n",
      "Epoch 22:  99%|▉| 79/80 [00:04<00:00, 18.66it/s, loss=0.58, v_num=8a20, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:04<00:00, 18.70it/s, loss=0.58, v_num=8a20, BTC_val_\u001b[A\n",
      "Epoch 23:  99%|▉| 79/80 [00:04<00:00, 19.59it/s, loss=0.587, v_num=8a20, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:04<00:00, 19.56it/s, loss=0.587, v_num=8a20, BTC_val\u001b[A\n",
      "Epoch 24:  99%|▉| 79/80 [00:04<00:00, 17.19it/s, loss=0.562, v_num=8a20, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:04<00:00, 17.23it/s, loss=0.562, v_num=8a20, BTC_val\u001b[A\n",
      "Epoch 25:  99%|▉| 79/80 [00:04<00:00, 17.55it/s, loss=0.558, v_num=8a20, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:04<00:00, 17.51it/s, loss=0.558, v_num=8a20, BTC_val\u001b[A\n",
      "Epoch 26:  99%|▉| 79/80 [00:04<00:00, 18.88it/s, loss=0.563, v_num=8a20, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:04<00:00, 18.85it/s, loss=0.563, v_num=8a20, BTC_val\u001b[A\n",
      "Epoch 27:  99%|▉| 79/80 [00:04<00:00, 17.75it/s, loss=0.569, v_num=8a20, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:04<00:00, 17.77it/s, loss=0.569, v_num=8a20, BTC_val\u001b[A\n",
      "Epoch 28:  99%|▉| 79/80 [00:04<00:00, 17.96it/s, loss=0.561, v_num=8a20, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.488. Signaling Trainer to stop.\n",
      "Epoch 28: 100%|█| 80/80 [00:04<00:00, 18.00it/s, loss=0.561, v_num=8a20, BTC_val\n",
      "Epoch 28: 100%|█| 80/80 [00:04<00:00, 17.98it/s, loss=0.561, v_num=8a20, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 53.82it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6774193644523621,\n",
      " 'BTC_test_f1': 0.6693548560142517,\n",
      " 'ETH_test_acc': 0.5806451439857483,\n",
      " 'ETH_test_f1': 0.5788955688476562,\n",
      " 'test_loss': 0.645462155342102}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 151016\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_113536-1oqj8a20/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_113536-1oqj8a20/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.63396\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 28\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2291\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621499880\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.73001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.71648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.71417\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.699\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.56457\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.55075\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.67742\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.66935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.58065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.5789\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.64546\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▅▆▂▂▇▅▆▃▇▄▃▄▄█▄▄▂▄▄▃▅▄▄▃▂█▂▅▅▄▄▄▄▁▃▅▄▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▁▅▇▆▆▆▆▇▇▆▆▇▇▆▇▇▇▇▇█▇█▇▇██▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▁▅▆▆▆▆▆▇▇▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██▇█▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▁▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇███▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▃▃▃▃▃▃▂▃▃▂▂▂▃▂▂▂▂▁▂▂▁▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁█▁█▁██▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▃▃▃▃▃▃▃▃▃▃▃▃▃▁▃▃▃▃█▃▃▃█▃█▄██▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▃▆▆█▆▆▆█▆▆▆▆▆▁▆█▆█▆██▆▃▆▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▃▆▆█▆▆▆█▆▆▆▆▆▁▆█▆█▆██▆▃▆▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▅▃▅▁▃▂▄▅▂▄▂▃▃█▂▅▃▃▄▂▅▄▄▃▅▅▅▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_multi_head_attention_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1oqj8a20\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 11:38:52.807622: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_multi_head_attention_loss_weighted_multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3lium95f\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_113851-3lium95f\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.5 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 79/80 [00:03<00:00, 21.63it/s, loss=1.16, v_num=m95f, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.091\n",
      "Epoch 0: 100%|█| 80/80 [00:03<00:00, 21.66it/s, loss=1.16, v_num=m95f, BTC_val_a\n",
      "Epoch 1:  99%|▉| 79/80 [00:03<00:00, 21.65it/s, loss=1.11, v_num=m95f, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 80/80 [00:03<00:00, 21.65it/s, loss=1.11, v_num=m95f, BTC_val_a\u001b[A\n",
      "Epoch 2:  99%|▉| 79/80 [00:03<00:00, 19.76it/s, loss=1.11, v_num=m95f, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:04<00:00, 19.75it/s, loss=1.11, v_num=m95f, BTC_val_a\u001b[A\n",
      "Epoch 3:  99%|▉| 79/80 [00:04<00:00, 19.36it/s, loss=1.12, v_num=m95f, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 1.087\n",
      "Epoch 3: 100%|█| 80/80 [00:04<00:00, 19.35it/s, loss=1.12, v_num=m95f, BTC_val_a\n",
      "Epoch 4:  99%|▉| 79/80 [00:04<00:00, 17.43it/s, loss=1.11, v_num=m95f, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:04<00:00, 17.42it/s, loss=1.11, v_num=m95f, BTC_val_a\u001b[A\n",
      "Epoch 5:  99%|▉| 79/80 [00:04<00:00, 16.76it/s, loss=1.09, v_num=m95f, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:04<00:00, 16.75it/s, loss=1.09, v_num=m95f, BTC_val_a\u001b[A\n",
      "Epoch 6:  99%|▉| 79/80 [00:04<00:00, 18.12it/s, loss=1.1, v_num=m95f, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:04<00:00, 18.15it/s, loss=1.1, v_num=m95f, BTC_val_ac\u001b[A\n",
      "Epoch 7:  99%|▉| 79/80 [00:04<00:00, 19.15it/s, loss=1.1, v_num=m95f, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:04<00:00, 19.17it/s, loss=1.1, v_num=m95f, BTC_val_ac\u001b[A\n",
      "Epoch 8:  99%|▉| 79/80 [00:04<00:00, 18.86it/s, loss=1.11, v_num=m95f, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:04<00:00, 18.91it/s, loss=1.11, v_num=m95f, BTC_val_a\u001b[A\n",
      "Epoch 9:  99%|▉| 79/80 [00:04<00:00, 17.18it/s, loss=1.1, v_num=m95f, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 1.082\n",
      "Epoch 9: 100%|█| 80/80 [00:04<00:00, 17.22it/s, loss=1.1, v_num=m95f, BTC_val_ac\n",
      "Epoch 10:  99%|▉| 79/80 [00:04<00:00, 15.99it/s, loss=1.1, v_num=m95f, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:04<00:00, 16.01it/s, loss=1.1, v_num=m95f, BTC_val_a\u001b[A\n",
      "Epoch 11:  99%|▉| 79/80 [00:04<00:00, 16.58it/s, loss=1.1, v_num=m95f, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:04<00:00, 16.62it/s, loss=1.1, v_num=m95f, BTC_val_a\u001b[A\n",
      "Epoch 12:  99%|▉| 79/80 [00:04<00:00, 19.35it/s, loss=1.1, v_num=m95f, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:04<00:00, 19.38it/s, loss=1.1, v_num=m95f, BTC_val_a\u001b[A\n",
      "Epoch 13:  99%|▉| 79/80 [00:04<00:00, 17.49it/s, loss=1.09, v_num=m95f, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:04<00:00, 17.52it/s, loss=1.09, v_num=m95f, BTC_val_\u001b[A\n",
      "Epoch 14:  99%|▉| 79/80 [00:03<00:00, 20.11it/s, loss=1.05, v_num=m95f, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:03<00:00, 20.10it/s, loss=1.05, v_num=m95f, BTC_val_\u001b[A\n",
      "Epoch 15:  99%|▉| 79/80 [00:03<00:00, 20.18it/s, loss=1.01, v_num=m95f, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:03<00:00, 20.13it/s, loss=1.01, v_num=m95f, BTC_val_\u001b[A\n",
      "Epoch 16:  99%|▉| 79/80 [00:04<00:00, 17.78it/s, loss=1, v_num=m95f, BTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:04<00:00, 17.82it/s, loss=1, v_num=m95f, BTC_val_acc\u001b[A\n",
      "Epoch 17:  99%|▉| 79/80 [00:04<00:00, 15.98it/s, loss=0.958, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:04<00:00, 16.04it/s, loss=0.958, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 18:  99%|▉| 79/80 [00:04<00:00, 17.20it/s, loss=1.01, v_num=m95f, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:04<00:00, 17.24it/s, loss=1.01, v_num=m95f, BTC_val_\u001b[A\n",
      "Epoch 19:  99%|▉| 79/80 [00:04<00:00, 17.32it/s, loss=0.974, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:04<00:00, 17.37it/s, loss=0.974, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 20:  99%|▉| 79/80 [00:04<00:00, 15.98it/s, loss=0.957, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:04<00:00, 16.03it/s, loss=0.957, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 21:  99%|▉| 79/80 [00:04<00:00, 16.81it/s, loss=0.992, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:04<00:00, 16.84it/s, loss=0.992, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 22:  99%|▉| 79/80 [00:04<00:00, 17.75it/s, loss=0.99, v_num=m95f, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:04<00:00, 17.79it/s, loss=0.99, v_num=m95f, BTC_val_\u001b[A\n",
      "Epoch 23:  99%|▉| 79/80 [00:03<00:00, 19.89it/s, loss=0.963, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:04<00:00, 19.91it/s, loss=0.963, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 24:  99%|▉| 79/80 [00:04<00:00, 18.45it/s, loss=0.991, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.020 >= min_delta = 0.003. New best score: 1.062\n",
      "Epoch 24: 100%|█| 80/80 [00:04<00:00, 18.47it/s, loss=0.991, v_num=m95f, BTC_val\n",
      "Epoch 25:  99%|▉| 79/80 [00:05<00:00, 15.78it/s, loss=0.976, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.061 >= min_delta = 0.003. New best score: 1.001\n",
      "Epoch 25: 100%|█| 80/80 [00:05<00:00, 15.82it/s, loss=0.976, v_num=m95f, BTC_val\n",
      "Epoch 26:  99%|▉| 79/80 [00:04<00:00, 17.94it/s, loss=0.981, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:04<00:00, 17.97it/s, loss=0.981, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 27:  99%|▉| 79/80 [00:03<00:00, 19.84it/s, loss=0.943, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:04<00:00, 19.85it/s, loss=0.943, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 28:  99%|▉| 79/80 [00:04<00:00, 17.77it/s, loss=0.974, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:04<00:00, 17.79it/s, loss=0.974, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 29:  99%|▉| 79/80 [00:05<00:00, 14.28it/s, loss=0.987, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:05<00:00, 14.34it/s, loss=0.987, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 30:  99%|▉| 79/80 [00:04<00:00, 17.62it/s, loss=0.976, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:04<00:00, 17.67it/s, loss=0.976, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 31:  99%|▉| 79/80 [00:05<00:00, 14.85it/s, loss=0.983, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 80/80 [00:05<00:00, 14.89it/s, loss=0.983, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 32:  99%|▉| 79/80 [00:05<00:00, 15.44it/s, loss=0.992, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 80/80 [00:05<00:00, 15.46it/s, loss=0.992, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 33:  99%|▉| 79/80 [00:04<00:00, 18.77it/s, loss=0.943, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 80/80 [00:04<00:00, 18.81it/s, loss=0.943, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 34:  99%|▉| 79/80 [00:04<00:00, 17.34it/s, loss=0.953, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 80/80 [00:04<00:00, 17.37it/s, loss=0.953, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 35:  99%|▉| 79/80 [00:04<00:00, 18.75it/s, loss=0.995, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 80/80 [00:04<00:00, 18.77it/s, loss=0.995, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 36:  99%|▉| 79/80 [00:05<00:00, 14.78it/s, loss=0.981, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 80/80 [00:05<00:00, 14.84it/s, loss=0.981, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 37:  99%|▉| 79/80 [00:06<00:00, 12.64it/s, loss=0.976, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 80/80 [00:06<00:00, 12.71it/s, loss=0.976, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 38:  99%|▉| 79/80 [00:04<00:00, 19.13it/s, loss=0.973, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 80/80 [00:04<00:00, 19.09it/s, loss=0.973, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 39:  99%|▉| 79/80 [00:04<00:00, 19.12it/s, loss=1, v_num=m95f, BTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 80/80 [00:04<00:00, 19.13it/s, loss=1, v_num=m95f, BTC_val_acc\u001b[A\n",
      "Epoch 40:  99%|▉| 79/80 [00:04<00:00, 17.22it/s, loss=0.941, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 80/80 [00:04<00:00, 17.24it/s, loss=0.941, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 41:  99%|▉| 79/80 [00:04<00:00, 18.23it/s, loss=0.985, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 80/80 [00:04<00:00, 18.25it/s, loss=0.985, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 42:  99%|▉| 79/80 [00:05<00:00, 13.47it/s, loss=1.02, v_num=m95f, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 80/80 [00:05<00:00, 13.54it/s, loss=1.02, v_num=m95f, BTC_val_\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43:  99%|▉| 79/80 [00:05<00:00, 13.48it/s, loss=0.939, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 80/80 [00:05<00:00, 13.55it/s, loss=0.939, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 44:  99%|▉| 79/80 [00:05<00:00, 14.79it/s, loss=0.963, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 80/80 [00:05<00:00, 14.84it/s, loss=0.963, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 45:  99%|▉| 79/80 [00:04<00:00, 16.92it/s, loss=0.956, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 80/80 [00:04<00:00, 16.96it/s, loss=0.956, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 46:  99%|▉| 79/80 [00:04<00:00, 16.70it/s, loss=0.959, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 80/80 [00:04<00:00, 16.76it/s, loss=0.959, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 47:  99%|▉| 79/80 [00:05<00:00, 14.43it/s, loss=0.966, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 80/80 [00:05<00:00, 14.41it/s, loss=0.966, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 48:  99%|▉| 79/80 [00:04<00:00, 17.03it/s, loss=0.977, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 80/80 [00:04<00:00, 17.07it/s, loss=0.977, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 49:  99%|▉| 79/80 [00:04<00:00, 16.16it/s, loss=0.959, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 80/80 [00:04<00:00, 16.17it/s, loss=0.959, v_num=m95f, BTC_val\u001b[A\n",
      "Epoch 50:  99%|▉| 79/80 [00:04<00:00, 19.52it/s, loss=0.944, v_num=m95f, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 80/80 [00:04<00:00, 19.55it/s, loss=0.944, v_num=m95f, BTC_valMonitored metric val_loss did not improve in the last 25 records. Best score: 1.001. Signaling Trainer to stop.\n",
      "\n",
      "Epoch 50: 100%|█| 80/80 [00:04<00:00, 19.53it/s, loss=0.944, v_num=m95f, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 51.68it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.3870967626571655,\n",
      " 'BTC_test_f1': 0.31241530179977417,\n",
      " 'ETH_test_acc': 0.5161290168762207,\n",
      " 'ETH_test_f1': 0.47014063596725464,\n",
      " 'test_loss': 0.8825052380561829}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 151489\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_113851-3lium95f/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_113851-3lium95f/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.12734\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4029\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 246\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621500177\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.41251\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.3623\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.42359\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.37538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.95225\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.22222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20635\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.11111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.09524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.21952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.3871\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.31242\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.51613\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.47014\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.88251\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▆▅▅▅▅▅▅▅▅▅▃▃▃▄▅▆▁▂▄▅▅▄█▄▄▄▃▁▄▅▇▃▂▃▆▃▂▃▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▂▁▂▁▁▅▃▄▆▆▄▅▇▅▆▇▇▇▇▇▆█▇▇▆▇▇▇▇▇▆▆▇▆▇██▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▂▁▂▂▂▂▃▂▃▃▂▆█▅▆▇▆▇▆▆▅▇▆▆▅▅▆▆▆▆▄▅▆▅▆▇▆▅▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▂▁▂▂▄▂▃▄▄▄▆▇▆▆█▆▇▆▆▇█▇▇▆█▇▇▇▆▇▇▇█▇█▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▁▂▂▂▃▂▂▁▁▂▂▆▇▅▇█▅▇▅▆▆█▆▆▅▇▆▇▆▅▆▆▆▇▇▇▆▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▆▆▅▆▆▆▆▅▆▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ██████████▁▁▆▁▆▃▆█▃▆▆▆▃▆▆▆▃▆▃▃▃▆▆▃█▃█▃▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▄▄▄▄▄▄▄▄▄▄▁▁▅▁▅▃▅█▃▅▇▅▄▅▅▅▃▅▃▄▄▅▅▄█▄█▄▆▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▂▂█▂█▂█▂▂█▁▁▁▂▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▃▃█▃█▃█▃▃█▁▃▂▆▂▂▂▂▂▆▆▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▃▃▃▃▃▃▃▃▃▃▃▅▄▇▅▇▅▅█▂▁▆▆▄▅▇█▅█▇▆▅▆▇▄▇▅▇▅▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_multi_head_attention_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3lium95f\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 11:43:08.806042: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_multi_head_attention_loss_weighted_multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/5h6zkfz5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_114307-5h6zkfz5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.5 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 79/80 [00:04<00:00, 19.70it/s, loss=1.13, v_num=kfz5, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.096\n",
      "Epoch 0: 100%|█| 80/80 [00:04<00:00, 19.71it/s, loss=1.13, v_num=kfz5, BTC_val_a\n",
      "Epoch 1:  99%|▉| 79/80 [00:04<00:00, 19.56it/s, loss=1.11, v_num=kfz5, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 80/80 [00:04<00:00, 19.59it/s, loss=1.11, v_num=kfz5, BTC_val_a\u001b[A\n",
      "Epoch 2:  99%|▉| 79/80 [00:04<00:00, 17.55it/s, loss=1.11, v_num=kfz5, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:04<00:00, 17.59it/s, loss=1.11, v_num=kfz5, BTC_val_a\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 1.091\n",
      "Epoch 3:  99%|▉| 79/80 [00:03<00:00, 19.81it/s, loss=1.11, v_num=kfz5, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:04<00:00, 19.84it/s, loss=1.11, v_num=kfz5, BTC_val_a\u001b[A\n",
      "Metric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 1.080\n",
      "Epoch 4:  99%|▉| 79/80 [00:05<00:00, 14.42it/s, loss=1.11, v_num=kfz5, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:05<00:00, 14.48it/s, loss=1.11, v_num=kfz5, BTC_val_a\u001b[A\n",
      "Epoch 5:  99%|▉| 79/80 [00:04<00:00, 18.40it/s, loss=1.11, v_num=kfz5, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:04<00:00, 18.44it/s, loss=1.11, v_num=kfz5, BTC_val_a\u001b[A\n",
      "Epoch 6:  99%|▉| 79/80 [00:04<00:00, 18.18it/s, loss=1.1, v_num=kfz5, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:04<00:00, 18.19it/s, loss=1.1, v_num=kfz5, BTC_val_ac\u001b[A\n",
      "Epoch 7:  99%|▉| 79/80 [00:04<00:00, 15.82it/s, loss=1.03, v_num=kfz5, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.046 >= min_delta = 0.003. New best score: 1.034\n",
      "Epoch 7: 100%|█| 80/80 [00:05<00:00, 15.88it/s, loss=1.03, v_num=kfz5, BTC_val_a\n",
      "Epoch 8:  99%|▉| 79/80 [00:04<00:00, 19.21it/s, loss=1.02, v_num=kfz5, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:04<00:00, 19.23it/s, loss=1.02, v_num=kfz5, BTC_val_a\u001b[A\n",
      "Epoch 9:  99%|▉| 79/80 [00:03<00:00, 20.01it/s, loss=0.949, v_num=kfz5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.049 >= min_delta = 0.003. New best score: 0.986\n",
      "Epoch 9: 100%|█| 80/80 [00:03<00:00, 20.02it/s, loss=0.949, v_num=kfz5, BTC_val_\n",
      "Epoch 10:  99%|▉| 79/80 [00:04<00:00, 19.10it/s, loss=1, v_num=kfz5, BTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:04<00:00, 19.11it/s, loss=1, v_num=kfz5, BTC_val_acc\u001b[A\n",
      "Epoch 11:  99%|▉| 79/80 [00:04<00:00, 19.25it/s, loss=1.02, v_num=kfz5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:04<00:00, 19.25it/s, loss=1.02, v_num=kfz5, BTC_val_\u001b[A\n",
      "Epoch 12:  99%|▉| 79/80 [00:04<00:00, 19.59it/s, loss=0.979, v_num=kfz5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:04<00:00, 19.55it/s, loss=0.979, v_num=kfz5, BTC_val\u001b[A\n",
      "Epoch 13:  99%|▉| 79/80 [00:04<00:00, 18.71it/s, loss=1.03, v_num=kfz5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:04<00:00, 18.67it/s, loss=1.03, v_num=kfz5, BTC_val_\u001b[A\n",
      "Epoch 14:  99%|▉| 79/80 [00:04<00:00, 16.47it/s, loss=0.993, v_num=kfz5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:04<00:00, 16.49it/s, loss=0.993, v_num=kfz5, BTC_val\u001b[A\n",
      "Epoch 15:  99%|▉| 79/80 [00:04<00:00, 16.81it/s, loss=0.981, v_num=kfz5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:04<00:00, 16.83it/s, loss=0.981, v_num=kfz5, BTC_val\u001b[A\n",
      "Epoch 16:  99%|▉| 79/80 [00:05<00:00, 14.51it/s, loss=0.944, v_num=kfz5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:05<00:00, 14.58it/s, loss=0.944, v_num=kfz5, BTC_val\u001b[A\n",
      "Epoch 17:  99%|▉| 79/80 [00:05<00:00, 14.74it/s, loss=1, v_num=kfz5, BTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:05<00:00, 14.80it/s, loss=1, v_num=kfz5, BTC_val_acc\u001b[A\n",
      "Epoch 18:  99%|▉| 79/80 [00:05<00:00, 14.79it/s, loss=0.965, v_num=kfz5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:05<00:00, 14.84it/s, loss=0.965, v_num=kfz5, BTC_val\u001b[A\n",
      "Epoch 19:  99%|▉| 79/80 [00:05<00:00, 13.91it/s, loss=0.968, v_num=kfz5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:05<00:00, 13.97it/s, loss=0.968, v_num=kfz5, BTC_val\u001b[A\n",
      "Epoch 20:  99%|▉| 79/80 [00:04<00:00, 17.49it/s, loss=0.971, v_num=kfz5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:04<00:00, 17.53it/s, loss=0.971, v_num=kfz5, BTC_val\u001b[A\n",
      "Epoch 21:  99%|▉| 79/80 [00:04<00:00, 17.26it/s, loss=1, v_num=kfz5, BTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:04<00:00, 17.28it/s, loss=1, v_num=kfz5, BTC_val_acc\u001b[A\n",
      "Epoch 22:  99%|▉| 79/80 [00:05<00:00, 15.42it/s, loss=1.01, v_num=kfz5, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:05<00:00, 15.48it/s, loss=1.01, v_num=kfz5, BTC_val_\u001b[A\n",
      "Epoch 23:  99%|▉| 79/80 [00:05<00:00, 15.54it/s, loss=1, v_num=kfz5, BTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:05<00:00, 15.53it/s, loss=1, v_num=kfz5, BTC_val_acc\u001b[A\n",
      "Epoch 24:  99%|▉| 79/80 [00:05<00:00, 14.37it/s, loss=1, v_num=kfz5, BTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:05<00:00, 14.31it/s, loss=1, v_num=kfz5, BTC_val_acc\u001b[A\n",
      "Epoch 25:  99%|▉| 79/80 [00:04<00:00, 18.10it/s, loss=0.987, v_num=kfz5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:04<00:00, 18.06it/s, loss=0.987, v_num=kfz5, BTC_val\u001b[A\n",
      "Epoch 26:  99%|▉| 79/80 [00:04<00:00, 16.65it/s, loss=0.974, v_num=kfz5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:04<00:00, 16.70it/s, loss=0.974, v_num=kfz5, BTC_val\u001b[A\n",
      "Epoch 27:  99%|▉| 79/80 [00:03<00:00, 20.02it/s, loss=0.994, v_num=kfz5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:03<00:00, 20.03it/s, loss=0.994, v_num=kfz5, BTC_val\u001b[A\n",
      "Epoch 28:  99%|▉| 79/80 [00:05<00:00, 13.46it/s, loss=0.962, v_num=kfz5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:05<00:00, 13.50it/s, loss=0.962, v_num=kfz5, BTC_val\u001b[A\n",
      "Epoch 29:  99%|▉| 79/80 [00:04<00:00, 17.09it/s, loss=1, v_num=kfz5, BTC_val_acc\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:04<00:00, 17.12it/s, loss=1, v_num=kfz5, BTC_val_acc\u001b[A\n",
      "Epoch 30:  99%|▉| 79/80 [00:04<00:00, 18.72it/s, loss=0.967, v_num=kfz5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:04<00:00, 18.74it/s, loss=0.967, v_num=kfz5, BTC_val\u001b[A\n",
      "Epoch 31:  99%|▉| 79/80 [00:05<00:00, 14.59it/s, loss=0.949, v_num=kfz5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 80/80 [00:05<00:00, 14.65it/s, loss=0.949, v_num=kfz5, BTC_val\u001b[A\n",
      "Epoch 32:  99%|▉| 79/80 [00:05<00:00, 14.40it/s, loss=0.938, v_num=kfz5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 80/80 [00:05<00:00, 14.44it/s, loss=0.938, v_num=kfz5, BTC_val\u001b[A\n",
      "Epoch 33:  99%|▉| 79/80 [00:04<00:00, 18.71it/s, loss=0.976, v_num=kfz5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 80/80 [00:04<00:00, 18.74it/s, loss=0.976, v_num=kfz5, BTC_val\u001b[A\n",
      "Epoch 34:  99%|▉| 79/80 [00:04<00:00, 17.68it/s, loss=0.933, v_num=kfz5, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.986. Signaling Trainer to stop.\n",
      "Epoch 34: 100%|█| 80/80 [00:04<00:00, 17.71it/s, loss=0.933, v_num=kfz5, BTC_val\n",
      "Epoch 34: 100%|█| 80/80 [00:04<00:00, 17.69it/s, loss=0.933, v_num=kfz5, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 54.59it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.3870967626571655,\n",
      " 'BTC_test_f1': 0.28440171480178833,\n",
      " 'ETH_test_acc': 0.4838709533214569,\n",
      " 'ETH_test_f1': 0.46731528639793396,\n",
      " 'test_loss': 0.9208070039749146}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 152053\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_114307-5h6zkfz5/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_114307-5h6zkfz5/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.84258\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 34\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2765\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621500362\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.40855\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.36063\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.42676\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.3806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.95555\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.22222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20635\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.11111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.09524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.33161\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.3871\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.2844\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.48387\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.46732\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.92081\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▆▅▆▅▅▆▅▄▃▄█▄▄▆▅▅▄▁▃▆▅▄▅▃▃▃▃▄▆█▃▆▅▅▅▇▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▁▄▅▄▇▆▄▅▇▇▆▇▇▇▇▆▇▇▆█▇▆▇▇█▇▆▇█▇▇▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▁▃▄▃▄▃▃▆█▇▇▇▇▆█▆▇▇▇▇▇▆▆▇▇▇▆▇▇▇▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▄▃▂▄▄▃▅▆▅▆▇▆▇▆▇▆▇▆▇▆▇▆▆▇▇▇▇▇▇▇▇█▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▁▃▃▂▃▂▃▆█▆▇▇▇▇▇▇▇▇▆▇▇▇▆▆▇▇▇▇▇▇███▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▆▆▆▆▆▄▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ██████▁█▆█▆▃▆█▃▆▆▆▃▃▆▃▃▆▃▆▆▆▃▆▃▆▆▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▄▄▄▄▄▄▁▆▅▄▅▄▅█▃▅▅▅▃▄▅▄▄▅▃▅▅▅▄▅▄▅▅▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc █▂████▁▂▁▄▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▅▂▅▅▅▅▁▄▁█▁▁▄▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▃▃▃▃▃▃▃▂▄▁▄▅▂▃▇▆▆▆▇▆▅▆▆▄▇▅▅▅▆▅▇▇▅██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_multi_head_attention_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/5h6zkfz5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 11:46:13.348558: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_multi_head_attention__binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2z74sag0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_114611-2z74sag0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|█| 80/80 [00:03<00:00, 20.14it/s, loss=0.629, v_num=sag0, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.484\n",
      "Epoch 0: 100%|█| 80/80 [00:03<00:00, 20.04it/s, loss=0.629, v_num=sag0, BTC_val_\n",
      "Epoch 1:  99%|▉| 79/80 [00:04<00:00, 19.17it/s, loss=0.651, v_num=sag0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 80/80 [00:04<00:00, 19.20it/s, loss=0.651, v_num=sag0, BTC_val_\u001b[A\n",
      "Epoch 2:  99%|▉| 79/80 [00:03<00:00, 20.68it/s, loss=0.649, v_num=sag0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:03<00:00, 20.69it/s, loss=0.649, v_num=sag0, BTC_val_\u001b[A\n",
      "Epoch 3:  99%|▉| 79/80 [00:05<00:00, 15.69it/s, loss=0.586, v_num=sag0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 0.478\n",
      "Epoch 3: 100%|█| 80/80 [00:05<00:00, 15.74it/s, loss=0.586, v_num=sag0, BTC_val_\n",
      "Epoch 4:  99%|▉| 79/80 [00:05<00:00, 14.72it/s, loss=0.611, v_num=sag0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:05<00:00, 14.74it/s, loss=0.611, v_num=sag0, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|▉| 79/80 [00:05<00:00, 13.34it/s, loss=0.6, v_num=sag0, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:05<00:00, 13.41it/s, loss=0.6, v_num=sag0, BTC_val_ac\u001b[A\n",
      "Epoch 6:  99%|▉| 79/80 [00:04<00:00, 19.53it/s, loss=0.6, v_num=sag0, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:04<00:00, 19.52it/s, loss=0.6, v_num=sag0, BTC_val_ac\u001b[A\n",
      "Epoch 7:  99%|▉| 79/80 [00:05<00:00, 14.56it/s, loss=0.594, v_num=sag0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:05<00:00, 14.62it/s, loss=0.594, v_num=sag0, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|▉| 79/80 [00:05<00:00, 14.66it/s, loss=0.624, v_num=sag0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:05<00:00, 14.72it/s, loss=0.624, v_num=sag0, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|▉| 79/80 [00:04<00:00, 18.74it/s, loss=0.581, v_num=sag0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:04<00:00, 18.78it/s, loss=0.581, v_num=sag0, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|▉| 79/80 [00:03<00:00, 20.70it/s, loss=0.562, v_num=sag0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:03<00:00, 20.70it/s, loss=0.562, v_num=sag0, BTC_val\u001b[A\n",
      "Epoch 11:  99%|▉| 79/80 [00:04<00:00, 16.70it/s, loss=0.601, v_num=sag0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:04<00:00, 16.75it/s, loss=0.601, v_num=sag0, BTC_val\u001b[A\n",
      "Epoch 12:  99%|▉| 79/80 [00:04<00:00, 18.15it/s, loss=0.606, v_num=sag0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:04<00:00, 18.06it/s, loss=0.606, v_num=sag0, BTC_val\u001b[A\n",
      "Epoch 13:  99%|▉| 79/80 [00:06<00:00, 12.11it/s, loss=0.538, v_num=sag0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:06<00:00, 12.17it/s, loss=0.538, v_num=sag0, BTC_val\u001b[A\n",
      "Epoch 14:  99%|▉| 79/80 [00:04<00:00, 17.97it/s, loss=0.584, v_num=sag0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:04<00:00, 17.95it/s, loss=0.584, v_num=sag0, BTC_val\u001b[A\n",
      "Epoch 15:  99%|▉| 79/80 [00:04<00:00, 18.13it/s, loss=0.591, v_num=sag0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:04<00:00, 18.13it/s, loss=0.591, v_num=sag0, BTC_val\u001b[A\n",
      "Epoch 16:  99%|▉| 79/80 [00:04<00:00, 18.16it/s, loss=0.557, v_num=sag0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:04<00:00, 18.10it/s, loss=0.557, v_num=sag0, BTC_val\u001b[A\n",
      "Epoch 17:  99%|▉| 79/80 [00:04<00:00, 18.08it/s, loss=0.58, v_num=sag0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:04<00:00, 18.11it/s, loss=0.58, v_num=sag0, BTC_val_\u001b[A\n",
      "Epoch 18:  99%|▉| 79/80 [00:04<00:00, 19.59it/s, loss=0.571, v_num=sag0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:04<00:00, 19.62it/s, loss=0.571, v_num=sag0, BTC_val\u001b[A\n",
      "Epoch 19:  99%|▉| 79/80 [00:04<00:00, 18.18it/s, loss=0.621, v_num=sag0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:04<00:00, 18.20it/s, loss=0.621, v_num=sag0, BTC_val\u001b[A\n",
      "Epoch 20:  99%|▉| 79/80 [00:04<00:00, 18.62it/s, loss=0.581, v_num=sag0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:04<00:00, 18.63it/s, loss=0.581, v_num=sag0, BTC_val\u001b[A\n",
      "Epoch 21:  99%|▉| 79/80 [00:04<00:00, 18.78it/s, loss=0.553, v_num=sag0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:04<00:00, 18.81it/s, loss=0.553, v_num=sag0, BTC_val\u001b[A\n",
      "Epoch 22:  99%|▉| 79/80 [00:04<00:00, 18.71it/s, loss=0.543, v_num=sag0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:04<00:00, 18.75it/s, loss=0.543, v_num=sag0, BTC_val\u001b[A\n",
      "Epoch 23:  99%|▉| 79/80 [00:04<00:00, 19.10it/s, loss=0.585, v_num=sag0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:04<00:00, 19.13it/s, loss=0.585, v_num=sag0, BTC_val\u001b[A\n",
      "Epoch 24:  99%|▉| 79/80 [00:04<00:00, 19.54it/s, loss=0.557, v_num=sag0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:04<00:00, 19.51it/s, loss=0.557, v_num=sag0, BTC_val\u001b[A\n",
      "Epoch 25:  99%|▉| 79/80 [00:04<00:00, 18.82it/s, loss=0.566, v_num=sag0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:04<00:00, 18.86it/s, loss=0.566, v_num=sag0, BTC_val\u001b[A\n",
      "Epoch 26:  99%|▉| 79/80 [00:04<00:00, 19.24it/s, loss=0.57, v_num=sag0, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:04<00:00, 19.25it/s, loss=0.57, v_num=sag0, BTC_val_\u001b[A\n",
      "Epoch 27:  99%|▉| 79/80 [00:04<00:00, 18.68it/s, loss=0.573, v_num=sag0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:04<00:00, 18.71it/s, loss=0.573, v_num=sag0, BTC_val\u001b[A\n",
      "Epoch 28:  99%|▉| 79/80 [00:04<00:00, 18.97it/s, loss=0.588, v_num=sag0, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.478. Signaling Trainer to stop.\n",
      "Epoch 28: 100%|█| 80/80 [00:04<00:00, 19.00it/s, loss=0.588, v_num=sag0, BTC_val\n",
      "Epoch 28: 100%|█| 80/80 [00:04<00:00, 18.98it/s, loss=0.588, v_num=sag0, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 53.18it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6774193644523621,\n",
      " 'BTC_test_f1': 0.6673067212104797,\n",
      " 'ETH_test_acc': 0.6129032373428345,\n",
      " 'ETH_test_f1': 0.6120177507400513,\n",
      " 'test_loss': 0.6505138874053955}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 152501\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_114611-2z74sag0/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_114611-2z74sag0/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.56193\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 28\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2291\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 141\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621500512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.71892\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.70077\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.70942\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.69347\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.56732\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.77778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.56328\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.67742\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.66731\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.61202\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.65051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▅█▁▄▅▄█▅▅▂▅▅▆▄▅▅▄▅▅▇▂▃█▆▂▅▆▄▄▃▃▂▅▄▃▂▅▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▁▅▆▆▇▆▇▇▆▆▇▇▇▇█▇▆▇▇▇█▇██▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▁▅▆▆▇▇▇▇▇▆▇▇▇▇█▇▆▇█▇█▇██▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▅▆▆▇▇▆▇▆▇▇▇▇▇▇█▇▇█▇████▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▁▅▆▅▇▇▆▇▆▇▇▇▇▇██▇▇█▆██▇█▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▄▃▃▃▃▂▃▃▂▂▂▂▁▁▂▂▁▂▁▁▁▁▁▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▁▁█▅▅▅▅▅▅▅▁▅▁▁▅▁▅▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▂▁▁▁▁▁▁▁█▅▄▅▅▅▅▅▂▅▂▂▅▂▅▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▃▃▃█████▃█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▄▄▄█████▄█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▄▆▁▁▂▂▃▃▃▃▄▆▂▄▇▄▅▅▇▅▅▃▅█▄▄▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_multi_head_attention__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2z74sag0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 11:48:43.824536: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_multi_head_attention__binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2uoij0bk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_114842-2uoij0bk\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|█| 80/80 [00:04<00:00, 18.46it/s, loss=0.671, v_num=j0bk, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.610\n",
      "Epoch 0: 100%|█| 80/80 [00:04<00:00, 18.38it/s, loss=0.671, v_num=j0bk, BTC_val_\n",
      "Epoch 1:  99%|▉| 79/80 [00:04<00:00, 18.56it/s, loss=0.601, v_num=j0bk, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.078 >= min_delta = 0.003. New best score: 0.532\n",
      "Epoch 1: 100%|█| 80/80 [00:04<00:00, 18.57it/s, loss=0.601, v_num=j0bk, BTC_val_\n",
      "Epoch 2:  99%|▉| 79/80 [00:04<00:00, 17.03it/s, loss=0.577, v_num=j0bk, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.024 >= min_delta = 0.003. New best score: 0.509\n",
      "Epoch 2: 100%|█| 80/80 [00:04<00:00, 17.07it/s, loss=0.577, v_num=j0bk, BTC_val_\n",
      "Epoch 3:  99%|▉| 79/80 [00:04<00:00, 15.84it/s, loss=0.582, v_num=j0bk, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 0.487\n",
      "Epoch 3: 100%|█| 80/80 [00:05<00:00, 15.90it/s, loss=0.582, v_num=j0bk, BTC_val_\n",
      "Epoch 4:  99%|▉| 79/80 [00:05<00:00, 14.99it/s, loss=0.587, v_num=j0bk, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:05<00:00, 15.05it/s, loss=0.587, v_num=j0bk, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|▉| 79/80 [00:04<00:00, 17.27it/s, loss=0.604, v_num=j0bk, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:04<00:00, 17.31it/s, loss=0.604, v_num=j0bk, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|▉| 79/80 [00:05<00:00, 14.74it/s, loss=0.599, v_num=j0bk, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:05<00:00, 14.73it/s, loss=0.599, v_num=j0bk, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|▉| 79/80 [00:05<00:00, 15.68it/s, loss=0.609, v_num=j0bk, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:05<00:00, 15.74it/s, loss=0.609, v_num=j0bk, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|▉| 79/80 [00:04<00:00, 17.98it/s, loss=0.597, v_num=j0bk, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:04<00:00, 17.99it/s, loss=0.597, v_num=j0bk, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|▉| 79/80 [00:05<00:00, 15.54it/s, loss=0.587, v_num=j0bk, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:05<00:00, 15.56it/s, loss=0.587, v_num=j0bk, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|▉| 79/80 [00:05<00:00, 14.91it/s, loss=0.587, v_num=j0bk, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:05<00:00, 14.96it/s, loss=0.587, v_num=j0bk, BTC_val\u001b[A\n",
      "Epoch 11:  99%|▉| 79/80 [00:04<00:00, 16.26it/s, loss=0.616, v_num=j0bk, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:04<00:00, 16.31it/s, loss=0.616, v_num=j0bk, BTC_val\u001b[A\n",
      "Epoch 12:  99%|▉| 79/80 [00:04<00:00, 17.93it/s, loss=0.564, v_num=j0bk, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:04<00:00, 17.97it/s, loss=0.564, v_num=j0bk, BTC_val\u001b[A\n",
      "Epoch 13:  99%|▉| 79/80 [00:04<00:00, 17.44it/s, loss=0.594, v_num=j0bk, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:04<00:00, 17.47it/s, loss=0.594, v_num=j0bk, BTC_val\u001b[A\n",
      "Epoch 14:  99%|▉| 79/80 [00:04<00:00, 17.46it/s, loss=0.582, v_num=j0bk, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:04<00:00, 17.50it/s, loss=0.582, v_num=j0bk, BTC_val\u001b[A\n",
      "Epoch 15:  99%|▉| 79/80 [00:04<00:00, 17.22it/s, loss=0.643, v_num=j0bk, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:04<00:00, 17.25it/s, loss=0.643, v_num=j0bk, BTC_val\u001b[A\n",
      "Epoch 16:  99%|▉| 79/80 [00:04<00:00, 17.39it/s, loss=0.597, v_num=j0bk, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:04<00:00, 17.43it/s, loss=0.597, v_num=j0bk, BTC_val\u001b[A\n",
      "Epoch 17:  99%|▉| 79/80 [00:04<00:00, 17.36it/s, loss=0.596, v_num=j0bk, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:04<00:00, 17.37it/s, loss=0.596, v_num=j0bk, BTC_val\u001b[A\n",
      "Epoch 18:  99%|▉| 79/80 [00:04<00:00, 17.91it/s, loss=0.569, v_num=j0bk, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:04<00:00, 17.92it/s, loss=0.569, v_num=j0bk, BTC_val\u001b[A\n",
      "Epoch 19:  99%|▉| 79/80 [00:04<00:00, 18.06it/s, loss=0.611, v_num=j0bk, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:04<00:00, 18.09it/s, loss=0.611, v_num=j0bk, BTC_val\u001b[A\n",
      "Epoch 20:  99%|▉| 79/80 [00:04<00:00, 17.90it/s, loss=0.557, v_num=j0bk, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:04<00:00, 17.92it/s, loss=0.557, v_num=j0bk, BTC_val\u001b[A\n",
      "Epoch 21:  99%|▉| 79/80 [00:04<00:00, 18.57it/s, loss=0.572, v_num=j0bk, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:04<00:00, 18.58it/s, loss=0.572, v_num=j0bk, BTC_val\u001b[A\n",
      "Epoch 22:  99%|▉| 79/80 [00:04<00:00, 17.45it/s, loss=0.556, v_num=j0bk, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:04<00:00, 17.49it/s, loss=0.556, v_num=j0bk, BTC_val\u001b[A\n",
      "Epoch 23:  99%|▉| 79/80 [00:04<00:00, 17.69it/s, loss=0.551, v_num=j0bk, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:04<00:00, 17.72it/s, loss=0.551, v_num=j0bk, BTC_val\u001b[A\n",
      "Epoch 24:  99%|▉| 79/80 [00:04<00:00, 17.80it/s, loss=0.579, v_num=j0bk, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:04<00:00, 17.82it/s, loss=0.579, v_num=j0bk, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25:  99%|▉| 79/80 [00:04<00:00, 18.33it/s, loss=0.555, v_num=j0bk, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:04<00:00, 18.31it/s, loss=0.555, v_num=j0bk, BTC_val\u001b[A\n",
      "Epoch 26:  99%|▉| 79/80 [00:05<00:00, 13.59it/s, loss=0.571, v_num=j0bk, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:05<00:00, 13.62it/s, loss=0.571, v_num=j0bk, BTC_val\u001b[A\n",
      "Epoch 27:  99%|▉| 79/80 [00:05<00:00, 15.22it/s, loss=0.556, v_num=j0bk, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 80/80 [00:05<00:00, 15.28it/s, loss=0.556, v_num=j0bk, BTC_val\u001b[A\n",
      "Epoch 28:  99%|▉| 79/80 [00:04<00:00, 18.66it/s, loss=0.578, v_num=j0bk, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.487. Signaling Trainer to stop.\n",
      "Epoch 28: 100%|█| 80/80 [00:04<00:00, 18.69it/s, loss=0.578, v_num=j0bk, BTC_val\n",
      "Epoch 28: 100%|█| 80/80 [00:04<00:00, 18.67it/s, loss=0.578, v_num=j0bk, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 50.30it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6774193644523621,\n",
      " 'BTC_test_f1': 0.6673067212104797,\n",
      " 'ETH_test_acc': 0.6129032373428345,\n",
      " 'ETH_test_f1': 0.6120177507400513,\n",
      " 'test_loss': 0.6390955448150635}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 152872\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_114842-2uoij0bk/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_114842-2uoij0bk/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.50008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 28\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2291\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 146\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621500668\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.73001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.71317\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.70467\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.68586\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.56666\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.77778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.775\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.88889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.88889\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.53189\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.67742\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.66731\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.6129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.61202\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.6391\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▄▆▃▃▃▅▄▆█▅▄▃▃▅▃▃▄▁▂▂▁▃▅▅▂▁▅▄▂▅▄▄█▂▅▄▃▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▁▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▁▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇████▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇██▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▁▆▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇██▇██▇▇█▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc █▃▃▃▃▃▃▃▃▃▃▃▃▃▆█▃▁▆▃▃▆▆▃▆▆▆▃▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 █▄▄▄▄▄▄▄▄▄▄▄▄▄▆█▄▁▆▄▄▆▆▄▆▆▆▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▃▆▆▆▆▆▆▆█▆█▆▆▆▁▁▆▆▁▁▁▁▁▁▅▅▁▁▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▃▆▆▆▆▆▆▆█▆█▆▆▆▁▁▆▆▁▁▁▁▁▁▅▅▁▁▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss █▄▂▁▂▄▂▃▁▂▁▂▁▂▂▄▂▃▃▄▃▄▅▄▃▃▃▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_multi_head_attention__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2uoij0bk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 11:51:21.953569: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_multi_head_attention__multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/16e41ned\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_115120-16e41ned\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.5 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 79/80 [00:04<00:00, 18.02it/s, loss=1.06, v_num=1ned, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 80/80 [00:04<00:00, 17.93it/s, loss=1.06, v_num=1ned, BTC_val_aMetric val_loss improved. New best score: 0.947\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  99%|▉| 79/80 [00:04<00:00, 17.72it/s, loss=1.03, v_num=1ned, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 80/80 [00:04<00:00, 17.75it/s, loss=1.03, v_num=1ned, BTC_val_a\u001b[A\n",
      "Epoch 2:  99%|▉| 79/80 [00:04<00:00, 19.64it/s, loss=1.02, v_num=1ned, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:04<00:00, 19.68it/s, loss=1.02, v_num=1ned, BTC_val_a\u001b[A\n",
      "Epoch 3:  99%|▉| 79/80 [00:03<00:00, 19.84it/s, loss=1.06, v_num=1ned, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:04<00:00, 19.87it/s, loss=1.06, v_num=1ned, BTC_val_a\u001b[A\n",
      "Epoch 4:  99%|▉| 79/80 [00:04<00:00, 18.08it/s, loss=1.04, v_num=1ned, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:04<00:00, 18.11it/s, loss=1.04, v_num=1ned, BTC_val_a\u001b[A\n",
      "Epoch 5:  99%|▉| 79/80 [00:04<00:00, 18.22it/s, loss=1.05, v_num=1ned, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:04<00:00, 18.25it/s, loss=1.05, v_num=1ned, BTC_val_a\u001b[A\n",
      "Epoch 6:  99%|▉| 79/80 [00:04<00:00, 19.12it/s, loss=1.05, v_num=1ned, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:04<00:00, 19.16it/s, loss=1.05, v_num=1ned, BTC_val_a\u001b[A\n",
      "Epoch 7:  99%|▉| 79/80 [00:04<00:00, 18.23it/s, loss=1.02, v_num=1ned, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:04<00:00, 18.26it/s, loss=1.02, v_num=1ned, BTC_val_a\u001b[A\n",
      "Epoch 8:  99%|▉| 79/80 [00:04<00:00, 17.24it/s, loss=0.967, v_num=1ned, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:04<00:00, 17.27it/s, loss=0.967, v_num=1ned, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|▉| 79/80 [00:04<00:00, 16.59it/s, loss=0.976, v_num=1ned, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:04<00:00, 16.63it/s, loss=0.976, v_num=1ned, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|▉| 79/80 [00:04<00:00, 17.76it/s, loss=0.967, v_num=1ned, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:04<00:00, 17.79it/s, loss=0.967, v_num=1ned, BTC_val\u001b[A\n",
      "Epoch 11:  99%|▉| 79/80 [00:05<00:00, 14.98it/s, loss=0.962, v_num=1ned, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:05<00:00, 15.04it/s, loss=0.962, v_num=1ned, BTC_val\u001b[A\n",
      "Epoch 12:  99%|▉| 79/80 [00:04<00:00, 19.35it/s, loss=0.983, v_num=1ned, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:04<00:00, 19.29it/s, loss=0.983, v_num=1ned, BTC_val\u001b[A\n",
      "Epoch 13:  99%|▉| 79/80 [00:05<00:00, 13.55it/s, loss=0.926, v_num=1ned, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:05<00:00, 13.61it/s, loss=0.926, v_num=1ned, BTC_val\u001b[A\n",
      "Epoch 14:  99%|▉| 79/80 [00:04<00:00, 17.90it/s, loss=0.958, v_num=1ned, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:04<00:00, 17.94it/s, loss=0.958, v_num=1ned, BTC_val\u001b[A\n",
      "Epoch 15:  99%|▉| 79/80 [00:04<00:00, 16.61it/s, loss=0.97, v_num=1ned, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:04<00:00, 16.66it/s, loss=0.97, v_num=1ned, BTC_val_\u001b[A\n",
      "Epoch 16:  99%|▉| 79/80 [00:04<00:00, 17.01it/s, loss=0.932, v_num=1ned, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:04<00:00, 17.05it/s, loss=0.932, v_num=1ned, BTC_val\u001b[A\n",
      "Epoch 17:  99%|▉| 79/80 [00:04<00:00, 17.43it/s, loss=0.947, v_num=1ned, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:04<00:00, 17.46it/s, loss=0.947, v_num=1ned, BTC_val\u001b[A\n",
      "Epoch 18:  99%|▉| 79/80 [00:04<00:00, 17.58it/s, loss=0.954, v_num=1ned, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:04<00:00, 17.56it/s, loss=0.954, v_num=1ned, BTC_val\u001b[A\n",
      "Epoch 19:  99%|▉| 79/80 [00:04<00:00, 17.97it/s, loss=0.966, v_num=1ned, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:04<00:00, 17.95it/s, loss=0.966, v_num=1ned, BTC_val\u001b[A\n",
      "Epoch 20:  99%|▉| 79/80 [00:04<00:00, 17.83it/s, loss=0.932, v_num=1ned, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:04<00:00, 17.87it/s, loss=0.932, v_num=1ned, BTC_val\u001b[A\n",
      "Epoch 21:  99%|▉| 79/80 [00:04<00:00, 18.28it/s, loss=0.948, v_num=1ned, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:04<00:00, 18.31it/s, loss=0.948, v_num=1ned, BTC_val\u001b[A\n",
      "Epoch 22:  99%|▉| 79/80 [00:04<00:00, 18.11it/s, loss=0.9, v_num=1ned, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:04<00:00, 18.14it/s, loss=0.9, v_num=1ned, BTC_val_a\u001b[A\n",
      "Epoch 23:  99%|▉| 79/80 [00:05<00:00, 14.69it/s, loss=0.941, v_num=1ned, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:05<00:00, 14.71it/s, loss=0.941, v_num=1ned, BTC_val\u001b[A\n",
      "Epoch 24:  99%|▉| 79/80 [00:06<00:00, 12.14it/s, loss=0.936, v_num=1ned, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:06<00:00, 12.19it/s, loss=0.936, v_num=1ned, BTC_val\u001b[A\n",
      "Epoch 25:  99%|▉| 79/80 [00:05<00:00, 13.38it/s, loss=0.927, v_num=1ned, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.947. Signaling Trainer to stop.\n",
      "Epoch 25: 100%|█| 80/80 [00:05<00:00, 13.39it/s, loss=0.927, v_num=1ned, BTC_val\n",
      "Epoch 25: 100%|█| 80/80 [00:05<00:00, 13.38it/s, loss=0.927, v_num=1ned, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 21.20it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5483871102333069,\n",
      " 'BTC_test_f1': 0.23566308617591858,\n",
      " 'ETH_test_acc': 0.4838709533214569,\n",
      " 'ETH_test_f1': 0.2169238030910492,\n",
      " 'test_loss': 1.0090892314910889}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 154337\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_115120-16e41ned/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_115120-16e41ned/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 1.08476\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2054\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 132\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621500812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 93\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.4996\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.28294\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.4806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.27761\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.94074\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.44444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.20513\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.26667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.05105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.54839\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.23566\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.48387\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.21692\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.00909\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▄█▅▃▃▅▁▅▃▅▅▆▅▆▅▅▁▄▂▁▃▄▃▃▃▃▂▄▄▁▅▃▄▂▄▅▁▄▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▁▆▆▇▇▇▇▇█▆▆▆▆▇▆▆▇▆▆▇▇▇▆▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▅▃▂▁▁▁▁▂▄▆▅▆▅▄▄▅▆▆▆▇▅▄█▃▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▆▅▇▇▇▇▇▇▆▆▇▆▆▇▇█▇▆▆▇▆▇▇▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▅▃▂▁▂▁▁▁▃▆▅▅▆▃▅▆▇▆▆▇▅▄█▂▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▅▅▅▄▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ████████▁████████▁▁██▁████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ████████▁████████▁▁██▁████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ████████▁█▆█████▆▁▁██▆▆███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ████████▁█▆█████▆▁▁██▆▆███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▁▁▂▂▂▁▁▆▆▇▅▄▄▆▁▅▃█▃█▆▆▆▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_multi_head_attention__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/16e41ned\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 11:53:43.114020: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_multi_head_attention__multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1odi2gkw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_115341-1odi2gkw\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.5 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 79/80 [00:03<00:00, 21.20it/s, loss=1.07, v_num=2gkw, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.945\n",
      "Epoch 0: 100%|█| 80/80 [00:03<00:00, 21.22it/s, loss=1.07, v_num=2gkw, BTC_val_a\n",
      "Epoch 1:  99%|▉| 79/80 [00:03<00:00, 21.95it/s, loss=1.07, v_num=2gkw, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 80/80 [00:03<00:00, 21.94it/s, loss=1.07, v_num=2gkw, BTC_val_a\u001b[A\n",
      "Epoch 2:  99%|▉| 79/80 [00:04<00:00, 19.16it/s, loss=1.05, v_num=2gkw, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 80/80 [00:04<00:00, 19.13it/s, loss=1.05, v_num=2gkw, BTC_val_a\u001b[A\n",
      "Epoch 3:  99%|▉| 79/80 [00:03<00:00, 20.08it/s, loss=1.03, v_num=2gkw, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 80/80 [00:03<00:00, 20.11it/s, loss=1.03, v_num=2gkw, BTC_val_a\u001b[A\n",
      "Epoch 4:  99%|▉| 79/80 [00:03<00:00, 21.05it/s, loss=1.03, v_num=2gkw, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 80/80 [00:03<00:00, 21.06it/s, loss=1.03, v_num=2gkw, BTC_val_a\u001b[A\n",
      "Epoch 5:  99%|▉| 79/80 [00:03<00:00, 21.06it/s, loss=1.03, v_num=2gkw, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 80/80 [00:03<00:00, 21.08it/s, loss=1.03, v_num=2gkw, BTC_val_a\u001b[A\n",
      "Epoch 6:  99%|▉| 79/80 [00:03<00:00, 20.67it/s, loss=1.04, v_num=2gkw, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 80/80 [00:03<00:00, 20.67it/s, loss=1.04, v_num=2gkw, BTC_val_a\u001b[A\n",
      "Epoch 7:  99%|▉| 79/80 [00:03<00:00, 20.97it/s, loss=1.05, v_num=2gkw, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 80/80 [00:03<00:00, 20.97it/s, loss=1.05, v_num=2gkw, BTC_val_a\u001b[A\n",
      "Epoch 8:  99%|▉| 79/80 [00:04<00:00, 19.03it/s, loss=1.05, v_num=2gkw, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 80/80 [00:04<00:00, 19.06it/s, loss=1.05, v_num=2gkw, BTC_val_a\u001b[A\n",
      "Epoch 9:  99%|▉| 79/80 [00:04<00:00, 19.47it/s, loss=1.05, v_num=2gkw, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 80/80 [00:04<00:00, 19.49it/s, loss=1.05, v_num=2gkw, BTC_val_a\u001b[A\n",
      "Epoch 10:  99%|▉| 79/80 [00:03<00:00, 20.27it/s, loss=1.03, v_num=2gkw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 80/80 [00:03<00:00, 20.29it/s, loss=1.03, v_num=2gkw, BTC_val_\u001b[A\n",
      "Epoch 11:  99%|▉| 79/80 [00:03<00:00, 20.38it/s, loss=1.03, v_num=2gkw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 80/80 [00:03<00:00, 20.40it/s, loss=1.03, v_num=2gkw, BTC_val_\u001b[A\n",
      "Epoch 12:  99%|▉| 79/80 [00:04<00:00, 17.76it/s, loss=1.03, v_num=2gkw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 80/80 [00:04<00:00, 17.79it/s, loss=1.03, v_num=2gkw, BTC_val_\u001b[A\n",
      "Epoch 13:  99%|▉| 79/80 [00:04<00:00, 18.11it/s, loss=1.02, v_num=2gkw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 80/80 [00:04<00:00, 18.11it/s, loss=1.02, v_num=2gkw, BTC_val_\u001b[A\n",
      "Epoch 14:  99%|▉| 79/80 [00:05<00:00, 13.58it/s, loss=0.965, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 80/80 [00:05<00:00, 13.61it/s, loss=0.965, v_num=2gkw, BTC_valMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 0.935\n",
      "\n",
      "Epoch 15:  99%|▉| 79/80 [00:05<00:00, 13.74it/s, loss=0.976, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 80/80 [00:05<00:00, 13.78it/s, loss=0.976, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 16:  99%|▉| 79/80 [00:05<00:00, 14.04it/s, loss=0.989, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 80/80 [00:05<00:00, 14.07it/s, loss=0.989, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 17:  99%|▉| 79/80 [00:05<00:00, 13.75it/s, loss=0.942, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 80/80 [00:05<00:00, 13.79it/s, loss=0.942, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 18:  99%|▉| 79/80 [00:05<00:00, 14.32it/s, loss=0.973, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 80/80 [00:05<00:00, 14.38it/s, loss=0.973, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 19:  99%|▉| 79/80 [00:05<00:00, 14.15it/s, loss=0.94, v_num=2gkw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 80/80 [00:05<00:00, 14.15it/s, loss=0.94, v_num=2gkw, BTC_val_\u001b[A\n",
      "Epoch 20:  99%|▉| 79/80 [00:05<00:00, 13.39it/s, loss=0.968, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 80/80 [00:05<00:00, 13.43it/s, loss=0.968, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 21:  99%|▉| 79/80 [00:05<00:00, 14.36it/s, loss=0.94, v_num=2gkw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 80/80 [00:05<00:00, 14.38it/s, loss=0.94, v_num=2gkw, BTC_val_\u001b[A\n",
      "Epoch 22:  99%|▉| 79/80 [00:08<00:00,  9.40it/s, loss=0.96, v_num=2gkw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 80/80 [00:08<00:00,  9.46it/s, loss=0.96, v_num=2gkw, BTC_val_\u001b[A\n",
      "Epoch 23:  99%|▉| 79/80 [00:07<00:00, 10.18it/s, loss=0.948, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 80/80 [00:07<00:00, 10.21it/s, loss=0.948, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 24:  99%|▉| 79/80 [00:05<00:00, 14.05it/s, loss=0.923, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 80/80 [00:05<00:00, 14.09it/s, loss=0.923, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 25:  99%|▉| 79/80 [00:05<00:00, 13.88it/s, loss=0.928, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 80/80 [00:05<00:00, 13.86it/s, loss=0.928, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 26:  99%|▉| 79/80 [00:09<00:00,  8.27it/s, loss=0.942, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 80/80 [00:09<00:00,  8.23it/s, loss=0.942, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 27:  99%|▉| 79/80 [00:08<00:00,  9.69it/s, loss=0.943, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.019 >= min_delta = 0.003. New best score: 0.916\n",
      "Epoch 27: 100%|█| 80/80 [00:08<00:00,  9.75it/s, loss=0.943, v_num=2gkw, BTC_val\n",
      "Epoch 28:  99%|▉| 79/80 [00:09<00:00,  8.46it/s, loss=0.96, v_num=2gkw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 80/80 [00:09<00:00,  8.52it/s, loss=0.96, v_num=2gkw, BTC_val_\u001b[A\n",
      "Epoch 29:  99%|▉| 79/80 [00:08<00:00,  9.84it/s, loss=0.95, v_num=2gkw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 80/80 [00:08<00:00,  9.89it/s, loss=0.95, v_num=2gkw, BTC_val_\u001b[A\n",
      "Epoch 30:  99%|▉| 79/80 [00:06<00:00, 11.44it/s, loss=0.919, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 80/80 [00:06<00:00, 11.48it/s, loss=0.919, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 31:  99%|▉| 79/80 [00:09<00:00,  8.51it/s, loss=0.928, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 80/80 [00:09<00:00,  8.57it/s, loss=0.928, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 32:  99%|▉| 79/80 [00:07<00:00, 11.09it/s, loss=0.918, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 80/80 [00:07<00:00, 11.13it/s, loss=0.918, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 33:  99%|▉| 79/80 [00:05<00:00, 13.25it/s, loss=0.943, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 80/80 [00:06<00:00, 13.26it/s, loss=0.943, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 34:  99%|▉| 79/80 [00:08<00:00,  9.41it/s, loss=0.931, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 80/80 [00:08<00:00,  9.47it/s, loss=0.931, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 35:  99%|▉| 79/80 [00:06<00:00, 12.25it/s, loss=0.914, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 80/80 [00:06<00:00, 12.27it/s, loss=0.914, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 36:  99%|▉| 79/80 [00:06<00:00, 12.41it/s, loss=0.919, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 80/80 [00:06<00:00, 12.46it/s, loss=0.919, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 37:  99%|▉| 79/80 [00:05<00:00, 13.54it/s, loss=0.916, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 80/80 [00:05<00:00, 13.59it/s, loss=0.916, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 38:  99%|▉| 79/80 [00:05<00:00, 13.69it/s, loss=0.941, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 80/80 [00:05<00:00, 13.69it/s, loss=0.941, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 39:  99%|▉| 79/80 [00:05<00:00, 13.62it/s, loss=0.929, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 80/80 [00:05<00:00, 13.63it/s, loss=0.929, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 40:  99%|▉| 79/80 [00:05<00:00, 13.50it/s, loss=0.939, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 80/80 [00:05<00:00, 13.55it/s, loss=0.939, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 41:  99%|▉| 79/80 [00:05<00:00, 13.63it/s, loss=0.933, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 80/80 [00:05<00:00, 13.68it/s, loss=0.933, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 42:  99%|▉| 79/80 [00:05<00:00, 13.63it/s, loss=0.925, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 80/80 [00:05<00:00, 13.67it/s, loss=0.925, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 43:  99%|▉| 79/80 [00:05<00:00, 13.56it/s, loss=0.914, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 80/80 [00:05<00:00, 13.60it/s, loss=0.914, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 44:  99%|▉| 79/80 [00:05<00:00, 13.55it/s, loss=0.956, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 80/80 [00:05<00:00, 13.58it/s, loss=0.956, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 45:  99%|▉| 79/80 [00:06<00:00, 11.51it/s, loss=0.947, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 80/80 [00:06<00:00, 11.56it/s, loss=0.947, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 46:  99%|▉| 79/80 [00:07<00:00,  9.89it/s, loss=0.914, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 80/80 [00:08<00:00,  9.94it/s, loss=0.914, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 47:  99%|▉| 79/80 [00:07<00:00, 10.60it/s, loss=0.902, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 80/80 [00:07<00:00, 10.66it/s, loss=0.902, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 48:  99%|▉| 79/80 [00:06<00:00, 11.83it/s, loss=0.915, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 80/80 [00:06<00:00, 11.82it/s, loss=0.915, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 49:  99%|▉| 79/80 [00:09<00:00,  8.40it/s, loss=0.899, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 80/80 [00:09<00:00,  8.37it/s, loss=0.899, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 50:  99%|▉| 79/80 [00:07<00:00, 10.95it/s, loss=0.915, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 80/80 [00:07<00:00, 10.98it/s, loss=0.915, v_num=2gkw, BTC_val\u001b[A\n",
      "Epoch 51:  99%|▉| 79/80 [00:08<00:00,  9.38it/s, loss=0.94, v_num=2gkw, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 80/80 [00:08<00:00,  9.32it/s, loss=0.94, v_num=2gkw, BTC_val_\u001b[A\n",
      "Epoch 52:  99%|▉| 79/80 [00:08<00:00,  9.19it/s, loss=0.932, v_num=2gkw, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.916. Signaling Trainer to stop.\n",
      "Epoch 52: 100%|█| 80/80 [00:08<00:00,  9.25it/s, loss=0.932, v_num=2gkw, BTC_val\n",
      "Epoch 52: 100%|█| 80/80 [00:08<00:00,  9.24it/s, loss=0.932, v_num=2gkw, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 42.34it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5483871102333069,\n",
      " 'BTC_test_f1': 0.23566308617591858,\n",
      " 'ETH_test_acc': 0.4838709533214569,\n",
      " 'ETH_test_f1': 0.2169238030910492,\n",
      " 'test_loss': 0.8647854924201965}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 154763\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_115341-1odi2gkw/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_115341-1odi2gkw/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.92454\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4187\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 336\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621501157\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 189\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.48298\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.31242\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.48456\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.33314\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.91908\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.18182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.55556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.37576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.12592\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.54839\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.23566\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.48387\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.21692\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.86479\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅█▅█▅▅▆▆▅▆▅▂▃▅▃▂▂▂▆▆▂▁▆▃▄▃▃▁▃█▂▅▃▁▁▃▂▇▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▁▆▇█▇▇▇▇▇▇▇█▆▇▆▇▇█▇▇▆█▇▇▇▇▇▇▆▇▇█▇▇▆▇▇▇▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▄▄▄▃▃▂▁▁▁▁▁▄▅▄▅▅█▄▆▅▃▅▅▆▄▆▆▇▅▆▄█▅▅▆▅▆▅▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▆▆▇▇▇█▇█▇▇▇▇▆▇▆▇▇▇▆▇▆▇▆▇▇▆█▆▆▇▇▆▇▇█▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▅▅▃▂▃▂▂▁▁▁▁▃▅▃▅▃▇▄▆▄▄▅▄▄▄▅▅█▆▄▅▆▄▄▅▆▇▅▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▅▅▅▅▅▅▅▅▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▄▄▄▄▄▄▄▄▄▄▄█▄▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▄▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▂▂▂▂▂▂▂▂▂▂▂█▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂██▂▂▂▁█▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁█▅▁▅▅▅▁▁▅▅▅▅▅▅▅▅▅▅▅██▁▅▅▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁█▂▁▂▂▂▁▁▂▂▂▂▂▂▂▂▅▂▂██▄▂▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▁▁▂▂▂▂▂▂▂▂▂▁▅█▇▅▅▆▃▃▁▃█▃▆▆▂▃▄▂▆▆▆▄▆▅▇▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_multi_head_attention__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1odi2gkw\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 11:59:28.355760: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_multi_head_attention_loss_weighted_binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2j6g79rd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_115926-2j6g79rd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|█| 73/73 [00:07<00:00, 10.06it/s, loss=0.609, v_num=79rd, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 73/73 [00:07<00:00, 10.01it/s, loss=0.609, v_num=79rd, BTC_val_Metric val_loss improved. New best score: 0.616\n",
      "\n",
      "Epoch 1:  99%|▉| 72/73 [00:08<00:00,  8.70it/s, loss=0.62, v_num=79rd, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 73/73 [00:08<00:00,  8.73it/s, loss=0.62, v_num=79rd, BTC_val_a\u001b[A\n",
      "Epoch 2:  99%|▉| 72/73 [00:07<00:00, 10.08it/s, loss=0.626, v_num=79rd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 73/73 [00:07<00:00, 10.12it/s, loss=0.626, v_num=79rd, BTC_val_\u001b[A\n",
      "Epoch 3:  99%|▉| 72/73 [00:07<00:00,  9.00it/s, loss=0.622, v_num=79rd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.026 >= min_delta = 0.003. New best score: 0.590\n",
      "Epoch 3: 100%|█| 73/73 [00:08<00:00,  9.04it/s, loss=0.622, v_num=79rd, BTC_val_\n",
      "Epoch 4:  99%|▉| 72/73 [00:07<00:00,  9.53it/s, loss=0.57, v_num=79rd, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 73/73 [00:07<00:00,  9.54it/s, loss=0.57, v_num=79rd, BTC_val_a\u001b[A\n",
      "Epoch 5:  99%|▉| 72/73 [00:08<00:00,  8.46it/s, loss=0.612, v_num=79rd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 73/73 [00:08<00:00,  8.51it/s, loss=0.612, v_num=79rd, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|▉| 72/73 [00:11<00:00,  6.11it/s, loss=0.578, v_num=79rd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 0.578\n",
      "Epoch 6: 100%|█| 73/73 [00:11<00:00,  6.15it/s, loss=0.578, v_num=79rd, BTC_val_\n",
      "Epoch 7:  99%|▉| 72/73 [00:11<00:00,  6.43it/s, loss=0.619, v_num=79rd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 73/73 [00:11<00:00,  6.48it/s, loss=0.619, v_num=79rd, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|▉| 72/73 [00:10<00:00,  7.05it/s, loss=0.581, v_num=79rd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 73/73 [00:10<00:00,  7.10it/s, loss=0.581, v_num=79rd, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|▉| 72/73 [00:09<00:00,  7.32it/s, loss=0.594, v_num=79rd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 73/73 [00:09<00:00,  7.35it/s, loss=0.594, v_num=79rd, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|▉| 72/73 [00:10<00:00,  6.80it/s, loss=0.591, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 73/73 [00:10<00:00,  6.85it/s, loss=0.591, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 11:  99%|▉| 72/73 [00:07<00:00,  9.60it/s, loss=0.601, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 73/73 [00:07<00:00,  9.64it/s, loss=0.601, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 12:  99%|▉| 72/73 [00:08<00:00,  8.97it/s, loss=0.604, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 73/73 [00:08<00:00,  9.01it/s, loss=0.604, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 13:  99%|▉| 72/73 [00:07<00:00,  9.37it/s, loss=0.609, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 73/73 [00:07<00:00,  9.42it/s, loss=0.609, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 14:  99%|▉| 72/73 [00:07<00:00,  9.48it/s, loss=0.552, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 73/73 [00:07<00:00,  9.53it/s, loss=0.552, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 15:  99%|▉| 72/73 [00:07<00:00,  9.35it/s, loss=0.585, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 0.575\n",
      "Epoch 15: 100%|█| 73/73 [00:07<00:00,  9.37it/s, loss=0.585, v_num=79rd, BTC_val\n",
      "Epoch 16:  99%|▉| 72/73 [00:09<00:00,  7.96it/s, loss=0.605, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 73/73 [00:09<00:00,  8.01it/s, loss=0.605, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 17:  99%|▉| 72/73 [00:07<00:00,  9.46it/s, loss=0.585, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 73/73 [00:07<00:00,  9.50it/s, loss=0.585, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 18:  99%|▉| 72/73 [00:09<00:00,  7.35it/s, loss=0.599, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 73/73 [00:09<00:00,  7.40it/s, loss=0.599, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 19:  99%|▉| 72/73 [00:11<00:00,  6.49it/s, loss=0.547, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 73/73 [00:11<00:00,  6.54it/s, loss=0.547, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 20:  99%|▉| 72/73 [00:11<00:00,  6.54it/s, loss=0.569, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 73/73 [00:11<00:00,  6.57it/s, loss=0.569, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 21:  99%|▉| 72/73 [00:11<00:00,  6.12it/s, loss=0.608, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 73/73 [00:11<00:00,  6.17it/s, loss=0.608, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 22:  99%|▉| 72/73 [00:09<00:00,  7.40it/s, loss=0.555, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 73/73 [00:09<00:00,  7.46it/s, loss=0.555, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 23:  99%|▉| 72/73 [00:09<00:00,  7.38it/s, loss=0.575, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 73/73 [00:09<00:00,  7.42it/s, loss=0.575, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 24:  99%|▉| 72/73 [00:09<00:00,  7.32it/s, loss=0.548, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 73/73 [00:09<00:00,  7.35it/s, loss=0.548, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 25:  99%|▉| 72/73 [00:10<00:00,  6.84it/s, loss=0.565, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 73/73 [00:10<00:00,  6.88it/s, loss=0.565, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 26:  99%|▉| 72/73 [00:10<00:00,  7.15it/s, loss=0.551, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 73/73 [00:10<00:00,  7.20it/s, loss=0.551, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 27:  99%|▉| 72/73 [00:08<00:00,  8.47it/s, loss=0.58, v_num=79rd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 73/73 [00:08<00:00,  8.51it/s, loss=0.58, v_num=79rd, BTC_val_\u001b[A\n",
      "Epoch 28:  99%|▉| 72/73 [00:07<00:00,  9.46it/s, loss=0.588, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 73/73 [00:07<00:00,  9.50it/s, loss=0.588, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 29:  99%|▉| 72/73 [00:11<00:00,  6.52it/s, loss=0.576, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 73/73 [00:11<00:00,  6.56it/s, loss=0.576, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 30:  99%|▉| 72/73 [00:11<00:00,  6.47it/s, loss=0.57, v_num=79rd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 73/73 [00:11<00:00,  6.52it/s, loss=0.57, v_num=79rd, BTC_val_\u001b[A\n",
      "Epoch 31:  99%|▉| 72/73 [00:10<00:00,  6.61it/s, loss=0.568, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 73/73 [00:10<00:00,  6.66it/s, loss=0.568, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 32:  99%|▉| 72/73 [00:07<00:00,  9.67it/s, loss=0.558, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 73/73 [00:07<00:00,  9.71it/s, loss=0.558, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 33:  99%|▉| 72/73 [00:08<00:00,  8.08it/s, loss=0.551, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 73/73 [00:08<00:00,  8.13it/s, loss=0.551, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 34:  99%|▉| 72/73 [00:10<00:00,  6.87it/s, loss=0.547, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 73/73 [00:10<00:00,  6.92it/s, loss=0.547, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 35:  99%|▉| 72/73 [00:10<00:00,  6.68it/s, loss=0.563, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 73/73 [00:10<00:00,  6.72it/s, loss=0.563, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 36:  99%|▉| 72/73 [00:11<00:00,  6.19it/s, loss=0.564, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 73/73 [00:11<00:00,  6.24it/s, loss=0.564, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 37:  99%|▉| 72/73 [00:10<00:00,  7.17it/s, loss=0.551, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 73/73 [00:10<00:00,  7.21it/s, loss=0.551, v_num=79rd, BTC_valMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 0.563\n",
      "\n",
      "Epoch 38:  99%|▉| 72/73 [00:11<00:00,  6.23it/s, loss=0.551, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 73/73 [00:11<00:00,  6.27it/s, loss=0.551, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 39:  99%|▉| 72/73 [00:08<00:00,  8.86it/s, loss=0.544, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 73/73 [00:08<00:00,  8.90it/s, loss=0.544, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 40:  99%|▉| 72/73 [00:08<00:00,  8.86it/s, loss=0.57, v_num=79rd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 73/73 [00:08<00:00,  8.90it/s, loss=0.57, v_num=79rd, BTC_val_\u001b[A\n",
      "Epoch 41:  99%|▉| 72/73 [00:08<00:00,  8.53it/s, loss=0.553, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 73/73 [00:08<00:00,  8.58it/s, loss=0.553, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 42:  99%|▉| 72/73 [00:07<00:00,  9.38it/s, loss=0.548, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 73/73 [00:07<00:00,  9.42it/s, loss=0.548, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 43:  99%|▉| 72/73 [00:09<00:00,  7.22it/s, loss=0.522, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 73/73 [00:10<00:00,  7.26it/s, loss=0.522, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 44:  99%|▉| 72/73 [00:11<00:00,  6.45it/s, loss=0.587, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 73/73 [00:11<00:00,  6.51it/s, loss=0.587, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 45:  99%|▉| 72/73 [00:08<00:00,  8.41it/s, loss=0.541, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 73/73 [00:08<00:00,  8.46it/s, loss=0.541, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 46:  99%|▉| 72/73 [00:08<00:00,  8.14it/s, loss=0.575, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 73/73 [00:08<00:00,  8.19it/s, loss=0.575, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 47:  99%|▉| 72/73 [00:11<00:00,  6.01it/s, loss=0.556, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 73/73 [00:12<00:00,  6.06it/s, loss=0.556, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 48:  99%|▉| 72/73 [00:08<00:00,  8.37it/s, loss=0.56, v_num=79rd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 73/73 [00:08<00:00,  8.39it/s, loss=0.56, v_num=79rd, BTC_val_\u001b[A\n",
      "Epoch 49:  99%|▉| 72/73 [00:09<00:00,  7.86it/s, loss=0.56, v_num=79rd, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 73/73 [00:09<00:00,  7.91it/s, loss=0.56, v_num=79rd, BTC_val_\u001b[A\n",
      "Epoch 50:  99%|▉| 72/73 [00:10<00:00,  7.19it/s, loss=0.549, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 73/73 [00:10<00:00,  7.23it/s, loss=0.549, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 51:  99%|▉| 72/73 [00:09<00:00,  7.48it/s, loss=0.566, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 73/73 [00:09<00:00,  7.52it/s, loss=0.566, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 52:  99%|▉| 72/73 [00:10<00:00,  6.68it/s, loss=0.527, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 73/73 [00:10<00:00,  6.73it/s, loss=0.527, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 53:  99%|▉| 72/73 [00:11<00:00,  6.01it/s, loss=0.535, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 73/73 [00:12<00:00,  6.06it/s, loss=0.535, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 54:  99%|▉| 72/73 [00:07<00:00,  9.08it/s, loss=0.567, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 73/73 [00:08<00:00,  9.12it/s, loss=0.567, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 55:  99%|▉| 72/73 [00:09<00:00,  7.47it/s, loss=0.547, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 73/73 [00:09<00:00,  7.53it/s, loss=0.547, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 56:  99%|▉| 72/73 [00:10<00:00,  7.09it/s, loss=0.539, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 73/73 [00:10<00:00,  7.14it/s, loss=0.539, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 57:  99%|▉| 72/73 [00:08<00:00,  8.85it/s, loss=0.512, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 73/73 [00:08<00:00,  8.88it/s, loss=0.512, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 58:  99%|▉| 72/73 [00:09<00:00,  7.30it/s, loss=0.526, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 73/73 [00:09<00:00,  7.35it/s, loss=0.526, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 59:  99%|▉| 72/73 [00:11<00:00,  6.46it/s, loss=0.585, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|█| 73/73 [00:11<00:00,  6.51it/s, loss=0.585, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 60:  99%|▉| 72/73 [00:08<00:00,  8.61it/s, loss=0.545, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 60: 100%|█| 73/73 [00:08<00:00,  8.67it/s, loss=0.545, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 61:  99%|▉| 72/73 [00:07<00:00,  9.58it/s, loss=0.545, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|█| 73/73 [00:07<00:00,  9.63it/s, loss=0.545, v_num=79rd, BTC_val\u001b[A\n",
      "Epoch 62:  99%|▉| 72/73 [00:09<00:00,  7.97it/s, loss=0.522, v_num=79rd, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.563. Signaling Trainer to stop.\n",
      "Epoch 62: 100%|█| 73/73 [00:09<00:00,  8.02it/s, loss=0.522, v_num=79rd, BTC_val\n",
      "Epoch 62: 100%|█| 73/73 [00:09<00:00,  8.02it/s, loss=0.522, v_num=79rd, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 13.71it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6785714030265808,\n",
      " 'BTC_test_f1': 0.6750550866127014,\n",
      " 'ETH_test_acc': 0.7142857313156128,\n",
      " 'ETH_test_f1': 0.7108843922615051,\n",
      " 'LTC_test_acc': 0.6785714030265808,\n",
      " 'LTC_test_f1': 0.6643990874290466,\n",
      " 'test_loss': 0.5547271966934204}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 155546\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_115926-2j6g79rd/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_115926-2j6g79rd/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.54377\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 62\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 610\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621501776\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 216\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.7302\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.71943\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.7215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.71021\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc 0.7154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 0.70411\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.54623\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.56364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.46667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.73333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.60119\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.67857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.67506\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.71429\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.71088\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.67857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.6644\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.55473\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅▄▄▃▃▂▄▅▃▄▅█▃▅▂▅▄▃▄▆▆▃▄▅▆▄▆▃▃▄▄▅▃▁▂▅▄▂▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▁▅▆▆▆▆▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇█▇▇▇██████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▁▄▆▆▆▆▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇███████▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▅▇▆▆▇▇▇▇▇▇▇█▇▇▇█▇█▇██▇▇▇▇▇█▇▇▇▇█████▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▁▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇███▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc ▁▄▆▆▆▆▆▆▆▇▇▇█▇█▇▇▇▇▇▇▇█▇▇██▇█▇▇▇▇▇██▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 ▁▄▆▆▆▆▆▆▆▇▇▇▇▇█▇▇▇▇▇▇▇█▇▇██▇█▇▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▁▁████▁▁█▁█▁█▁█▁▁▁██▁▁█▁▁█▁▁████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁▁▁████▁▁█▁█▁█▁█▁▁▁██▁▁█▁▁█▁▁████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▅▅▅█▅▅▅▅▅█▁▁▁▁▅█▁▅▁█▁▁▁▁▁▁▁▁▅▅▁▁▁▁▅▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▅▅▅█▅▅▅▅▅█▁▁▁▁▅█▁▅▁█▁▁▁▁▁▁▁▁▅▅▁▁▁▁▅▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▃▃▆▃▆▃▆▆▆▃▃▃▁▁▃▆▆▃▃▃▆█▆▆▃▃▆▃▆▆▃▃▆▃▆▃▆▆▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▄▄▆▄▆▄▆▆▆▄▄▄▁▁▄▆▆▄▃▄▆█▆▆▄▄▆▃▆▆▃▄▆▃▆▄▆▆▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄█▃▃▂▆▃▃▃▃▂▃▃▃█▇▂▄▃▅▂▃▃▃▄▆▂▂▆▅▁▃▃▂▅▄▃▂▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_multi_head_attention_loss_weighted_binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2j6g79rd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 12:09:48.804152: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_multi_head_attention_loss_weighted_binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/3d26ctr3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_120947-3d26ctr3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|█| 73/73 [00:07<00:00,  9.99it/s, loss=0.672, v_num=ctr3, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.604\n",
      "Epoch 0: 100%|█| 73/73 [00:07<00:00,  9.94it/s, loss=0.672, v_num=ctr3, BTC_val_\n",
      "Epoch 1:  99%|▉| 72/73 [00:11<00:00,  6.43it/s, loss=0.63, v_num=ctr3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 73/73 [00:11<00:00,  6.47it/s, loss=0.63, v_num=ctr3, BTC_val_a\u001b[A\n",
      "Epoch 2:  99%|▉| 72/73 [00:09<00:00,  7.61it/s, loss=0.645, v_num=ctr3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 73/73 [00:09<00:00,  7.66it/s, loss=0.645, v_num=ctr3, BTC_val_\u001b[A\n",
      "Epoch 3:  99%|▉| 72/73 [00:08<00:00,  8.74it/s, loss=0.603, v_num=ctr3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|█| 73/73 [00:08<00:00,  8.75it/s, loss=0.603, v_num=ctr3, BTC_val_\u001b[A\n",
      "Epoch 4:  99%|▉| 72/73 [00:08<00:00,  8.28it/s, loss=0.614, v_num=ctr3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 73/73 [00:08<00:00,  8.33it/s, loss=0.614, v_num=ctr3, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|▉| 72/73 [00:07<00:00,  9.18it/s, loss=0.592, v_num=ctr3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.021 >= min_delta = 0.003. New best score: 0.583\n",
      "Epoch 5: 100%|█| 73/73 [00:07<00:00,  9.22it/s, loss=0.592, v_num=ctr3, BTC_val_\n",
      "Epoch 6:  99%|▉| 72/73 [00:09<00:00,  7.61it/s, loss=0.565, v_num=ctr3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 73/73 [00:09<00:00,  7.66it/s, loss=0.565, v_num=ctr3, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|▉| 72/73 [00:07<00:00,  9.10it/s, loss=0.592, v_num=ctr3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 73/73 [00:07<00:00,  9.15it/s, loss=0.592, v_num=ctr3, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|▉| 72/73 [00:09<00:00,  7.24it/s, loss=0.606, v_num=ctr3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 73/73 [00:10<00:00,  7.29it/s, loss=0.606, v_num=ctr3, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|▉| 72/73 [00:10<00:00,  6.89it/s, loss=0.601, v_num=ctr3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.012 >= min_delta = 0.003. New best score: 0.570\n",
      "Epoch 9: 100%|█| 73/73 [00:10<00:00,  6.94it/s, loss=0.601, v_num=ctr3, BTC_val_\n",
      "Epoch 10:  99%|▉| 72/73 [00:07<00:00,  9.66it/s, loss=0.561, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 73/73 [00:07<00:00,  9.70it/s, loss=0.561, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 11:  99%|▉| 72/73 [00:10<00:00,  6.79it/s, loss=0.564, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 73/73 [00:10<00:00,  6.82it/s, loss=0.564, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 12:  99%|▉| 72/73 [00:08<00:00,  8.44it/s, loss=0.572, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 73/73 [00:08<00:00,  8.48it/s, loss=0.572, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 13:  99%|▉| 72/73 [00:07<00:00,  9.50it/s, loss=0.607, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 73/73 [00:07<00:00,  9.55it/s, loss=0.607, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 14:  99%|▉| 72/73 [00:07<00:00,  9.52it/s, loss=0.574, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 73/73 [00:07<00:00,  9.56it/s, loss=0.574, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 15:  99%|▉| 72/73 [00:11<00:00,  6.34it/s, loss=0.62, v_num=ctr3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 73/73 [00:11<00:00,  6.38it/s, loss=0.62, v_num=ctr3, BTC_val_\u001b[A\n",
      "Epoch 16:  99%|▉| 72/73 [00:08<00:00,  8.16it/s, loss=0.565, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 73/73 [00:08<00:00,  8.21it/s, loss=0.565, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 17:  99%|▉| 72/73 [00:08<00:00,  8.26it/s, loss=0.584, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 73/73 [00:08<00:00,  8.29it/s, loss=0.584, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 18:  99%|▉| 72/73 [00:08<00:00,  8.21it/s, loss=0.556, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 73/73 [00:08<00:00,  8.26it/s, loss=0.556, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 19:  99%|▉| 72/73 [00:07<00:00,  9.36it/s, loss=0.57, v_num=ctr3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 73/73 [00:07<00:00,  9.41it/s, loss=0.57, v_num=ctr3, BTC_val_\u001b[A\n",
      "Epoch 20:  99%|▉| 72/73 [00:10<00:00,  6.67it/s, loss=0.599, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 73/73 [00:10<00:00,  6.71it/s, loss=0.599, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 21:  99%|▉| 72/73 [00:08<00:00,  8.70it/s, loss=0.538, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 73/73 [00:08<00:00,  8.76it/s, loss=0.538, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 22:  99%|▉| 72/73 [00:12<00:00,  5.94it/s, loss=0.56, v_num=ctr3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 73/73 [00:12<00:00,  5.99it/s, loss=0.56, v_num=ctr3, BTC_val_\u001b[A\n",
      "Epoch 23:  99%|▉| 72/73 [00:10<00:00,  7.18it/s, loss=0.581, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 73/73 [00:10<00:00,  7.23it/s, loss=0.581, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 24:  99%|▉| 72/73 [00:09<00:00,  7.54it/s, loss=0.604, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 73/73 [00:09<00:00,  7.59it/s, loss=0.604, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 25:  99%|▉| 72/73 [00:07<00:00,  9.19it/s, loss=0.573, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 73/73 [00:07<00:00,  9.24it/s, loss=0.573, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 26:  99%|▉| 72/73 [00:07<00:00,  9.54it/s, loss=0.565, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 73/73 [00:07<00:00,  9.58it/s, loss=0.565, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 27:  99%|▉| 72/73 [00:08<00:00,  8.21it/s, loss=0.583, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 73/73 [00:08<00:00,  8.26it/s, loss=0.583, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 28:  99%|▉| 72/73 [00:08<00:00,  8.08it/s, loss=0.532, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 73/73 [00:08<00:00,  8.13it/s, loss=0.532, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 29:  99%|▉| 72/73 [00:08<00:00,  8.13it/s, loss=0.569, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 73/73 [00:08<00:00,  8.17it/s, loss=0.569, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 30:  99%|▉| 72/73 [00:07<00:00,  9.54it/s, loss=0.552, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 73/73 [00:07<00:00,  9.58it/s, loss=0.552, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 31:  99%|▉| 72/73 [00:09<00:00,  7.87it/s, loss=0.56, v_num=ctr3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.022 >= min_delta = 0.003. New best score: 0.548\n",
      "Epoch 31: 100%|█| 73/73 [00:09<00:00,  7.91it/s, loss=0.56, v_num=ctr3, BTC_val_\n",
      "Epoch 32:  99%|▉| 72/73 [00:10<00:00,  6.85it/s, loss=0.56, v_num=ctr3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 73/73 [00:10<00:00,  6.89it/s, loss=0.56, v_num=ctr3, BTC_val_\u001b[A\n",
      "Epoch 33:  99%|▉| 72/73 [00:10<00:00,  7.05it/s, loss=0.547, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 73/73 [00:10<00:00,  7.10it/s, loss=0.547, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 34:  99%|▉| 72/73 [00:07<00:00,  9.21it/s, loss=0.522, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 73/73 [00:07<00:00,  9.25it/s, loss=0.522, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 35:  99%|▉| 72/73 [00:10<00:00,  6.83it/s, loss=0.569, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 73/73 [00:10<00:00,  6.88it/s, loss=0.569, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 36:  99%|▉| 72/73 [00:09<00:00,  7.83it/s, loss=0.555, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 73/73 [00:09<00:00,  7.88it/s, loss=0.555, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 37:  99%|▉| 72/73 [00:07<00:00,  9.78it/s, loss=0.557, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 73/73 [00:07<00:00,  9.81it/s, loss=0.557, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 38:  99%|▉| 72/73 [00:08<00:00,  8.98it/s, loss=0.565, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 73/73 [00:08<00:00,  9.03it/s, loss=0.565, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 39:  99%|▉| 72/73 [00:07<00:00,  9.80it/s, loss=0.56, v_num=ctr3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 73/73 [00:07<00:00,  9.82it/s, loss=0.56, v_num=ctr3, BTC_val_\u001b[A\n",
      "Epoch 40:  99%|▉| 72/73 [00:09<00:00,  7.45it/s, loss=0.537, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 73/73 [00:09<00:00,  7.50it/s, loss=0.537, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 41:  99%|▉| 72/73 [00:07<00:00,  9.03it/s, loss=0.533, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 73/73 [00:08<00:00,  9.08it/s, loss=0.533, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 42:  99%|▉| 72/73 [00:07<00:00,  9.29it/s, loss=0.563, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 73/73 [00:07<00:00,  9.34it/s, loss=0.563, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 43:  99%|▉| 72/73 [00:10<00:00,  7.02it/s, loss=0.584, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 73/73 [00:10<00:00,  7.06it/s, loss=0.584, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 44:  99%|▉| 72/73 [00:12<00:00,  5.95it/s, loss=0.541, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 73/73 [00:12<00:00,  5.98it/s, loss=0.541, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 45:  99%|▉| 72/73 [00:11<00:00,  6.15it/s, loss=0.569, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 73/73 [00:11<00:00,  6.19it/s, loss=0.569, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 46:  99%|▉| 72/73 [00:11<00:00,  6.19it/s, loss=0.544, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 73/73 [00:11<00:00,  6.24it/s, loss=0.544, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 47:  99%|▉| 72/73 [00:09<00:00,  7.90it/s, loss=0.557, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 73/73 [00:09<00:00,  7.94it/s, loss=0.557, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 48:  99%|▉| 72/73 [00:07<00:00,  9.50it/s, loss=0.568, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 73/73 [00:07<00:00,  9.55it/s, loss=0.568, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 49:  99%|▉| 72/73 [00:08<00:00,  8.10it/s, loss=0.548, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 73/73 [00:08<00:00,  8.14it/s, loss=0.548, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 50:  99%|▉| 72/73 [00:08<00:00,  8.21it/s, loss=0.561, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 73/73 [00:08<00:00,  8.25it/s, loss=0.561, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 51:  99%|▉| 72/73 [00:10<00:00,  7.02it/s, loss=0.526, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 73/73 [00:10<00:00,  7.07it/s, loss=0.526, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 52:  99%|▉| 72/73 [00:09<00:00,  7.90it/s, loss=0.542, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 73/73 [00:09<00:00,  7.93it/s, loss=0.542, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 53:  99%|▉| 72/73 [00:08<00:00,  8.07it/s, loss=0.567, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 73/73 [00:09<00:00,  8.10it/s, loss=0.567, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 54:  99%|▉| 72/73 [00:07<00:00,  9.49it/s, loss=0.586, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 73/73 [00:07<00:00,  9.54it/s, loss=0.586, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 55:  99%|▉| 72/73 [00:09<00:00,  7.86it/s, loss=0.558, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 73/73 [00:09<00:00,  7.90it/s, loss=0.558, v_num=ctr3, BTC_val\u001b[A\n",
      "Epoch 56:  99%|▉| 72/73 [00:11<00:00,  6.30it/s, loss=0.582, v_num=ctr3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.548. Signaling Trainer to stop.\n",
      "Epoch 56: 100%|█| 73/73 [00:11<00:00,  6.34it/s, loss=0.582, v_num=ctr3, BTC_val\n",
      "Epoch 56: 100%|█| 73/73 [00:11<00:00,  6.34it/s, loss=0.582, v_num=ctr3, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 23.72it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6785714030265808,\n",
      " 'BTC_test_f1': 0.6750550866127014,\n",
      " 'ETH_test_acc': 0.7142857313156128,\n",
      " 'ETH_test_f1': 0.7108843922615051,\n",
      " 'LTC_test_acc': 0.75,\n",
      " 'LTC_test_f1': 0.7454982399940491,\n",
      " 'test_loss': 0.5720449686050415}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 156833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_120947-3d26ctr3/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_120947-3d26ctr3/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.56926\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 56\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4104\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 535\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621502322\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 196\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.72672\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.71165\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.7215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.7069\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc 0.71279\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 0.6999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.54434\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.73333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.46667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.73333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.59123\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.67857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.67506\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.71429\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.71088\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.7455\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.57204\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▆▄▇▅▄▇▄▆▃▇▅▂▆▅▆▄█▆▅▃▁▂▇▆▄▃▆▃▃▃▁▅▂▂▅▂▃▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▁▅▇▆▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇█▇▇▇█▇█▇████████▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▁▆▆▇▇▇▆▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇▇█▇█▇█▇█▇█▇▇█▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇▇█▇██▇▇█▇▇█▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▁▆▆▆▆▇▇▇▇▇▇▇▇█▇▇█▇█▇▇▇▇█▇██▇▇██▇█▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc ▁▆▇▇▇▇▇█▇▇▇██▇▇██▇████▇███████████▇████▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 ▁▆▇▇▇▇▇▇▇▇▇██▇▇██▇████▇█▇█████████▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▅▁▅█▅▅▁▅▁▅▁▅▁▅▅▁▅▅▁▁▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁▅▁▅█▅▅▁▅▁▅▁▅▁▅▅▁▅▅▁▁▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc █▅▁▅███▁▁█▁▁▁▁▅▁▅▁█▁▅▁▁▅▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 █▅▁▅███▁▁█▁▁▁▁▅▁▅▁█▁▅▁▁▅▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc █▅▅▅▅█▅▅▅█▅▅▅▅█▅▅▅█▅█▁▅█▅▅██▅██▅███▅█▅██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 █▅▅▅▅█▅▅▅█▅▃▅▅█▅▅▅█▅█▁▃█▅▅██▅▆▆▅██▆▃█▅██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▃▄█▅▂▃▃▆▄▃▃▂▃▄▅▄▆▄▅▃▄▂▁▄▃▂▅▄▃▁▂▃▃▃▂▂▃▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_multi_head_attention_loss_weighted_binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/3d26ctr3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 12:18:55.588442: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_multi_head_attention_loss_weighted_multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/1hevqp8e\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_121854-1hevqp8e\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.5 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/73 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 72/73 [00:07<00:00,  9.64it/s, loss=1.13, v_num=qp8e, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.105\n",
      "Epoch 0: 100%|█| 73/73 [00:07<00:00,  9.68it/s, loss=1.13, v_num=qp8e, BTC_val_a\n",
      "Epoch 1:  99%|▉| 72/73 [00:07<00:00,  9.13it/s, loss=1.11, v_num=qp8e, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 73/73 [00:07<00:00,  9.18it/s, loss=1.11, v_num=qp8e, BTC_val_a\u001b[A\n",
      "Epoch 2:  99%|▉| 72/73 [00:08<00:00,  8.26it/s, loss=1.09, v_num=qp8e, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 73/73 [00:08<00:00,  8.31it/s, loss=1.09, v_num=qp8e, BTC_val_a\u001b[A\n",
      "Epoch 3:  99%|▉| 72/73 [00:08<00:00,  8.04it/s, loss=1.11, v_num=qp8e, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 73/73 [00:09<00:00,  8.09it/s, loss=1.11, v_num=qp8e, BTC_val_a\u001b[A\n",
      "                                                                                Metric val_loss improved by 0.014 >= min_delta = 0.003. New best score: 1.092\n",
      "Epoch 4:  99%|▉| 72/73 [00:08<00:00,  8.18it/s, loss=1.08, v_num=qp8e, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 73/73 [00:08<00:00,  8.23it/s, loss=1.08, v_num=qp8e, BTC_val_a\u001b[A\n",
      "Epoch 5:  99%|▉| 72/73 [00:10<00:00,  7.16it/s, loss=1.03, v_num=qp8e, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 1.078\n",
      "Epoch 5: 100%|█| 73/73 [00:10<00:00,  7.21it/s, loss=1.03, v_num=qp8e, BTC_val_a\n",
      "Epoch 6:  99%|▉| 72/73 [00:08<00:00,  8.60it/s, loss=1.02, v_num=qp8e, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 73/73 [00:08<00:00,  8.65it/s, loss=1.02, v_num=qp8e, BTC_val_a\u001b[A\n",
      "Epoch 7:  99%|▉| 72/73 [00:09<00:00,  7.23it/s, loss=1, v_num=qp8e, BTC_val_acc=\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 73/73 [00:10<00:00,  7.26it/s, loss=1, v_num=qp8e, BTC_val_acc=\u001b[A\n",
      "Epoch 8:  99%|▉| 72/73 [00:08<00:00,  8.85it/s, loss=1.01, v_num=qp8e, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 73/73 [00:08<00:00,  8.89it/s, loss=1.01, v_num=qp8e, BTC_val_a\u001b[A\n",
      "Epoch 9:  99%|▉| 72/73 [00:10<00:00,  6.97it/s, loss=1.01, v_num=qp8e, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 73/73 [00:10<00:00,  7.02it/s, loss=1.01, v_num=qp8e, BTC_val_a\u001b[A\n",
      "Epoch 10:  99%|▉| 72/73 [00:10<00:00,  6.68it/s, loss=0.976, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 73/73 [00:10<00:00,  6.73it/s, loss=0.976, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 11:  99%|▉| 72/73 [00:08<00:00,  8.89it/s, loss=0.96, v_num=qp8e, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 73/73 [00:08<00:00,  8.94it/s, loss=0.96, v_num=qp8e, BTC_val_\u001b[A\n",
      "Epoch 12:  99%|▉| 72/73 [00:07<00:00,  9.03it/s, loss=0.983, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 73/73 [00:08<00:00,  9.07it/s, loss=0.983, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 13:  99%|▉| 72/73 [00:07<00:00,  9.13it/s, loss=0.989, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 73/73 [00:07<00:00,  9.19it/s, loss=0.989, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 14:  99%|▉| 72/73 [00:07<00:00,  9.32it/s, loss=0.988, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 73/73 [00:07<00:00,  9.32it/s, loss=0.988, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 15:  99%|▉| 72/73 [00:06<00:00, 11.70it/s, loss=0.993, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 73/73 [00:06<00:00, 11.73it/s, loss=0.993, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 16:  99%|▉| 72/73 [00:05<00:00, 12.52it/s, loss=0.961, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 73/73 [00:05<00:00, 12.57it/s, loss=0.961, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 17:  99%|▉| 72/73 [00:05<00:00, 12.58it/s, loss=0.959, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 73/73 [00:05<00:00, 12.62it/s, loss=0.959, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 18:  99%|▉| 72/73 [00:06<00:00, 10.63it/s, loss=0.984, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 73/73 [00:06<00:00, 10.68it/s, loss=0.984, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 19:  99%|▉| 72/73 [00:05<00:00, 12.09it/s, loss=0.99, v_num=qp8e, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 73/73 [00:06<00:00, 12.12it/s, loss=0.99, v_num=qp8e, BTC_val_\u001b[A\n",
      "Epoch 20:  99%|▉| 72/73 [00:08<00:00,  8.90it/s, loss=0.968, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 73/73 [00:08<00:00,  8.93it/s, loss=0.968, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 21:  99%|▉| 72/73 [00:11<00:00,  6.22it/s, loss=0.992, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 73/73 [00:11<00:00,  6.26it/s, loss=0.992, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 22:  99%|▉| 72/73 [00:07<00:00,  9.29it/s, loss=0.99, v_num=qp8e, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 73/73 [00:07<00:00,  9.33it/s, loss=0.99, v_num=qp8e, BTC_val_\u001b[A\n",
      "Epoch 23:  99%|▉| 72/73 [00:09<00:00,  7.42it/s, loss=0.97, v_num=qp8e, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 73/73 [00:09<00:00,  7.44it/s, loss=0.97, v_num=qp8e, BTC_val_\u001b[A\n",
      "Epoch 24:  99%|▉| 72/73 [00:07<00:00,  9.27it/s, loss=0.967, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 73/73 [00:07<00:00,  9.33it/s, loss=0.967, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 25:  99%|▉| 72/73 [00:07<00:00,  9.94it/s, loss=0.955, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 73/73 [00:07<00:00, 10.01it/s, loss=0.955, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 26:  99%|▉| 72/73 [00:06<00:00, 11.28it/s, loss=0.994, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.006 >= min_delta = 0.003. New best score: 1.073\n",
      "Epoch 26: 100%|█| 73/73 [00:06<00:00, 11.34it/s, loss=0.994, v_num=qp8e, BTC_val\n",
      "Epoch 27:  99%|▉| 72/73 [00:06<00:00, 11.62it/s, loss=0.968, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 73/73 [00:06<00:00, 11.68it/s, loss=0.968, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 28:  99%|▉| 72/73 [00:05<00:00, 12.39it/s, loss=0.977, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 73/73 [00:05<00:00, 12.44it/s, loss=0.977, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 29:  99%|▉| 72/73 [00:11<00:00,  6.23it/s, loss=0.939, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 73/73 [00:11<00:00,  6.27it/s, loss=0.939, v_num=qp8e, BTC_valMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 1.064\n",
      "\n",
      "Epoch 30:  99%|▉| 72/73 [00:11<00:00,  6.15it/s, loss=0.971, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 73/73 [00:11<00:00,  6.19it/s, loss=0.971, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 31:  99%|▉| 72/73 [00:12<00:00,  5.97it/s, loss=0.953, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 73/73 [00:12<00:00,  6.02it/s, loss=0.953, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 32:  99%|▉| 72/73 [00:09<00:00,  7.50it/s, loss=0.967, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 73/73 [00:09<00:00,  7.52it/s, loss=0.967, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 33:  99%|▉| 72/73 [00:08<00:00,  8.19it/s, loss=0.955, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 73/73 [00:08<00:00,  8.24it/s, loss=0.955, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 34:  99%|▉| 72/73 [00:08<00:00,  8.54it/s, loss=0.973, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 73/73 [00:08<00:00,  8.56it/s, loss=0.973, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 35:  99%|▉| 72/73 [00:09<00:00,  7.38it/s, loss=0.977, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 73/73 [00:09<00:00,  7.44it/s, loss=0.977, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 36:  99%|▉| 72/73 [00:11<00:00,  6.12it/s, loss=0.958, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 73/73 [00:11<00:00,  6.17it/s, loss=0.958, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 37:  99%|▉| 72/73 [00:12<00:00,  5.96it/s, loss=0.95, v_num=qp8e, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 73/73 [00:12<00:00,  6.00it/s, loss=0.95, v_num=qp8e, BTC_val_\u001b[A\n",
      "Epoch 38:  99%|▉| 72/73 [00:11<00:00,  6.28it/s, loss=0.955, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 73/73 [00:11<00:00,  6.32it/s, loss=0.955, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 39:  99%|▉| 72/73 [00:11<00:00,  6.33it/s, loss=0.932, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 73/73 [00:11<00:00,  6.38it/s, loss=0.932, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 40:  99%|▉| 72/73 [00:15<00:00,  4.51it/s, loss=0.956, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 73/73 [00:16<00:00,  4.56it/s, loss=0.956, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 41:  99%|▉| 72/73 [00:12<00:00,  5.69it/s, loss=0.946, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 73/73 [00:12<00:00,  5.73it/s, loss=0.946, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 42:  99%|▉| 72/73 [00:12<00:00,  5.86it/s, loss=0.97, v_num=qp8e, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 73/73 [00:12<00:00,  5.91it/s, loss=0.97, v_num=qp8e, BTC_val_\u001b[A\n",
      "Epoch 43:  99%|▉| 72/73 [00:15<00:00,  4.65it/s, loss=0.954, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 73/73 [00:15<00:00,  4.70it/s, loss=0.954, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 44:  99%|▉| 72/73 [00:08<00:00,  8.95it/s, loss=0.907, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 73/73 [00:08<00:00,  8.99it/s, loss=0.907, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 45:  99%|▉| 72/73 [00:09<00:00,  7.71it/s, loss=0.93, v_num=qp8e, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 73/73 [00:09<00:00,  7.75it/s, loss=0.93, v_num=qp8e, BTC_val_\u001b[A\n",
      "Epoch 46:  99%|▉| 72/73 [00:11<00:00,  6.29it/s, loss=0.945, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 73/73 [00:11<00:00,  6.25it/s, loss=0.945, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 47:  99%|▉| 72/73 [00:08<00:00,  8.69it/s, loss=0.928, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 73/73 [00:08<00:00,  8.74it/s, loss=0.928, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 48:  99%|▉| 72/73 [00:07<00:00,  9.38it/s, loss=0.933, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 73/73 [00:07<00:00,  9.43it/s, loss=0.933, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 49:  99%|▉| 72/73 [00:08<00:00,  8.83it/s, loss=0.92, v_num=qp8e, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 73/73 [00:08<00:00,  8.88it/s, loss=0.92, v_num=qp8e, BTC_val_\u001b[A\n",
      "Epoch 50:  99%|▉| 72/73 [00:07<00:00,  9.66it/s, loss=0.934, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 73/73 [00:07<00:00,  9.71it/s, loss=0.934, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 51:  99%|▉| 72/73 [00:09<00:00,  7.75it/s, loss=0.945, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 73/73 [00:09<00:00,  7.77it/s, loss=0.945, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 52:  99%|▉| 72/73 [00:10<00:00,  6.89it/s, loss=0.97, v_num=qp8e, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 73/73 [00:10<00:00,  6.93it/s, loss=0.97, v_num=qp8e, BTC_val_\u001b[A\n",
      "Epoch 53:  99%|▉| 72/73 [00:09<00:00,  7.43it/s, loss=0.951, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 73/73 [00:09<00:00,  7.49it/s, loss=0.951, v_num=qp8e, BTC_val\u001b[A\n",
      "Epoch 54:  99%|▉| 72/73 [00:07<00:00,  9.81it/s, loss=0.947, v_num=qp8e, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 1.064. Signaling Trainer to stop.\n",
      "Epoch 54: 100%|█| 73/73 [00:07<00:00,  9.84it/s, loss=0.947, v_num=qp8e, BTC_val\n",
      "Epoch 54: 100%|█| 73/73 [00:07<00:00,  9.83it/s, loss=0.947, v_num=qp8e, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 29.08it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.3928571343421936,\n",
      " 'BTC_test_f1': 0.30113381147384644,\n",
      " 'ETH_test_acc': 0.5,\n",
      " 'ETH_test_f1': 0.4555381238460541,\n",
      " 'LTC_test_acc': 0.5,\n",
      " 'LTC_test_f1': 0.43877553939819336,\n",
      " 'test_loss': 0.9381815791130066}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 158072\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_121854-1hevqp8e/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_121854-1hevqp8e/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.8968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 54\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 3960\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 519\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621502853\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 189\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.40644\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.34765\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.41601\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.35916\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc 0.40905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 0.35519\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.94893\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.33333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.22857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.13333\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.06617\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.39286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.30113\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.45554\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.43878\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.93818\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▇▆▅▃▆▇▄▂▃▄▅▃▅▅▄▇▃▄▂▄▁▄▅▅▄▂▅▆▂▂▂▁▃▁▃▃▇▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▃▁▄▄▇▇▆▆▆▆▇▆▆▆▆▇▇▆▇█▆▇▆▇▇▅▇▅▇▇▇▇▇▇█▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▃▁▃▄██▇▅▇▅▇▆▇▆▆▆▆▅▇█▅▆▅▆▆▄▆▄▆▆▆▅▅▆▆▆▆▆▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▂▁▂▅▅▇▆▇▇▅▆██▇▆▇▇▆▇▇▇▇▇▆▇▆█▇▆▇▆▇█▇▇█▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▂▁▁▆▆▇▇█▇▅▆█▇▆▅▇▆▅▆▇▆▇▆▅▆▅▆▆▅▆▅▅▆▆▆▇▆▇▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc ▁▂▄▆▅▆▆▇▇▅▆█▇▆▇▆▇▆▇▇▇▇▇▇▇▇▇▇▆▆▇▆█▇▇█▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 ▁▁▃▆▆▇▇▇▇▅▇██▆▇▆▇▆▇▇▆▆▆▇▆▆▆▇▆▅▅▆▇▇▇▇▆▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▆▆▄▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ██▅▅▅▁▅▅█▅▁▅▅█▁▁▅▁▁▅▅█▅████▅██▁███▅▅███▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▇▇▁▆▆▁▅▅█▆▃▆▅█▃▃▆▃▂▆▅█▆████▅██▂███▆▅███▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc █▆▅▁▁▁▁▁▃▃▁▃▁▃▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 █▆▃▁▁▁▂▂▄▄▁▄▂▄▂▂▂▂▁▂▂▄▄▄▄▄▄▄▄▄▂▄▄▄▄▄▄▄▄▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▅▃█▁▃▃▃▃▁▃▃▃▃▁▃▃▁▃▃▁▃▁▃▁▁▁▁▃▁▁▃▁▁▁▁▃▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▇▅█▃▇▇▇▇▂▇▇▇▇▃▇▇▃▇▇▃▇▂▇▃▃▂▃▇▁▂▇▂▂▂▃▇▃▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄▄▄▃▂█▅▆▇▆▅▄▆▅▃▄▄▅▆▂▅▁▅▄▂▃▃▂▃▁▄▁▄▃▃▅▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_multi_head_attention_loss_weighted_multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/1hevqp8e\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 12:27:45.533406: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_multi_head_attention_loss_weighted_multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/2b3a2lov\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_122743-2b3a2lov\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.5 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/73 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 72/73 [00:07<00:00,  9.39it/s, loss=1.12, v_num=2lov, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 73/73 [00:07<00:00,  9.44it/s, loss=1.12, v_num=2lov, BTC_val_a\u001b[A\n",
      "Metric val_loss improved. New best score: 1.105\n",
      "Epoch 1:  99%|▉| 72/73 [00:06<00:00, 10.39it/s, loss=1.12, v_num=2lov, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.010 >= min_delta = 0.003. New best score: 1.096\n",
      "Epoch 1: 100%|█| 73/73 [00:06<00:00, 10.44it/s, loss=1.12, v_num=2lov, BTC_val_a\n",
      "Epoch 2:  99%|▉| 72/73 [00:10<00:00,  7.09it/s, loss=1.11, v_num=2lov, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 73/73 [00:10<00:00,  7.15it/s, loss=1.11, v_num=2lov, BTC_val_a\u001b[A\n",
      "Epoch 3:  99%|▉| 72/73 [00:07<00:00,  9.40it/s, loss=1.1, v_num=2lov, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 73/73 [00:07<00:00,  9.44it/s, loss=1.1, v_num=2lov, BTC_val_ac\u001b[A\n",
      "Epoch 4:  99%|▉| 72/73 [00:10<00:00,  7.13it/s, loss=1.11, v_num=2lov, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 73/73 [00:10<00:00,  7.18it/s, loss=1.11, v_num=2lov, BTC_val_a\u001b[A\n",
      "Epoch 5:  99%|▉| 72/73 [00:13<00:00,  5.45it/s, loss=1.1, v_num=2lov, BTC_val_ac\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 73/73 [00:13<00:00,  5.49it/s, loss=1.1, v_num=2lov, BTC_val_ac\u001b[A\n",
      "Epoch 6:  99%|▉| 72/73 [00:07<00:00,  9.83it/s, loss=1.11, v_num=2lov, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 73/73 [00:07<00:00,  9.88it/s, loss=1.11, v_num=2lov, BTC_val_a\u001b[A\n",
      "Epoch 7:  99%|▉| 72/73 [00:08<00:00,  8.43it/s, loss=1.07, v_num=2lov, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 73/73 [00:08<00:00,  8.49it/s, loss=1.07, v_num=2lov, BTC_val_a\u001b[A\n",
      "Epoch 8:  99%|▉| 72/73 [00:07<00:00, 10.28it/s, loss=1.03, v_num=2lov, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 73/73 [00:07<00:00, 10.33it/s, loss=1.03, v_num=2lov, BTC_val_a\u001b[A\n",
      "Epoch 9:  99%|▉| 72/73 [00:08<00:00,  8.31it/s, loss=0.995, v_num=2lov, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 73/73 [00:08<00:00,  8.35it/s, loss=0.995, v_num=2lov, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|▉| 72/73 [00:06<00:00, 10.57it/s, loss=1.01, v_num=2lov, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 73/73 [00:06<00:00, 10.63it/s, loss=1.01, v_num=2lov, BTC_val_\u001b[A\n",
      "Epoch 11:  99%|▉| 72/73 [00:08<00:00,  8.60it/s, loss=1.04, v_num=2lov, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 73/73 [00:08<00:00,  8.65it/s, loss=1.04, v_num=2lov, BTC_val_\u001b[A\n",
      "Epoch 12:  99%|▉| 72/73 [00:06<00:00, 11.87it/s, loss=1.01, v_num=2lov, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 1.088\n",
      "Epoch 12: 100%|█| 73/73 [00:06<00:00, 11.92it/s, loss=1.01, v_num=2lov, BTC_val_\n",
      "Epoch 13:  99%|▉| 72/73 [00:05<00:00, 13.97it/s, loss=1.01, v_num=2lov, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 73/73 [00:05<00:00, 14.01it/s, loss=1.01, v_num=2lov, BTC_val_\u001b[A\n",
      "Epoch 14:  99%|▉| 72/73 [00:05<00:00, 14.27it/s, loss=0.98, v_num=2lov, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 73/73 [00:05<00:00, 14.28it/s, loss=0.98, v_num=2lov, BTC_val_\u001b[A\n",
      "Epoch 15:  99%|▉| 72/73 [00:05<00:00, 14.25it/s, loss=0.982, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 73/73 [00:05<00:00, 14.30it/s, loss=0.982, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 16:  99%|▉| 72/73 [00:05<00:00, 14.16it/s, loss=0.983, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 73/73 [00:05<00:00, 14.21it/s, loss=0.983, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 17:  99%|▉| 72/73 [00:05<00:00, 14.22it/s, loss=1.02, v_num=2lov, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 73/73 [00:05<00:00, 14.19it/s, loss=1.02, v_num=2lov, BTC_val_\u001b[A\n",
      "Epoch 18:  99%|▉| 72/73 [00:07<00:00, 10.04it/s, loss=0.985, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 73/73 [00:07<00:00, 10.10it/s, loss=0.985, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 19:  99%|▉| 72/73 [00:06<00:00, 11.79it/s, loss=0.97, v_num=2lov, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 73/73 [00:06<00:00, 11.81it/s, loss=0.97, v_num=2lov, BTC_val_\u001b[A\n",
      "Epoch 20:  99%|▉| 72/73 [00:11<00:00,  6.07it/s, loss=0.997, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 73/73 [00:11<00:00,  6.11it/s, loss=0.997, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 21:  99%|▉| 72/73 [00:12<00:00,  6.00it/s, loss=0.992, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 73/73 [00:12<00:00,  6.01it/s, loss=0.992, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 22:  99%|▉| 72/73 [00:11<00:00,  6.27it/s, loss=0.97, v_num=2lov, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 73/73 [00:11<00:00,  6.31it/s, loss=0.97, v_num=2lov, BTC_val_\u001b[A\n",
      "Epoch 23:  99%|▉| 72/73 [00:10<00:00,  6.95it/s, loss=0.993, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 73/73 [00:10<00:00,  7.00it/s, loss=0.993, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 24:  99%|▉| 72/73 [00:08<00:00,  8.64it/s, loss=0.992, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.004 >= min_delta = 0.003. New best score: 1.084\n",
      "Epoch 24: 100%|█| 73/73 [00:08<00:00,  8.68it/s, loss=0.992, v_num=2lov, BTC_val\n",
      "Epoch 25:  99%|▉| 72/73 [00:07<00:00,  9.61it/s, loss=0.966, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 73/73 [00:07<00:00,  9.65it/s, loss=0.966, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 26:  99%|▉| 72/73 [00:07<00:00, 10.18it/s, loss=0.959, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 73/73 [00:07<00:00, 10.22it/s, loss=0.959, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 27:  99%|▉| 72/73 [00:07<00:00,  9.99it/s, loss=0.946, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 73/73 [00:07<00:00, 10.02it/s, loss=0.946, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 28:  99%|▉| 72/73 [00:08<00:00,  8.14it/s, loss=0.992, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 1.075\n",
      "Epoch 28: 100%|█| 73/73 [00:08<00:00,  8.18it/s, loss=0.992, v_num=2lov, BTC_val\n",
      "Epoch 29:  99%|▉| 72/73 [00:13<00:00,  5.35it/s, loss=0.985, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 73/73 [00:13<00:00,  5.38it/s, loss=0.985, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 30:  99%|▉| 72/73 [00:11<00:00,  6.09it/s, loss=0.959, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 73/73 [00:11<00:00,  6.14it/s, loss=0.959, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 31:  99%|▉| 72/73 [00:11<00:00,  6.10it/s, loss=0.978, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 73/73 [00:11<00:00,  6.15it/s, loss=0.978, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 32:  99%|▉| 72/73 [00:09<00:00,  7.24it/s, loss=0.976, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 73/73 [00:10<00:00,  7.29it/s, loss=0.976, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 33:  99%|▉| 72/73 [00:08<00:00,  8.30it/s, loss=0.952, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 73/73 [00:08<00:00,  8.32it/s, loss=0.952, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 34:  99%|▉| 72/73 [00:08<00:00,  8.79it/s, loss=0.968, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 73/73 [00:08<00:00,  8.84it/s, loss=0.968, v_num=2lov, BTC_val\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 1.071\n",
      "Epoch 35:  99%|▉| 72/73 [00:07<00:00,  9.07it/s, loss=0.966, v_num=2lov, BTC_val\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 73/73 [00:08<00:00,  9.12it/s, loss=0.966, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 36:  99%|▉| 72/73 [00:08<00:00,  8.48it/s, loss=0.954, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 73/73 [00:08<00:00,  8.37it/s, loss=0.954, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 37:  99%|▉| 72/73 [00:10<00:00,  6.86it/s, loss=0.965, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 73/73 [00:10<00:00,  6.87it/s, loss=0.965, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 38:  99%|▉| 72/73 [00:12<00:00,  5.87it/s, loss=0.979, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 73/73 [00:12<00:00,  5.91it/s, loss=0.979, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 39:  99%|▉| 72/73 [00:10<00:00,  6.57it/s, loss=0.94, v_num=2lov, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 73/73 [00:11<00:00,  6.60it/s, loss=0.94, v_num=2lov, BTC_val_\u001b[A\n",
      "Epoch 40:  99%|▉| 72/73 [00:11<00:00,  6.31it/s, loss=0.999, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 73/73 [00:11<00:00,  6.35it/s, loss=0.999, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 41:  99%|▉| 72/73 [00:09<00:00,  7.25it/s, loss=0.972, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 73/73 [00:10<00:00,  7.30it/s, loss=0.972, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 42:  99%|▉| 72/73 [00:08<00:00,  8.21it/s, loss=0.966, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 73/73 [00:08<00:00,  8.25it/s, loss=0.966, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 43:  99%|▉| 72/73 [00:08<00:00,  8.90it/s, loss=0.934, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 73/73 [00:08<00:00,  8.94it/s, loss=0.934, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 44:  99%|▉| 72/73 [00:08<00:00,  8.52it/s, loss=0.978, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 73/73 [00:08<00:00,  8.57it/s, loss=0.978, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 45:  99%|▉| 72/73 [00:11<00:00,  6.41it/s, loss=0.958, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 73/73 [00:11<00:00,  6.29it/s, loss=0.958, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 46:  99%|▉| 72/73 [00:10<00:00,  7.09it/s, loss=0.972, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 73/73 [00:10<00:00,  7.12it/s, loss=0.972, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 47:  99%|▉| 72/73 [00:09<00:00,  7.31it/s, loss=0.961, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 73/73 [00:09<00:00,  7.34it/s, loss=0.961, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 48:  99%|▉| 72/73 [00:08<00:00,  8.39it/s, loss=0.909, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 73/73 [00:08<00:00,  8.45it/s, loss=0.909, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 49:  99%|▉| 72/73 [00:07<00:00,  9.49it/s, loss=0.956, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 73/73 [00:07<00:00,  9.54it/s, loss=0.956, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 50:  99%|▉| 72/73 [00:10<00:00,  6.98it/s, loss=0.983, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 73/73 [00:10<00:00,  7.03it/s, loss=0.983, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 51:  99%|▉| 72/73 [00:10<00:00,  7.05it/s, loss=0.96, v_num=2lov, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 73/73 [00:10<00:00,  7.08it/s, loss=0.96, v_num=2lov, BTC_val_\u001b[A\n",
      "Epoch 52:  99%|▉| 72/73 [00:12<00:00,  5.74it/s, loss=0.963, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 73/73 [00:12<00:00,  5.78it/s, loss=0.963, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 53:  99%|▉| 72/73 [00:13<00:00,  5.14it/s, loss=0.942, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 73/73 [00:14<00:00,  5.18it/s, loss=0.942, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 54:  99%|▉| 72/73 [00:10<00:00,  7.19it/s, loss=0.953, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 73/73 [00:10<00:00,  7.22it/s, loss=0.953, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 55:  99%|▉| 72/73 [00:08<00:00,  8.36it/s, loss=0.958, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 73/73 [00:08<00:00,  8.40it/s, loss=0.958, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 56:  99%|▉| 72/73 [00:08<00:00,  8.26it/s, loss=0.936, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 73/73 [00:08<00:00,  8.29it/s, loss=0.936, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 57:  99%|▉| 72/73 [00:10<00:00,  6.75it/s, loss=0.965, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 73/73 [00:10<00:00,  6.80it/s, loss=0.965, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 58:  99%|▉| 72/73 [00:08<00:00,  8.93it/s, loss=0.935, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 73/73 [00:08<00:00,  8.97it/s, loss=0.935, v_num=2lov, BTC_val\u001b[A\n",
      "Epoch 59:  99%|▉| 72/73 [00:09<00:00,  7.62it/s, loss=0.932, v_num=2lov, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 1.071. Signaling Trainer to stop.\n",
      "Epoch 59: 100%|█| 73/73 [00:09<00:00,  7.66it/s, loss=0.932, v_num=2lov, BTC_val\n",
      "Epoch 59: 100%|█| 73/73 [00:09<00:00,  7.65it/s, loss=0.932, v_num=2lov, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00,  3.95it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.3571428656578064,\n",
      " 'BTC_test_f1': 0.25623583793640137,\n",
      " 'ETH_test_acc': 0.4642857015132904,\n",
      " 'ETH_test_f1': 0.43356016278266907,\n",
      " 'LTC_test_acc': 0.4285714328289032,\n",
      " 'LTC_test_f1': 0.3619047701358795,\n",
      " 'test_loss': 0.9396761059761047}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 159364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_122743-2b3a2lov/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_122743-2b3a2lov/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.92089\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 59\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4320\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 559\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621503422\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 206\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.40122\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.34582\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.42385\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.37113\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc 0.40818\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 0.3513\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.94425\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.30159\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.22222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.35714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.092\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.35714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.25624\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.46429\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.43356\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.42857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.3619\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.93968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step █▆█▇▆█▅▅▄▃▇▆▄▁▂▆▆▃▆▄▆▄▃▄▅▆▄▄▅▂▇▂▃▂▃▄▄▃▁▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▃▂▂▅▄▁▅▆▆▇▇▇▇▄█▆▅▇▇▅▇▇▇▇█▆▇▇▇▆▇██▇▇▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▃▄▂▃▁▃▇█▆▇▇██▅█▆▅▇▇▅▇██▇█▆▇▇▇▆▇▆█▆▆▇▆▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▁▄▅▆▂▆█▆▆▆▇▇▆▇█▇█▇▇▇██▇▇▇▇█▇▇▇▇▆█▇▆██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▁▂▃▃▄▃▆█▆▆▅▇▆▆▆▇▅▇▆▆▆▇▇▆▅▅▆▇▆▆▆▅▅▆▆▆▆▆▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc ▅▂▁▅▅▁▆▅▆▇▇▇▆▇▅█▆▇▇▆▇█▇▆▇▇▇▇█▇▆█▆▇█▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 ▄▂▁▃▃▃▇▆▅▆▆▇▆▇▅█▆▇▆▅▆█▇▅▇▆▆▅▇▆▆▆▅▆▆▆▆▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▇▇▇▆▆▄▃▃▃▃▃▂▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▅▁▅▅█▁▅▁▅█▅█▅██▅▅▁▅▁▁▁▁▁▅▁▁▅▅▁▁▁▅▅▁▁▁▅▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▃▁▂▅█▁▆▂▆█▆█▅██▆▆▂▆▂▂▂▂▂▅▂▂▅▅▂▂▂▅▅▂▂▂▅▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▆▁█▆▆▆▃▃▁▃▃▃▁▃▃▃▁▁▃▁▁▁▁▁▃▁▁▁▃▁▁▁▃▁▁▁▁▁▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▅▁▅▄█▄▅▅▂▅▄▄▂▄▄▄▂▂▄▂▂▂▂▂▄▂▂▂▄▂▂▂▄▂▂▂▂▂▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▄█▇▂▅▄▄▄▂▂▂▄▂▂▂▂▄▂▂▄▂▄▄▄▄▄▄▄▄▅▅▄▄▅▅▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▄▆▆▂▅▆▆▆▄▄▄▆▃▃▄▄▆▄▄▆▄▆▆▆▆▆▆▆▆██▆▆██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▃▂▃▃▂█▅▄▂▄▃▅▂▄▃▄▂▃▄▁▃▂▂▁▂▂▂▂▂▁▂▂▃▂▂▂▂▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_multi_head_attention_loss_weighted_multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/2b3a2lov\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 12:37:14.931607: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_multi_head_attention__binary_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/ki61csbl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_123713-ki61csbl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|█| 73/73 [00:06<00:00, 10.48it/s, loss=0.682, v_num=csbl, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 73/73 [00:06<00:00, 10.43it/s, loss=0.682, v_num=csbl, BTC_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved. New best score: 0.611\n",
      "Epoch 1:  99%|▉| 72/73 [00:06<00:00, 10.46it/s, loss=0.632, v_num=csbl, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 73/73 [00:06<00:00, 10.49it/s, loss=0.632, v_num=csbl, BTC_val_\u001b[A\n",
      "Epoch 2:  99%|▉| 72/73 [00:07<00:00,  9.81it/s, loss=0.621, v_num=csbl, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 73/73 [00:07<00:00,  9.85it/s, loss=0.621, v_num=csbl, BTC_val_\u001b[A\n",
      "Epoch 3:  99%|▉| 72/73 [00:07<00:00, 10.14it/s, loss=0.577, v_num=csbl, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 73/73 [00:07<00:00, 10.17it/s, loss=0.577, v_num=csbl, BTC_val_\u001b[A\n",
      "Epoch 4:  99%|▉| 72/73 [00:09<00:00,  7.71it/s, loss=0.585, v_num=csbl, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 73/73 [00:09<00:00,  7.76it/s, loss=0.585, v_num=csbl, BTC_val_\u001b[A\n",
      "                                                                                \u001b[AMetric val_loss improved by 0.028 >= min_delta = 0.003. New best score: 0.583\n",
      "Epoch 5:  99%|▉| 72/73 [00:07<00:00,  9.20it/s, loss=0.583, v_num=csbl, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 73/73 [00:07<00:00,  9.25it/s, loss=0.583, v_num=csbl, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|▉| 72/73 [00:07<00:00,  9.23it/s, loss=0.603, v_num=csbl, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 73/73 [00:07<00:00,  9.26it/s, loss=0.603, v_num=csbl, BTC_val_\u001b[A\n",
      "Epoch 7:  99%|▉| 72/73 [00:07<00:00,  9.31it/s, loss=0.612, v_num=csbl, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.005 >= min_delta = 0.003. New best score: 0.579\n",
      "Epoch 7: 100%|█| 73/73 [00:07<00:00,  9.35it/s, loss=0.612, v_num=csbl, BTC_val_\n",
      "Epoch 8:  99%|▉| 72/73 [00:10<00:00,  7.19it/s, loss=0.591, v_num=csbl, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.023 >= min_delta = 0.003. New best score: 0.556\n",
      "Epoch 8: 100%|█| 73/73 [00:10<00:00,  7.22it/s, loss=0.591, v_num=csbl, BTC_val_\n",
      "Epoch 9:  99%|▉| 72/73 [00:07<00:00,  9.34it/s, loss=0.597, v_num=csbl, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 73/73 [00:07<00:00,  9.38it/s, loss=0.597, v_num=csbl, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|▉| 72/73 [00:07<00:00,  9.29it/s, loss=0.571, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 73/73 [00:07<00:00,  9.33it/s, loss=0.571, v_num=csbl, BTC_val\u001b[A\n",
      "Epoch 11:  99%|▉| 72/73 [00:08<00:00,  8.10it/s, loss=0.59, v_num=csbl, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 73/73 [00:08<00:00,  8.15it/s, loss=0.59, v_num=csbl, BTC_val_\u001b[A\n",
      "Epoch 12:  99%|▉| 72/73 [00:07<00:00,  9.95it/s, loss=0.541, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 73/73 [00:07<00:00,  9.95it/s, loss=0.541, v_num=csbl, BTC_val\u001b[A\n",
      "Epoch 13:  99%|▉| 72/73 [00:07<00:00,  9.89it/s, loss=0.535, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 73/73 [00:07<00:00,  9.94it/s, loss=0.535, v_num=csbl, BTC_val\u001b[A\n",
      "Epoch 14:  99%|▉| 72/73 [00:11<00:00,  6.51it/s, loss=0.559, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 73/73 [00:11<00:00,  6.55it/s, loss=0.559, v_num=csbl, BTC_val\u001b[A\n",
      "Epoch 15:  99%|▉| 72/73 [00:09<00:00,  7.33it/s, loss=0.571, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 73/73 [00:09<00:00,  7.38it/s, loss=0.571, v_num=csbl, BTC_val\u001b[A\n",
      "Epoch 16:  99%|▉| 72/73 [00:09<00:00,  7.89it/s, loss=0.587, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 73/73 [00:09<00:00,  7.85it/s, loss=0.587, v_num=csbl, BTC_val\u001b[A\n",
      "Epoch 17:  99%|▉| 72/73 [00:10<00:00,  6.82it/s, loss=0.581, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 73/73 [00:10<00:00,  6.87it/s, loss=0.581, v_num=csbl, BTC_val\u001b[A\n",
      "Epoch 18:  99%|▉| 72/73 [00:11<00:00,  6.54it/s, loss=0.572, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 73/73 [00:11<00:00,  6.56it/s, loss=0.572, v_num=csbl, BTC_val\u001b[A\n",
      "Epoch 19:  99%|▉| 72/73 [00:12<00:00,  5.77it/s, loss=0.58, v_num=csbl, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 73/73 [00:12<00:00,  5.78it/s, loss=0.58, v_num=csbl, BTC_val_\u001b[A\n",
      "Epoch 20:  99%|▉| 72/73 [00:14<00:00,  5.03it/s, loss=0.539, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 73/73 [00:14<00:00,  5.08it/s, loss=0.539, v_num=csbl, BTC_val\u001b[A\n",
      "Epoch 21:  99%|▉| 72/73 [00:09<00:00,  7.21it/s, loss=0.575, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 73/73 [00:10<00:00,  7.26it/s, loss=0.575, v_num=csbl, BTC_val\u001b[A\n",
      "Epoch 22:  99%|▉| 72/73 [00:11<00:00,  6.23it/s, loss=0.573, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 73/73 [00:11<00:00,  6.28it/s, loss=0.573, v_num=csbl, BTC_val\u001b[A\n",
      "Epoch 23:  99%|▉| 72/73 [00:07<00:00, 10.03it/s, loss=0.564, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 73/73 [00:07<00:00, 10.07it/s, loss=0.564, v_num=csbl, BTC_val\u001b[A\n",
      "Epoch 24:  99%|▉| 72/73 [00:09<00:00,  7.48it/s, loss=0.577, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 73/73 [00:09<00:00,  7.52it/s, loss=0.577, v_num=csbl, BTC_val\u001b[A\n",
      "Epoch 25:  99%|▉| 72/73 [00:07<00:00,  9.08it/s, loss=0.544, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 73/73 [00:07<00:00,  9.13it/s, loss=0.544, v_num=csbl, BTC_val\u001b[A\n",
      "Epoch 26:  99%|▉| 72/73 [00:09<00:00,  7.70it/s, loss=0.543, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 73/73 [00:09<00:00,  7.75it/s, loss=0.543, v_num=csbl, BTC_val\u001b[A\n",
      "Epoch 27:  99%|▉| 72/73 [00:09<00:00,  7.87it/s, loss=0.558, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 73/73 [00:09<00:00,  7.91it/s, loss=0.558, v_num=csbl, BTC_val\u001b[A\n",
      "Epoch 28:  99%|▉| 72/73 [00:07<00:00,  9.37it/s, loss=0.543, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 73/73 [00:07<00:00,  9.41it/s, loss=0.543, v_num=csbl, BTC_val\u001b[A\n",
      "Epoch 29:  99%|▉| 72/73 [00:07<00:00,  9.39it/s, loss=0.54, v_num=csbl, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 73/73 [00:07<00:00,  9.38it/s, loss=0.54, v_num=csbl, BTC_val_\u001b[A\n",
      "Epoch 30:  99%|▉| 72/73 [00:12<00:00,  5.93it/s, loss=0.556, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 73/73 [00:12<00:00,  5.96it/s, loss=0.556, v_num=csbl, BTC_val\u001b[A\n",
      "Epoch 31:  99%|▉| 72/73 [00:13<00:00,  5.48it/s, loss=0.544, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 73/73 [00:13<00:00,  5.52it/s, loss=0.544, v_num=csbl, BTC_val\u001b[A\n",
      "Epoch 32:  99%|▉| 72/73 [00:09<00:00,  7.53it/s, loss=0.549, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 73/73 [00:09<00:00,  7.58it/s, loss=0.549, v_num=csbl, BTC_val\u001b[A\n",
      "Epoch 33:  99%|▉| 72/73 [00:07<00:00,  9.35it/s, loss=0.562, v_num=csbl, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.556. Signaling Trainer to stop.\n",
      "Epoch 33: 100%|█| 73/73 [00:07<00:00,  9.40it/s, loss=0.562, v_num=csbl, BTC_val\n",
      "Epoch 33: 100%|█| 73/73 [00:07<00:00,  9.39it/s, loss=0.562, v_num=csbl, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 39.76it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6071428656578064,\n",
      " 'BTC_test_f1': 0.6003202199935913,\n",
      " 'ETH_test_acc': 0.6428571343421936,\n",
      " 'ETH_test_f1': 0.6329985857009888,\n",
      " 'LTC_test_acc': 0.6785714030265808,\n",
      " 'LTC_test_f1': 0.6643990874290466,\n",
      " 'test_loss': 0.6182862520217896}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 160735\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_123713-ki61csbl/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_123713-ki61csbl/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.67444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 33\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2448\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 324\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621503757\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 116\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.72324\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.71014\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.72585\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.71307\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc 0.72063\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 0.7061\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.55893\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.56364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.46667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.46667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.59254\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.60714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.60032\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.64286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.633\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.67857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.6644\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.61829\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▅▅█▃▃▅▄▅▆▄▅▆▅▃█▄▃▄▇▄▆▄▆▄▆▄▄▇▅▃▆▁▅▃▃▅▂▃▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▁▆▆▆▇▆▆▇▆█▇▇▇▇▇▇▇▇█▇▇███████▇█▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▁▆▆▆▆▆▆▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇█▇▇███▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▅▆▆▇▆▇▇▆▆▆▆▇▇▇▇▇▆▆▆▇▇▆▆▇▇▇█▇▇▇▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▁▅▅▆▇▆▇▆▆▆▆▆▆▆▇▇▇▆▆▆▇▇▆▆▇▇▇█▇▇▇▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc ▁▅▇▆▇▆▆▇▇▇▇▇▇▇████▇▇▇▇█▇▇█▇▇▇█▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 ▁▅▇▆▇▆▆▆▇▇▇▇▆▇█▇██▆▇▇▇█▇▇▇▇▇▇█▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▄▄▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▁▂▂▁▁▁▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▅▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▅▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▅▅▅▅█▅██▁▅▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▅▅▅▅█▅██▁▅▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▅▅▅▅▅███▁▅▅▅█▅█▅▅▁█▅▁▅▅▅▁▅▁▅▅▁▅▅▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▅▅▅▅▅███▁▅▅▅█▅█▅▅▁█▃▁▅▅▅▁▅▁▅▅▁▅▅▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▄▇▆█▃▅▃▂▁▄▃▇█▄▆▃▅▃▇▂▄▆▅▅▄▆▃▇▅▃▄▄▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_multi_head_attention__binary_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/ki61csbl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 12:42:49.337751: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_multi_head_attention__binary_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/10wmihbx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_124247-10wmihbx\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.4 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 72/73 [00:04<00:00, 14.54it/s, loss=0.651, v_num=ihbx, BTC_val_\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 0.586\n",
      "Epoch 0: 100%|█| 73/73 [00:05<00:00, 14.59it/s, loss=0.651, v_num=ihbx, BTC_val_\n",
      "Epoch 1:  99%|▉| 72/73 [00:05<00:00, 13.99it/s, loss=0.591, v_num=ihbx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 73/73 [00:05<00:00, 14.05it/s, loss=0.591, v_num=ihbx, BTC_val_\u001b[A\n",
      "Epoch 2:  99%|▉| 72/73 [00:05<00:00, 12.99it/s, loss=0.597, v_num=ihbx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 73/73 [00:05<00:00, 13.04it/s, loss=0.597, v_num=ihbx, BTC_val_\u001b[A\n",
      "Epoch 3:  99%|▉| 72/73 [00:07<00:00,  9.98it/s, loss=0.598, v_num=ihbx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 73/73 [00:07<00:00, 10.01it/s, loss=0.598, v_num=ihbx, BTC_val_\u001b[A\n",
      "Epoch 4:  99%|▉| 72/73 [00:12<00:00,  5.54it/s, loss=0.601, v_num=ihbx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 73/73 [00:13<00:00,  5.58it/s, loss=0.601, v_num=ihbx, BTC_val_\u001b[A\n",
      "Epoch 5:  99%|▉| 72/73 [00:11<00:00,  6.48it/s, loss=0.593, v_num=ihbx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 73/73 [00:11<00:00,  6.52it/s, loss=0.593, v_num=ihbx, BTC_val_\u001b[A\n",
      "Epoch 6:  99%|▉| 72/73 [00:13<00:00,  5.52it/s, loss=0.603, v_num=ihbx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 73/73 [00:13<00:00,  5.52it/s, loss=0.603, v_num=ihbx, BTC_val_\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  99%|▉| 72/73 [00:12<00:00,  5.55it/s, loss=0.602, v_num=ihbx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 73/73 [00:13<00:00,  5.40it/s, loss=0.602, v_num=ihbx, BTC_val_\u001b[A\n",
      "Epoch 8:  99%|▉| 72/73 [00:08<00:00,  8.54it/s, loss=0.576, v_num=ihbx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 73/73 [00:08<00:00,  8.59it/s, loss=0.576, v_num=ihbx, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|▉| 72/73 [00:08<00:00,  8.02it/s, loss=0.579, v_num=ihbx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.022 >= min_delta = 0.003. New best score: 0.565\n",
      "Epoch 9: 100%|█| 73/73 [00:09<00:00,  8.07it/s, loss=0.579, v_num=ihbx, BTC_val_\n",
      "Epoch 10:  99%|▉| 72/73 [00:13<00:00,  5.19it/s, loss=0.553, v_num=ihbx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 73/73 [00:13<00:00,  5.23it/s, loss=0.553, v_num=ihbx, BTC_val\u001b[A\n",
      "Epoch 11:  99%|▉| 72/73 [00:12<00:00,  5.75it/s, loss=0.592, v_num=ihbx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 73/73 [00:12<00:00,  5.79it/s, loss=0.592, v_num=ihbx, BTC_val\u001b[A\n",
      "Epoch 12:  99%|▉| 72/73 [00:09<00:00,  7.27it/s, loss=0.623, v_num=ihbx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 73/73 [00:09<00:00,  7.32it/s, loss=0.623, v_num=ihbx, BTC_val\u001b[A\n",
      "Epoch 13:  99%|▉| 72/73 [00:06<00:00, 10.56it/s, loss=0.583, v_num=ihbx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 73/73 [00:06<00:00, 10.59it/s, loss=0.583, v_num=ihbx, BTC_val\u001b[A\n",
      "Epoch 14:  99%|▉| 72/73 [00:09<00:00,  7.38it/s, loss=0.55, v_num=ihbx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 73/73 [00:09<00:00,  7.44it/s, loss=0.55, v_num=ihbx, BTC_val_\u001b[A\n",
      "Epoch 15:  99%|▉| 72/73 [00:08<00:00,  8.57it/s, loss=0.555, v_num=ihbx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 73/73 [00:08<00:00,  8.63it/s, loss=0.555, v_num=ihbx, BTC_val\u001b[A\n",
      "Epoch 16:  99%|▉| 72/73 [00:07<00:00,  9.27it/s, loss=0.561, v_num=ihbx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 73/73 [00:07<00:00,  9.32it/s, loss=0.561, v_num=ihbx, BTC_val\u001b[A\n",
      "Epoch 17:  99%|▉| 72/73 [00:07<00:00, 10.12it/s, loss=0.586, v_num=ihbx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 73/73 [00:07<00:00, 10.17it/s, loss=0.586, v_num=ihbx, BTC_val\u001b[A\n",
      "Epoch 18:  99%|▉| 72/73 [00:06<00:00, 10.52it/s, loss=0.57, v_num=ihbx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 73/73 [00:06<00:00, 10.57it/s, loss=0.57, v_num=ihbx, BTC_val_\u001b[A\n",
      "Epoch 19:  99%|▉| 72/73 [00:06<00:00, 10.58it/s, loss=0.612, v_num=ihbx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 73/73 [00:06<00:00, 10.62it/s, loss=0.612, v_num=ihbx, BTC_val\u001b[A\n",
      "Epoch 20:  99%|▉| 72/73 [00:07<00:00,  9.23it/s, loss=0.579, v_num=ihbx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 73/73 [00:07<00:00,  9.28it/s, loss=0.579, v_num=ihbx, BTC_val\u001b[A\n",
      "Epoch 21:  99%|▉| 72/73 [00:09<00:00,  7.49it/s, loss=0.57, v_num=ihbx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 73/73 [00:09<00:00,  7.54it/s, loss=0.57, v_num=ihbx, BTC_val_\u001b[A\n",
      "Epoch 22:  99%|▉| 72/73 [00:08<00:00,  8.59it/s, loss=0.585, v_num=ihbx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 73/73 [00:08<00:00,  8.64it/s, loss=0.585, v_num=ihbx, BTC_val\u001b[A\n",
      "Epoch 23:  99%|▉| 72/73 [00:06<00:00, 11.26it/s, loss=0.571, v_num=ihbx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 73/73 [00:06<00:00, 11.29it/s, loss=0.571, v_num=ihbx, BTC_val\u001b[A\n",
      "Epoch 24:  99%|▉| 72/73 [00:06<00:00, 10.41it/s, loss=0.562, v_num=ihbx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 73/73 [00:06<00:00, 10.45it/s, loss=0.562, v_num=ihbx, BTC_val\u001b[A\n",
      "Epoch 25:  99%|▉| 72/73 [00:09<00:00,  7.58it/s, loss=0.539, v_num=ihbx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 73/73 [00:09<00:00,  7.63it/s, loss=0.539, v_num=ihbx, BTC_val\u001b[A\n",
      "Epoch 26:  99%|▉| 72/73 [00:10<00:00,  7.12it/s, loss=0.542, v_num=ihbx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 73/73 [00:10<00:00,  7.18it/s, loss=0.542, v_num=ihbx, BTC_val\u001b[A\n",
      "Epoch 27:  99%|▉| 72/73 [00:08<00:00,  8.25it/s, loss=0.543, v_num=ihbx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 73/73 [00:08<00:00,  8.29it/s, loss=0.543, v_num=ihbx, BTC_val\u001b[A\n",
      "Epoch 28:  99%|▉| 72/73 [00:09<00:00,  7.41it/s, loss=0.577, v_num=ihbx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 73/73 [00:09<00:00,  7.46it/s, loss=0.577, v_num=ihbx, BTC_val\u001b[A\n",
      "Epoch 29:  99%|▉| 72/73 [00:08<00:00,  8.15it/s, loss=0.516, v_num=ihbx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 73/73 [00:08<00:00,  8.21it/s, loss=0.516, v_num=ihbx, BTC_val\u001b[A\n",
      "Epoch 30:  99%|▉| 72/73 [00:07<00:00,  9.57it/s, loss=0.576, v_num=ihbx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 73/73 [00:07<00:00,  9.62it/s, loss=0.576, v_num=ihbx, BTC_val\u001b[A\n",
      "Epoch 31:  99%|▉| 72/73 [00:06<00:00, 10.89it/s, loss=0.573, v_num=ihbx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 73/73 [00:06<00:00, 10.94it/s, loss=0.573, v_num=ihbx, BTC_val\u001b[A\n",
      "Epoch 32:  99%|▉| 72/73 [00:09<00:00,  7.22it/s, loss=0.57, v_num=ihbx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 73/73 [00:10<00:00,  7.27it/s, loss=0.57, v_num=ihbx, BTC_val_\u001b[A\n",
      "Epoch 33:  99%|▉| 72/73 [00:08<00:00,  8.22it/s, loss=0.56, v_num=ihbx, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 73/73 [00:08<00:00,  8.27it/s, loss=0.56, v_num=ihbx, BTC_val_\u001b[A\n",
      "Epoch 34:  99%|▉| 72/73 [00:07<00:00,  9.56it/s, loss=0.589, v_num=ihbx, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 0.565. Signaling Trainer to stop.\n",
      "Epoch 34: 100%|█| 73/73 [00:07<00:00,  9.60it/s, loss=0.589, v_num=ihbx, BTC_val\n",
      "Epoch 34: 100%|█| 73/73 [00:07<00:00,  9.59it/s, loss=0.589, v_num=ihbx, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 21.94it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.6428571343421936,\n",
      " 'BTC_test_f1': 0.6344671249389648,\n",
      " 'ETH_test_acc': 0.6071428656578064,\n",
      " 'ETH_test_f1': 0.5980335474014282,\n",
      " 'LTC_test_acc': 0.6785714030265808,\n",
      " 'LTC_test_f1': 0.6778711676597595,\n",
      " 'test_loss': 0.6061872839927673}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 161633\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_124247-10wmihbx/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_124247-10wmihbx/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.69728\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 34\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2520\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 320\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621504087\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 120\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.71976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.70812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.71018\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.69069\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc 0.72324\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 0.7053\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.56479\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.56364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.46667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.61905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 0.62054\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.64286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.63447\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.60714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.59803\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.67857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.67787\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 0.60619\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▇▄▃▄▃▅▂▄▇▆▅▂▃▆▁▇▆▅▆▅▄▃█▅▄▃▇▅▇▆▂▃▆▅▃▃▂▅▁▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▁▅▆▇▇▆▇▆▇▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▁▅▆▇▇▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▅▆▆▇▇▇▇▇▇▇▇▇█▇▇▇▇█▇▇▇████▇██▇████▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▁▅▆▆▇▇▇▇▇▇▇▇▇█▇▇▇▇█▇▇▇██▇▇▇██▇████▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc ▁▆▆▇▇▇▇▇▆▇▇▇▇▇▇▇▇███▇▇█▇▇▇███▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 ▁▅▆▇▇▇▇▇▆▇▇▆▇▇▇▇▇▇██▇▇▇█▇▇█▇█▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▄▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▁█▁███▁▁█▁▁██▁████▁█▁█▁▁██▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁▁█▁███▁▁█▁▁██▁████▁█▁█▁▁██▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc █▅▅▅▅▅▅██▁█▁▁▁▅█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 █▅▅▅▅▅▅██▁█▁▁▁▅█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▅▅███▅█▅█▅▅▅▅▅▅█▁██▁▅▅▅▅▅▅▅▁█▅▅█▁▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▅▅███▅█▅█▅▅▅▅▅▅█▁██▁▅▅▅▅▅▅▅▁█▅▅█▁▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▂█▄▄▄▆▃▂▄▁▃▂▃▂▄▅▃▅▇▄▆▇▅▅▃▅▄▄▆▄▄▅▂▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_multi_head_attention__binary_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/10wmihbx\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maysenurk\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 12:48:19.361951: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_multi_head_attention__multi_classification_trend_removed\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/38svhfww\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_124817-38svhfww\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.5 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0: 100%|█| 73/73 [00:10<00:00,  6.94it/s, loss=1.06, v_num=hfww, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.038\n",
      "Epoch 0: 100%|█| 73/73 [00:10<00:00,  6.91it/s, loss=1.06, v_num=hfww, BTC_val_a\n",
      "Epoch 1:  99%|▉| 72/73 [00:09<00:00,  7.38it/s, loss=1.07, v_num=hfww, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 1.031\n",
      "Epoch 1: 100%|█| 73/73 [00:09<00:00,  7.43it/s, loss=1.07, v_num=hfww, BTC_val_a\n",
      "Epoch 2:  99%|▉| 72/73 [00:06<00:00, 11.46it/s, loss=1.04, v_num=hfww, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 73/73 [00:06<00:00, 11.49it/s, loss=1.04, v_num=hfww, BTC_val_a\u001b[A\n",
      "Epoch 3:  99%|▉| 72/73 [00:06<00:00, 11.81it/s, loss=1.04, v_num=hfww, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 73/73 [00:06<00:00, 11.86it/s, loss=1.04, v_num=hfww, BTC_val_a\u001b[A\n",
      "Epoch 4:  99%|▉| 72/73 [00:07<00:00, 10.16it/s, loss=1.04, v_num=hfww, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 73/73 [00:07<00:00, 10.21it/s, loss=1.04, v_num=hfww, BTC_val_a\u001b[A\n",
      "Epoch 5:  99%|▉| 72/73 [00:05<00:00, 12.16it/s, loss=1.02, v_num=hfww, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 73/73 [00:05<00:00, 12.20it/s, loss=1.02, v_num=hfww, BTC_val_a\u001b[A\n",
      "Epoch 6:  99%|▉| 72/73 [00:08<00:00,  8.53it/s, loss=1.03, v_num=hfww, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.008 >= min_delta = 0.003. New best score: 1.023\n",
      "Epoch 6: 100%|█| 73/73 [00:08<00:00,  8.57it/s, loss=1.03, v_num=hfww, BTC_val_a\n",
      "Epoch 7:  99%|▉| 72/73 [00:08<00:00,  8.56it/s, loss=1.02, v_num=hfww, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 1.014\n",
      "Epoch 7: 100%|█| 73/73 [00:08<00:00,  8.61it/s, loss=1.02, v_num=hfww, BTC_val_a\n",
      "Epoch 8:  99%|▉| 72/73 [00:06<00:00, 10.46it/s, loss=0.974, v_num=hfww, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 73/73 [00:06<00:00, 10.50it/s, loss=0.974, v_num=hfww, BTC_val_\u001b[A\n",
      "Epoch 9:  99%|▉| 72/73 [00:06<00:00, 10.60it/s, loss=0.982, v_num=hfww, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 73/73 [00:06<00:00, 10.65it/s, loss=0.982, v_num=hfww, BTC_val_\u001b[A\n",
      "Epoch 10:  99%|▉| 72/73 [00:06<00:00, 10.64it/s, loss=0.956, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 73/73 [00:06<00:00, 10.69it/s, loss=0.956, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 11:  99%|▉| 72/73 [00:06<00:00, 10.54it/s, loss=0.958, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 73/73 [00:06<00:00, 10.58it/s, loss=0.958, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 12:  99%|▉| 72/73 [00:06<00:00, 10.55it/s, loss=0.974, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 73/73 [00:06<00:00, 10.59it/s, loss=0.974, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 13:  99%|▉| 72/73 [00:06<00:00, 10.55it/s, loss=0.969, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 73/73 [00:06<00:00, 10.59it/s, loss=0.969, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 14:  99%|▉| 72/73 [00:09<00:00,  7.60it/s, loss=0.928, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 73/73 [00:09<00:00,  7.65it/s, loss=0.928, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 15:  99%|▉| 72/73 [00:06<00:00, 11.02it/s, loss=0.96, v_num=hfww, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 73/73 [00:06<00:00, 11.07it/s, loss=0.96, v_num=hfww, BTC_val_\u001b[A\n",
      "Epoch 16:  99%|▉| 72/73 [00:08<00:00,  8.38it/s, loss=0.961, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 73/73 [00:08<00:00,  8.43it/s, loss=0.961, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 17:  99%|▉| 72/73 [00:07<00:00, 10.13it/s, loss=0.946, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 73/73 [00:07<00:00, 10.16it/s, loss=0.946, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 18:  99%|▉| 72/73 [00:06<00:00, 10.29it/s, loss=0.974, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 73/73 [00:07<00:00, 10.32it/s, loss=0.974, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 19:  99%|▉| 72/73 [00:07<00:00,  9.82it/s, loss=0.958, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 73/73 [00:07<00:00,  9.85it/s, loss=0.958, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 20:  99%|▉| 72/73 [00:09<00:00,  7.21it/s, loss=0.958, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 73/73 [00:10<00:00,  7.26it/s, loss=0.958, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 21:  99%|▉| 72/73 [00:07<00:00,  9.29it/s, loss=0.951, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 73/73 [00:07<00:00,  9.33it/s, loss=0.951, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 22:  99%|▉| 72/73 [00:08<00:00,  8.31it/s, loss=0.936, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 73/73 [00:08<00:00,  8.36it/s, loss=0.936, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 23:  99%|▉| 72/73 [00:06<00:00, 11.36it/s, loss=0.942, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 73/73 [00:06<00:00, 11.39it/s, loss=0.942, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 24:  99%|▉| 72/73 [00:07<00:00,  9.18it/s, loss=0.956, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 73/73 [00:07<00:00,  9.24it/s, loss=0.956, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 25:  99%|▉| 72/73 [00:07<00:00, 10.13it/s, loss=0.937, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 73/73 [00:07<00:00, 10.18it/s, loss=0.937, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 26:  99%|▉| 72/73 [00:08<00:00,  8.23it/s, loss=0.973, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 73/73 [00:08<00:00,  8.29it/s, loss=0.973, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 27:  99%|▉| 72/73 [00:06<00:00, 11.95it/s, loss=0.988, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 73/73 [00:06<00:00, 12.00it/s, loss=0.988, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 28:  99%|▉| 72/73 [00:05<00:00, 13.76it/s, loss=0.948, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 73/73 [00:05<00:00, 13.80it/s, loss=0.948, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 29:  99%|▉| 72/73 [00:05<00:00, 14.34it/s, loss=0.952, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 73/73 [00:05<00:00, 14.37it/s, loss=0.952, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 30:  99%|▉| 72/73 [00:04<00:00, 14.61it/s, loss=0.956, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 73/73 [00:05<00:00, 14.57it/s, loss=0.956, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 31:  99%|▉| 72/73 [00:06<00:00, 11.93it/s, loss=0.928, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 73/73 [00:06<00:00, 11.99it/s, loss=0.928, v_num=hfww, BTC_val\u001b[A\n",
      "Epoch 32:  99%|▉| 72/73 [00:05<00:00, 14.13it/s, loss=0.935, v_num=hfww, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 1.014. Signaling Trainer to stop.\n",
      "Epoch 32: 100%|█| 73/73 [00:05<00:00, 14.17it/s, loss=0.935, v_num=hfww, BTC_val\n",
      "Epoch 32: 100%|█| 73/73 [00:05<00:00, 14.16it/s, loss=0.935, v_num=hfww, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 37.50it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5357142686843872,\n",
      " 'BTC_test_f1': 0.2323809713125229,\n",
      " 'ETH_test_acc': 0.5,\n",
      " 'ETH_test_f1': 0.2212051898241043,\n",
      " 'LTC_test_acc': 0.4285714328289032,\n",
      " 'LTC_test_f1': 0.1999756544828415,\n",
      " 'test_loss': 1.001727819442749}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 162855\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_124817-38svhfww/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_124817-38svhfww/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.97067\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2376\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 252\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621504349\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 113\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.48738\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.26992\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.47781\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.2597\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc 0.47781\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 0.27237\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.93992\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.18182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.22222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.25641\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.0731\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.53571\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.23238\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.22121\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.42857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.19998\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.00173\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▅▄▅▇▄▅▆▄▆▅▆▃▅▃▅▄▃▄▃▂▃▄▃▄█▃▂▂▄▂▃▅▃▄▁▄▃▅▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▁▅▆▅▆▇▆▆▆▇█▆▅▇▇█▇▆▇▆▆▅▇▆▇▆▆▇▇▆▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▄▃▃▂▁▁▁▂▃▆█▄▄▆▄▅▃▂▃▃▂▄▄▂▄▂▃▂▄▄▁▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▅▆▆▇█▇▇▇▆▅▇▆▇▆▇▇▇▇▇▇▆██▇▇▇█▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▆▄▃▁▂▁▁▂▅██▆▇▇▅▇▃▄▃▄▃▆▆▆▃▄▅▂▆▇▂▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc ▁▆▇▆▆▇▇▆▆▇▇▆▇▇▇▇▇▇▇▇▇▆▇▇███▇▇▆▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 ▅▅▃▂▂▂▁▁▃▇█▅▅▅▄▆▄▄▃▄▃▆▄▄▄▄▅▂▅▅▁▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▅▅▅▅▅▅▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▅▅▅▅▅▅▅▅▅█▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▂▂▂▂▂▂▂▂▂█▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ██████████████▁████████████▁█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▃▃▃▃▃▃▃▃▃█▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▁▃▃▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc █████████▁████▁████████████▁█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂█▂▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▃▂▂▂▂▂▂▁▃▄▇█▄▇▇▁▃▄▂▃▄▇▂▃▂▃▇▇▃▅▂▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_multi_head_attention__multi_classification_trend_removed\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/38svhfww\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
      "2021-05-20 12:52:46.441165: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti_task_BTC_ETH_LTC_multi_head_attention__multi_classification_\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aysenurk/price_change_2/runs/37qp4ff3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_125244-37qp4ff3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Linear             | 128   \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 133 K \n",
      "3 | output_net          | ModuleList         | 4.5 K \n",
      "4 | f1_score            | F1                 | 0     \n",
      "5 | accuracy_score      | Accuracy           | 0     \n",
      "6 | cross_entropy_loss  | ModuleList         | 0     \n",
      "-----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Validation sanity check:   0%|                            | 0/1 [00:00<?, ?it/s]/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  99%|▉| 72/73 [00:04<00:00, 17.52it/s, loss=1.08, v_num=4ff3, BTC_val_a\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved. New best score: 1.034\n",
      "Epoch 0: 100%|█| 73/73 [00:04<00:00, 17.54it/s, loss=1.08, v_num=4ff3, BTC_val_a\n",
      "Epoch 1:  99%|▉| 72/73 [00:04<00:00, 14.90it/s, loss=1.04, v_num=4ff3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 73/73 [00:04<00:00, 14.94it/s, loss=1.04, v_num=4ff3, BTC_val_a\u001b[A\n",
      "Epoch 2:  99%|▉| 72/73 [00:04<00:00, 16.39it/s, loss=1.04, v_num=4ff3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 73/73 [00:04<00:00, 16.42it/s, loss=1.04, v_num=4ff3, BTC_val_a\u001b[A\n",
      "Epoch 3:  99%|▉| 72/73 [00:04<00:00, 15.15it/s, loss=1.04, v_num=4ff3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 73/73 [00:04<00:00, 15.15it/s, loss=1.04, v_num=4ff3, BTC_val_a\u001b[A\n",
      "Epoch 4:  99%|▉| 72/73 [00:04<00:00, 15.13it/s, loss=1.02, v_num=4ff3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 73/73 [00:04<00:00, 15.16it/s, loss=1.02, v_num=4ff3, BTC_val_a\u001b[A\n",
      "Epoch 5:  99%|▉| 72/73 [00:05<00:00, 13.82it/s, loss=1.07, v_num=4ff3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 73/73 [00:05<00:00, 13.86it/s, loss=1.07, v_num=4ff3, BTC_val_a\u001b[A\n",
      "Epoch 6:  99%|▉| 72/73 [00:05<00:00, 13.92it/s, loss=1.04, v_num=4ff3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 73/73 [00:05<00:00, 13.98it/s, loss=1.04, v_num=4ff3, BTC_val_a\u001b[A\n",
      "Epoch 7:  99%|▉| 72/73 [00:05<00:00, 13.81it/s, loss=1.04, v_num=4ff3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.003 >= min_delta = 0.003. New best score: 1.030\n",
      "Epoch 7: 100%|█| 73/73 [00:05<00:00, 13.76it/s, loss=1.04, v_num=4ff3, BTC_val_a\n",
      "Epoch 8:  99%|▉| 72/73 [00:04<00:00, 15.53it/s, loss=1.02, v_num=4ff3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMetric val_loss improved by 0.011 >= min_delta = 0.003. New best score: 1.019\n",
      "Epoch 8: 100%|█| 73/73 [00:04<00:00, 15.56it/s, loss=1.02, v_num=4ff3, BTC_val_a\n",
      "Epoch 9:  99%|▉| 72/73 [00:06<00:00, 10.44it/s, loss=1.01, v_num=4ff3, BTC_val_a\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 73/73 [00:06<00:00, 10.49it/s, loss=1.01, v_num=4ff3, BTC_val_aMetric val_loss improved by 0.013 >= min_delta = 0.003. New best score: 1.007\n",
      "\n",
      "Epoch 10:  99%|▉| 72/73 [00:05<00:00, 12.19it/s, loss=1.01, v_num=4ff3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 73/73 [00:05<00:00, 12.23it/s, loss=1.01, v_num=4ff3, BTC_val_\u001b[A\n",
      "Epoch 11:  99%|▉| 72/73 [00:06<00:00, 11.76it/s, loss=0.977, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 73/73 [00:06<00:00, 11.79it/s, loss=0.977, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 12:  99%|▉| 72/73 [00:06<00:00, 11.27it/s, loss=0.963, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 73/73 [00:06<00:00, 11.31it/s, loss=0.963, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 13:  99%|▉| 72/73 [00:06<00:00, 10.88it/s, loss=0.977, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 73/73 [00:06<00:00, 10.92it/s, loss=0.977, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 14:  99%|▉| 72/73 [00:06<00:00, 11.04it/s, loss=0.966, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 73/73 [00:06<00:00, 11.08it/s, loss=0.966, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 15:  99%|▉| 72/73 [00:07<00:00,  9.83it/s, loss=0.967, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 73/73 [00:07<00:00,  9.88it/s, loss=0.967, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 16:  99%|▉| 72/73 [00:06<00:00, 11.55it/s, loss=0.942, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 73/73 [00:06<00:00, 11.58it/s, loss=0.942, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 17:  99%|▉| 72/73 [00:07<00:00,  9.79it/s, loss=0.945, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 73/73 [00:07<00:00,  9.83it/s, loss=0.945, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 18:  99%|▉| 72/73 [00:06<00:00, 11.86it/s, loss=0.939, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 73/73 [00:06<00:00, 11.89it/s, loss=0.939, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 19:  99%|▉| 72/73 [00:06<00:00, 11.12it/s, loss=0.957, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 73/73 [00:06<00:00, 11.15it/s, loss=0.957, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 20:  99%|▉| 72/73 [00:06<00:00, 10.93it/s, loss=0.934, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 73/73 [00:06<00:00, 10.87it/s, loss=0.934, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 21:  99%|▉| 72/73 [00:08<00:00,  8.91it/s, loss=0.976, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 73/73 [00:08<00:00,  8.96it/s, loss=0.976, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 22:  99%|▉| 72/73 [00:07<00:00,  9.37it/s, loss=0.955, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 73/73 [00:07<00:00,  9.43it/s, loss=0.955, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 23:  99%|▉| 72/73 [00:07<00:00, 10.09it/s, loss=0.957, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 73/73 [00:07<00:00, 10.12it/s, loss=0.957, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 24:  99%|▉| 72/73 [00:06<00:00, 11.08it/s, loss=0.942, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 73/73 [00:06<00:00, 11.11it/s, loss=0.942, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 25:  99%|▉| 72/73 [00:06<00:00, 11.08it/s, loss=0.959, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 73/73 [00:06<00:00, 11.11it/s, loss=0.959, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 26:  99%|▉| 72/73 [00:06<00:00, 10.99it/s, loss=0.954, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 73/73 [00:06<00:00, 11.04it/s, loss=0.954, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 27:  99%|▉| 72/73 [00:06<00:00, 10.99it/s, loss=0.93, v_num=4ff3, BTC_val_\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 73/73 [00:06<00:00, 11.04it/s, loss=0.93, v_num=4ff3, BTC_val_\u001b[A\n",
      "Epoch 28:  99%|▉| 72/73 [00:06<00:00, 11.04it/s, loss=0.931, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 73/73 [00:06<00:00, 11.08it/s, loss=0.931, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 29:  99%|▉| 72/73 [00:08<00:00,  8.46it/s, loss=0.957, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 73/73 [00:08<00:00,  8.52it/s, loss=0.957, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 30:  99%|▉| 72/73 [00:07<00:00,  9.50it/s, loss=0.935, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 73/73 [00:07<00:00,  9.55it/s, loss=0.935, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 31:  99%|▉| 72/73 [00:09<00:00,  7.95it/s, loss=0.923, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 73/73 [00:09<00:00,  8.00it/s, loss=0.923, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 32:  99%|▉| 72/73 [00:08<00:00,  8.61it/s, loss=0.961, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 73/73 [00:08<00:00,  8.66it/s, loss=0.961, v_num=4ff3, BTC_val\u001b[A\n",
      "Epoch 33:  99%|▉| 72/73 [00:08<00:00,  8.45it/s, loss=0.962, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 73/73 [00:08<00:00,  8.50it/s, loss=0.962, v_num=4ff3, BTC_val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34:  99%|▉| 72/73 [00:08<00:00,  8.50it/s, loss=0.971, v_num=4ff3, BTC_val\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[AMonitored metric val_loss did not improve in the last 25 records. Best score: 1.007. Signaling Trainer to stop.\n",
      "Epoch 34: 100%|█| 73/73 [00:08<00:00,  8.56it/s, loss=0.971, v_num=4ff3, BTC_val\n",
      "Epoch 34: 100%|█| 73/73 [00:08<00:00,  8.55it/s, loss=0.971, v_num=4ff3, BTC_val\u001b[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|████████████████████████████████████| 2/2 [00:00<00:00, 29.24it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5357142686843872,\n",
      " 'BTC_test_f1': 0.2323809713125229,\n",
      " 'ETH_test_acc': 0.5,\n",
      " 'ETH_test_f1': 0.2212051898241043,\n",
      " 'LTC_test_acc': 0.4285714328289032,\n",
      " 'LTC_test_f1': 0.1999756544828415,\n",
      " 'test_loss': 1.0075188875198364}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 163522\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_125244-37qp4ff3/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/aysenurk/Projects/OzU/CS_540_MLF/multi_task_price_change_prediction/notebooks/wandb/run-20210520_125244-37qp4ff3/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step 0.88783\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 34\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2520\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 239\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1621504603\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 120\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc 0.48651\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 0.24687\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc 0.49086\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 0.26132\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc 0.4839\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 0.25356\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch 0.93924\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 0.18182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 0.22222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 0.25641\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss 1.03202\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc 0.53571\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 0.23238\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 0.22121\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc 0.42857\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 0.19998\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss 1.00752\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_step ▆▄▄▇▅▄▇▆█▃▆▃▅▄▂▆▄▄▄▂▄▂▄▃▃▅▃▂▄▃▃▃▂▂▂▅▁▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         BTC_train_acc ▁▅▅▆▅▆▆▇▇▆▆▆▆▅▇▆▅▅▆▆█▅▇▅▆▆▅▆▇▅▆▆▇▆▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_train_f1 ▆▆▃▃▃▁▁▁▄▃▂▄▆▆█▇▅▅▆▄▅▄▃▆▄▄▄▃▇▂▂▃▇▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         ETH_train_acc ▁▇▇▇▇▇▆▇▇▆▇▅▄▅▆█▇▆▇█▆▇▇▄▆▆▅▇▇▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_train_f1 ▇▇▄▃▄▂▁▁▃▄▁▃▆▅▇▆█▇▇▆▃▄▂▅▅▂▄▄▆▃▃▃▄▃▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         LTC_train_acc ▁▃▅▅▅▇▇▇▇▅▆▇▃▇▄▇▅▇▆▆▄▇▇▇▆▇▆█▇▄▆▅▅▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_train_f1 █▅▃▂▂▂▂▁▃▃▁▅▅▆▆▆▇█▅▅▃▄▁▇▃▃▅▄▆▂▂▂▄▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_loss_epoch █▆▆▆▆▆▆▅▅▅▄▄▃▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            BTC_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            ETH_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            LTC_val_f1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_loss ▃▄▃▄▃▄▃▃▂▁▁▃▅▃▇▇▃▄▆▆▇▅▆▆▅▆▅▄▅▅█▅▆▇▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          BTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           BTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          ETH_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ETH_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          LTC_test_acc ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           LTC_test_f1 ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             test_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mmulti_task_BTC_ETH_LTC_multi_head_attention__multi_classification_\u001b[0m: \u001b[34mhttps://wandb.ai/aysenurk/price_change_2/runs/37qp4ff3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for c in (ParameterGrid(param_grid)):\n",
    "    config = CONFIG.copy()\n",
    "    config.update(c)\n",
    "    script = \"--currency-list \" + \" \".join([i for i in c[\"currency_list\"]])\n",
    "    script += \" -trend \" + str(1 if c[\"remove_trend\"] else 0)\n",
    "    script += \" -classes \" + str(c[\"n_classes\"] )\n",
    "    script += \" -weight \" + str(1 if c[\"loss_weight_calculate\"] else 0) \n",
    "\n",
    "    experiment(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "04984682def58a97e4300fcfdea82226e95c772fd8b0b63e42875ad1781ae0ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
