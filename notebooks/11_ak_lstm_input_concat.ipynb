{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from src.TimeSeriesLearningUtils import *\n",
    "from src.LSTMModel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_model(config):\n",
    "    name =[]\n",
    "    if len(config[\"currency_list\"])  > 1:\n",
    "        name.append(\"multi_task_\" + \"_\".join(config[\"currency_list\"]))\n",
    "    else:\n",
    "        name.append(config[\"currency_list\"][0])\n",
    "        \n",
    "    if config[\"indicators\"] or config[\"imfs\"] or config [\"ohlv\"]:\n",
    "        name.append(\"multi_variate\")\n",
    "    \n",
    "    lstm = \"stack_lstm\" if config[\"n_lstm_layers\"] > 1 else \"lstm\"\n",
    "    name.append(lstm)\n",
    "    \n",
    "    name.append(config[\"pred_frequency\"])\n",
    "    classification = \"multi_clf\" if config[\"num_classes\"] > 2 else \"binary_clf\"\n",
    "    name.append(classification)\n",
    "    \n",
    "    return \"_\".join(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"window_size\": 50, \n",
    "          \"dataset_percentages\": [0.96, 0.02, 0.02],\n",
    "          \"data_frequency\": \"6h\", \n",
    "          \"pred_frequency\": \"6h\",\n",
    "          \"ma_period\": 10, \n",
    "          \"log_price\": True,\n",
    "          \"neutral_quantile\": 0.33,\n",
    "          \"batch_size\": 64,\n",
    "          \"bidirectional\": True, \n",
    "          \"num_classes\": 2,\n",
    "          \"currency_list\": ['BTC', 'ETH', 'LTC'],#['LTC'],# \n",
    "          \"dropout_after_each_lstm_layer\": 0.5,\n",
    "          \"dropout_before_output_layer\": 0.5,\n",
    "          \"remove_trend\": True,\n",
    "          \"lstm_hidden_size\": 128,\n",
    "          \"n_lstm_layers\": 3,\n",
    "          \"calculate_loss_weights\": True, \n",
    "          \"last_layer_fsz\": 128,\n",
    "          \"warmup_epoch\": 10,\n",
    "          \"learning_rate\": 1e-3,\n",
    "          \"weight_decay\": 1e-2,\n",
    "          \"indicators\": True, \n",
    "          \"imfs\": False,\n",
    "          \"ohlv\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multi_task_BTC_ETH_LTC_multi_variate_stack_lstm_6h_binary_clf'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = name_model(config)\n",
    "name_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/ta/trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n"
     ]
    }
   ],
   "source": [
    "X, y, features, dfs = get_data(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5350, 84)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.03468741e+04,  2.92840762e+04, -1.96873182e-02, ...,\n",
       "        -1.93443631e+00,  2.67114606e+02, -1.93443631e-02],\n",
       "       [ 2.87209002e+04,  2.59601113e+04, -1.32567221e-02, ...,\n",
       "        -2.83072132e+00,  2.56868320e+02, -2.83072132e-02],\n",
       "       [ 3.10818191e+04,  2.95377875e+04, -2.21909886e-02, ...,\n",
       "         4.02990270e+00,  2.71543478e+02,  4.02990270e-02],\n",
       "       ...,\n",
       "       [ 5.38031281e+06,  1.60726087e+06, -5.44086359e-02, ...,\n",
       "        -1.42354554e+00,  9.22403901e+02, -1.42354554e-02],\n",
       "       [ 5.38521346e+06,  1.61593551e+06, -7.80766603e-02, ...,\n",
       "         6.30375228e-01,  9.28869238e+02,  6.30375228e-03],\n",
       "       [ 5.38898243e+06,  1.62071936e+06, -1.12914380e-01, ...,\n",
       "         1.00455474e+00,  9.39256880e+02,  1.00455474e-02]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5086, 107, 107]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset = [TimeSeriesDataset(X,\n",
    "                                                              y,\n",
    "                                                              dtype, \n",
    "                                                              **config) for dtype in ['train', 'val', 'test']]\n",
    "\n",
    "config[\"dataset_sizes\"] = [len(train_dataset), len(val_dataset), len(test_dataset)]\n",
    "config[\"dataset_sizes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BTC_window': tensor([[-9.6904e-01, -7.2636e-01, -6.6448e-01,  ..., -9.3108e-01,\n",
       "           1.4858e-01, -9.3108e-01],\n",
       "         [-9.7011e-01, -7.3054e-01, -6.1313e-01,  ..., -1.3592e+00,\n",
       "           1.1644e-01, -1.3592e+00],\n",
       "         [-9.6855e-01, -7.2604e-01, -6.8448e-01,  ...,  1.9179e+00,\n",
       "           1.6247e-01,  1.9179e+00],\n",
       "         ...,\n",
       "         [-9.4988e-01, -7.4503e-01,  4.1288e-01,  ..., -4.8375e-01,\n",
       "          -3.1387e-02, -4.8375e-01],\n",
       "         [-9.4836e-01, -7.3860e-01,  3.8059e-01,  ...,  1.5194e+00,\n",
       "           1.5797e-04,  1.5194e+00],\n",
       "         [-9.4891e-01, -7.4413e-01,  3.0799e-01,  ..., -8.4158e-01,\n",
       "          -1.7212e-02, -8.4158e-01]]),\n",
       " 'BTC_label': tensor(1),\n",
       " 'ETH_window': tensor([[-0.7030, -0.4120,  0.7236,  ...,  1.6966,  0.0949,  1.6966],\n",
       "         [-0.7034, -0.4103,  0.1582,  ...,  1.2730,  0.1261,  1.2730],\n",
       "         [-0.7029, -0.4093,  0.5725,  ...,  2.1572,  0.1812,  2.1572],\n",
       "         ...,\n",
       "         [-0.6940, -0.4169,  0.8242,  ...,  1.9771,  0.1945,  1.9771],\n",
       "         [-0.6932, -0.4148,  0.8246,  ...,  1.3933,  0.2324,  1.3933],\n",
       "         [-0.6934, -0.4169,  0.6992,  ..., -0.5707,  0.2171, -0.5707]]),\n",
       " 'ETH_label': tensor(1),\n",
       " 'LTC_window': tensor([[-0.6877, -0.4671, -1.8627,  ..., -2.6736,  2.7442, -2.6736],\n",
       "         [-0.6879, -0.4668, -3.8276,  ...,  0.3196,  2.7815,  0.3196],\n",
       "         [-0.6878, -0.4665, -2.0237,  ...,  0.9339,  2.8938,  0.9339],\n",
       "         ...,\n",
       "         [-0.6839, -0.4719,  0.5751,  ...,  0.0869,  2.4977,  0.0869],\n",
       "         [-0.6839, -0.4712,  0.5029,  ...,  0.7284,  2.5790,  0.7284],\n",
       "         [-0.6841, -0.4717,  0.5348,  ..., -1.1338,  2.4518, -1.1338]]),\n",
       " 'LTC_label': tensor(1)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 168])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((train_dataset[0]['BTC_window'], train_dataset[0]['BTC_window'] ), axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_based_classification_model(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 train_dataset,\n",
    "                 val_dataset,\n",
    "                 test_dataset,\n",
    "                 calculate_loss_weights,\n",
    "                 currency_list,\n",
    "                 num_classes,\n",
    "                 window_size,\n",
    "                 batch_size,\n",
    "                 lstm_hidden_size,\n",
    "                 n_lstm_layers,\n",
    "                 bidirectional,\n",
    "                 last_layer_fsz,\n",
    "                 dropout_after_each_lstm_layer,\n",
    "                 dropout_before_output_layer,\n",
    "                 input_concat,\n",
    "                 warmup_epoch = 5,\n",
    "                 learning_rate = 1e-3,\n",
    "                 weight_decay = 1e-2,\n",
    "                 **kwargs\n",
    "                #  scheduler_step = 10,\n",
    "                #  scheduler_gamma = 0.1,\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.currency_list = currency_list\n",
    "        self.num_tasks = len(currency_list)\n",
    "        self.window_size = window_size\n",
    "        self.input_concat = input_concat ###########\n",
    "        self.input_size = train_dataset.x.shape[-1] * self.num_tasks if self.input_concat else train_dataset.x.shape[-1]\n",
    "        self.batch_size = batch_size\n",
    "        self.n_lstm_layers = n_lstm_layers\n",
    "        self.lstm_hidden_sizes = [lstm_hidden_size] * self.n_lstm_layers\n",
    "        self.bidirectional = bidirectional \n",
    "        self.loss_weightening = calculate_loss_weights\n",
    "        self.dropout_after_each_lstm_layer = dropout_after_each_lstm_layer\n",
    "        self.dropout_before_output_layer = dropout_before_output_layer\n",
    "        self.last_layer_fsz = last_layer_fsz\n",
    "        self.learning_rate = learning_rate\n",
    "        self.warmup_epoch = warmup_epoch\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        if calculate_loss_weights:\n",
    "            loss_weights = []\n",
    "            for i in range(self.num_tasks):\n",
    "                train_labels = [int(train_dataset[n][self.currency_list[i] +\"_label\"] )for n in range(train_dataset.__len__())]\n",
    "                samples_size = pd.DataFrame({\"label\": train_labels}).groupby(\"label\").size().to_numpy()\n",
    "                loss_weights.append((1 / samples_size) * sum(samples_size)/2)\n",
    "            self.weights = loss_weights\n",
    "        else:\n",
    "            self.weights = None\n",
    "        \n",
    "        # self.lstm_1 = nn.LSTM(input_size = self.input_size, \n",
    "        #                       num_layers=1, \n",
    "        #                       batch_first=True, \n",
    "        #                       hidden_size = self.lstm_hidden_sizes[0], \n",
    "        #                       bidirectional = bidirectional)\n",
    "        # self.batch_norm1 = nn.BatchNorm2d(num_features=self.lstm_hidden_sizes[0]*2 if bidirectional else self.lstm_hidden_sizes[0])\n",
    "        \n",
    "        # if len(self.lstm_hidden_sizes) > 1:\n",
    "        #     self.lstm_2 = nn.LSTM(input_size = self.lstm_hidden_sizes[0] *2 if bidirectional else self.lstm_hidden_sizes[0], \n",
    "        #                           num_layers=1, \n",
    "        #                           batch_first=True, \n",
    "        #                           hidden_size = self.lstm_hidden_sizes[1], \n",
    "        #                           bidirectional = bidirectional)\n",
    "        #     self.batch_norm2 = nn.BatchNorm2d(num_features=self.lstm_hidden_sizes[1]*2 if bidirectional else self.lstm_hidden_sizes[1])\n",
    "\n",
    "        #     self.lstm_3 = nn.LSTM(input_size = self.lstm_hidden_sizes[1]*2 if bidirectional else self.lstm_hidden_sizes[1], \n",
    "        #                           num_layers=1, \n",
    "        #                           batch_first=True, \n",
    "        #                           hidden_size = self.lstm_hidden_sizes[2], \n",
    "        #                           bidirectional = bidirectional)\n",
    "        #     self.batch_norm3 = nn.BatchNorm2d(num_features=self.lstm_hidden_sizes[2]*2 if bidirectional else self.lstm_hidden_sizes[2])\n",
    "        \n",
    "        \n",
    "        # self.dropout = nn.Dropout(self.dropout_ratio)\n",
    "        \n",
    "        self.lstm_blocks = nn.ModuleList()\n",
    "        \n",
    "        for i in range(self.n_lstm_layers):\n",
    "\n",
    "            if i == 0:\n",
    "              input_size = self.input_size \n",
    "            else:\n",
    "              input_size = self.lstm_hidden_sizes[i-1]*2 if self.bidirectional else self.lstm_hidden_sizes[i-1]   \n",
    "            \n",
    "            lstm_layer = nn.LSTM(input_size = input_size, \n",
    "                                  num_layers=1, \n",
    "                                  batch_first=True, \n",
    "                                  hidden_size = self.lstm_hidden_sizes[i], \n",
    "                                  bidirectional = self.bidirectional)\n",
    "            \n",
    "            n_feature = self.lstm_hidden_sizes[i]*2 if self.bidirectional else self.lstm_hidden_sizes[i]   \n",
    "            batch_norm = nn.BatchNorm2d(num_features=n_feature)\n",
    "            lst = [('lstm', lstm_layer), ('batch_norm', batch_norm)]\n",
    "  \n",
    "            if self.dropout_after_each_lstm_layer:\n",
    "                dropout = nn.Dropout(self.dropout_after_each_lstm_layer)\n",
    "                lst.append(('dropout', dropout))\n",
    "                \n",
    "            module_dict = nn.ModuleDict(lst)\n",
    "            \n",
    "            self.lstm_blocks.append(module_dict)\n",
    "        \n",
    "        n_feature = self.lstm_hidden_sizes[-1] *2 if bidirectional else self.lstm_hidden_sizes[-1]\n",
    "        \n",
    "        self.linear1 =[nn.Linear(n_feature, self.last_layer_fsz)] * self.num_tasks\n",
    "        self.linear1 = torch.nn.ModuleList(self.linear1)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        if self.dropout_before_output_layer:\n",
    "          self.dropout1 = nn.Dropout(self.dropout_before_output_layer)\n",
    "          \n",
    "        self.output_layers = [nn.Linear(self.last_layer_fsz, self.num_classes)] * self.num_tasks\n",
    "        self.output_layers = torch.nn.ModuleList(self.output_layers)\n",
    "        \n",
    "        if self.weights != None:\n",
    "            self.cross_entropy_loss = [nn.CrossEntropyLoss(weight= torch.tensor(weights).float()) for weights in self.weights]\n",
    "        else:\n",
    "            self.cross_entropy_loss = [nn.CrossEntropyLoss() for _ in range(self.num_tasks)]\n",
    "        \n",
    "        self.cross_entropy_loss = torch.nn.ModuleList(self.cross_entropy_loss)\n",
    "        \n",
    "        self.f1_score = pl.metrics.F1(num_classes=self.num_classes, average=\"macro\")\n",
    "        self.accuracy_score = pl.metrics.Accuracy()\n",
    "        \n",
    "        self.train_dl = DataLoader(train_dataset, batch_size=self.batch_size, shuffle = True)\n",
    "        self.val_dl = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "        self.test_dl = DataLoader(test_dataset, batch_size=self.batch_size)\n",
    "        \n",
    "        # self.scheduler_step = scheduler_step\n",
    "        # self.scheduler_gamma = scheduler_gamma\n",
    "        \n",
    "    def forward(self, x, n):\n",
    "\n",
    "        batch_size = x.size()[0]\n",
    "        \n",
    "        # x = x.view(batch_size, self.window_size, self.input_size) #(batch, window_len, feature_size)\n",
    "        # x, _  = self.lstm_1(x)\n",
    "        \n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        # x = x.reshape(x.size()[-1], batch_size, self.window_size) #(feature_size, batch, window_len)\n",
    "        # x = self.batch_norm1(x.unsqueeze(0))\n",
    "        \n",
    "        # if len(self.lstm_hidden_sizes) > 1:\n",
    "            \n",
    "        #     x = x.view(batch_size, self.window_size, x.size()[1])\n",
    "        #     x, _  = self.lstm_2(x)\n",
    "\n",
    "        #     x = self.dropout(x)\n",
    "\n",
    "        #     x = x.reshape(x.size()[-1], batch_size, self.window_size) #(feature_size, batch, window_len)\n",
    "        #     x = self.batch_norm2(x.unsqueeze(0))\n",
    "\n",
    "        #     x = x.view(batch_size, self.window_size, x.size()[1])\n",
    "        #     x, _  = self.lstm_3(x)\n",
    "\n",
    "        #     x = self.dropout(x)\n",
    "\n",
    "        #     x = x.reshape(x.size()[-1], batch_size, self.window_size) #(feature_size, batch, window_len)\n",
    "        #     x = self.batch_norm3(x.unsqueeze(0))\n",
    "        \n",
    "        for i, block in enumerate(self.lstm_blocks):\n",
    "\n",
    "            if i == 0:\n",
    "              n_feature = self.input_size \n",
    "            else:\n",
    "              n_feature = self.lstm_hidden_sizes[i-1]*2 if self.bidirectional else self.lstm_hidden_sizes[i-1]   \n",
    " \n",
    "            x = x.view(batch_size, self.window_size, n_feature) #(batch, window_len, feature_size)\n",
    "            x, _ = block['lstm'](x)\n",
    "        \n",
    "            if 'dropout' in block:\n",
    "                x = block['dropout'](x)\n",
    "           \n",
    "            x = x.reshape(x.size()[-1], batch_size, self.window_size) #(feature_size, batch, window_len)\n",
    "        \n",
    "            x = block['batch_norm'](x.unsqueeze(0))\n",
    "  \n",
    "            if len(x.shape) == 4: #error handling\n",
    "              x = x.squeeze() \n",
    "              \n",
    "        #x = x.view(batch_size, self.window_size, x.size()[1])\n",
    "        n_feature = self.lstm_hidden_sizes[-1]*2 if self.bidirectional else self.lstm_hidden_sizes[-1]\n",
    "        x = x.view(batch_size, self.window_size, n_feature)\n",
    "        x = x[:, -1, :] # equivalent to return sequence = False on keras :)\n",
    "        \n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear1[n](x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        if self.dropout_before_output_layer:\n",
    "            x = self.dropout1(x)\n",
    "                   \n",
    "        output = self.output_layers[n](x)\n",
    "    \n",
    "        return output\n",
    "    \n",
    "    def step(self, batch, step_type = 'train'):\n",
    "        loss = (torch.tensor(0.0, device=\"cuda:0\", requires_grad=True) + \\\n",
    "                torch.tensor(0.0, device=\"cuda:0\", requires_grad=True)) \n",
    "        \n",
    "        if self.input_concat and self.num_tasks > 1:\n",
    "            x = torch.cat(tuple([batch[self.currency_list[i] + \"_window\"] for i in range(self.num_tasks)]),axis =1)\n",
    "        \n",
    "        for i in range(self.num_tasks):\n",
    "            if self.input_concat and self.num_tasks > 1:\n",
    "                y = batch[self.currency_list[i] + \"_label\"]\n",
    "            else:\n",
    "                x, y = batch[self.currency_list[i] + \"_window\"], batch[self.currency_list[i] + \"_label\"]\n",
    "            output = self.forward(x, i)\n",
    "            #loss = F.nll_loss(output, y)\n",
    "            loss += self.cross_entropy_loss[i](output, y)\n",
    "            \n",
    "            acc = self.accuracy_score(torch.max(output, dim=1)[1], y)\n",
    "            self.log(f\"{self.currency_list[i]}_{step_type}_acc\", acc, on_epoch=True, prog_bar=True)\n",
    "\n",
    "            f1 = self.f1_score(torch.max(output, dim=1)[1], y)\n",
    "            self.log(f\"{self.currency_list[i]}_{step_type}_f1\", f1, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        loss = loss / torch.tensor(self.num_tasks)\n",
    "        self.log(f\"{step_type}_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(self.parameters(), \n",
    "                                      lr= self.learning_rate, \n",
    "                                      weight_decay=self.weight_decay)\n",
    "\n",
    "#         scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
    "#                                                     step_size=self.scheduler_step, \n",
    "#                                                     gamma=self.scheduler_gamma)\n",
    "        \n",
    "        self.lr_scheduler = CosineWarmupScheduler(optimizer, \n",
    "                                                  warmup = self.train_dl.__len__() * self.warmup_epoch, \n",
    "                                                  max_iters = MAX_EPOCHS * self.train_dl.__len__())\n",
    "        return [optimizer]#, [{\"scheduler\": scheduler}]\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        \n",
    "        loss = self.step(batch, \"train\")\n",
    "        \n",
    "        return loss \n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        \n",
    "        self.step(batch, \"val\")\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        \n",
    "        self.step(batch, \"test\")\n",
    "    \n",
    "    def optimizer_step(self, *args, **kwargs):\n",
    "        super().optimizer_step(*args, **kwargs)\n",
    "        self.lr_scheduler.step() # Step per iteration\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_dl\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_dl\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type       | Params\n",
      "--------------------------------------------------\n",
      "0 | lstm_blocks        | ModuleList | 1.0 M \n",
      "1 | linear1            | ModuleList | 32.9 K\n",
      "2 | activation         | ReLU       | 0     \n",
      "3 | dropout1           | Dropout    | 0     \n",
      "4 | output_layers      | ModuleList | 258   \n",
      "5 | cross_entropy_loss | ModuleList | 0     \n",
      "6 | f1_score           | F1         | 0     \n",
      "7 | accuracy_score     | Accuracy   | 0     \n",
      "--------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.177     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ffc3b20b164f49b07a0d211252bf6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.699\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.009 >= min_delta = 0.003. New best score: 0.690\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 10 records. Best score: 0.690. Signaling Trainer to stop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6c9156edde4b26b7da5ccf89bcaa4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BTC_test_acc': 0.5233644843101501,\n",
      " 'BTC_test_f1': 0.5123544335365295,\n",
      " 'ETH_test_acc': 0.5233644843101501,\n",
      " 'ETH_test_f1': 0.4658462703227997,\n",
      " 'LTC_test_acc': 0.5981308221817017,\n",
      " 'LTC_test_f1': 0.596709668636322,\n",
      " 'test_loss': 0.6924201250076294}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'BTC_test_acc': 0.5233644843101501,\n",
       "  'BTC_test_f1': 0.5123544335365295,\n",
       "  'ETH_test_acc': 0.5233644843101501,\n",
       "  'ETH_test_f1': 0.4658462703227997,\n",
       "  'LTC_test_acc': 0.5981308221817017,\n",
       "  'LTC_test_f1': 0.596709668636322,\n",
       "  'test_loss': 0.6924201250076294}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM_based_classification_model(train_dataset = train_dataset,\n",
    "                                        val_dataset = val_dataset,\n",
    "                                        test_dataset = test_dataset,\n",
    "                                        input_concat = False,\n",
    "                                        **config)\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='val_loss',\n",
    "   min_delta=0.003,\n",
    "   patience=10,\n",
    "   verbose=True,\n",
    "   mode='min'\n",
    ")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='../output/',\n",
    "    filename = MODEL_NAME +'-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(gpus=-1, \n",
    "                     max_epochs= 80,\n",
    "                     #logger = logger, \n",
    "                     callbacks=[early_stop_callback, checkpoint_callback])\n",
    "trainer.fit(model)\n",
    "\n",
    "trainer.test(ckpt_path = checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input concat\n",
    "--------------------------------------------------------------------------------\n",
    "DATALOADER:0 TEST RESULTS\n",
    "{'BTC_test_acc': 0.5046728849411011,\n",
    " 'BTC_test_f1': 0.3353511095046997,\n",
    " 'ETH_test_acc': 0.5700934529304504,\n",
    " 'ETH_test_f1': 0.3804537355899811,\n",
    " 'LTC_test_acc': 0.5327102541923523,\n",
    " 'LTC_test_f1': 0.3617068827152252,\n",
    " 'test_loss': 0.6925621628761292}\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "[{'BTC_test_acc': 0.5046728849411011,\n",
    "  'BTC_test_f1': 0.3353511095046997,\n",
    "  'ETH_test_acc': 0.5700934529304504,\n",
    "  'ETH_test_f1': 0.3804537355899811,\n",
    "  'LTC_test_acc': 0.5327102541923523,\n",
    "  'LTC_test_f1': 0.3617068827152252,\n",
    "  'test_loss': 0.6925621628761292}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(dataloaders= model.val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(dataloaders= model.train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
