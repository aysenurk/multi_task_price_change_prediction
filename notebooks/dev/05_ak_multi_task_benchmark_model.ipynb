{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from PyEMD import EEMD\n",
    "except:\n",
    "    !pip install EMD-signal\n",
    "    from PyEMD import EEMD\n",
    "\n",
    "from ta import add_all_ta_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics import F1\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENCY_LST = ['BTC', 'ETH', 'LTC']\n",
    "PRICE_TYPE = 'close'\n",
    "FREQUENCY = \"D\"\n",
    "WINDOW_SIZE = 14\n",
    "NEUTRAL_QUANTILE = 0.25\n",
    "LOG_PRICE = True\n",
    "INDICATORS = False\n",
    "IMFS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 2 ###\n",
    "TRAIN_PERCENTAGE, VAL_PERCENTAGE, TEST_PERCENTAGE = 0.80, 0.10, 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_HIDDEN_SIZES = [128, 128, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(currency_lst,\n",
    "             frequency, \n",
    "             window_size,\n",
    "             neutral_quantile = 0.25,\n",
    "             beg_date = pd.Timestamp(2013,1,1),\n",
    "             end_date = pd.Timestamp.now(),\n",
    "             log_price = True, \n",
    "             include_indicators = True,\n",
    "             include_imfs = True):\n",
    "        \n",
    "        X, y, dfs = {}, {}, {}     \n",
    "        \n",
    "        for cur in currency_lst:\n",
    "            df = pd.read_csv(f\"../data/0_raw/Binance/{str.lower(cur)}_usdt_1d.csv\", index_col=0).reset_index()   \n",
    "            \n",
    "            if include_indicators:\n",
    "                df = add_all_ta_features(df, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\", \n",
    "                                         fillna=True)\n",
    "            else:\n",
    "                df.drop(\"Volume\", axis=1, inplace=True)\n",
    "            \n",
    "            df.Date = df.Date.apply(pd.Timestamp)\n",
    "            df.sort_values(\"Date\", ascending=True, inplace=True)\n",
    "            df.set_index(\"Date\", inplace=True)\n",
    "            df.drop([\"Timestamp\", \"Open\", \"High\", \"Low\"], axis=1, inplace=True)\n",
    "            df.rename(str.lower, axis=1, inplace=True)\n",
    "            \n",
    "            if log_price:\n",
    "                df[\"close\"] = df[\"close\"].apply(np.log)\n",
    "      \n",
    "            if include_imfs:\n",
    "                eemd = EEMD()\n",
    "                imfs = eemd(df[PRICE_TYPE].values)\n",
    "                imf_features = [\"imf_\"+str(i) for i in range(imfs.shape[0])]\n",
    "                df = pd.concat((df, pd.DataFrame(imfs.T, columns=imf_features, index=df.index)), axis=1)\n",
    "                \n",
    "            price_diff = df[\"close\"].diff().dropna()\n",
    "            rolling_quantiles = price_diff.abs().rolling(window_size).quantile(neutral_quantile).dropna()\n",
    "#             conditions = [(price_diff[window_size-1:] < 0) & (price_diff[window_size-1:].abs() > rolling_quantiles),\n",
    "#                           (price_diff[window_size-1:] > 0) & (price_diff[window_size-1:].abs() > rolling_quantiles)]\n",
    "            conditions = [(price_diff[window_size-1:] < 0),\n",
    "                          (price_diff[window_size-1:] > 0)]\n",
    "\n",
    "#             classes = [1,2] #1 is decrease, 2 is decrease, and 0 is neutral if none of conditions is met\n",
    "            classes = [0,1]\n",
    "    \n",
    "            y = pd.Series(np.select(conditions, classes, default=0), index=price_diff[window_size-1:].index)\n",
    "            df.insert(loc=0, column=\"change_dir\", value=y)\n",
    "            dfs[cur] = df\n",
    "            \n",
    "        min_dates = [df.index.min() for cur, df in dfs.items()]\n",
    "        max_dates = [df.index.max() for cur, df in dfs.items()]\n",
    "        beg_date = max([max(min_dates), beg_date])\n",
    "        end_date = min([min(max_dates), end_date])\n",
    "        common_range = pd.date_range(beg_date, end_date, freq=frequency)\n",
    "        \n",
    "        X = np.array([dfs[cur].drop(\"change_dir\", axis=1).loc[common_range].values for cur in currency_lst])\n",
    "        y = np.array([dfs[cur].loc[common_range, \"change_dir\"].values for cur in currency_lst])\n",
    "        features = df.columns.tolist()\n",
    "        \n",
    "        return X, y, features, dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, features, dfs = get_data(CURRENCY_LST,\n",
    "                                 FREQUENCY, \n",
    "                                 WINDOW_SIZE,\n",
    "                                 neutral_quantile = NEUTRAL_QUANTILE,\n",
    "                                 log_price=LOG_PRICE,\n",
    "                                 include_indicators = INDICATORS, #True diyince patlıyor\n",
    "                                 include_imfs = IMFS\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CURRENCIES = X.shape[0]\n",
    "INPUT_FEATURE_SIZE = X.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 n_currencies,\n",
    "                 x: np.ndarray, \n",
    "                 y: np.ndarray,\n",
    "                 data_use_type,\n",
    "                 train_percentage = TRAIN_PERCENTAGE,\n",
    "                 val_percentage = VAL_PERCENTAGE,\n",
    "                 test_percentage = TEST_PERCENTAGE,\n",
    "                 seq_len = WINDOW_SIZE, \n",
    "                 ):\n",
    "        \n",
    "        self.x = torch.tensor(x[:n_currencies]).float()\n",
    "        self.y = torch.tensor(y[:n_currencies]).long()\n",
    "        self.seq_len = seq_len\n",
    "        self.data_use_type = data_use_type\n",
    "        \n",
    "        #self.train_size = int(len(self.x[0]) * train_percentage)\n",
    "        self.val_size = int(len(self.x[0]) * val_percentage)\n",
    "        self.test_size = int(len(self.x[0]) * test_percentage)\n",
    "        self.train_size = len(self.x[0]) - self.val_size - self.test_size \n",
    "        \n",
    "#         self.train_mean = [self.x[i][:self.train_size].mean() for i in range(n_currencies)]\n",
    "#         self.train_std = [self.x[i][:self.train_size].std() for i in range(n_currencies)]\n",
    "        \n",
    "#         self.train_min = [self.x[i][:self.train_size].min() for i in range(n_currencies)]\n",
    "#         self.train_max = [self.x[i][:self.train_size].max() for i in range(n_currencies)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        if self.data_use_type == \"train\":\n",
    "            return self.train_size - ( self.seq_len)\n",
    "\n",
    "        elif self.data_use_type == \"val\":\n",
    "            return self.val_size\n",
    "  \n",
    "        else:\n",
    "            return self.test_size\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        item = dict()\n",
    "        \n",
    "        if self.data_use_type ==\"val\":\n",
    "            index = self.train_size + index - self.seq_len\n",
    "            \n",
    "        elif self.data_use_type ==\"test\":\n",
    "            index = self.train_size + self.val_size + index - self.seq_len\n",
    "        \n",
    "        for i in range(N_CURRENCIES):\n",
    "            window = self.x[i][index:index+self.seq_len]\n",
    "            #window = (window -self.train_mean[i]) / self.train_std[i]\n",
    "            \n",
    "            item[\"currency_\" + str(i) + \"_window\"] = window\n",
    "            item[\"currency_\" + str(i) + \"_label\"]  = self.y[i][index+self.seq_len]\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = [MultiTimeSeriesDataset(N_CURRENCIES, X, y, dtype) for dtype in ['train', 'val', 'test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.524841546163962"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1][:2].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    collated_batch = dict()\n",
    "    for i in range(N_CURRENCIES):\n",
    "        collated_batch[\"currency_\" + str(i) + \"_window\"] = []\n",
    "        collated_batch[\"currency_\" + str(i) + \"_label\"] = []\n",
    "        \n",
    "    for sample in batch: \n",
    "        for i in range(N_CURRENCIES):\n",
    "            collated_batch[\"currency_\" + str(i) + \"_window\"].append(sample[\"currency_\" + str(i) + \"_window\"])\n",
    "            collated_batch[\"currency_\" + str(i) + \"_label\"].append(sample[\"currency_\" + str(i) + \"_label\"])\n",
    "    \n",
    "    for i in range(N_CURRENCIES):\n",
    "        collated_batch[\"currency_\" + str(i) + \"_window\"] = torch.stack(collated_batch[\"currency_\" + str(i) + \"_window\"])\n",
    "        collated_batch[\"currency_\" + str(i) + \"_label\"] = torch.stack(collated_batch[\"currency_\" + str(i) + \"_label\"])\n",
    "    \n",
    "    return collated_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#calculate loss' weights\n",
    "train_labels = [train_dataset[i][1].item() for i in range (train_dataset.__len__())]\n",
    "\n",
    "cnt = Counter(train_labels)\n",
    "samples_size = np.array([cnt[key] for key in  sorted(cnt.keys())])\n",
    "\n",
    "loss_weights = (1 / samples_size) * sum(samples_size)/2\n",
    "loss_weights, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'currency_0_window': tensor([[9.6824],\n",
       "         [9.7011],\n",
       "         [9.7722],\n",
       "         [9.8576],\n",
       "         [9.8448],\n",
       "         [9.8446],\n",
       "         [9.7582],\n",
       "         [9.7104],\n",
       "         [9.6481],\n",
       "         [9.4975],\n",
       "         [9.4955],\n",
       "         [9.5104],\n",
       "         [9.5251],\n",
       "         [9.6607]]),\n",
       " 'currency_0_label': tensor(0),\n",
       " 'currency_1_window': tensor([[6.5279],\n",
       "         [6.5217],\n",
       "         [6.5146],\n",
       "         [6.5262],\n",
       "         [6.5619],\n",
       "         [6.6644],\n",
       "         [6.6821],\n",
       "         [6.6723],\n",
       "         [6.6624],\n",
       "         [6.4552],\n",
       "         [6.4620],\n",
       "         [6.4892],\n",
       "         [6.5722],\n",
       "         [6.6201]]),\n",
       " 'currency_1_label': tensor(0),\n",
       " 'currency_2_window': tensor([[5.6699],\n",
       "         [5.6073],\n",
       "         [5.6836],\n",
       "         [5.6835],\n",
       "         [5.7411],\n",
       "         [5.8636],\n",
       "         [5.8245],\n",
       "         [5.7127],\n",
       "         [5.7104],\n",
       "         [5.5254],\n",
       "         [5.5351],\n",
       "         [5.5718],\n",
       "         [5.5653],\n",
       "         [5.6275]]),\n",
       " 'currency_2_label': tensor(0)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'currency_0_window': tensor([[[9.1319],\n",
      "         [9.1129],\n",
      "         [9.1321],\n",
      "         [9.1850],\n",
      "         [9.1813],\n",
      "         [9.1966],\n",
      "         [9.1756],\n",
      "         [9.1447],\n",
      "         [9.1256],\n",
      "         [9.1388],\n",
      "         [9.1052],\n",
      "         [9.0360],\n",
      "         [9.0438],\n",
      "         [9.0687]],\n",
      "\n",
      "        [[9.0687],\n",
      "         [9.0669],\n",
      "         [9.0433],\n",
      "         [9.0276],\n",
      "         [8.9924],\n",
      "         [9.0167],\n",
      "         [9.0160],\n",
      "         [9.0510],\n",
      "         [9.0338],\n",
      "         [8.9843],\n",
      "         [8.9229],\n",
      "         [8.9326],\n",
      "         [8.9169],\n",
      "         [8.9003]],\n",
      "\n",
      "        [[9.0406],\n",
      "         [9.0289],\n",
      "         [9.0613],\n",
      "         [9.0947],\n",
      "         [9.1457],\n",
      "         [9.1379],\n",
      "         [9.1604],\n",
      "         [9.1434],\n",
      "         [9.1468],\n",
      "         [9.1412],\n",
      "         [9.1369],\n",
      "         [9.1266],\n",
      "         [9.1708],\n",
      "         [9.1873]]]), 'currency_0_label': tensor([0, 1, 1]), 'currency_1_window': tensor([[[6.5084],\n",
      "         [6.5103],\n",
      "         [6.5346],\n",
      "         [6.6580],\n",
      "         [6.6669],\n",
      "         [6.7087],\n",
      "         [6.6775],\n",
      "         [6.6234],\n",
      "         [6.6187],\n",
      "         [6.6224],\n",
      "         [6.5827],\n",
      "         [6.5176],\n",
      "         [6.5281],\n",
      "         [6.5918]],\n",
      "\n",
      "        [[6.5918],\n",
      "         [6.5896],\n",
      "         [6.5581],\n",
      "         [6.5600],\n",
      "         [6.5037],\n",
      "         [6.5405],\n",
      "         [6.5468],\n",
      "         [6.5739],\n",
      "         [6.5464],\n",
      "         [6.4630],\n",
      "         [6.3582],\n",
      "         [6.4006],\n",
      "         [6.3726],\n",
      "         [6.3733]],\n",
      "\n",
      "        [[5.0909],\n",
      "         [5.0774],\n",
      "         [5.1231],\n",
      "         [5.1363],\n",
      "         [5.1684],\n",
      "         [5.1574],\n",
      "         [5.2187],\n",
      "         [5.1929],\n",
      "         [5.2128],\n",
      "         [5.2388],\n",
      "         [5.2454],\n",
      "         [5.2413],\n",
      "         [5.3170],\n",
      "         [5.3622]]]), 'currency_1_label': tensor([0, 0, 1]), 'currency_2_window': tensor([[[4.9997],\n",
      "         [4.9992],\n",
      "         [5.0211],\n",
      "         [5.0838],\n",
      "         [5.1325],\n",
      "         [5.1876],\n",
      "         [5.1494],\n",
      "         [5.1035],\n",
      "         [5.0682],\n",
      "         [5.0545],\n",
      "         [4.9983],\n",
      "         [4.9150],\n",
      "         [4.9528],\n",
      "         [4.9715]],\n",
      "\n",
      "        [[4.9715],\n",
      "         [4.9933],\n",
      "         [4.9370],\n",
      "         [4.9339],\n",
      "         [4.8851],\n",
      "         [4.9123],\n",
      "         [4.9049],\n",
      "         [4.9391],\n",
      "         [4.8997],\n",
      "         [4.8512],\n",
      "         [4.7738],\n",
      "         [4.8088],\n",
      "         [4.7780],\n",
      "         [4.7745]],\n",
      "\n",
      "        [[3.9960],\n",
      "         [3.9750],\n",
      "         [4.0296],\n",
      "         [4.0699],\n",
      "         [4.1030],\n",
      "         [4.0910],\n",
      "         [4.2192],\n",
      "         [4.2195],\n",
      "         [4.2587],\n",
      "         [4.2475],\n",
      "         [4.2435],\n",
      "         [4.2205],\n",
      "         [4.2848],\n",
      "         [4.2986]]]), 'currency_2_label': tensor([1, 0, 1])}\n"
     ]
    }
   ],
   "source": [
    "for batch in DataLoader(train_dataset, batch_size=3, shuffle = True):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_based_classification_model(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 train_dataset = train_dataset,\n",
    "                 val_dataset = val_dataset,\n",
    "                 test_dataset = test_dataset,\n",
    "#                  weights = loss_weights,\n",
    "                 num_tasks = N_CURRENCIES,\n",
    "                 num_classes = N_CLASSES,\n",
    "                 window_size = WINDOW_SIZE,\n",
    "                 input_size = INPUT_FEATURE_SIZE,\n",
    "                 batch_size=8,\n",
    "                 lstm_hidden_sizes = LSTM_HIDDEN_SIZES,\n",
    "                 bidirectional = False,\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_tasks = num_tasks\n",
    "        self.window_size = window_size\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.lstm_hidden_sizes = lstm_hidden_sizes\n",
    "        self.bidirectional = bidirectional \n",
    "        \n",
    "#         self.stack_lstm = nn.LSTM(input_size = self.input_size, \n",
    "#                 hidden_size = self.lstm_hidden_size, \n",
    "#                 num_layers= self.lstm_stack_size,\n",
    "#                 dropout = self.lstm_dropout, # sadece stack arasına koyuyor\n",
    "#                 bidirectional = self.bidirectional, \n",
    "#                 batch_first=True,) \n",
    "        \n",
    "        self.lstm_1 = nn.LSTM(input_size = self.input_size, num_layers=1, batch_first=True, hidden_size = self.lstm_hidden_sizes[0])\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=self.lstm_hidden_sizes[0])\n",
    "        \n",
    "        self.lstm_2 = nn.LSTM(input_size = self.lstm_hidden_sizes[0], num_layers=1, batch_first=True, hidden_size = self.lstm_hidden_sizes[1])\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=self.lstm_hidden_sizes[1])\n",
    "        \n",
    "        self.lstm_3 = nn.LSTM(input_size = self.lstm_hidden_sizes[1], num_layers=1, batch_first=True, hidden_size = self.lstm_hidden_sizes[2])\n",
    "        self.batch_norm3 = nn.BatchNorm2d(num_features=self.lstm_hidden_sizes[2])\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.lstm_hidden_sizes[2], int(self.lstm_hidden_sizes[2]/2))\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.output_layers = [nn.Linear(int(self.lstm_hidden_sizes[2]/2), self.num_classes)] * N_CURRENCIES\n",
    "        self.output_layers = torch.nn.ModuleList(self.output_layers)\n",
    "        \n",
    "        \n",
    "#         self.cross_entropy_loss = nn.CrossEntropyLoss(weight= torch.tensor(weights).float()) # loss weight\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.f1_score = pl.metrics.F1(num_classes=self.num_classes, average=\"macro\")\n",
    "        self.accuracy_score = pl.metrics.Accuracy()\n",
    "        \n",
    "        self.train_dl = DataLoader(train_dataset, batch_size=self.batch_size, shuffle = True)\n",
    "        self.val_dl = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "        self.test_dl = DataLoader(test_dataset, batch_size=self.batch_size)\n",
    "        \n",
    "    def forward(self, x, i):\n",
    "\n",
    "        batch_size = x.size()[0]\n",
    "        \n",
    "        x = x.view(batch_size, self.window_size, self.input_size) #(batch, window_len, feature_size)\n",
    "        x, _  = self.lstm_1(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x.reshape(x.size()[-1], batch_size, self.window_size) #(feature_size, batch, window_len)\n",
    "        x = self.batch_norm1(x.unsqueeze(0))\n",
    "\n",
    "        x = x.view(batch_size, self.window_size, x.size()[1])\n",
    "        x, _  = self.lstm_2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.reshape(x.size()[-1], batch_size, self.window_size) #(feature_size, batch, window_len)\n",
    "        x = self.batch_norm2(x.unsqueeze(0))\n",
    "        \n",
    "        x = x.view(batch_size, self.window_size, x.size()[1])\n",
    "        x, _  = self.lstm_3(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.reshape(x.size()[-1], batch_size, self.window_size) #(feature_size, batch, window_len)\n",
    "        x = self.batch_norm3(x.unsqueeze(0))\n",
    "        \n",
    "        x = x.view(batch_size, self.window_size, x.size()[1])\n",
    "        x = x[:, -1, :] # equivalent to return sequence = False on keras :)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "#         x = self.dropout(x)\n",
    "        \n",
    "#         x = self.linear2(x)\n",
    "#         x = self.activation(x)\n",
    "#         x = self.dropout(x)\n",
    "            \n",
    "        output = self.output_layers[i](x)\n",
    "\n",
    "        #output = F.log_softmax(output, dim = 1)\n",
    "        #output = F.softmax(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        \n",
    "        loss = (torch.tensor(0.0, device=\"cuda:0\", requires_grad=True) + \\\n",
    "                torch.tensor(0.0, device=\"cuda:0\", requires_grad=True)) \n",
    "        # araştırılabilir\n",
    "        for i in range(self.num_tasks):\n",
    "            x, y = batch[\"currency_\" + str(i) + \"_window\"], batch[\"currency_\" + str(i) + \"_label\"]\n",
    "\n",
    "            output = self.forward(x, i)\n",
    "            #loss = F.nll_loss(output, y)\n",
    "            loss += self.cross_entropy_loss(output, y)\n",
    "            \n",
    "            acc = self.accuracy_score(torch.max(output, dim=1)[1], y)\n",
    "            self.log(\"currency_\" + str(i) +'_train_acc', acc, on_epoch=True, prog_bar=True)\n",
    "\n",
    "            f1 = self.f1_score(torch.max(output, dim=1)[1], y)\n",
    "            self.log(\"currency_\" + str(i) +'_train_f1', f1, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        loss = loss / torch.tensor(3)\n",
    "        return loss \n",
    "    \n",
    "#     def validation_step(self, batch, batch_nb):\n",
    "#         x, y = batch\n",
    "#         output = self(x)\n",
    "#         #loss = F.nll_loss(output, y)\n",
    "#         loss = self.cross_entropy_loss(output, y)\n",
    "#         self.log('val_loss', loss, on_epoch=True, reduce_fx=torch.mean, prog_bar=True)\n",
    "        \n",
    "#         #print(torch.max(output, dim=1)[1])\n",
    "#         acc = self.accuracy_score(torch.max(output, dim=1)[1], y)\n",
    "#         self.log('val_acc', acc, on_epoch=True, reduce_fx=torch.mean, prog_bar=True)\n",
    "        \n",
    "#         f1 = self.f1_score(torch.max(output, dim=1)[1], y)\n",
    "#         self.log('val_f1', f1, on_epoch=True, reduce_fx=torch.mean, prog_bar=True)\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        loss = torch.tensor(0.0, device=\"cuda:0\") + torch.tensor(0.0, device=\"cuda:0\")\n",
    "        \n",
    "        for i in range(self.num_tasks):\n",
    "            x, y = batch[\"currency_\" + str(i) + \"_window\"], batch[\"currency_\" + str(i) + \"_label\"]\n",
    "\n",
    "            output = self(x, i)\n",
    "            #loss = F.nll_loss(output, y)\n",
    "            loss += self.cross_entropy_loss(output, y)\n",
    " \n",
    "            acc = self.accuracy_score(torch.max(output, dim=1)[1], y)\n",
    "            self.log(\"currency_\" + str(i) +'_val_acc', acc, on_epoch=True, prog_bar=True, reduce_fx=torch.mean)\n",
    "\n",
    "            f1 = self.f1_score(torch.max(output, dim=1)[1], y)\n",
    "            self.log(\"currency_\" + str(i) +'_val_f1', f1, on_epoch=True, prog_bar=True, reduce_fx=torch.mean)\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        loss = torch.tensor(0.0, device=\"cuda:0\") + torch.tensor(0.0, device=\"cuda:0\")\n",
    "        \n",
    "        for i in range(self.num_tasks):\n",
    "            x, y = batch[\"currency_\" + str(i) + \"_window\"], batch[\"currency_\" + str(i) + \"_label\"]\n",
    "\n",
    "            output = self(x, i)\n",
    "            #loss = F.nll_loss(output, y)\n",
    "            loss += self.cross_entropy_loss(output, y)\n",
    "            \n",
    "            acc = self.accuracy_score(torch.max(output, dim=1)[1], y)\n",
    "            self.log(\"currency_\" + str(i) +'_test_acc', acc, on_epoch=True, reduce_fx=torch.mean)\n",
    "\n",
    "            f1 = self.f1_score(torch.max(output, dim=1)[1], y)\n",
    "            self.log(\"currency_\" + str(i) +'_test_f1', f1, on_epoch=True, reduce_fx=torch.mean)\n",
    "        \n",
    "        self.log('test_loss', loss, on_epoch=True, reduce_fx=torch.mean)\n",
    "        \n",
    "#     def test_step(self, batch, batch_nb):\n",
    "#         x, y = batch\n",
    "#         output = self(x)\n",
    "        \n",
    "#         #print(y, torch.max(output, dim=1)[1])\n",
    "#         #print(output)\n",
    "#         print(F.softmax(output)) # mantıken fark etmiyor\n",
    "#         print(y, torch.max(output, dim=1)[1])\n",
    "        \n",
    "#         #loss = F.nll_loss(output, y)\n",
    "#         loss = self.cross_entropy_loss(output, y)\n",
    "#         self.log('test_loss', loss, on_epoch=True, reduce_fx=torch.mean)\n",
    "        \n",
    "#         acc = self.accuracy_score(torch.max(output, dim=1)[1], y)\n",
    "#         self.log('test_acc', acc, on_epoch=True, reduce_fx=torch.mean)\n",
    "        \n",
    "#         f1 = self.f1_score(torch.max(output, dim=1)[1], y)\n",
    "#         self.log('test_f1', f1, on_epoch=True, reduce_fx=torch.mean)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr= 1e-3)#AdamW does weight decay\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "        return [optimizer], [{\"scheduler\": scheduler}]\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_dl\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_dl\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./lightning_logs/version_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb_logger = WandbLogger(name='lstm.v1',project='pytorchlightning')\n",
    "logger = TensorBoardLogger(\"../output/models/lstm_model_logs\", name=\"lstm_multi_task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_based_classification_model(batch_size=BATCH_SIZE)\n",
    "trainer = pl.Trainer(gpus=-1, \n",
    "                     logger = logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type             | Params\n",
      "---------------------------------------------------------\n",
      "0  | lstm_1             | LSTM             | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d      | 256   \n",
      "2  | lstm_2             | LSTM             | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d      | 256   \n",
      "4  | lstm_3             | LSTM             | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d      | 256   \n",
      "6  | dropout            | Dropout          | 0     \n",
      "7  | linear1            | Linear           | 8.3 K \n",
      "8  | activation         | ReLU             | 0     \n",
      "9  | output_layers      | ModuleList       | 130   \n",
      "10 | cross_entropy_loss | CrossEntropyLoss | 0     \n",
      "11 | f1_score           | F1               | 0     \n",
      "12 | accuracy_score     | Accuracy         | 0     \n",
      "---------------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ff227e941b4bffab8808ad1bd53657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2f0a06c380469c93442fe95590af92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'currency_0_test_acc': 0.5,\n",
      " 'currency_0_test_f1': 0.31474655866622925,\n",
      " 'currency_1_test_acc': 0.5967742204666138,\n",
      " 'currency_1_test_f1': 0.35683563351631165,\n",
      " 'currency_2_test_acc': 0.5564516186714172,\n",
      " 'currency_2_test_f1': 0.33917051553726196,\n",
      " 'test_loss': 2.0648059844970703}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'currency_0_test_acc': 0.5,\n",
       "  'currency_0_test_f1': 0.31474655866622925,\n",
       "  'currency_1_test_acc': 0.5967742204666138,\n",
       "  'currency_1_test_f1': 0.35683563351631165,\n",
       "  'currency_2_test_acc': 0.5564516186714172,\n",
       "  'currency_2_test_f1': 0.33917051553726196,\n",
       "  'test_loss': 2.0648059844970703}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropout, batch normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f3502a56730>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_multi_task_price_change_prediction",
   "language": "python",
   "name": "py3_multi_task_price_change_prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
