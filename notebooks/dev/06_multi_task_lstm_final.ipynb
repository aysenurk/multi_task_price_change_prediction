{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aysenur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics import F1\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataPreparation import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENCY_LST = ['BTC', 'ETH']#, 'LTC']\n",
    "FREQUENCY = \"D\"\n",
    "\n",
    "WINDOW_SIZE = 50\n",
    "NEUTRAL_QUANTILE = 0.33 # çok olduğunu düşünüyorum\n",
    "N_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PERCENTAGE, VAL_PERCENTAGE, TEST_PERCENTAGE = 0.97, 0.007, 0.023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_HIDDEN_SIZES = [128, 128, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_WEIGHT_CALCULATE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE= 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, features, dfs = get_data(CURRENCY_LST,\n",
    "                                N_CLASSES,\n",
    "                                 FREQUENCY, \n",
    "                                 WINDOW_SIZE,\n",
    "                                 neutral_quantile = NEUTRAL_QUANTILE,\n",
    "                                 log_price=True,\n",
    "                                 remove_trend=False,\n",
    "                                 include_indicators = False,\n",
    "                                 include_imfs = False\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_CURRENCIES = X.shape[0]\n",
    "INPUT_FEATURE_SIZE = X.shape[-1]\n",
    "\n",
    "N_CURRENCIES, INPUT_FEATURE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 n_currencies,\n",
    "                 x: np.ndarray, \n",
    "                 y: np.ndarray,\n",
    "                 data_use_type,\n",
    "                 train_percentage = TRAIN_PERCENTAGE,\n",
    "                 val_percentage = VAL_PERCENTAGE,\n",
    "                 test_percentage = TEST_PERCENTAGE,\n",
    "                 seq_len = WINDOW_SIZE, \n",
    "                 ):\n",
    "        \n",
    "        self.x = torch.tensor(x[:n_currencies]).float()\n",
    "        self.y = torch.tensor(y[:n_currencies]).long()\n",
    "        self.seq_len = seq_len\n",
    "        self.data_use_type = data_use_type\n",
    "        \n",
    "        #self.train_size = int(len(self.x[0]) * train_percentage)\n",
    "        self.val_size = int(len(self.x[0]) * val_percentage)\n",
    "        self.test_size = int(len(self.x[0]) * test_percentage)\n",
    "        self.train_size = len(self.x[0]) - self.val_size - self.test_size \n",
    "        \n",
    "        self.train_mean = [self.x[i][:self.train_size].mean() for i in range(n_currencies)]\n",
    "        self.train_std = [self.x[i][:self.train_size].std() for i in range(n_currencies)]\n",
    "        \n",
    "        self.train_min = [self.x[i][:self.train_size].min() for i in range(n_currencies)]\n",
    "        self.train_max = [self.x[i][:self.train_size].max() for i in range(n_currencies)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        if self.data_use_type == \"train\":\n",
    "            return self.train_size - ( self.seq_len)\n",
    "\n",
    "        elif self.data_use_type == \"val\":\n",
    "            return self.val_size\n",
    "  \n",
    "        else:\n",
    "            return self.test_size\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        item = dict()\n",
    "        \n",
    "        if self.data_use_type ==\"val\":\n",
    "            index = self.train_size + index - self.seq_len\n",
    "            \n",
    "        elif self.data_use_type ==\"test\":\n",
    "            index = self.train_size + self.val_size + index - self.seq_len\n",
    "        \n",
    "        for i in range(N_CURRENCIES):\n",
    "            window = self.x[i][index:index+self.seq_len]\n",
    "            window = (window -self.train_mean[i]) / self.train_std[i]\n",
    "            \n",
    "            item[\"currency_\" + str(i) + \"_window\"] = window\n",
    "            item[\"currency_\" + str(i) + \"_label\"]  = self.y[i][index+self.seq_len]\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = [MultiTimeSeriesDataset(N_CURRENCIES, X, y, dtype) for dtype in ['train', 'val', 'test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1269, 9, 31)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOSS_WEIGHT_CALCULATE:\n",
    "    loss_weights = []\n",
    "    for n in range(N_CURRENCIES):\n",
    "        train_labels = [int(train_dataset[i][\"currency_\"+ str(n)+\"_label\"] )for i in range(train_dataset.__len__())]\n",
    "        samples_size = pd.DataFrame({\"label\": train_labels}).groupby(\"label\").size().to_numpy()\n",
    "        print(samples_size)\n",
    "        loss_weights.append((1 / samples_size) * sum(samples_size)/2)\n",
    "    loss_weights\n",
    "else:\n",
    "    loss_weights = None\n",
    "\n",
    "loss_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_based_classification_model(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 train_dataset = train_dataset,\n",
    "                 val_dataset = val_dataset,\n",
    "                 test_dataset = test_dataset,\n",
    "                 weights = loss_weights,\n",
    "                 num_tasks = N_CURRENCIES,\n",
    "                 num_classes = N_CLASSES,\n",
    "                 window_size = WINDOW_SIZE,\n",
    "                 input_size = INPUT_FEATURE_SIZE,\n",
    "                 batch_size=BATCH_SIZE,\n",
    "                 lstm_hidden_sizes = LSTM_HIDDEN_SIZES,\n",
    "                 bidirectional = False,\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_tasks = num_tasks\n",
    "        self.window_size = window_size\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.lstm_hidden_sizes = lstm_hidden_sizes\n",
    "        self.bidirectional = bidirectional \n",
    "        self.weights = weights\n",
    "        \n",
    "        \n",
    "        self.lstm_1 = nn.LSTM(input_size = self.input_size, num_layers=1, batch_first=True, hidden_size = self.lstm_hidden_sizes[0])\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=self.lstm_hidden_sizes[0])\n",
    "        \n",
    "        self.lstm_2 = nn.LSTM(input_size = self.lstm_hidden_sizes[0], num_layers=1, batch_first=True, hidden_size = self.lstm_hidden_sizes[1])\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=self.lstm_hidden_sizes[1])\n",
    "        \n",
    "        self.lstm_3 = nn.LSTM(input_size = self.lstm_hidden_sizes[1], num_layers=1, batch_first=True, hidden_size = self.lstm_hidden_sizes[2])\n",
    "        self.batch_norm3 = nn.BatchNorm2d(num_features=self.lstm_hidden_sizes[2])\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        #self.linear1 = nn.Linear(self.lstm_hidden_sizes[2], int(self.lstm_hidden_sizes[2]/2))\n",
    "        self.linear1 =[nn.Linear(self.lstm_hidden_sizes[2], int(self.lstm_hidden_sizes[2]/2))] * self.num_tasks\n",
    "        self.linear1 = torch.nn.ModuleList(self.linear1)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.output_layers = [nn.Linear(int(self.lstm_hidden_sizes[2]/2), self.num_classes)] * self.num_tasks\n",
    "        self.output_layers = torch.nn.ModuleList(self.output_layers)\n",
    "        \n",
    "        \n",
    "        #self.cross_entropy_loss = nn.CrossEntropyLoss(weight= torch.tensor(weights).float()) # loss weight\n",
    "        #self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        if self.weights != None:\n",
    "            self.cross_entropy_loss = [nn.CrossEntropyLoss(weight= torch.tensor(weights).float()) for weights in self.weights]\n",
    "        else:\n",
    "            self.cross_entropy_loss = [nn.CrossEntropyLoss() for _ in range(self.num_tasks)]\n",
    "        \n",
    "        self.cross_entropy_loss = torch.nn.ModuleList(self.cross_entropy_loss)\n",
    "        \n",
    "        self.f1_score = pl.metrics.F1(num_classes=self.num_classes, average=\"macro\")\n",
    "        self.accuracy_score = pl.metrics.Accuracy()\n",
    "        \n",
    "        self.train_dl = DataLoader(train_dataset, batch_size=self.batch_size, shuffle = True)\n",
    "        self.val_dl = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "        self.test_dl = DataLoader(test_dataset, batch_size=self.batch_size)\n",
    "        \n",
    "    def forward(self, x, i):\n",
    "\n",
    "        batch_size = x.size()[0]\n",
    "        \n",
    "        x = x.view(batch_size, self.window_size, self.input_size) #(batch, window_len, feature_size)\n",
    "        x, _  = self.lstm_1(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x.reshape(x.size()[-1], batch_size, self.window_size) #(feature_size, batch, window_len)\n",
    "        x = self.batch_norm1(x.unsqueeze(0))\n",
    "\n",
    "        x = x.view(batch_size, self.window_size, x.size()[1])\n",
    "        x, _  = self.lstm_2(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.reshape(x.size()[-1], batch_size, self.window_size) #(feature_size, batch, window_len)\n",
    "        x = self.batch_norm2(x.unsqueeze(0))\n",
    "        \n",
    "        x = x.view(batch_size, self.window_size, x.size()[1])\n",
    "        x, _  = self.lstm_3(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.reshape(x.size()[-1], batch_size, self.window_size) #(feature_size, batch, window_len)\n",
    "        x = self.batch_norm3(x.unsqueeze(0))\n",
    "        \n",
    "        x = x.view(batch_size, self.window_size, x.size()[1])\n",
    "        x = x[:, -1, :] # equivalent to return sequence = False on keras :)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear1[i](x)\n",
    "        x = self.activation(x)\n",
    "                 \n",
    "        output = self.output_layers[i](x)\n",
    "    \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        \n",
    "        loss = (torch.tensor(0.0, device=\"cuda:0\", requires_grad=True) + \\\n",
    "                torch.tensor(0.0, device=\"cuda:0\", requires_grad=True)) \n",
    "        # araştırılabilir\n",
    "        for i in range(self.num_tasks):\n",
    "            x, y = batch[\"currency_\" + str(i) + \"_window\"], batch[\"currency_\" + str(i) + \"_label\"]\n",
    "\n",
    "            output = self.forward(x, i)\n",
    "            #loss = F.nll_loss(output, y)\n",
    "            loss += self.cross_entropy_loss[i](output, y)\n",
    "            \n",
    "            acc = self.accuracy_score(torch.max(output, dim=1)[1], y)\n",
    "            self.log(\"currency_\" + str(i) +'_train_acc', acc, on_epoch=True, prog_bar=True)\n",
    "\n",
    "            f1 = self.f1_score(torch.max(output, dim=1)[1], y)\n",
    "            self.log(\"currency_\" + str(i) +'_train_f1', f1, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        loss = loss / torch.tensor(self.num_tasks)\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss \n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        loss = torch.tensor(0.0, device=\"cuda:0\") + torch.tensor(0.0, device=\"cuda:0\")\n",
    "        \n",
    "        for i in range(self.num_tasks):\n",
    "            x, y = batch[\"currency_\" + str(i) + \"_window\"], batch[\"currency_\" + str(i) + \"_label\"]\n",
    "\n",
    "            output = self(x, i)\n",
    "            #loss = F.nll_loss(output, y)\n",
    "            loss += self.cross_entropy_loss[i](output, y)\n",
    " \n",
    "            acc = self.accuracy_score(torch.max(output, dim=1)[1], y)\n",
    "            self.log(\"currency_\" + str(i) +'_val_acc', acc, on_epoch=True, prog_bar=True, reduce_fx=torch.mean)\n",
    "\n",
    "            f1 = self.f1_score(torch.max(output, dim=1)[1], y)\n",
    "            self.log(\"currency_\" + str(i) +'_val_f1', f1, on_epoch=True, prog_bar=True, reduce_fx=torch.mean)\n",
    "        \n",
    "        loss = loss / torch.tensor(self.num_tasks)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        loss = torch.tensor(0.0, device=\"cuda:0\") + torch.tensor(0.0, device=\"cuda:0\")\n",
    "        \n",
    "        for i in range(self.num_tasks):\n",
    "            x, y = batch[\"currency_\" + str(i) + \"_window\"], batch[\"currency_\" + str(i) + \"_label\"]\n",
    "\n",
    "            output = self(x, i)\n",
    "            print(y, torch.max(output, dim=1)[1])\n",
    "            print(F.softmax(output)) # mantıken fark etmiyor\n",
    "            #loss = F.nll_loss(output, y)\n",
    "            loss += self.cross_entropy_loss[i](output, y)\n",
    "            \n",
    "            acc = self.accuracy_score(torch.max(output, dim=1)[1], y)\n",
    "            self.log(\"currency_\" + str(i) +'_test_acc', acc, on_epoch=True, reduce_fx=torch.mean)\n",
    "\n",
    "            f1 = self.f1_score(torch.max(output, dim=1)[1], y)\n",
    "            self.log(\"currency_\" + str(i) +'_test_f1', f1, on_epoch=True, reduce_fx=torch.mean)\n",
    "        \n",
    "        loss = loss / torch.tensor(self.num_tasks)\n",
    "        self.log('test_loss', loss, on_epoch=True, reduce_fx=torch.mean)\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr= 1e-3)#AdamW does weight decay\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "        return [optimizer], [{\"scheduler\": scheduler}]\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_dl\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_dl\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logger = WandbLogger(name='lstm.v1',project='pytorchlightning')\n",
    "logger = TensorBoardLogger(\"../output/models/lstm_model_logs\", name=\"lstm_multi_task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_based_classification_model()\n",
    "trainer = pl.Trainer(gpus=-1, \n",
    "                     max_epochs= 150,\n",
    "                     logger = logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type        | Params\n",
      "----------------------------------------------------\n",
      "0  | lstm_1             | LSTM        | 67.1 K\n",
      "1  | batch_norm1        | BatchNorm2d | 256   \n",
      "2  | lstm_2             | LSTM        | 132 K \n",
      "3  | batch_norm2        | BatchNorm2d | 256   \n",
      "4  | lstm_3             | LSTM        | 132 K \n",
      "5  | batch_norm3        | BatchNorm2d | 256   \n",
      "6  | dropout            | Dropout     | 0     \n",
      "7  | linear1            | ModuleList  | 8.3 K \n",
      "8  | activation         | ReLU        | 0     \n",
      "9  | output_layers      | ModuleList  | 195   \n",
      "10 | cross_entropy_loss | ModuleList  | 0     \n",
      "11 | f1_score           | F1          | 0     \n",
      "12 | accuracy_score     | Accuracy    | 0     \n",
      "----------------------------------------------------\n",
      "340 K     Trainable params\n",
      "0         Non-trainable params\n",
      "340 K     Total params\n",
      "1.362     Total estimated model params size (MB)\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b895549148c54001a1df2ab3d0924508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/aysenurk/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6876637023f24c74a1a3ea342b515696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 1, 2, 2, 1, 2, 2, 0, 0, 0, 2, 1, 0, 0], device='cuda:0') tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "tensor([[0.3377, 0.3581, 0.3041],\n",
      "        [0.3367, 0.3560, 0.3074],\n",
      "        [0.3373, 0.3572, 0.3055],\n",
      "        [0.3379, 0.3571, 0.3050],\n",
      "        [0.3373, 0.3560, 0.3067],\n",
      "        [0.3386, 0.3572, 0.3042],\n",
      "        [0.3363, 0.3550, 0.3087],\n",
      "        [0.3369, 0.3569, 0.3061],\n",
      "        [0.3386, 0.3575, 0.3039],\n",
      "        [0.3361, 0.3562, 0.3077],\n",
      "        [0.3371, 0.3577, 0.3053],\n",
      "        [0.3377, 0.3582, 0.3041],\n",
      "        [0.3374, 0.3593, 0.3033],\n",
      "        [0.3361, 0.3555, 0.3084],\n",
      "        [0.3372, 0.3560, 0.3068],\n",
      "        [0.3369, 0.3565, 0.3066]], device='cuda:0')\n",
      "tensor([0, 1, 2, 1, 2, 2, 1, 1, 1, 0, 0, 0, 0, 1, 2, 2], device='cuda:0') tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "tensor([[0.3377, 0.3590, 0.3033],\n",
      "        [0.3366, 0.3567, 0.3067],\n",
      "        [0.3371, 0.3581, 0.3048],\n",
      "        [0.3379, 0.3579, 0.3043],\n",
      "        [0.3372, 0.3568, 0.3061],\n",
      "        [0.3387, 0.3575, 0.3038],\n",
      "        [0.3363, 0.3556, 0.3081],\n",
      "        [0.3371, 0.3572, 0.3056],\n",
      "        [0.3383, 0.3584, 0.3033],\n",
      "        [0.3363, 0.3565, 0.3073],\n",
      "        [0.3373, 0.3578, 0.3049],\n",
      "        [0.3379, 0.3583, 0.3037],\n",
      "        [0.3375, 0.3597, 0.3028],\n",
      "        [0.3362, 0.3559, 0.3079],\n",
      "        [0.3373, 0.3562, 0.3065],\n",
      "        [0.3371, 0.3566, 0.3063]], device='cuda:0')\n",
      "tensor([2, 0, 0, 1, 1, 2, 0, 1, 2, 0, 2, 0, 1, 0, 2], device='cuda:0') tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "tensor([[0.3347, 0.3680, 0.2973],\n",
      "        [0.3299, 0.3626, 0.3075],\n",
      "        [0.3367, 0.3590, 0.3043],\n",
      "        [0.3377, 0.3579, 0.3044],\n",
      "        [0.3369, 0.3567, 0.3064],\n",
      "        [0.3369, 0.3591, 0.3041],\n",
      "        [0.3380, 0.3580, 0.3040],\n",
      "        [0.3377, 0.3588, 0.3036],\n",
      "        [0.3382, 0.3588, 0.3030],\n",
      "        [0.3376, 0.3572, 0.3052],\n",
      "        [0.3377, 0.3573, 0.3050],\n",
      "        [0.3367, 0.3608, 0.3024],\n",
      "        [0.3369, 0.3573, 0.3058],\n",
      "        [0.3372, 0.3562, 0.3066],\n",
      "        [0.3370, 0.3566, 0.3064]], device='cuda:0')\n",
      "tensor([2, 0, 1, 1, 1, 1, 2, 2, 1, 2, 1, 0, 1, 2, 2], device='cuda:0') tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "tensor([[0.3345, 0.3692, 0.2964],\n",
      "        [0.3302, 0.3632, 0.3066],\n",
      "        [0.3366, 0.3597, 0.3037],\n",
      "        [0.3377, 0.3582, 0.3041],\n",
      "        [0.3370, 0.3570, 0.3061],\n",
      "        [0.3368, 0.3595, 0.3037],\n",
      "        [0.3380, 0.3582, 0.3038],\n",
      "        [0.3377, 0.3590, 0.3033],\n",
      "        [0.3383, 0.3590, 0.3028],\n",
      "        [0.3377, 0.3573, 0.3050],\n",
      "        [0.3377, 0.3574, 0.3048],\n",
      "        [0.3367, 0.3610, 0.3022],\n",
      "        [0.3368, 0.3575, 0.3056],\n",
      "        [0.3371, 0.3565, 0.3065],\n",
      "        [0.3369, 0.3569, 0.3062]], device='cuda:0')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'currency_0_test_acc': 0.25806450843811035,\n",
      " 'currency_0_test_f1': 0.1367289274930954,\n",
      " 'currency_1_test_acc': 0.4193548262119293,\n",
      " 'currency_1_test_f1': 0.19648092985153198,\n",
      " 'test_loss': 1.1045910120010376}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-56c20af0ae69>:156: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(F.softmax(output)) # mantıken fark etmiyor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'currency_0_test_acc': 0.25806450843811035,\n",
       "  'currency_0_test_f1': 0.1367289274930954,\n",
       "  'currency_1_test_acc': 0.4193548262119293,\n",
       "  'currency_1_test_f1': 0.19648092985153198,\n",
       "  'test_loss': 1.1045910120010376}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    12\n",
      "1     8\n",
      "2    11\n",
      "dtype: int64\n",
      "label\n",
      "0     7\n",
      "1    13\n",
      "2    11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for n in range(N_CURRENCIES):\n",
    "    labels = [int(test_dataset[i][\"currency_\"+ str(n)+\"_label\"] )for i in range(test_dataset.__len__())]\n",
    "    print(pd.DataFrame({\"label\": labels}).groupby(\"label\").size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "04984682def58a97e4300fcfdea82226e95c772fd8b0b63e42875ad1781ae0ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
